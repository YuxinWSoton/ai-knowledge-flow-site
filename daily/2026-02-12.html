<!DOCTYPE html>
<html lang="zh-CN">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<title>知识流日报 2026-02-12</title>
<style>body{font-family:system-ui,sans-serif;max-width:720px;margin:2rem auto;padding:0 1rem;} a{color:#0066cc;} h1,h2,h3{margin-top:1.5rem;} pre{white-space:pre-wrap;}</style>
</head>
<body>
<p><a href="https://YuxinWSoton.github.io/ai-knowledge-flow-site/">← 返回首页</a></p>
<h1>知识流日报 2026-02-12</h1>
<div class="content">
<h1>知识流日报 2026-02-12</h1>
<p>共 9 条（来自 Google Scholar / Bing 等）。</p>
<p>（以下含抓取正文，便于阅读后判断是否接入 AI。未调用任何 AI API。）</p>
<h2>1. zhihu.com https://zhuanlan.zhihu.com</h2>
<ul>
<li>链接：https://zhuanlan.zhihu.com/p/620342675</li>
<li>来源：bing</li>
<li>摘要：2023年6月16日 · Embodied AI = Embodied Intelligence = 具象AI = 具身智能 Internet AI = Disembodied AI = 非具身智能 时间线： 2015年， 伊尔亚·苏茨克维（Ilya Sutskeve） 离开谷歌参与创办了 OpenAI …</li>
</ul>
<h3>正文</h3>
<p>（抓取失败或无可提取正文）</p>
<h2>2. github.com https://github.com › tianxingchen › Embodied-AI-Guide</h2>
<ul>
<li>链接：https://github.com/tianxingchen/Embodied-AI-Guide</li>
<li>来源：bing</li>
<li>摘要：2025年3月15日 · [Lumina Embodied AI] 具身智能技术指南 Embodied-AI-Guide. Contribute to TianxingChen/Embodied-AI-Guide development by creating an account on GitHub.</li>
</ul>
<h3>正文（抓取，非 AI）</h3>
<p>GitHub - TianxingChen/Embodied-AI-Guide: [Lumina Embodied AI] 具身智能技术指南 Embodied-AI-Guide<br>
Skip to content<br>
You signed in with another tab or window.<br>
Reload<br>
to refresh your session.<br>
You signed out in another tab or window.<br>
Reload<br>
to refresh your session.<br>
You switched accounts on another tab or window.<br>
Reload<br>
to refresh your session.<br>
Dismiss alert<br>
TianxingChen<br>
/<br>
Embodied-AI-Guide<br>
Public<br>
Notifications<br>
You must be signed in to change notification settings<br>
Fork<br>
784<br>
Star<br>
11.8k<br>
[Lumina Embodied AI] 具身智能技术指南 Embodied-AI-Guide<br>
github.com/TianxingChen/Embodied-AI-Guide<br>
License<br>
View license<br>
11.8k<br>
stars<br>
784<br>
forks<br>
Branches<br>
Tags<br>
Activity<br>
Star<br>
Notifications<br>
You must be signed in to change notification settings<br>
TianxingChen/Embodied-AI-Guide<br>
main<br>
Branches<br>
Tags<br>
Go to file<br>
Code<br>
Open more actions menu<br>
Folders and files<br>
Name<br>
Name<br>
Last commit message<br>
Last commit date<br>
Latest commit<br>
History<br>
509 Commits<br>
509 Commits<br>
files<br>
files<br>
topics<br>
topics<br>
.gitignore<br>
.gitignore<br>
LICENSE<br>
LICENSE<br>
README.md<br>
README.md<br>
View all files<br>
Repository files navigation<br>
具身智能技术指南 Embodied-AI-Guide<br>
📚 国内最热门的具身智能技术指南，一个偏「百科全书」定位的具身智能中文知识库与资料索引。欢迎<br>
Star / 分享 / 提 PR<br>
，欢迎邮件联系<br>
lumina.embodiedai@gmail.com<br>
或<br>
项目创始人<br>
微信<br>
TianxingChen_2002<br>
（请备注机构+姓名与来意）。<br>
📢 News｜项目进展<br>
📷<br>
2026-01-15: Embodied-AI-Guide重组织完成<br>
⭐️<br>
2025-12-18: GitHub Stars 突破 10,000<br>
❤️<br>
2025-03-15: Embodied-AI-Guide正式开源<br>
🧑‍💻 Related Open-source Projects｜相关开源项目<br>
⭐️ Lumina Robotics Talent Call (具身智能招贤榜):<br>
Repo<br>
⭐️ Datawhale Easy-Embodied:<br>
Repo<br>
🦉 Lumina具身智能社区:<br>
点击访问<br>
扫描右下图加入<br>
Lumina具身智能<br>
社区<br>
:<br>
🐣 (1) Start From Here - 从这里开始<br>
具身智能是指一种基于物理实体进行感知和行动的智能系统, 其通过智能体与环境的交互获取信息、理解问题、做出决策并实现行动, 从而产生智能行为和适应性。<br>
(1.1) How - 如何使用这份指南<br>
我们希望的是帮助新人快速建立领域认知, 所以设计理念是：<br>
简要<br>
以一个实践项目带大家动手学习具身智能，同时以<br>
百科全书形式<br>
介绍目前具身智能涉及到的主要技术, 让大家知道不同的技术能够解决什么问题, 未来想要深入发展的时候能够有头绪。<br>
(1.2) About us - 关于我们<br>
我们是一个由具身初学者组成的团队, 希望能够通过我们自己的学习经验, 为后来者提供一些帮助, 加快具身智能的普及。欢迎更多朋友加入我们的项目, 也很欢迎交友、学术合作, 有任何问题, 可以联系邮箱<br>
chentianxing2002@gmail.com<br>
。<br>
⚒️ (2) 动手学习具身智能操作<br>
建议一周内完成学习<br>
，使用<br>
RoboTwin 2.0<br>
平台走通一个操作策略“生命周期”的全流程<br>
完成此教程需要至少16GB显存的显卡<br>
(2.1) 为什么这样选择这个教程<br>
具身智能操作是一个很复杂的问题：<br>
数据从哪来<br>
、<br>
策略怎么设计（架构与训练细节）<br>
、<br>
怎么评测模型性能（平台与任务设计）<br>
。<br>
数据从哪来<br>
：具身智能的数据有很多种源头，比如真机数据采集、人类视频数据、仿真合成数据、世界模型合成数据等等，其中各有各的问题，比如真机数据采集成本高、人类视频数据信息含量低、仿真合成数据Sim2Real Gap与Scaleup难题、世界模型合成数据存在幻觉等。<br>
策略怎么设计<br>
：不同的网络架构选择影响模型的表现、收敛效果、推理速度等。<br>
怎么评测模型性能<br>
：评测是非常重要的，否则我们不知道科学评价模型效果如何，也没办法推动技术发展。<br>
面对以上问题，<br>
RoboTwin 2.0平台<br>
为广大科研学者提供了非常好的学习平台，RoboTwin 2.0基于易配置的<br>
SAPIEN<br>
仿真平台开发，提供了50个双臂自动化数据合成、主流操作策略训测集成、评测系统，能够辅助大家快速走起来具身智能操作策略的生命周期。过程中也可以多看看数据与评测视频，了解数据分布与策略表现。<br>
(2.2) 学习流程<br>
RoboTwin 2.0：<br>
代码<br>
｜<br>
主页<br>
｜<br>
文档<br>
｜<br>
论文<br>
展开学习流程<br>
(2.2.1) 了解RoboTwin 2.0做了什么 (～1天)<br>
阅读RoboTwin 2.0论文<br>
paper<br>
，了解仿真数据合成的方案，深入理解对于合成一条机器人数据需要什么信息，机器人有什么可以做的任务，了解<br>
Aloha<br>
硬件。<br>
(2.2.2) 配置RoboTwin 2.0平台，数据采集 (~0.5天)<br>
环境安装教学:<br>
Tutorial<br>
，根据以下数据采集脚本采集<br>
beat_block_hammer<br>
任务50条:<br>
bash collect_data.sh ${task_name} ${task_config} ${gpu_id}</p>
<h2>Clean Data Example: bash collect_data.sh beat_block_hammer demo_clean 0</h2>
<h2>Radomized Data Example: bash collect_data.sh beat_block_hammer demo_randomized 0</h2>
<p>(2.2.4) 策略训练（～1天）<br>
选择ACT策略进行复现<br>
Tutorial<br>
，ACT是非常经典的操作策略算法，训练此策略大约需要12GB显存，<br>
(2.2.5) 测试策略（～1天）<br>
在<br>
demo_clean<br>
下评测ACT成功率大约是56%（详见<br>
Leaderboard<br>
）。<br>
📄 (3) Useful Info - 有利于搭建认知的资料<br>
这一章主要用于<br>
快速建立对具身智能领域的整体认知<br>
，适合在系统学习算法、工程或硬件之前，用来了解技术版图、社区生态与研究脉络。<br>
方向性与方法论资料<br>
具身智能基础技术路线（Yunlong Dong）：<br>
PDF<br>
｜<br>
bilibili<br>
斯坦福机器人学导论：<br>
website<br>
Cyber Nachos（偏系统与工程思维）：<br>
website<br>
社区 / 社交媒体（长期跟进价值高）<br>
公众号：<br>
石麻日记（强烈推荐）<br>
、Lumina具身智能、机器之心、新智元、量子位、具身智能研究室、具身纪元、Human Five、Xbot具身知识库、具身智能之心、自动驾驶之心、3D视觉工坊、将门创投、RLCN强化学习研究、CVHub<br>
博主（小红书）：WhynotTV、穆尧_YaoMarkMu、许华哲Harry、周博宇、高飞、李弘扬、朱政、丁琰、YY硕、Mango-Man、RHOSLab #PI-李永露、正合时宜、心言任永亮、York Yang-Dyna Robotics、哲伦班长<br>
实验室与学术生态参考<br>
Robotics 实验室总结：<br>
zhihu link1<br>
｜<br>
zhihu link2<br>
具身智能华人高引榜：<br>
repo<br>
Lumina 具身智能社区：<br>
website<br>
高质量会议与期刊（论文检索时重点关注）<br>
Science Robotics, TRO, IJRR, JFR, RSS, RAL, IROS, ICRA, ICCV, ECCV, ICML, CVPR, NeurIPS, CoRL, ICLR, AAAI, ACL<br>
长期跟进研究进展与选题调研<br>
Awesome Humanoid Robot Learning（Yanjie Ze）：<br>
repo<br>
Paper Reading List（DeepTimber Community）：<br>
repo<br>
Paper List（Yanjie Ze）：<br>
repo<br>
RoboScholar / Embodied AI Paper List（Tianxing Chen）：<br>
repo<br>
SOTA Paper Rating（Weiyang Jin）：<br>
website<br>
Awesome LLM Robotics：<br>
repo<br>
Awesome Video Robotic Papers：<br>
repo<br>
Awesome Embodied Robotics and Agent：<br>
repo<br>
awesome-embodied-vla / va / vln：<br>
repo<br>
Awesome Affordance Learning：<br>
repo<br>
Embodied AI Paper TopConf：<br>
repo<br>
Awesome<br>
RL-VLA<br>
for Robotic Manipulation (Haoyuan Deng)：<br>
repo<br>
年度趋势总结<br>
State of Robot Learning (Dec 2025)：<br>
website<br>
许华哲 - 具身智能：2025回望，<br>
website<br>
林天威 - 具身VLA的2025：从 Demo 到通用的距离，<br>
website<br>
🍎 (4) Algorithm - 算法篇<br>
这一篇把具身智能中最常用的“算法能力栈”从下往上串了起来：底层是工程工具与几何/标定/控制这类决定系统能否稳定运行的基础；中层是视觉与多模态表征（2D/3D/4D、prompting、affordance），它们把复杂世界压缩成可泛化、可对齐、可被策略利用的中间表示；上层则是学习与决策（RL/IL、VLA、LLM+Planner、快慢系统），把感知与任务目标转成可执行动作，并逐步走向更长程、更通用、更可部署的系统形态。<br>
Common Tools —— 常用工程工具<br>
Vision Foundation Models —— 视觉基础模型<br>
Robot Learning —— 机器人学习<br>
LLM for Robotics —— LLM+机器人<br>
VLA —— Vision-Language-Action Models<br>
5.0 参考与综述<br>
5.1 经典工作<br>
5.2 分层双系统 VLA<br>
5.3 最新 VLA 工作<br>
Computer Vision —— 计算机视觉<br>
6.1 2D/3D/4D Vision<br>
6.2 Visual Prompting &amp; Affordance<br>
Computer Graphics —— 计算机图形学<br>
Multimodal Models —— 多模态模型<br>
Robot Navigation —— 机器人导航<br>
Embodied AI for X —— 具身智能+X<br>
10.1 Healthcare<br>
10.2 UAV<br>
10.3 Autonomous Driving<br>
🏋️‍♂️ (5) Infrastruture - 软件基础设施篇<br>
这一章关注的不是“具体某个模型”，而是<br>
支撑具身智能研究与系统落地的软件基础设施（Infrastructure）<br>
。仿真器决定你能构建怎样的世界，基准集决定你如何比较方法优劣，数据集决定模型最终学到什么样的行为分布。它们共同构成了具身智能中<br>
最容易被忽视、但最影响上限与复现性的部分<br>
。<br>
(1) Simulators - 仿真器<br>
(2) Benchmarks - 基准集<br>
(3) Datasets - 数据集<br>
🎮 (6) Control - 控制篇<br>
这一章并不是为了让你“立刻跑一个模型”，而是为具身智能系统提供<br>
稳定性、可解释性与工程底座<br>
。控制论保证系统在高频下不崩溃，机器人学提供几何与动力学约束，SLAM 与状态估计让机器人“知道自己在哪里”，ROS 与工程库则把理论变成可复现的系统。<br>
(1) Control and Robotics —— 控制论与机器人学基础<br>
(1.1) 经典课程<br>
(2) 控制理论基础（Control Foundations）<br>
(2.1) 经典控制（Classical Control）<br>
(2.2) 现代控制（最优控制）<br>
(2.3) 先进控制（Advanced Control）<br>
(3) 机器人学导论（Robotics Foundations）<br>
(3.1) 推荐教材与材料<br>
(3.2) 运动学与动力学<br>
(3.3) 里程计与 SLAM<br>
(3.4) 工程生态与工具<br>
🦾 (7) Hardware - 硬件篇<br>
具身智能硬件涵盖多个技术栈：嵌入式软硬件、机械设计、机器人系统集成与传感器等。它们知识面很杂，但共同目标只有一个：把“算法”变成真实世界里稳定可复现的系统。关于硬件学习，最有效的方式几乎永远是<br>
从实践出发<br>
：先做出一个能跑起来的最小系统，再逐步扩展复杂度与可靠性。<br>
(1) Embedded —— 嵌入式<br>
(2) Mechanical Design —— 机械设计<br>
(3) Robot System Design —— 机器人系统设计<br>
(4) Sensors —— 传感器<br>
(4.1) 深度相机（Depth Camera）<br>
(5) Tactile Sensing —— 触觉感知<br>
(5.1) 视触觉传感器<br>
(5.2) 电子皮肤<br>
(5.3) 触觉应用与算法<br>
(5.4) 传感器购买<br>
(6) Data Collection —— 数据采集硬件<br>
(7) Companies —— 公司与硬件生态<br>
👍 Citation - 引用<br>
If you find this repository helpful, please consider citing:<br>
@misc{embodiedaiguide2025,<br>
title = {Embodied-AI-Guide},<br>
author = {Embodied-AI-Guide-Contributors, Lumina-Embodied-AI-Community},<br>
month = {January},<br>
year = {2025},<br>
url = {https://github.com/tianxingchen/Embodied-AI-Guide},<br>
}<br>
🏷️ License - 许可协议<br>
本项目为<br>
非商业使用（Non-Commercial Use）<br>
协议：<br>
允许：个人学习、学术研究、非盈利使用；<br>
禁止：任何形式的商业使用，包括但不限于公司/企业内部使用、<br>
集成到收费产品或服务中、或用于任何营利目的。<br>
详情请查看仓库中的<br>
LICENSE<br>
文件。<br>
如需商业授权（例如在公司产品或商业项目中使用），请联系项目负责人：<br>
chentianxing2002@gmail.com<br>
。<br>
⭐️ Star History - Star历史<br>
🤝 Sponsors - 支持机构<br>
感谢<br>
无界智航、超维动力、香港大学MMLab、地瓜机器人、松灵机器人<br>
对本项目的支持<br>
About<br>
[Lumina Embodied AI] 具身智能技术指南 Embodied-AI-Guide<br>
github.com/TianxingChen/Embodied-AI-Guide<br>
Topics<br>
tutorial<br>
guide<br>
embodied-ai<br>
Resources<br>
Readme<br>
License<br>
View license<br>
Uh oh!<br>
There was an error while loading.<br>
Please reload this page<br>
.<br>
Activity<br>
Stars<br>
11.8k<br>
stars<br>
Watchers<br>
93<br>
watching<br>
Forks<br>
784<br>
forks<br>
Report repository<br>
Releases<br>
No releases published<br>
Packages<br>
0<br>
No packages published<br>
Contributors<br>
54<br>
Uh oh!<br>
There was an error while loading.<br>
Please reload this page<br>
.<br>
+ 40 contributors<br>
You can’t perform that action at this time.</p>
<h2>3. baidu.com https://baike.baidu.com › item › 具身智能</h2>
<ul>
<li>链接：https://baike.baidu.com/item/%E5%85%B7%E8%BA%AB%E6%99%BA%E8%83%BD/63286570</li>
<li>来源：bing</li>
<li>摘要：2025年3月5日 · 具身智能（Embodied Intelligence）是人工智能与机器人学交叉的前沿领域，强调智能体通过身体与环境的动态交互实现自主学习和进化，其核心在于将感知、行动与认知深度融合‌。 1950 …</li>
</ul>
<h3>正文（抓取，非 AI）</h3>
<p>具身智能（智能体通过身体将感知、行动与认知深度融合‌的智能系统）_百度百科<br>
网页<br>
新闻<br>
贴吧<br>
知道<br>
网盘<br>
图片<br>
视频<br>
地图<br>
文库<br>
资讯<br>
采购<br>
百科<br>
百度首页<br>
登录<br>
注册<br>
进入词条<br>
全站搜索<br>
国际版<br>
帮助<br>
首页<br>
秒懂百科<br>
特色百科<br>
知识专题<br>
加入百科<br>
百科团队<br>
权威合作<br>
个人中心<br>
具身智能<br>
播报<br>
讨论<br>
上传视频<br>
智能体通过身体将感知、行动与认知深度融合‌的智能系统<br>
展开<br>
4个同名词条<br>
收藏<br>
查看<br>
我的收藏<br>
0<br>
有用+1<br>
0<br>
本词条由<br>
中国科学院大学人工智能学院<br>
参与编辑并审核，经<br>
科普中国·科学百科<br>
认证 。<br>
具身智能（Embodied Intelligence）是人工智能与机器人学交叉的前沿领域，强调智能体通过身体与环境的动态交互实现自主学习和进化，其核心在于将感知、行动与认知深度融合‌。<br>
1950年，在图灵的论文《Computing Machinery and Intelligence》中首次提出。1986年，布鲁克斯从控制论角度出发，提出行为式机器人概念，认为智能是具身化和情境化的。1991年，布鲁克斯提出“行为智能”。<br>
2023年6月，第七届世界智能大会智能科技展，人形机器人的逐步完善为具身智能的落地提供了方向。<br>
[21]<br>
2024年3月，OpenAI与Figure公司合作推出了Figure 01人形机器人；<br>
[17]<br>
8月，中科源码服务机器人研究院发布了全国首个“温江造”基于物流场景的具身智能机器人；<br>
[22]<br>
10月，具身小脑模型被列入人工智能十大前沿技术趋势之一。<br>
[7]<br>
2025年3月5日，《<br>
2025年国务院政府工作报告<br>
》提到，要培育具身智能等未来产业。<br>
[19]<br>
2025年10月，在《中共中央关于制定国民经济和社会发展第十五个五年规划的建议》中，提出推动具身智能等成为新的经济增长点。<br>
[33]<br>
具身智能领域蕴含着巨大的市场潜力和发展机遇，随着技术的不断成熟、应用的不断拓展，具身智能产品将在智能制造、<br>
智能家居<br>
、智慧医疗、智能服务等多个领域发挥重要作用。在技术方面要通过优化算法、提高硬件性能等方式，采用加密技术、数据脱敏等手段保护用户数据安全和隐私。此外也要制定具身智能技术伦理和道德准则加强对具身智能技术的监管和评估确保其行为符合人类道德和价值观。<br>
[25]<br>
中文名<br>
具身智能<br>
外文名<br>
Embodied AI<br>
所属学科<br>
人工智能<br>
特    质<br>
以主人公的视角去自主感知物理世界<br>
提出者<br>
图灵<br>
提出时间<br>
1950年<br>
代表作品<br>
《Computing Machinery and Intelligence》<br>
[28]<br>
目录<br>
1<br>
定义<br>
2<br>
发展历程<br>
▪<br>
诞生前夜<br>
3<br>
20世纪80年代<br>
▪<br>
20世纪90年代<br>
▪<br>
21世纪初<br>
▪<br>
2010年代<br>
▪<br>
2020年代至今<br>
4<br>
主要特点<br>
▪<br>
身体与智能的相互依存<br>
▪<br>
环境中的实时感知与反馈<br>
▪<br>
感知-动作循环<br>
▪<br>
学习与适应性<br>
5<br>
算法路径<br>
▪<br>
分层方法<br>
▪<br>
端到端方法<br>
▪<br>
总结<br>
6<br>
相关概念辨析<br>
▪<br>
具身智能与具身认知<br>
▪<br>
具身智能与传统人工智能<br>
▪<br>
具身智能与行为主义<br>
▪<br>
具身智能与分布式智能<br>
▪<br>
具身智能与仿生机器人<br>
▪<br>
具身智能与自适应控制<br>
7<br>
发展现状<br>
▪<br>
多模态融合技术的应用<br>
▪<br>
深度强化学习与自我探索<br>
▪<br>
模拟到现实迁移技术的成熟<br>
▪<br>
人机交互与协作技术的进展<br>
8<br>
未来发展趋势<br>
▪<br>
更高层次的自我学习与自主性<br>
▪<br>
多智能体协作与分布式具身智能<br>
▪<br>
仿生设计与灵巧度增强<br>
▪<br>
全感知整合与情境智能<br>
▪<br>
伦理和安全框架的完善<br>
9<br>
政策指导<br>
10<br>
具身智能机器人<br>
11<br>
所获荣誉<br>
定义<br>
播报<br>
编辑<br>
具身智能是指智能体通过身体与环境的互动产生的智能行为，强调智能体的认知和行动在物理环境中的相互依赖。从字面理解是“具身化的人工智能”，是将人工智能融入机器人、新能源汽车等物理实体，为“大脑”赋予了“身体”，使得它们拥有像人一样感知、学习和与环境动态交互的能力。<br>
[24]<br>
发展历程<br>
播报<br>
编辑<br>
诞生前夜<br>
具身智能的概念源于1950年，当时图灵在其著名论文《Computing Machinery andIntelligence》中首次提出了具身智能的设想。这一设想奠定了智能与物理形态相结合的理论基础<br>
[1]<br>
。在此后的达特茅斯会议后，人工智能研究主要集中于符号处理模型（符号主义），依赖符号操作来模拟人类智能<br>
[2]<br>
。然而，符号主义的局限性在实际应用中逐渐暴露，这为连接主义的发展铺平了道路，诞生了多层感知机、前向神经网络和循环神经网络等方法。尽管神经网络在学习和适应方面取得了显著进展，但仍难以解决智能体与物理世界交互的复杂性问题。由此，“具身智能”应运而生，作为新的人工智能方向开始发展。<br>
20世纪80年代<br>
播报<br>
编辑<br>
在20世纪80年代，受“具身认知”理论启发，具身智能的概念逐渐成型。研究者开始质疑传统的符号处理模型，认为智能不仅是大脑的功能，而是身体、环境和智能体之间互动的产物。1986年，布鲁克斯从控制论角度出发，提出了行为式机器人概念，强调去除表征，推动具身智能以行为为核心发展，认为智能是具身化（Embodied）和情境化（Contextualized）的<br>
[3]<br>
。这一理论为具身智能奠定了基础，并开始影响当时的机器人学领域。<br>
20世纪90年代<br>
进入20世纪90年代，具身智能的发展受到机器人学的进一步推动。1991年，布鲁克斯提出了“行为智能”（Behavior-Based AI），认为智能系统应直接通过与环境互动来体现其高度的适应性，而非依赖内部模型<br>
[4]<br>
。这一突破性工作成为具身智能的一个里程碑，引导研究者从计算能力转向身体与环境的交互，推动了机器人在适应性和环境交互方面的创新发展。<br>
21世纪初<br>
在21世纪初，具身智能的研究逐渐深入，并扩展到人类智能领域。研究者们提出了基于感知、动作和环境相互作用的具身认知模型，以更好地理解人类的认知过程<br>
[5]<br>
。同时，类人机器人和仿生机器人领域的技术进展进一步推动了具身智能的应用，使机器人能够更真实地应对复杂物理环境中的挑战。<br>
2010年代<br>
进入2010年代，随着深度学习和机器学习技术的迅猛发展，具身智能进入了新阶段。研究者将深度学习技术与具身智能结合，通过深度强化学习赋予机器人自我探索和适应性行为的能力，使其能够在未知环境中进行自我学习<br>
[6]<br>
。这一时期的研究拓宽了具身智能在导航、机械操作和人机互动等领域的应用范围。<br>
2020年代至今<br>
到2020年代，具身智能已成为人工智能和机器人学的重要研究方向。在2023年，英伟达创始人黄仁勋在半导体大会上指出，具身智能是能够理解、推理并与物理世界互动的智能系统，预示着其将成为人工智能的下一波浪潮<br>
[16]<br>
。2024年3月，OpenAI与Figure公司合作推出了Figure 01人形机器人，展现了具身智能在理解、判断和自我评估方面的前沿进展<br>
[17]<br>
。同年10月，具身小脑模型被列入人工智能十大前沿技术趋势之一<br>
[7]<br>
，标志着具身智能正在进一步结合多模态感知系统、仿生设计和大模型技术，使机器人具备更接近人类的自适应性和决策能力，推动人机协作迈向新的未来。<br>
2025年3月5日，《<br>
2025年国务院政府工作报告<br>
》提到，建立未来产业投入增长机制，培育具身智能等未来产业<br>
；同时“具身智能”入选2025年两会上的新词<br>
矩阵。<br>
[19-20]<br>
[23]<br>
被正式纳入国家战略。作为国家重点培育的未来产业。<br>
[24]<br>
截至2024年底，广东深圳已聚集了约210家具身智能机器人相关企业，并推出“18条”行动计划推动产业发展。<br>
[29]<br>
2025年4月2日，智元机器人宣布与国际顶尖具身智能公司Physical Intelligence（Pi）携手，双方将围绕动态环境下的长周期复杂任务，在具身智能领域展开深度技术合作。<br>
[26]<br>
2025年，“具身智能”首次被写入政府工作报告，成为未来产业发展的重点方向之一。在深圳，围绕具身智能产业，关键零部件的创新和升级正在加速，包含灵巧手、核心关节、减速器、雷达等人形机器人产业链关键技术不断取得新突破。<br>
[27]<br>
2025年10月，在党的二十届四中全会审议通过了《中共中央关于制定国民经济和社会发展第十五个五年规划的建议》中，提出前瞻布局未来产业，推动具身智能等成为新的经济增长点。<br>
[33]<br>
2025年11月，教育部公示了7所高校布局“具身智能”新专业。<br>
[34]<br>
主要特点<br>
播报<br>
编辑<br>
身体与智能的相互依存<br>
具身智能的基本原理之一是身体和智能相互依存<br>
[3]<br>
。智能体的身体形态不仅限制了其在环境中的行动能力，也在一定程度上塑造了智能体的认知方式。这一观点认为，智能并非仅仅存在于“头脑”中，而是通过智能体的身体及其在环境中的互动表现出来的。身体在环境中的感知、反馈和适应使智能体能够更灵活地应对复杂任务，因此，身体的形态和运动方式直接影响智能体的认知和决策过程。<br>
环境中的实时感知与反馈<br>
具身智能强调智能体对外部环境的实时感知和反馈<br>
[8]<br>
。这种感知能力使智能体能够直接从环境中获取信息，减少对内部模型和复杂计算的依赖。智能体可以通过触觉、视觉、听觉等多种感官来实时感知周围环境的变化，从而快速调整自身行为以适应环境。例如，机器人在导航时，能依据实时的障碍物信息进行路径规划，这种直接的感知反馈机制使得智能体在动态环境中展现出更强的适应性。<br>
感知-动作循环<br>
具身智能的核心之一是感知-动作循环，即智能体通过持续的感知反馈来动态调整自身行为。智能体在与环境互动过程中，会不断地接收感知信息，并根据该信息调整动作，从而形成闭环系统。这种循环使得智能体能够灵活应对复杂环境中的变化，使其行为更加适应任务需求。通过感知-动作循环，具身智能体可以在不完全依赖内部模型的情况下，通过身体与环境的互动来完成复杂任务。<br>
学习与适应性<br>
具身智能强调智能体的学习与适应能力，即智能体可以在未知的环境中不断调整和优化自身行为，以更高效地完成任务<br>
[9]<br>
。具身智能体通常采用强化学习、进化算法或深度学习等技术，通过反复试错来找到最佳行为策略。这种适应性学习的机制使得具身智能体不仅能在训练环境中执行任务，还能够自主地应对新环境的挑战，从而实现更为灵活和高效的任务执行。<br>
算法路径<br>
播报<br>
编辑<br>
在具身智能的研究中，主要的算法路径分为分层方法和端到端方法。每种路径都各有优缺点，适用于不同的任务需求和复杂性。<br>
分层方法<br>
感知、规划与控制分层<br>
分层方法中，最经典的算法路径将智能体的任务拆分为感知、规划和控制三个主要层次。感知层负责从环境中收集信息，规划层根据环境和任务需求制定行动计划，控制层则执行具体的动作。这种架构常见于机器人导航和任务执行中，因为其各层任务划分清晰，可以使得感知和控制独立优化，适合复杂任务中的稳定性需求。然而，这种方法通常依赖高效的信息传递机制，如果各层之间缺乏良好协调，可能会影响实时响应性。<br>
行为式控制与层次化强化学习<br>
行为式控制是分层方法的一种变体，将复杂任务分解为多个独立的行为模块，智能体通过不同模块对不同任务进行分步解决。层次化强化学习（HRL）也遵循这一思想，在高层设计策略而在低层执行具体动作。例如，高层通过奖励机制选择子任务，低层则执行指定的动作策略<br>
[10]<br>
。HRL在复杂任务中表现出色，适合需要多步决策的情境，但其训练和调试往往更为复杂。<br>
模块化与可迁移性<br>
分层方法的一个重要优势是其模块化设计，使得每个模块可以在不同任务或场景间迁移或重新配置。这对于多种任务场景中的具身智能应用具有很大帮助，例如同一感知模块可以在不同的控制场景中复用，避免从零开始重新训练。但这种方法的实现依赖高质量的模块设计和接口定义，因此在设计之初需要深度考虑不同模块的协作性和可扩展性。<br>
典型模型<br>
分层模型典型例子为OpenAI与Figure合作推出的Figure01机器人所采用的模型架构<br>
[11]<br>
。这个模型分为三层：第一层是策略控制系统（SLC），通过大型模型整合任务要求、环境感知信息和机器人本体的反馈；第二层是环境交互控制系统（ELC），该层通过具身模型实现更精确的环境感知和动作规划；第三层是行为控制系统（PLC），该层通过传统控制算法输出机器人所需的具体力矩，以实现最终的动作。分层决策模型结构清晰，每一层在功能上独立，但各步骤之间的整合和一致性是一个技术挑战。这种架构适合多种应用场景，尤其是在需要分步决策和细节控制的任务中。<br>
端到端方法<br>
深度强化学习（DRL）<br>
端到端<br>
方法中，<br>
深度强化学习<br>
（DRL）是一种广泛应用的路径，能够通过神经网络直接从感知输入生成控制输出。DRL使得智能体可以在没有明确划分的感知、规划和控制模块的情况下，直接从环境中学习最优策略。该方法在复杂环境中的表现十分突出，尤其适合具有高度动态性和不确定性的任务，但往往需要大量数据和计算资源用于训练。<br>
模拟到现实（Sim-to-Real）技术<br>
为了克服DRL在真实环境中训练的高成本，端到端方法中也出现了模拟到现实的技术。这种技术通过在模拟环境中进行大量训练，然后将学习成果迁移到现实环境中。这一方法极大降低了成本和风险，但其成功依赖于模拟环境和现实环境的一致性，尤其是在物理细节和感知数据上，一些微小差异可能会影响算法的有效性。<br>
多模态学习<br>
具身智能中的端到端算法路径还包括多模态学习方法，通过将视觉、触觉、听觉等多模态数据直接输入神经网络，实现跨感知的融合决策。多模态学习可以在复杂环境中实现更灵活和鲁棒的表现，尤其适用于需要多层次感知的任务。但其训练难度较大，模型容易产生过拟合，因此在真实应用中通常需要丰富的多模态数据支持。<br>
典型模型<br>
端到端模型以Google的RT-2为典型代表<br>
[12]<br>
。RT-2模型首先在大量互联网数据上预训练视觉语言模型（VLMs），然后在机器人任务上进行微调。整个流程是通过一个神经网络从任务和对象的组合输入开始，完成从感知、推理、决策到动作指令的全过程。这种方法使得机器人能够直接从输入推导出相应的动作序列，谷歌甚至在RT-2模型中观察到涌现能力的表现，认为这一方法具有较强的可扩展性。然而，端到端模型也面临挑战，例如需要大量数据才能实现良好泛化，并且由于全程调用大型模型，其资源消耗较高，导致机器人执行速度相对较慢。这种方法更适用于计算资源充足且任务要求精准的场景。<br>
总结<br>
分层方法在具身智能的应用中提供了清晰的架构分工，易于调试和模块化迁移，适合稳定性要求较高的复杂任务；端到端方法则依赖深度学习技术，能够直接从感知生成行动决策，适用于快速响应和高度动态的任务，但对数据量和计算资源的需求更大。<br>
相关概念辨析<br>
播报<br>
编辑<br>
具身智能与具身认知<br>
具身智能和具身认知的核心思想都强调身体在智能或认知过程中的重要性<br>
[13]<br>
。具身认知主要源于认知科学，关注身体如何影响和塑造心智的认知过程，尤其是感知、记忆和学习等心理活动。具身智能则是具身认知的延伸，强调智能体在物理环境中通过身体和环境的互动来展现智能行为。具身智能更侧重于实际任务的执行和智能体的行为表现，常用于机器人和人工智能领域，而具身认知的重点是理解人类和生物的认知机制。<br>
具身智能与传统人工智能<br>
传统人工智能（AI）通常依赖符号处理和逻辑推理，通过内部算法和计算实现智能行为。其核心在于通过复杂计算来实现高效的决策过程<br>
[14]<br>
，但它忽视了智能体与环境的直接互动。具身智能则强调智能的生成依赖于身体和环境的动态交互，智能体在物理空间中感知和行动，进而影响其行为。相比之下，具身智能对环境的依赖更强，更适合动态、多变的场景，而传统AI通常在静态和结构化任务中表现较好。<br>
具身智能与行为主义<br>
行为主义是心理学中强调刺激-反应（S-R）关系的理论，主张行为是对外部环境的直接反应<br>
[15]<br>
。具身智能虽然也重视环境对智能体的影响，但它超越了单一的刺激-反应机制，强调智能体与环境之间的深度交互和学习过程。具身智能关注的是在动态环境中如何通过身体、感知和行动形成智能行为，而行为主义较少涉及内部过程和学习能力。此外，具身智能更倾向于通过自适应和自主学习实现智能行为，不是仅依赖简单的条件反射。<br>
具身智能与分布式智能<br>
分布式智能是一种将智能体的智能任务分配到多个节点或系统中的方法，适用于多智能体系统中的协调和协作。具身智能则更多关注单一智能体如何在复杂环境中通过身体与环境的互动产生智能行为。分布式智能通常用于团队或集群中的多体协作，而具身智能注重个体在动态环境中的自适应性。尽管在多机器人系统中，具身智能也可以应用分布式智能的思想，但它的关注点主要是每个个体的身体和环境交互，而非多个智能体之间的协作。<br>
具身智能与仿生机器人<br>
仿生机器人是以模仿生物特性和行为为设计基础的机器人，借鉴自然界的生物结构来实现特定功能。具身智能虽然在某些应用中借鉴生物，但其重点并非模仿生物，而是通过环境交互实现自主智能行为。仿生机器人多注重形态的模仿，具身智能则专注智能行为的产生和适应性。仿生机器人往往会利用具身智能的方法来提升其环境适应能力，但具身智能的范畴更为广泛，涵盖所有依赖身体互动的智能体，不限于仿生形态。<br>
具身智能与自适应控制<br>
自适应控制是一种通过调整系统控制参数以适应环境变化的控制策略，常用于系统动态控制中。具身智能则涉及更广泛的智能行为生成，不仅限于控制参数的调节。具身智能通过持续的环境交互和学习，逐步形成适应性行为，因此具身智能所包含的自适应能力不仅限于调整系统参数，还涵盖感知、行为规划和决策等方面。<br>
发展现状<br>
播报<br>
编辑<br>
多模态融合技术的应用<br>
当前，具身智能的研究越来越关注多模态融合，通过整合视觉、触觉、听觉等多种感知形式，提升智能体对环境的理解和反应能力<br>
[9]<br>
。例如，多模态感知的结合使得智能体可以在视觉受限的情况下，通过触觉来感知物体的特征，或者在复杂环境中结合视觉和听觉信息，形成更为全面的环境认知。这种技术在自动驾驶、机器人导航和医疗辅助等领域具有广泛应用前景，提升了智能体的任务适应能力和决策准确性。<br>
深度强化学习与自我探索<br>
随着深度学习和强化学习的发展，具身智能中的深度强化学习（DRL）成为重要路径之一<br>
[11-12]<br>
。通过在虚拟环境中训练智能体，智能体能够自我探索并学习复杂的行为策略，减少了人工设定规则的需求。在这一过程中，DRL使得具身智能可以适应各种动态、不确定的环境，广泛应用于机器人自主导航和机械手臂操作等复杂任务。虽然这种方法训练成本较高，但其在高动态环境中的表现和适应性已得到显著提升。<br>
模拟到现实迁移技术的成熟<br>
为了降低具身智能在真实环境中训练的成本和风险，模拟到现实（Sim-to-Real）技术逐渐成熟。这种技术允许智能体先在虚拟环境中进行大量训练，然后将学习的知识迁移到现实环境中，从而实现快速适应。目前该技术已广泛应用于自动驾驶、无人机飞行和服务机器人等领域，通过增强虚拟环境的逼真度和调整策略，使得现实环境中的智能体表现更为稳定。<br>
人机交互与协作技术的进展<br>
具身智能的发展还体现在人机协作的进步上，智能体不仅能够在环境中自适应行动，还能够识别和理解人类的意图，与人类进行合作。例如，服务机器人和协作机器人领域的具身智能应用日益增加，这些机器人可以感知人类动作、情绪，并根据需求调整其行为，使得人与机器能够在同一任务中高效协同。这样的发展使得具身智能在医疗、制造和家庭服务等领域具有重要应用价值，显著提升了智能体的社会适应能力。<br>
未来发展趋势<br>
播报<br>
编辑<br>
更高层次的自我学习与</p>
<h2>4. cuhk.edu.cn https://airs.cuhk.edu.cn › article</h2>
<ul>
<li>链接：https://airs.cuhk.edu.cn/article/1124</li>
<li>来源：bing</li>
<li>摘要：2024年5月6日 · 具身智能（Embodied Artificial Intelligence，简称 EAI）将人工智能融入机器人等物理实体，赋予它们感知、学习和与环境动态交互的能力。本文简要回顾了 EAI 的历史、当前发展以及未来 …</li>
</ul>
<h3>正文</h3>
<p>（抓取失败或无可提取正文）</p>
<h2>5. tencent.com https://cloud.tencent.com › developer › article</h2>
<ul>
<li>链接：https://cloud.tencent.com/developer/article/2492854</li>
<li>来源：bing</li>
<li>摘要：2025年2月3日 · 具身人工智能（Embodied AI）是实现通用人工智能的关键，连接网络空间与物理世界。本文综述了具身AI的最新进展，涵盖具身机器人、模拟器、具身感知、具身交互、具身代理及模拟到 …</li>
</ul>
<h3>正文（抓取，非 AI）</h3>
<p>一文全面了解目前具身智能的核心内容和未来发展-腾讯云开发者社区-腾讯云<br>
点云PCL博主<br>
一文全面了解目前具身智能的核心内容和未来发展<br>
关注作者<br>
腾讯云<br>
开发者社区<br>
文档<br>
建议反馈<br>
控制台<br>
登录/注册<br>
首页<br>
学习<br>
活动<br>
专区<br>
圈层<br>
工具<br>
MCP广场<br>
文章/答案/技术大牛<br>
搜索<br>
搜索<br>
关闭<br>
发布<br>
点云PCL博主<br>
社区首页</p>
<blockquote>
<p>专栏</p>
<p>一文全面了解目前具身智能的核心内容和未来发展<br>
一文全面了解目前具身智能的核心内容和未来发展<br>
点云PCL博主<br>
关注<br>
发布<br>
于<br>
2025-02-03 21:56:16<br>
发布<br>
于<br>
2025-02-03 21:56:16<br>
2.9K<br>
0<br>
举报<br>
文章被收录于专栏：<br>
点云PCL<br>
点云PCL<br>
文章：Aligning Cyber Space with Physical World: A Comprehensive Survey on Embodied AI<br>
作者：Yang Liu, Weixing Chen, Yongjie Bai, Jingzhou Luo, Xinshuai Song, Kaixuan Jiang, Zhida Li, Ganlong Zhao, Junyi Lin, Guanbin Li, Liang Lin.<br>
欢迎各位加入知识星球，获取PDF论文，欢迎转发朋友圈。文章仅做学术分享，如有侵权联系删文。<br>
公众号致力于点云处理，SLAM，三维视觉，高精地图等领域相关内容的干货分享，欢迎各位加入，有兴趣的可联系dianyunpcl@163.com。<br>
文章未申请原创，未经过本人允许请勿转载，有意转载联系微信920177957。<br>
摘要<br>
具身人工智能（Embodied AI）是实现通用人工智能（AGI）的关键，同时也是连接网络空间与物理世界的各种应用的基础。近年来多模态大模型（MLMs）和世界模型（WMs）的出现因其卓越的感知、交互和推理能力而引起了广泛关注，使其成为具身智能体“大脑”的一种有前景的架构。然而，目前针对多模态大模型时代的具身人工智能尚无全面的综述。在这篇综述中对具身人工智能的最新进展进行了全面探讨。首先梳理了具身机器人和模拟器领域的代表性工作，深入了解其研究重点及其局限性。随后分析了四个主要研究目标：1）具身感知，2）具身交互，3）具身智能体，以及4）仿真到现实的适应（sim-to-real adaptation），涵盖了最先进的方法、核心范式以及完整的数据集。此外还探讨了多模态大模型在虚拟和真实具身智能体中的复杂性，强调其在动态数字和物理环境中促进交互的意义。最后总结了具身人工智能领域的挑战与局限，并讨论了其潜在的发展方向。希望这篇综述能够为研究社区提供重要的参考基础，并激发更多创新。<br>
主要贡献<br>
具身人工智能最初是由艾伦·图灵于 1950 年通过具身图灵测试提出的。旨在确定代理是否能够表现出不仅限于解决虚拟环境中的抽象问题的智能，而且还能够应对物理世界的复杂性和不可预测性。网络空间中的代理通常被称为非实体人工智能，而物理空间中的代理则是实体人工智能（表一）。多模态大型模型 (MLM) 的最新进展为实体模型注入了强大的感知、交互和规划能力，从而开发出能够主动与虚拟和物理环境交互的通用实体代理和机器人。因此具身化主体被广泛认为是传销的最佳载体。近期具有代表性的具身化模型有RT-2和 RT-H。然而目前的MLM在长期记忆、理解复杂意图、分解复杂任务等方面的能力还十分有限。<br>
图 1：Google Scholar 搜索结果显示 Embodied AI 相关主题，纵轴和横轴分别表示相关出版物数量和年份。自 2023 年 MLM 突破以来，出版物数量呈指数级增长。<br>
要实现通用人工智能 (AGI)，发展具身人工智能是一条根本途径。与 ChatGPT 等对话代理不同，具身人工智能认为，真正的通用人工智能可以通过控制物理实体并与模拟和物理环境交互来实现。我们正站在 AGI 驱动创新的前沿，深入研究具身人工智能领域、揭示其复杂性、评估其当前发展阶段并思考其未来可能遵循的潜在轨迹至关重要。如今具身人工智能涵盖计算机视觉 (CV)、自然语言处理 (NLP) 和机器人技术等各种关键技术，其中最具代表性的是具身感知、具身交互、具身代理和模拟到现实的机器人控制。因此在追求 AGI 的过程中，通过全面的调查来捕捉具身人工智能不断发展的格局势在必行。<br>
表一：无实体人工智能与实体人工智能的比较。<br>
图2. 基于多模态大模型（MLMs）和世界模型（WMs）的具身智能体整体框架。具身智能体以具身世界模型作为其“大脑”。它具有理解虚拟-物理环境的能力，能够主动感知多模态元素。它能够充分理解人类意图，与人类价值观对齐，分解复杂任务，执行精准的动作，并与人类互动，利用知识库和工具。<br>
具身人工智能的发展呈现快速发展态势，引起了研究界的极大关注，被认为是实现 AGI 最可行的途径。Google Scholar 报告了大量具身人工智能出版物，仅 2023 年就发表了约 10,700 篇论文。这相当于平均每天 29 篇论文或每小时一篇以上论文。尽管人们对从 MLM 中获取强大的感知和推理能力有着浓厚的兴趣，但研究界缺乏全面的调查来帮助理清现有的具身人工智能研究、面临的挑战以及未来的研究方向。在 MLM 时代，我们旨在通过对跨网络空间到物理世界的具身人工智能进行系统调查来填补这一空白。我们从不同的角度进行调查，包括具身机器人、模拟器、四个具有代表性的具身任务（视觉主动感知、具身交互、多模态代理和模拟到现实的机器人控制）以及未来的研究方向。相信这次调查将清楚地展示我们所取得的成就，并可以沿着这个新兴但非常有前景的研究方向取得进一步的成就。<br>
本文提出了这项综合调查，涵盖了代表性的具身机器人、模拟器和四个主要研究任务：具身感知、具身交互、具身代理和模拟到现实的机器人控制。主要贡献有三点：首先，它系统地回顾了具身人工智能，包括具身机器人、模拟器和四个主要研究任务：视觉主动感知、具身交互、具身代理和模拟到现实的机器人控制。据我们所知，这是首次从基于 MLM 和 WM 的网络空间和物理空间对齐的角度对具身人工智能进行全面调查，提供了广泛的概述，并对现有研究进行了全面的总结和分类。其次，它研究了具身人工智能的最新进展，对多个模拟器和数据集的当前工作进行了全面的基准测试和讨论。第三，它确定了具身人工智能通用人工智能未来研究的几个研究挑战和潜在方向。<br>
主要内容<br>
具身机器人<br>
具身人工智能与物理环境积极互动，涵盖范围广泛的具身化，包括机器人、智能家电、智能眼镜、自动驾驶汽车等。其中，机器人是最突出的具身化之一。根据应用的不同，机器人被设计成各种形式，以利用其硬件特性完成特定任务，如图4所示。<br>
图4：具现机器人包括定基机器人、四足机器人、人形机器人、轮式机器人、履带式机器人、仿生机器人等。<br>
固定基座机器人<br>
：如图 4 (a) 所示，固定基座机器人由于结构紧凑、操作精度高，广泛应用于实验室自动化、教育培训和工业制造。这些机器人具有坚固的底座和结构，可确保操作过程中的稳定性和高精度。它们配备高精度传感器和执行器，精度可达微米级，适合需要高精度和可重复性的任务 。此外，固定基座机器人具有高度可编程性，允许用户根据各种任务场景对其进行调整，例如弗兰卡（Franka Emika panda） 、库卡iiwa（KUKA）和 Sawyer (Rethink Robotics)  。尽管固定基座机器人在很多领域都有着出色的表现，但也存在一定的缺点。首先固定基座的设计限制了它们的操作范围和灵活性，使得它们无法在大范围内移动或调整位置，从而影响它们与人类和其他机器人的协作。其次固定基座机器人通常价格昂贵，需要专业人员进行安装和维护，这增加了它们的初始投资和运营成本。<br>
轮式机器人和履带式机器人<br>
：对于移动机器人来说，它们面临的应用场景更加复杂多样。如图 4（b）所示的轮式机器人，以其高效的移动性而闻名，广泛应用于物流、仓储和安全检查。轮式机器人的优点包括结构简单、成本相对较低、能源效率高、能够在平坦表面上快速移动 。这些机器人通常配备高精度传感器，如激光雷达和摄像头，实现自主导航和环境感知，使其在自动化仓库管理和检查任务中非常有效，例如 Kiva 机器人（Kiva Systems  ）和 Jackal 机器人（Clearpath Robotics） 。然而轮式机器人在复杂地形和恶劣环境下，尤其是在不平坦的地面上，机动性有限，负载能力和机动性也受到一定限制。相比之下，履带式机器人越野能力强、机动性高，在农业、建筑、灾害恢复等领域具有巨大潜力，如图 4（c）所示。履带系统提供更大的地面接触面积，分散机器人的重量，降低在泥沙等软地形中下沉的风险。此外履带式机器人通常配备强大的动力和悬架系统，使其能够在复杂地形上保持稳定性和牵引力 。因此可靠的履带式机器人也用于军事等敏感领域。iRobot PackBot 是一款多功能军用履带式机器人，能够执行侦察、爆炸物处理以及搜索和救援等任务。然而由于履带系统摩擦力较大，履带式机器人往往存在能量效率低的问题，而且在平面上的移动速度不如轮式机器人快，灵活性和机动性也相对较低。<br>
四足机器人<br>
：四足机器人以其稳定性和适应性而闻名，非常适合复杂地形探索、救援任务和军事应用。受四足动物的启发，这些机器人可以在不平坦的地面上保持平衡和移动，如图 4 (d) 所示。多关节设计使它们能够模仿生物运动，实现复杂的步态和姿势调整。高适应性使机器人能够自动调整其姿势以适应不断变化的地形，从而增强机动性和稳定性。传感系统（例如 LiDAR 和摄像头）提供环境感知，使机器人能够自主导航并避开障碍物。研究人员通常使用几种类型的四足机器人作为研究平台：Unitree Robotics、Boston Dynamics Spot 和 ANYmal C。Unitree Robotics 的 Unitree A1 和 Go1 以其成本效益和灵活性而闻名。A1和 Go1具备强大的机动性和智能避障能力，适用于各种应用。波士顿动力的 Spot 以其出色的稳定性和操作灵活性而闻名，常用于工业巡检和救援任务。它具有强大的承载能力和适应性，能够在恶劣环境下执行复杂的任务。ANYbotics 的 ANYmal C 具有模块化设计和高耐用性，广泛应用于工业检查和维护。ANYmal C 配备自主导航和远程操作功能，适合长时间户外任务，甚至极端的月球任务。与固定基座机器人一样，四足机器人也面临类似的缺点，例如成本高。四足机器人设计复杂、制造成本高，需要大量的初始投资，限制了它们在成本敏感领域的使用。此外四足机器人在复杂环境中的电池续航能力有限，需要频繁充电或更换电池才能长时间运行。<br>
人形机器人<br>
：继固定基座机器人和四足机器人的讨论之后，人形机器人以其类似人类的外形而著称，在服务业、医疗保健和协作环境等领域越来越普遍。这些机器人可以模仿人类的动作和行为模式，提供个性化的服务和支持。它们灵巧的手部设计使它们能够执行复杂的任务，从而有别于其他类型的机器人，如图 4 (e) 所示。这些手通常具有多个自由度和高精度传感器，使它们能够模拟人手的抓握和操纵能力，这在医疗手术和精密制造等领域尤为重要 。在目前的人形机器人中，Atlas（波士顿动力公司）以其出色的机动性和稳定性而闻名。Atlas可以执行复杂的动态动作，例如跑步、跳跃和滚动，展示了人形机器人在高度动态环境中的潜力。HRP系列（AIST）用于各种研究和工业应用，设计重点是高稳定性和灵活性，使其在复杂环境中有效，特别是在与人类的协作任务中最著名的人形机器人之一ASIMO（本田），可以行走、跑步、爬楼梯，还能识别人脸和手势，适合用于接待、导游服务。此外小型社交机器人 Pepper（软银机器人公司）可以识别情绪并进行自然语言交流，广泛应用于客户服务和教育环境。尽管人形机器人在许多领域表现优异，但由于其控制系统复杂，在复杂环境下保持运行稳定性和可靠性方面面临重大挑战。这些挑战包括稳健的双足步行控制算法和灵巧的手抓取算法 。此外传统基于液压系统的人形机器人体积庞大、维护成本高，正逐渐被电机驱动系统取代。近期特斯拉和宇树机器人等公司均推出了基于电机系统的人形机器人。随着LLMs发展整合，人形机器人有望更智能地处理各种复杂任务，填补制造业、医疗服务业的劳动力空白，从而提高效率和安全性。<br>
仿生机器人<br>
：与前面提到的机器人不同，仿生机器人通过模拟自然生物的有效运动和功能，在复杂而动态的环境中执行任务。通过模仿生物实体的形态和运动机制，这些机器人在医疗保健、环境监测和生物研究等领域展现出巨大的潜力 。通常它们利用柔性材料和结构来实现逼真、敏捷的动作。这些材料不仅增强了机器人的适应性和灵活性，而且还最大限度地减少了对环境的影响。此外仿生机器人通常配备先进的传感器和控制系统，能够实时感知环境并快速响应，从而增强其自主导航和任务执行能力。重要的是，仿生设计可以通过模仿生物体高效的运动机制，显着提高机器人的能源效率，使其在能源消耗方面更经济 。这些仿生机器人包括类似鱼类的机器人、类昆虫机器人和软体机器人，如图 4（f）所示。尽管仿生机器人性能优异，但也面临诸多挑战。首先其设计和制造过程往往复杂且成本高昂，限制了其大规模生产和广泛应用。其次由于仿生机器人采用柔性材料，运动机制复杂，在极端环境下的耐久性和可靠性有待提高。<br>
具身模拟器<br>
数据稀缺一直是具身人工智能研究面临的长期挑战。尽管如此收集现实世界的机器人数据仍面临诸多挑战。首先现实世界的机器人训练由于其实时性而进展缓慢，无法并行化。相关成本高得令人望而却步，需要专门的部署站点、专家操作控制数据收集以及大量的硬件费用。此外，最大的挑战在于可重复性，这源于机器人硬件配置、控制方法和实施框架的巨大差异，阻碍了数据的可移植性。在这种情况下，模拟器为收集和训练具身人工智能数据提供了一种新颖的解决方案。具身模拟器对于具身人工智能至关重要，因为它们提供经济高效的实验，通过模拟潜在危险场景来确保安全，可扩展性以在不同环境中进行测试，快速原型设计能力，更广泛的研究社区的可访问性，精确研究的受控环境，训练和评估的数据生成，以及算法比较的标准化基准。为了使代理能够与环境交互，必须构建一个逼真的模拟环境。这需要考虑环境的物理特性、对象的属性及其相互作用。这里将介绍常用的仿真平台：基于底层仿真的通用模拟器和基于真实场景的模拟器。<br>
通用模拟器<br>
：现实环境中的物理交互和动态变化是不可替代的。然而在物理世界中部署具身模型往往需要高昂的成本，并面临诸多挑战。具身人工智能的最终目标是将虚拟环境中的发现转移到现实世界的应用中，研究人员可以选择最适合他们需求的模拟器来辅助他们的研究。通用模拟器提供了一个与物理世界紧密相似的虚拟环境，允许算法开发和模型训练，这具有显著的成本、时间和安全优势。<br>
Isaac Sim<br>
是由 NVIDIA 开发的一款专为机器人和人工智能研究量身定制的先进仿真平台。Isaac Sim 的主要功能包括高保真物理模拟、实时光线追踪、丰富的机器人模型库和深度学习支持。还引入了 Pixar 的 USD（通用场景描述）格式来描述机器人和复杂场景。Isaac Sim 提供各种预构建的机器人模型和环境，并支持用户定义的模型。其应用场景包括机器人导航和控制、自动驾驶、工业自动化和人机交互。通过提供强大而多功能的平台，Isaac Sim 显著提高了机器人和人工智能研究的效率和效果。<br>
Gazebo<br>
是 Open Robotics 开发的一款开源模拟器，广泛应用于机器人研究和教育。它提供高保真物理模拟和丰富的功能，是研究人员和开发人员的首选工具。Gazebo 的主要功能包括高保真物理模拟、多样化的传感器模拟、广泛的机器人库以及与 ROS 的紧密集成。Gazebo 支持各种传感器的模拟，包括摄像头、激光雷达和声纳，并提供大量预构建的机器人模型和环境，支持自定义模型。其应用场景包括机器人导航和控制以及多机器人系统。<br>
PyBullet<br>
是 Bullet 物理引擎的 Python 接口，提供易于使用的模拟环境。PyBullet 的主要特点包括易用性、实时物理模拟、多样化传感器模拟和深度学习集成。PyBullet 支持实时物理模拟，包括刚体动力学、碰撞检测和约束求解。其应用场景包括机器人导航和控制、强化学习和计算机图形学。<br>
表 II：通用模拟器。HFPS：高保真物理仿真；HQGR：高质量图形渲染；RRL：丰富的机器人库；DLS：深度学习支持；LSPC：大规模并行计算；ROS：与ROS的紧密集成；MSS：多传感器仿真；CP：跨平台；Nav：机器人导航；AD：自动驾驶；RL：强化学习；LSPS：大规模并行仿真；MR：多机器人系统；RS：机器人仿真。○ 表示该模拟器在此方面表现突出。<br>
基于真实场景的模拟器：<br>
实现家居活动中的通用具身智能体一直是具身人工智能研究领域的重点。这些具身智能体需要深入了解人类的日常生活，并执行复杂的具身任务，例如室内环境中的导航和交互。为了满足这些复杂任务的需求，模拟环境需要尽可能接近现实世界，这对模拟器的复杂性和真实感提出了很高的要求。这导致了基于现实世界环境的模拟器的创建。这些模拟器主要从现实世界收集数据，创建逼真的 3D 资源，并使用 UE5 和 Unity 等 3D 游戏引擎构建场景，丰富逼真的场景使基于现实世界环境的模拟器成为家居活动中具身人工智能研究的首选。<br>
图6：基于真实场景的模拟器的示例<br>
表三总结所有基于真实场景的模拟器。<br>
具身感知<br>
视觉感知未来的“north stars”是以具身为中心的视觉推理和社会智能。与仅仅识别图像中的物体不同，具有具身感知的智能体必须在物理世界中移动并与环境交互，这需要对三维空间和动态环境有更深入的理解。具身感知需要视觉感知和推理，理解场景内的三维关系，并根据视觉信息预测和执行复杂任务。<br>
01<br>
主动视觉感知<br>
主动视觉感知系统需要状态估计、场景感知和环境探索等基本能力。如图7所示，这些能力已在视觉同步定位与地图构建 ( vSLAM ) 领域得到广泛研究。3D场景理解和积极探索。这些研究领域有助于开发强大的主动视觉感知系统，促进在复杂、动态的环境中改善环境交互和导航。<br>
图 7：主动视觉感知的示意图，视觉SLAM和3D场景理解为被动视觉感知提供了基础，而主动探索可以为被动感知系统提供主动性。这三个要素相辅相成，对主动视觉感知系统至关重要。<br>
视觉同步定位与地图构建<br>
：同步定位与地图构建 (SLAM) 是一种确定移动机器人在未知环境中的位置并同时构建该环境地图的技术。基于深度的 SLAM使用测距仪（例如激光扫描仪、雷达和/或声纳）创建点云表示，但成本高昂且提供的环境信息有限。视觉 SLAM (vSLAM)使用机载摄像头捕捉帧并构建环境表示。它因硬件成本低、小规模场景中的准确性高以及能够捕捉丰富的环境信息而广受欢迎。经典的 vSLAM 技术可分为传统 vSLAM 和语义 vSLAM。<br>
3D 场景理解<br>
：3D 场景理解旨在从 3D 场景数据中区分物体的语义、识别其位置并推断其几何属性，这是自动驾驶的基础。机器人导航，以及人机交互等。可以使用 3D 扫描工具（如 LiDAR 或 RGB-D 传感器）将场景记录为 3D 点云。与图像不同，点云稀疏、无序且不规则使得场景解读极具挑战性。<br>
前面介绍的3D场景理解方法赋予机器人被</p>
</blockquote>
<h2>6. arxiv.org https://arxiv.org › abs</h2>
<ul>
<li>链接：https://arxiv.org/abs/2509.20021</li>
<li>来源：bing</li>
<li>摘要：2025年9月24日 · Embodied Artificial Intelligence (AI) is an intelligent system paradigm for achieving Artificial General Intelligence (AGI), serving as the cornerstone for various applications and driving …</li>
</ul>
<h3>正文（抓取，非 AI）</h3>
<p>[2509.20021] Embodied AI: From LLMs to World Models<br>
Computer Science &gt; Artificial Intelligence<br>
arXiv:2509.20021<br>
(cs)<br>
[Submitted on 24 Sep 2025]<br>
Title:<br>
Embodied AI: From LLMs to World Models<br>
Authors:<br>
Tongtong Feng<br>
,<br>
Xin Wang<br>
,<br>
Yu-Gang Jiang<br>
,<br>
Wenwu Zhu<br>
View a PDF of the paper titled Embodied AI: From LLMs to World Models, by Tongtong Feng and 3 other authors<br>
View PDF<br>
HTML (experimental)<br>
Abstract:<br>
Embodied Artificial Intelligence (AI) is an intelligent system paradigm for achieving Artificial General Intelligence (AGI), serving as the cornerstone for various applications and driving the evolution from cyberspace to physical systems. Recent breakthroughs in Large Language Models (LLMs) and World Models (WMs) have drawn significant attention for embodied AI. On the one hand, LLMs empower embodied AI via semantic reasoning and task decomposition, bringing high-level natural language instructions and low-level natural language actions into embodied cognition. On the other hand, WMs empower embodied AI by building internal representations and future predictions of the external world, facilitating physical law-compliant embodied interactions. As such, this paper comprehensively explores the literature in embodied AI from basics to advances, covering both LLM driven and WM driven works. In particular, we first present the history, key technologies, key components, and hardware systems of embodied AI, as well as discuss its development via looking from unimodal to multimodal angle. We then scrutinize the two burgeoning fields of embodied AI, i.e., embodied AI with LLMs/multimodal LLMs (MLLMs) and embodied AI with WMs, meticulously delineating their indispensable roles in end-to-end embodied cognition and physical laws-driven embodied interactions. Building upon the above advances, we further share our insights on the necessity of the joint MLLM-WM driven embodied AI architecture, shedding light on its profound significance in enabling complex tasks within physical worlds. In addition, we examine representative applications of embodied AI, demonstrating its wide applicability in real-world scenarios. Last but not least, we point out future research directions of embodied AI that deserve further investigation.<br>
Comments:<br>
Accepted by IEEE CASM<br>
Subjects:<br>
Artificial Intelligence (cs.AI)<br>
; Computation and Language (cs.CL); Robotics (cs.RO)<br>
Cite as:<br>
arXiv:2509.20021<br>
[cs.AI]<br>
(or<br>
arXiv:2509.20021v1<br>
[cs.AI]<br>
for this version)<br>
https://doi.org/10.48550/arXiv.2509.20021<br>
Focus to learn more<br>
arXiv-issued DOI via DataCite<br>
Submission history<br>
From: Tongtong Feng [<br>
view email<br>
]<br>
[v1]<br>
Wed, 24 Sep 2025 11:37:48 UTC (4,147 KB)<br>
Full-text links:<br>
Access Paper:<br>
View a PDF of the paper titled Embodied AI: From LLMs to World Models, by Tongtong Feng and 3 other authors<br>
View PDF<br>
HTML (experimental)<br>
TeX Source<br>
view license<br>
Current browse context:<br>
cs.AI<br>
&lt; prev<br>
|<br>
next &gt;<br>
new<br>
|<br>
recent<br>
|<br>
2025-09<br>
Change to browse by:<br>
cs<br>
cs.CL<br>
cs.RO<br>
References &amp; Citations<br>
NASA ADS<br>
Google Scholar<br>
Semantic Scholar<br>
export BibTeX citation<br>
Loading...<br>
BibTeX formatted citation<br>
×<br>
loading...<br>
Data provided by:<br>
Bookmark<br>
Bibliographic Tools<br>
Bibliographic and Citation Tools<br>
Bibliographic Explorer Toggle<br>
Bibliographic Explorer<br>
(<br>
What is the Explorer?<br>
)<br>
Connected Papers Toggle<br>
Connected Papers<br>
(<br>
What is Connected Papers?<br>
)<br>
Litmaps Toggle<br>
Litmaps<br>
(<br>
What is Litmaps?<br>
)<br>
scite.ai Toggle<br>
scite Smart Citations<br>
(<br>
What are Smart Citations?<br>
)<br>
Code, Data, Media<br>
Code, Data and Media Associated with this Article<br>
alphaXiv Toggle<br>
alphaXiv<br>
(<br>
What is alphaXiv?<br>
)<br>
Links to Code Toggle<br>
CatalyzeX Code Finder for Papers<br>
(<br>
What is CatalyzeX?<br>
)<br>
DagsHub Toggle<br>
DagsHub<br>
(<br>
What is DagsHub?<br>
)<br>
GotitPub Toggle<br>
Gotit.pub<br>
(<br>
What is GotitPub?<br>
)<br>
Huggingface Toggle<br>
Hugging Face<br>
(<br>
What is Huggingface?<br>
)<br>
Links to Code Toggle<br>
Papers with Code<br>
(<br>
What is Papers with Code?<br>
)<br>
ScienceCast Toggle<br>
ScienceCast<br>
(<br>
What is ScienceCast?<br>
)<br>
Demos<br>
Demos<br>
Replicate Toggle<br>
Replicate<br>
(<br>
What is Replicate?<br>
)<br>
Spaces Toggle<br>
Hugging Face Spaces<br>
(<br>
What is Spaces?<br>
)<br>
Spaces Toggle<br>
TXYZ.AI<br>
(<br>
What is TXYZ.AI?<br>
)<br>
Related Papers<br>
Recommenders and Search Tools<br>
Link to Influence Flower<br>
Influence Flower<br>
(<br>
What are Influence Flowers?<br>
)<br>
Core recommender toggle<br>
CORE Recommender<br>
(<br>
What is CORE?<br>
)<br>
Author<br>
Venue<br>
Institution<br>
Topic<br>
About arXivLabs<br>
arXivLabs: experimental projects with community collaborators<br>
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.<br>
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.<br>
Have an idea for a project that will add value for arXiv's community?<br>
Learn more about arXivLabs<br>
.<br>
Which authors of this paper are endorsers?<br>
|<br>
Disable MathJax<br>
(<br>
What is MathJax?<br>
)</p>
<h2>7. csdn.net https://blog.csdn.net › penriver › article › details</h2>
<ul>
<li>链接：https://blog.csdn.net/penriver/article/details/136287650</li>
<li>来源：bing</li>
<li>摘要：2026年1月20日 · 在ChatGPT之后，具身智能（Embodied AI）这个大模型概念火了，那什么是具身智能呢？ 什么是具身智能？ 具身智能 作为人工智能发展的一个重要分支，正在迅速崭露头角，成为科技 …</li>
</ul>
<h3>正文</h3>
<p>（抓取失败或无可提取正文）</p>
<h2>8. wiley.com https://onlinelibrary.wiley.com › doi</h2>
<ul>
<li>链接：https://onlinelibrary.wiley.com/doi/10.1002/smb2.70003</li>
<li>来源：bing</li>
<li>摘要：2025年9月26日 · Exploring the trends of Embodied AI, this review presents a structural framework that details how AI technologies enable intelligent robot behaviors. It delves into the critical aspects of …</li>
</ul>
<h3>正文</h3>
<p>（抓取失败或无可提取正文）</p>
<h2>9. zhihu.com https://zhuanlan.zhihu.com</h2>
<ul>
<li>链接：https://zhuanlan.zhihu.com/p/712662612</li>
<li>来源：bing</li>
<li>摘要：2024年8月4日 · 具身人工智能（Embodied AI）对于实现通用人工智能（AGI）至关重要，并作为连接网络空间和物理世界的各种应用的基础。最近，多模态大型模型（MLMs）和世界模型（WMs）的出现 …</li>
</ul>
<h3>正文</h3>
<p>（抓取失败或无可提取正文）</p>
</div>
</body>
</html>
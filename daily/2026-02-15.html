<!DOCTYPE html>
<html lang="zh-CN">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<title>知识流日报 2026-02-15：3D卷积、3D CNN、图神经网络、注意力机制、Transformer、扩散模型、强化学习、多模态、大模型、知识蒸馏、神经架构搜索、第一性原理、具身智能、预训练、自监督学习、伴随函数</title>
<style>
  * { box-sizing: border-box; }
  body {
    font-family: system-ui, sans-serif;
    max-width: 720px;
    margin: 0 auto;
    padding: 2rem 1rem;
    color: #fff;
    min-height: 100vh;
    background: #0a0e1a;
    background-image:
      radial-gradient(2px 2px at 20px 30px, rgba(255,255,255,0.9), transparent),
      radial-gradient(2px 2px at 40px 70px, rgba(255,255,255,0.6), transparent),
      radial-gradient(1.5px 1.5px at 90px 40px, rgba(255,255,255,0.8), transparent),
      radial-gradient(2px 2px at 130px 80px, rgba(255,255,255,0.5), transparent),
      radial-gradient(1.5px 1.5px at 160px 120px, rgba(255,255,255,0.7), transparent),
      radial-gradient(ellipse 120% 100% at 50% 0%, rgba(88,60,120,0.25), transparent 50%),
      radial-gradient(ellipse 80% 80% at 80% 20%, rgba(40,60,100,0.2), transparent 40%);
    background-size: 200px 200px;
    background-repeat: repeat;
  }
  a { color: #a8d4ff; text-decoration: none; }
  a:hover { text-decoration: underline; }
 h1,h2,h3{margin-top:1.5rem;color:#fff;} pre{white-space:pre-wrap;color:rgba(255,255,255,0.9);} .toolbar{margin:0.5rem 0;color:rgba(255,255,255,0.8);} .content{color:rgba(255,255,255,0.95);} .content a{color:#a8d4ff;} .meta{color:rgba(255,255,255,0.65);font-size:0.9em;} .content p,.content div,.content h2,.content h3,.content h4,.content li,.content pre{cursor:pointer;-webkit-tap-highlight-color:rgba(255,255,255,0.15);}</style>
</head>
<body>
<p><a href="https://YuxinWSoton.github.io/ai-knowledge-flow-site/">← 返回首页</a></p>
<h1>知识流日报 2026-02-15：3D卷积、3D CNN、图神经网络、注意力机制、Transformer、扩散模型、强化学习、多模态、大模型、知识蒸馏、神经架构搜索、第一性原理、具身智能、预训练、自监督学习、伴随函数</h1>
<p class="meta" style="margin-top:0;">最后更新：2026-02-17 11:07</p>
<p class="toolbar"><button id="btnNext" onclick="nextArticle()">下一篇</button> <button id="btnSel" onclick="readSelected()">朗读这段</button> <span id="readStatus"></span></p>
<p class="meta" style="margin-top:0;">（点任意段落即可朗读，手机/触屏友好；也可选中后点「朗读这段」或点「下一篇」按条朗读）</p>
<div class="content">
<h1>知识流日报 2026-02-15：3D卷积、3D CNN、图神经网络、注意力机制、Transformer、扩散模型、强化学习、多模态、大模型、知识蒸馏、神经架构搜索、第一性原理、具身智能、预训练、自监督学习、伴随函数</h1>
<p>共 13 条（来自百度学术 / Google / Bing 等，仅含正文抓取成功的条目；按正文长度从短到长排列，便于先听短的）。</p>
<p>（以下含抓取正文，便于阅读。未调用任何 AI API。）</p>
<h2>1. 3D CNN - Picassooo - 博客园</h2>
<ul>
<li>链接：https://www.cnblogs.com/picassooo/p/13458454.html</li>
<li>来源：bing</li>
<li>摘要：2020年8月8日 · 从2D卷积到3D卷积，都有什么不一样(动态图演示) 3D卷积（3D Convolution) 论文笔记：基于3D卷积神经网络的人体行为识别(3D CNN) 理解3D CNN 第一种理解方式： 视频输 …</li>
</ul>
<h3>正文（抓取，非 AI）</h3>
<p>3D CNN - Picassooo - 博客园 Picassooo 博客园 首页 新随笔 联系 订阅 管理 3D CNN 从2D卷积到3D卷积，都有什么不一样(动态图演示) 3D卷积（3D Convolution) 论文笔记：基于3D卷积神经网络的人体行为识别(3D CNN) 理解3D CNN 第一种理解方式： 视频输入的维度：input_C x input_T x input_W x input_H； 3D卷积核的维度： C个并列的 维度为 kernel_T x kernel_W x kernel_H 的卷积核； 3D卷积核在T, W, H三个方向上移动。 第二种理解方式： posted @ 2020-08-08 16:54 Picassooo 阅读( 1839 ) 评论( 0 ) 收藏 举报 刷新页面 返回顶部 公告 博客园 © 2004-2026 浙公网安备 33010602011771号 浙ICP备2021040463号-3</p>
<h2>2. 【3DCNN基础】-CSDN博客</h2>
<ul>
<li>链接：https://blog.csdn.net/qq_43561603/article/details/130123847</li>
<li>来源：bing</li>
<li>摘要：2024年9月21日 · 3DCNN是一种处理3D输入数据的深度学习模型，其结构类似2DCNN，但计算资源需求更大。3D卷积层和池化层分别用于提取和减少数据维度，全连接层则用于分类或回归任 …</li>
</ul>
<h3>正文（抓取，非 AI）</h3>
<p>【概述】 3DCNN是可以处理３D输入数据的卷积神经网络，结构与２DCNN相同，但是比２DCNN占用更多的内存空间和运行时间。另一方面，由于输入数据的信息很丰富，３DCNN可以给出更精确的结果。CNN架构包括resnet, LeNet, Densenet等，这些架构也以三维形式提供。 【３D卷积层】 卷积层采用卷积来检索图像信号中包含的特征。卷积层的输出称为特征图或者激活图。３D卷积操作比２D卷积操作更为复杂。 【３D池化层】 池化层用于减少图像的空间维度，同时仅保留最具描述性的像素。有 3 种常用方法可供使用：最大池化（选择最大值）、最小池化（选择最低值）、平均池化（值的平均值）。 【３D全连接层】 全连接层适用于之前已经展平的输入，它将一层中的每个神经元连接到另一层中的所有神经元。 【激活函数】 激活函数是一个数学函数，它考虑权重和偏差来确定哪个结果将转移到下一个神经元，可以分为线性激活函数和非线性激活函数。这种激活函数的选择取决于要解决的问题的类型。 非线性激活函数： 【３DCNN和２ＤＣＮＮ比较】 原文链接： https://www.reachiteasily.com/2021/06/3d-convolutional-neural-network-pytorch.html 此笔记仅供自己复习使用！</p>
<h2>3. 王小川、朱军、文继荣等联袂推荐—《扩散模型：生成式AI ...</h2>
<ul>
<li>链接：https://www.zhihu.com/tardis/zm/art/647227802</li>
<li>来源：bing</li>
<li>摘要：2023年7月31日 · 书籍实拍展示 3. 扩散模型应用 得益于扩散模型的强大性能，图片生成的应用Stable Diffusion、DALLE·2、Midjourney、妙鸭相机等在实际生产中都有利用扩散模型进行创造性内容生成 …</li>
</ul>
<h3>正文（抓取，非 AI）</h3>
<ol>
<li>AIGC关键技术 无论是ChatGPT，还是Midjourney、妙鸭相机，都属于人工智能创造内容（AI Generated Content，AIGC）！ AIGC有两项关键技术： 一个是ChatGPT所代表的 大模型（Large Language Model，LLM）技术 另一个是Midjourney、妙鸭相机等AI绘画作图背后的 扩散模型（Diffusion Model）技术 AI绘画、AI对话、AI游戏创作等这些产物的背后是 深度生成模型 ，它可以根据已有的数据和计算机程序生成新的数据。 扩散模型 能够捕捉复杂的数据分布、产生真实、新颖的内容，并且能够实现个性化的、高效的生产。因此，引起了人们的广泛关注。 2. 扩散模型 扩散模型 于2020年被提出，但其发源可以追溯到2015年，理论背景甚至可以追溯到20世纪对于随机过程、随机微分方程的研究。 扩散模型通过向原始数据逐步加入噪声来破坏原始信息，然后再逆转这一过程生成样本。相较于以往的深度生成模型，扩散模型生成的数据质量更高、多样性更强，并且扩散模型的结构也很灵活，这使得扩散模型很快成为了研究和应用的热点。在 《扩散模型：生成式AI模型的理论、应用与代码实践》 一书中就详细讨论了扩散模型与其他深度生成模型的关系。 我们可以考虑一个物理过程来通俗地理解扩散模型。把真实世界的数据比作空气中的一团分子，它们互相交织，形成了具有特定结构的整体。由于这个分子团过于复杂，我们无法直接了解其结构，但我们可以理解在空气中做无规则运动的某种粒子，即对应着服从标准高斯分布的某个变量。从无规则运动的粒子出发，我们不断变换这些粒子的相对位置，每次只变换一小步，最终将这些粒子的分布状态变换为我们想要的复杂的分子的形态。也就是说，从纯噪声开始，我们进行了很多小的“去噪”变换，逐渐地将噪声的分布转换为数据的分布，这样就可以利用得到的数据分布进行采样，得到新的数据。可以看到，我们需要知道的信息就是——该如何进行每一步的变换。这比直接学习原始数据的分布简单得多，并且朴素地解释了扩散模型的有效性。 《扩散模型：生成式AI模型的理论、应用与代码实践》 一书会详细、严格地介绍扩散模型的原理和算法。 扩散模型也有其内在的缺点，如采样速度慢、对结构化数据处理能力较差，等等。例如，扩散模型在将噪声分布逐步转换为数据分布的过程中需要大量调用神经网络，这就导致了生成高质量图片时采样时间较长。后续大量的研究就是致力于提升扩散模型各个方面的性能，使扩散模型可以真正帮助人们高效解决现实问题。 《扩散模型：生成式AI模型的理论、应用与代码实践》 一书将详细分析扩散模型的优缺点，并系统地讲解扩散模型的进一步发展。 书籍实拍展示 3. 扩散模型应用 得益于扩散模型的强大性能，图片生成的应用Stable Diffusion、DALLE·2、Midjourney、妙鸭相机等在实际生产中都有利用扩散模型进行创造性内容生成。 这些应用程序利用扩散模型进行条件生成，即基于输入，引导、生成符合条件的内容。这种引导可以是自然语句，可以是部分图像，也可以用低分辨率的图像作为引导，生成高分辨率的图像，等等。 此外还有利用扩散模型生成语音、视频等各种模态数据的应用。艺术创作者们可以使用这些应用进行直接创作，或者使用它来提供灵感。在生成内容上进行修改可以大大提升工作效率。 此外，扩散模型在科学研究领域也有应用，比如分子结构生成、分子动力学模拟。扩散模型可以生成表示分子的3D表示、分子的图结构，或者二者同时生成，以及控制生成分子的性质。这对于AI制药领域是又一大研究贡献。总的来看，扩散模型在各个领域正处于一个百花齐放的状态。 《扩散模型：生成式AI模型的理论、应用与代码实践》 一书也会详细介绍扩散模型在各个领域的应用研究。 为了推进扩散模型的发展和应用，需要多个学科领域的合作，包括机器学习算法、深度生成学习理论、随机分析理论，各领域的应用研究、隐私保护、法律与监管要求等。 目前扩散模型在各领域的发展和应用的介绍分散于论文和网络上，因此有必要在 《扩散模型：生成式AI模型的理论、应用与代码实践》 这本书中进行系统地介绍。 《扩散模型 : 生成式AI模型的理论、应用与代码实践》是一本从浅入深、全面系统地介绍扩散模型的书籍，其具备丰富的实践案例，以及前沿视角，受到一众专家、学者的认可、推荐。</li>
</ol>
<h2>4. 注意力机制到底在做什么，Q/K/V怎么来的？一文读懂 ...</h2>
<ul>
<li>链接：https://www.zhihu.com/tardis/zm/art/414084879</li>
<li>来源：bing</li>
<li>摘要：2023年2月2日 · Transformer [^1]论文中使用了注意力Attention机制，注意力Attention机制的最核心的公式为： 这个公式中的 Q 、 K 和 V 分别代表Query、Key和Value，他们之间进行的数学计算并不容易 …</li>
</ul>
<h3>正文（抓取，非 AI）</h3>
<p>本文同时发布于我的个人网站，公式图片显示效果更好，欢迎访问： https:// lulaoshi.info/machine-l earning/attention/transformer-attention.html Transformer[^1]论文中使用了注意力Attention机制，注意力Attention机制的最核心的公式为： 这个公式中的 Q 、 K 和 V 分别代表Query、Key和Value，他们之间进行的数学计算并不容易理解。 从向量点乘说起 我们先从 这样一个公式开始。 首先需要复习一下向量点乘（Dot Product）的概念。对于两个行向量 和 ： 向量点乘的几何意义是：向量 在向量 方向上的投影再与向量 的乘积，能够反应两个向量的相似度。向量点乘结果大，两个向量越相似。 一个矩阵 由 行向量组成。比如，我们可以将某一行向量 理解成一个词的词向量，共有 个行向量组成 的方形矩阵： 矩阵 与矩阵的转置 相乘， 中的每一行与 的每一列相乘得到目标矩阵的一个元素， 可表示为： 以 中的第一行第一列元素为例，其实是向量 与 自身做点乘，其实就是 自身与自身的相似度，那第一行第二列元素就是 与 之间的相似度。 下面以词向量矩阵为例，这个矩阵中，每行为一个词的词向量。矩阵与自身的转置相乘，生成了目标矩阵，目标矩阵其实就是一个词的词向量与各个词的词向量的相似度。 词向量矩阵相乘 如果再加上Softmax呢？我们进行下面的计算： 。Softmax的作用是对向量做归一化，那么就是对相似度的归一化，得到了一个归一化之后的权重矩阵，矩阵中，某个值的权重越大，表示相似度越高。 在这个基础上，再进一步： ，将得到的归一化的权重矩阵与词向量矩阵相乘。权重矩阵中某一行分别与词向量的一列相乘，词向量矩阵的一列其实代表着不同词的某一维度。经过这样一个矩阵相乘，相当于一个加权求和的过程，得到结果词向量是经过加权求和之后的新表示，而权重矩阵是经过相似度和归一化计算得到的。 通过与权重矩阵相乘，完成加权求和过程 Q、K、V 注意力Attention机制的最核心的公式为： ，与我们刚才分析的 有几分相似。Transformer[^1]论文中将这个Attention公式描述为：Scaled Dot-Product Attention。其中，Q为Query、K为Key、V为Value。Q、K、V是从哪儿来的呢？Q、K、V其实都是从同样的输入矩阵X线性变换而来的。我们可以简单理解成： 用图片演示为： X分别乘以三个矩阵，生成Q、K、V矩阵 其中， ， 和 是三个可训练的参数矩阵。输入矩阵 分别与 ， 和 相乘，生成 、 和 ，相当于经历了一次线性变换。Attention不直接使用 ，而是使用经过矩阵乘法生成的这三个矩阵，因为使用三个可训练的参数矩阵，可增强模型的拟合能力。 Scaled Dot-Product Attention 在这张图中， 与 经过MatMul，生成了相似度矩阵。对相似度矩阵每个元素除以 ， 为 的维度大小。这个除法被称为Scale。当 很大时， 的乘法结果方差变大，进行Scale可以使方差变小，训练时梯度更新更稳定。 Mask是机器翻译等自然语言处理任务中经常使用的环节。在机器翻译等NLP场景中，每个样本句子的长短不同，对于句子结束之后的位置，无需参与相似度的计算，否则影响Softmax的计算结果。 我们用国外博主Transformer详解博文[^2]中的例子来将上述计算串联起来解释。 输入为词向量矩阵X，每个词为矩阵中的一行，经过与W进行矩阵乘法，首先生成Q、K和V。 q1 = X1 * WQ ， q1 为 Q 矩阵中的行向量， k1 等与之类似。 从词向量到Q、K、V 第二步是进行 计算，得到相似度。 Q与K相乘，得到相似度 第三步，将刚得到的相似度除以 ，再进行Softmax。经过Softmax的归一化后，每个值是一个大于0小于1的权重系数，且总和为0，这个结果可以被理解成一个权重矩阵。 Scale &amp; Softmax 第四步是使用刚得到的权重矩阵，与V相乘，计算加权求和。 使用权重矩阵与V相乘，得到加权求和 多头注意力 为了增强拟合性能，Transformer对Attention继续扩展，提出了多头注意力（Multiple Head Attention）。刚才我们已经理解了， 、 、 是输入 与 、 和 分别相乘得到的， 、 和 是可训练的参数矩阵。现在，对于同样的输入 ，我们定义多组不同的 、 、 ，比如 、 、 ， 、 和 ，每组分别计算生成不同的 、 、 ，最后学习到不同的参数。 定义多组W，生成多组Q、K、V 比如我们定义8组参数，同样的输入 ，将得到8个不同的输出 到 。 定义8组参数 在输出到下一层前，我们需要将8个输出拼接到一起，乘以矩阵 ，将维度降低回我们想要的维度。 将多组输出拼接后乘以矩阵Wo以降低维度 多头注意力的计算过程如下图所示。对于下图中的第2）步，当前为第一层时，直接对输入词进行编码，生成词向量X；当前为后续层时，直接使用上一层输出。 多头注意力计算过程 再去观察Transformer论文中给出的多头注意力图示，似乎更容易理解了： Transformer论文给出的多头注意力图示 [^1]: Vaswani A, Shazeer N, Parmar N, et al. Attention is all you need. 31st Conference on Neural Information Processing Systems 2017(NIPS 2017). Long Beach, CA, USA: 2017: 5998–6008. [^2]: https:// jalammar.github.io/illu strated-transformer/</p>
<h2>5. 使用 3D 卷积神经网络 (CNN) 进行视频分类 - TensorFlow Core</h2>
<ul>
<li>链接：https://tensorflow.google.cn/tutorials/video/video_classification?hl=zh-cn</li>
<li>来源：bing</li>
<li>摘要：2023年11月7日 · 本教程演示了如何使用 UCF101 动作识别数据集训练一个用于视频分类的 3D 卷积神经网络。3D CNN 使用三维过滤器来执行卷积。内核能够在三个维度上滑动，而在 2D …</li>
</ul>
<h3>正文（抓取，非 AI）</h3>
<p>使用 3D 卷积神经网络 (CNN) 进行视频分类 | TensorFlow Core 跳至主要内容 / English 中文 – 简体 日本語 GitHub TensorFlow Core TensorFlow 学习 TensorFlow Core 使用 3D 卷积神经网络 (CNN) 进行视频分类 使用集合让一切井井有条 根据您的偏好保存内容并对其进行分类。 在 TensorFlow.org 上查看 在 Google Colab 中运行 在 GitHub 上查看源代码 下载笔记本 本教程演示了如何使用 UCF101 动作识别数据集训练一个用于视频分类的 3D 卷积神经网络。3D CNN 使用三维过滤器来执行卷积。内核能够在三个维度上滑动，而在 2D CNN 中，它可以在两个维度上滑动。此模型基于 D. Tran 等人在 A Closer Look at Spatiotemporal Convolutions for Action Recognition （2017 年）中发表的工作。在本教程中，您将完成以下任务： 构建输入流水线 使用 Keras 函数式 API 构建具有残差连接的 3D 卷积神经网络模型 训练模型 评估和测试模型 安装 首先，安装和导入一些必要的库，包括：用于检查 ZIP 文件内容的 remotezip ，用于使用进度条的 tqdm ，用于处理视频文件的 OpenCV ，用于执行更复杂张量运算的 einops ，以及用于在 Jupyter 笔记本中嵌入数据的 tensorflow_docs 。 加载并预处理视频数据 下面的隐藏单元定义了从 UCF-101 数据集下载数据切片并将其加载到 tf.data.Dataset 中的函数。可以在 加载视频数据教程 中详细了解特定的预处理步骤，此教程将更详细地介绍此代码。 Toggle code 创建训练集、验证集和测试集（ train_ds 、 val_ds 和 test_ds ）。 Toggle code 创建模型 以下 3D 卷积神经网络模型基于 D. Tran 等人的论文 A Closer Look at Spatiotemporal Convolutions for Action Recognition （2017 年）。这篇论文比较了数个版本的 3D ResNet。与标准 ResNet 一样，它们并非对具有维度 (height, width) 的单个图像进行运算，而是对视频体积 (time, height, width) 进行运算。解决这一问题的最明显方式是将每个 2D 卷积 ( layers.Conv2D ) 替换为 3D 卷积 ( layers.Conv3D )。 本教程使用具有 残差连接 的 (2 + 1)D 卷积。(2 + 1)D 卷积允许对空间和时间维度进行分解，进而创建两个单独的步骤。这种方式的一个优势在于，将卷积因式分解为空间和时间维度有助于节省参数。 对于每个输出位置，3D 卷积将体积的 3D 补丁中的所有向量组合在一起，以在输出体积中创建一个向量。 此运算需要 time * height * width * channels 个输入并产生 channels 个输出（假设输入和输出通道的数量相同。这样，内核大小为 (3 x 3 x 3) 的 3D 卷积层需要一个具有 27 * channels <strong> 2 个条目的权重矩阵。根据参考论文的发现，更有效且高效的方式是对卷积进行因式分解。他们提出了一个 (2+1)D 卷积来分别处理空间和时间维度，而不是用单个 3D 卷积来处理时间和空间维度。下图显示了一个 (2 + 1)D 卷积因式分解后的空间和时间卷积。 这种方式的主要优点是减少了参数数量。在 (2 + 1)D 卷积中，空间卷积接受形状为 (1, width, height) 的数据，而时间卷积接受形状为 (time, 1, 1) 的数据。例如，内核大小为 (3 x 3 x 3) 的 (2 + 1)D 卷积需要大小为 (9 * channels</strong>2) + (3 * channels**2) 的权重矩阵，不到完整 3D 卷积的一半。本教程实现了 (2 + 1)D ResNet18，其中 ResNet 中的每个卷积都被替换为 (2+1)D 卷积。 ResNet 模型由一系列残差块组成。一个残差块有两个分支。主分支执行计算，但难以让梯度流过。残差分支绕过主计算，大部分只是将输入添加到主分支的输出中。梯度很容易流过此分支。因此，将存在从损失函数到任何残差块的主分支的简单路径。这有助于避免梯度消失的问题。 使用以下类创建残差块的主分支。与标准 ResNet 结构相比，它使用自定义的 Conv2Plus1D 层而不是 layers.Conv2D 。 要将残差分支添加到主分支，它需要具有相同的大小。下面的 Project 层处理分支上通道数发生变化的情况。特别是，添加了一系列密集连接层，然后添加了归一化。 使用 add_residual_block 在模型的各层之间引入跳跃连接。 必须调整视频大小才能执行数据的下采样。特别是，对视频帧进行下采样允许模型检查帧的特定部分，以检测可能特定于某个动作的模式。通过下采样，可以丢弃非必要信息。此外，调整视频大小将允许降维，从而加快模型的处理速度。 使用 Keras 函数式 API 构建残差网络。 训练模型 对于本教程，选择 tf.keras.optimizers.Adam 优化器和 tf.keras.losses.SparseCategoricalCrossentropy 损失函数。使用 metrics 参数查看每个步骤中模型性能的准确率。 使用 Keras Model.fit 方法将模型训练 50 个周期。 注：此示例模型在较少的数据点（300 个训练样本和 100 个验证样本）上进行训练，以保持本教程具有合理的训练时间。此外，此示例模型可能需要超过一个小时来训练。 呈现结果 在训练集和验证集上创建损失和准确率的图表： 评估模型 使用 Keras Model.evaluate 获取测试数据集的损失和准确率。 注：本教程中的示例模型使用 UCF101 数据集的子集来保持合理的训练时间。通过进一步的超参数调优或更多的训练数据，可以改善准确率和损失。 要进一步呈现模型性能，请使用 混淆矩阵 。混淆矩阵允许评估分类模型的性能，而不仅仅是准确率。为了构建此多类分类问题的混淆矩阵，需要获得测试集中的实际值和预测值。 另外，还可以使用混淆矩阵计算每个类的准确率和召回率值。 如未另行说明，那么本页面中的内容已根据 知识共享署名 4.0 许可 获得了许可，并且代码示例已根据 Apache 2.0 许可 获得了许可。有关详情，请参阅 Google 开发者网站政策 。Java 是 Oracle 和/或其关联公司的注册商标。 最后更新时间 (UTC)：2024-01-11。</p>
<h2>6. 3D CNN技术深度解析及其在3D物体识别中的应用-百度开发 ...</h2>
<ul>
<li>链接：https://developer.baidu.com/article/detail.html?id=3407282</li>
<li>来源：bing</li>
<li>摘要：2024年11月29日 · 本文深入探讨了3D卷积神经网络（3D CNN）的概念、结构及其在3D物体识别领域的应用。通过对比分析2D CNN与3D CNN的区别，揭示了3D CNN在处理具有时间或深度 …</li>
</ul>
<h3>正文（抓取，非 AI）</h3>
<p>3D CNN技术深度解析及其在3D物体识别中的应用-百度开发者中心 推荐 云原生 文心快码 Baidu Comate 飞桨PaddlePaddle 人工智能 超级链 数据库 百度安全 物联网 开源技术 云计算 大数据 开发者 企业服务 更多内容 千帆大模型平台 客悦智能客服 3D CNN技术深度解析及其在3D物体识别中的应用 作者： 菠萝爱吃肉 2024.11.29 21:39 浏览量： 192 简介： 本文深入探讨了3D卷积神经网络（3D CNN）的概念、结构及其在3D物体识别领域的应用。通过对比分析2D CNN与3D CNN的区别，揭示了3D CNN在处理具有时间或深度信息的三维数据时的优势。同时，结合具体算法和实例，阐述了3D CNN在识别和定位3D对象中的实际效果。 在当今的计算机视觉领域，三维卷积 神经网络 （3D CNN）正逐渐成为处理和分析三维数据的重要工具，特别是在3D物体识别方面展现出巨大的潜力。本文将深入探讨3D CNN的基本概念、结构特点及其在3D物体识别算法中的应用，通过对比分析，揭示3D CNN相比于传统二维卷积神经网络（2D CNN）的优势。 一、3D CNN的基本概念与结构 三维卷积神经网络（3D CNN）是一种 深度学习 模型，它能够处理 视频 、医学图像等具有时间或深度信息的三维数据。与2D CNN不同，3D CNN采用三维卷积核来处理三维数据，卷积核在三个方向上移动并执行卷积操作，从而捕捉三维数据中的空间和时间/深度特征。 3D CNN通常由卷积层、池化层、批量归一化层和全连接层组成。卷积层和池化层可以有效地减少数据维度并提取特征，批量归一化层可以加速收敛和提高模型的泛化能力，而全连接层则将特征映射到具体的输出类别。 二、3D CNN与2D CNN的对比分析 在处理二维图像时，2D CNN表现出色，但在处理具有时间或深度信息的三维数据时，其性能受限。相比之下，3D CNN具有以下显著优势： 时空特征捕捉 ：3D CNN可以对连续帧的视频数据进行处理，理解视频中的运动和动态变化，对于视频分类、动作识别等任务具有明显优势。 多通道数据处理 ：3D CNN可以在一个模型中同时处理多个通道的数据，如RGB和深度数据，将不同的数据类型结合在一起进行处理。 特征提取能力 ：3D CNN利用3D卷积核进行卷积操作，可以提取出空间上更加丰富的特征，从而提高模型的准确性。 三、3D CNN在3D物体识别中的应用 在3D物体识别领域，3D CNN的应用主要体现在以下几个方面： 点云数据处理 ：在激光雷达（LiDAR）点云物体检测中，3D CNN能够处理点云数据，提取出物体的三维特征，实现精确的3D物体检测和定位。例如，VoxelNet和PointNet++等代表性方法，在点云物体检测领域取得了显著成果。 视频分析 ：在视频分析中，3D CNN能够捕捉视频中的时空特征，用于视频分类、动作识别等任务。通过提取视频中的连续帧信息，3D CNN可以实现对视频中物体的精确识别和跟踪。 医学影像分析 ：在医学影像分析领域，3D CNN能够处理医学图像数据，提取出病变部位的三维特征，辅助医生进行疾病诊断和治疗。例如，在肺部CT图像分析中，3D CNN可以实现对肺结节的精确检测和分类。 四、具体算法与实例 以pcl_recognition模块为例，该模块利用相关组算法对从3D描述器算法中提取的特征点进行聚类，将当前的场景与模型进行匹配。对于每一次聚类，描绘出一个在场景中的可能模型实例，并输出标识6DOF位姿估计的转换矩阵。这种方法在3D物体识别中取得了良好的效果。 此外，在3D 人脸识别 等应用中，3D CNN也展现出巨大的潜力。传统2D人脸识别由于无法记录脸部的深度三维信息，存在 安全 隐患。而3D CNN能够提取人脸的三维特征，实现更加准确和安全的人脸识别。 五、结论与展望 综上所述，3D CNN在3D物体识别领域具有广泛的应用前景和巨大的潜力。随着深度学习技术的不断发展和计算能力的不断提升，3D CNN将在更多领域发挥重要作用。未来，我们可以期待3D CNN在自动驾驶、机器人视觉、医学影像分析等领域取得更加显著的成果。 同时，为了进一步提高3D CNN的性能和泛化能力，我们需要不断探索新的网络结构、优化算法和训练策略。此外，结合其他先进技术如传感器融合、强化学习等，也将为3D CNN的应用拓展新的可能性。在选择相关技术平台时， 千帆 大模型开发 与服务平台 凭借其强大的模型开发能力，能够为3D CNN的研究与应用提供有力支持。 相关文章推荐 文心一言接入指南：通过百度智能云千帆大模型平台API调用 本文介绍了如何通过百度智能云千帆大模型平台接入文心一言，包括创建千帆应用、API授权、获取访问凭证及调用API接口的详细流程。文心一言作为百度的人工智能大语言模型，拥有强大的语义理解与生成能力，通过千帆平台可轻松实现多场景应用。 十万个为什么 2023.10.20 16:56 257267 19 10 从 MLOps 到 LMOps 的关键技术嬗变 本文整理自 QCon 全球软件开发大会 -从 MLOps 到 LMOps 分论坛的同名主题演讲 百度智能云开发者中心 2023.11.15 18:03 34868 9 5 Sugar BI教你怎么做数据可视化 - 拓扑图，让节点连接信息一目了然 Sugar BI教你怎么做数据可视化 - 拓扑图，让节点连接信息一目了然 百度智能云开发者中心 2023.03.21 10:56 30806 3 1 更轻量的百度百舸，CCE Stack 智算版发布 百度百舸·AI 异构计算平台，是百度智能云将百度内部强大的 AI 工程能力面向市场推出的解决方案。 百度智能云开发者中心 2023.03.02 12:17 26680 1 1 打造合规数据闭环，加速自动驾驶技术研发 今天跟大家的演讲主题，主要是想交流如何去构建这样两个自动驾驶的数据闭环链路。 百度智能云开发者中心 2023.03.02 15:00 28144 0 1 LMOps 工具链与千帆大模型平台 LMOps 相关的概念以及关键技术 百度智能云开发者中心 2023.11.17 15:49 24328 3 3 发表评论 登录后可评论，请前往 登录 或 注册 评 论 开发者关注产品榜 1 百度千帆·大模型服务及Agent开发平台 企业级一站式大模型开发及服务平台 模型训练限时免费 2 百度千帆·数据智能平台 一站式多模态数据管理、加工和分析应用平台 平台体验全免费 3 秒哒-生成式应用开发平台 不用写代码，就能实现任意想法 全功能免费体验 4 百度智能云客悦智能客服平台 大模型重塑营销与客服体验 0元试用一个月 最热文章 零基础调用文心大模型4.5API实操手册 生产力UP！文心快码 Rules 功能实战指南 Redis 数据恢复的月光宝盒，闪回到任意指定时间 用文心快码Zulu打造太阳系3D模拟器：从需求到落地的全流程实践 关于作者 被阅读数 被赞数 被收藏数 关 注 活动 咨询</p>
<h2>7. 注意力机制 - 知乎</h2>
<ul>
<li>链接：https://www.zhihu.com/topic/20682987/intro</li>
<li>来源：bing</li>
<li>摘要：2020年4月24日 · 注意力机制是非常优美而神奇的机制，在神经网络「信息过载」的今天，让 NN 学会只关注特定的部分，无疑会大幅度提升任务的效果与效率。借助注意力机制，神经机器翻译、预训练语 …</li>
</ul>
<h3>正文（抓取，非 AI）</h3>
<p>注意力机制 - 知乎 注意力机制 「注意力机制」（Attention Mechanism）是机器学习中的一种数据处理方法，广泛应用在自然语言处理、图像识别及语音识别等各种不同类型的机器学习任务中。... 查看全部内容 关注话题 ​ 管理 ​ 分享 ​ 百科 讨论 精华 等待回答 详细内容 简介 注意力机制（Attention Mechanism）是机器学习中的一种数据处理方法，广泛应用在自然语言处理、图像识别及语音识别等各种不同类型的机器学习任务中。 注意力机制分成两个部分： 一、注意力机制为什么有必要存在（科普向） 二、注意力机制具体是如何实现的（知识向） 存在意义 一、注意力机制为什么有必要存在 注意力这个词本来是属于人类才有的动作。 也就是说，注意力机制可以看做是一种仿生，是机器通过对人类阅读、听说中的注意力行为进行模拟。 那为何要对注意力进行仿生，按理说，计算机理应拥有无限的记忆力和注意力。 这是因为，人脑在进行阅读任务、读图任务时，并不是严格的解码过程，而是接近于一种模式识别。 大脑会自动忽略低可能、低价值的信息。 大脑会自动忽略可能性低的答案 上面的这段话，好几个词都存在顺序混乱，但是阅读的第一感受，不会感觉到这种乱序。 甚至不用停下来思考，这种乱序是不是存在笔误，不仔细看甚至发现不了问题的存在。 这不是眼睛都出了问题，而是大脑在识别文字的过程中，自动就把低可能的解释忽略了。 这一点在读图任务上，还更为明显一些，读图的过程中，大脑总是会优先获取认为有用的信息，而将次要的内容直接抛弃。 这是因为，大脑在阅读或读图的过程中，会直接抛弃低可能性答案，将阅读的内容更正为“大脑认为正确的版本”。 同样的文字随着对话主题的不同，含义也会发生变化 上下文联系影响文字的意义 这是网络上的一个段子，所谓的中文十级考试。 这段话的第二句话里，两个“对”字代表了不同的含义。 但如果单独把第二局话挑出来，即使是中国人也会对里面的意义产生疑义。因为本身这句话的“对了”可以解释成“已经校对过了”、“正确了”或者“无意义的承接词”。 但如果有了第一句话的限定，这个“对了”就只能是“对答案了”的意思。 也就是说，理解一句话的含义，不仅仅取决于这句话本身，而与上下文相关联的词也有很大影响。 还不单单如此，通常一段对话中，都会存在一个反复出现的概念，例如这段话中的“对答案”，其他的词语或多或少都能与这个概念产生联系。也就是说，这个概念就是这段话的主题。而在更复杂一些的段落里，还会出现部分与主题没有什么关联的内容，通常这些内容都会被我们弱化或者自动遗忘。 结合上面的两个例子，人们在阅读、交流的过程中，本身就存在着信息的舍弃。虽然每段文字可能字号、粗细都相同，但注意力却不是那样均衡地分配给每一个词。 如果计算机不能模拟人类的注意力状态，就可能让无关的信息对处理结果造成干扰，最后导致处理结果偏离实际的应用场景。 例如，聊天场景中，用户输入了错别字，导致了歧义。如果是人工场景，就很容易忽略错别字的影响，理解文字的本来含义。 又或者，同样的句子，在不同语境中含义发生变化，导致机器翻译在段落和文章的翻译上，似是而非，语言不通顺。 这些干扰，都让人工智能显得像是“人工智障”，逻辑硬伤导致无法执行较为复杂的任务。 为了让计算机更加适应人类交流场景，必须教会计算机选择遗忘和关联上下文，这种机制就是所谓的注意力机制。 实现过程 严格来说，注意力机制更像是一种方法论。没有严格的数学定义，而是根据具体任务目标，对关注的方向和加权模型进行调整。 简单的理解就是，在神经网络的隐藏层，增加注意力机制的加权。 使不符合注意力模型的内容弱化或者遗忘。 Google 2017年论文中，Attention Is All You Need曾经为Attention做了一个抽象定义： Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_k}})V An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key. 注意力是将一个查询和键值对映射到输出的方法，Q、K、V均为向量，输出通过对V进行加权求和得到，权重就是Q、K相似度。 1）机器视觉中的应用（精细分类、图像分割、图像焦点） 例如，识别鸟类的品种问题。对于鸟品种的精细分类，对结果影响最大的可能是鸟类的头部，通过注意力机制将头部的特征强化，而忽略其他部分（羽毛、爪子），以实现区分鸟类的具体品种。 2）机器翻译中的应用（LSTM+注意力模型） LSTM单元 LSTM（Long Short Term Memory）是RNN（循环神经网络）的一种应用。可以简单理解为，每一个神经元都具有输入门、输出门、遗忘门。 输入门、输出门将LSTM神经元首尾连接在一起，而遗忘门将无意义内容弱化或遗忘。注意力机制就应用在LSTM的遗忘门，使得机器阅读更加贴近于人类阅读的习惯，也使得翻译结果具有上下文联系。 参考资料 【计算机视觉】深入理解Attention机制 - Slow down, Keep learning and Enjoy life - CSDN博客 周知瑞：Attention的梳理、随想与尝试 瑟木：计算机视觉中的注意力机制 百科摘录 3 2019 ICCV 用于检测、去除阴影区域的注意力循环生成对抗网路 下的内容摘录 B1gme 总是再不停的突破自己的b1gme 注意力机制是用来编码序列数据，这些数据基于给每个参数分配重要的权值的方式。它为自然语言处理方向、语音识别、计算机视觉、图像描述和视觉问答方向提供重大帮助。不同于上述方法，论文采取了一种使用渐进式和循环方法来整合不同层特征的多内容信息。并且能处理复杂环境下阴影去除和检测。 知乎小知 摘录于 2020-04-24 哈希算法、爱因斯坦求和约定，这是2020年的注意力机制 下的内容摘录 机器之心 ​ 数学等 2 个话题下的优秀答主 注意力机制是非常优美而神奇的机制，在神经网络「信息过载」的今天，让 NN 学会只关注特定的部分，无疑会大幅度提升任务的效果与效率。借助注意力机制，神经机器翻译、预训练语言模型等任务获得了前所未有的提升。 知乎小知 摘录于 2020-04-24 【ACL 2019】为知识图谱添加注意力机制 下的内容摘录 超正经学术君 让更多人读懂科学 注意力机制（Attention）是近些年来提出的一种改进神经网络的方法，在图像识别、自然语言处理和图网络表示等领域都取得了很好的效果，可以说注意力机制的加入极大地丰富了神经网络的表示能力。 知乎小知 摘录于 2020-04-24 浏览量 3677 万 讨论量 1.6 万</p>
<h2>8. 图卷积网络（GCN）入门详解</h2>
<ul>
<li>链接：https://www.zhihu.com/tardis/zm/art/107162772</li>
<li>来源：bing</li>
<li>摘要：2020年3月5日 · 图卷积网络（GCN）入门详解 什么是GCN GCN 概述 模型定义 数学推导 Graph Laplacian ref 图神经网络领域算是一个比较新的领域，有非常多的探索潜力，所以我也一直想着要入 …</li>
</ul>
<h3>正文（抓取，非 AI）</h3>
<p>图卷积网络（GCN）入门详解 什么是GCN GCN 概述 模型定义 数学推导 Graph Laplacian ref 图神经网络领域算是一个比较新的领域，有非常多的探索潜力，所以我也一直想着要入门。其中图卷积网络就非常热门，我找到了一篇教程： 图卷积网络(GCN)新手村完全指南 , 想着借此走出新手村，结果因为个人资质有限，简直成了劝退文，看完了之后还是没有非常的明白，所以决定自己写一篇入门介绍。当然这篇文章介绍得非常好，我也参考了很多，所以我会引用很多其中的公式，加上相关的推导补充。 本文主要分为两部分，第一部分介绍什么是GCN，第二部分将进行详细的数学推导。 什么是GCN GCN 概述 本文讲的GCN 来源于论文： SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS ，这是在GCN领域最经典的论文之一。 我们可以根据这个GCN的图看到，一个拥有 个input channel的graph作为输入，经过中间的hidden layers，得到 个 output channel的输出。 图卷积网络主要可以由两个级别的作用变换组成： 注意本文讲的图都特指无向无权重的图。 graph level： 例如说通过引入一些形式的pooling 操作 (see, e.g. Duvenaud et al ., NIPS 2015). 然后改变图的结构。但是本次讲过GCN并没有进行这个级别的操作。所以看到上图我们的网络结构的输出和输出的graph的结构是一样的。 node level： 通常说node level的作用是不改变graph的结构的，仅通过对graph的特征/信号（features/signals） 作为输入：一个 的矩阵( : 输入图的nodes的个数， 输入的特征维度) ，得到输出 ：一个 的矩阵( 输出的特征维度)。 a) 一个特征描述（feature description） : 指的是每个节点 的特征表示 b) 每一个graph 的结构都可以通过邻接矩阵 表示（或者其他根据它推导的矩阵） 我们可以很容易的根据一个邻接矩阵重构出一个graph。 例如下图： 其中 代表节点， 代表边 我们通过构造 的矩阵可以得到邻接矩阵 , 其中 如果节点 和节点 相连，否则 ， 我们根据graph可以得到 ， 同理通过 也可以得到graph 的结构。 所以网络中间的每一个隐藏层可以写成以下的非线性函数： 其中输入层 , 输出层 , 是层数。 不同的GCN模型，采用不同 函数。 模型定义 论文中采用的函数如下： 刚开始看的时候，都会被吓到！这个函数未免也太抽象了。但是我们先了解一下它在起的作用，然后再从头一步一步引出这个公式，以及为什么它起到了这些作用。 首先物理上它起的作用是，每一个节点下一层的信息是由前一层本身的信息以及相邻的节点的信息加权加和得到，然后再经过线性变换 以及非线性变换 。 我们一步一步分解，我们要定义一个简单的 函数，作为基础的网络层。 我们可以很容易的采用最简单的层级传导（ layer-wise propagation ）规则 我们直接将 做矩阵相乘，然后再通过一个权重矩阵 做线性变换，之后再经过非线性激活函数 , 比如说 ReLU，最后得到下一层的输入 。 我们需要特别注意的是 做矩阵相乘，这代表了什么意思呢？ 我们先看看，对于下图。 假设每个节点 , 那么在经过矩阵相乘之后，它会变成什么呢。 输入层的 , 根据矩阵的运算公式我们可以很容易地得到下一层的该节点的表示 , 也很容易发现 ，而 就是节点1的相邻节点。具体计算结果可以参考下面的代码。 所以我们直到 就是把通过邻接矩阵快速的方式，快速将相邻的节点的信息相加得到自己下一层的输入。 但是这就完美了吗？ 问题一：我们虽然获得了周围节点的信息了，但是自己本身的信息却没了（除非自己有一条边指向自己）。 我们采用的解决方案是，对每个节点手动增加一条self-loop 到每一个节点，即 , 其中 是单位矩阵identity matrix。 问题二：从上面的结果也可以看出，在经过一次的 矩阵变换后，得到的输出会变大，即特征向量 的scale会改变，在经过多层的变化之后，将和输入的scale差距越来越大。 所以我们是否可以将邻接矩阵 做归一化使得最后的每一行的加和为1，使得 获得的是weighted sum。 我们可以将 的每一行除以行的和，这就可以得到normalized的 。而其中每一行的和，就是每个节点的度degree。用矩阵表示则为： , 对于 我们还是按照上面图的graph来看。 但是在实际运用中我们采用的是对称的normalization： 对于 这跟Laplacian Matrix 有关，下一部分会介绍。 我们可以发现 把这两个tricks结合起来，我们可以得到 其中 , 是 的degree matrix。 而 是对 做了一个对称的归一化。 数学推导 Graph Laplacian 首先我们表示一个graph的方式有很多，我们可以用邻接矩阵，也可以用Incidence matrix。 这个matrix 中，每一行表示一个边，每一列表示一个节点。每一行中，边的节点的起点用记为1，边的终点记为-1。 我们将这个metrix 记为 . 具体如下图。 那么 graph Laplacian 定义为： 我们可以发现，对角线的值 , 其中如果 , 则其积 = 0，如果 或者 , 则其积 = 1。所以我们可以知道对角线代表的是每个节点的度（Degree） 对于非对角线的值 , 我们可以看出来，如果节点 和 没有相连，那么 否则 ， 于是知道非对角线的值就是邻接矩阵的负值。 所以我们可以推导得到 如下图（注意这边W表示的是邻接矩阵） 总结来说： 具体计算参考下面的代码 我们需要知道 laplacian 的性质： 是对称矩阵 有实数的，非负的特征值（eigen values） 有实数的，正交的特征矩阵（eigen vectors), i.e. 对此，我们假设 的特征值为 特征向量为 : 对于特征值我们有 对称归一化的Laplacian （Symmetric normalized Laplacian） 其元素值，对角线为1，非对角线为 我们要知道两个函数的卷积可以由以下公式得到，具体 参考 其中 代表傅立叶变换 而Graph Fourier变换对应的就是以下： 其中Graph Fourier 逆变换对应的就是以下： 其中 是laplacian 的特征矩阵 具体的对应关系： 我们知道普通的卷积公式： 那么相应的图卷积的公式为： 作为图的特征 的filter，我们希望它的作用域跟CNN一样，都是在中心节点附近的区域，所以我们定义 是一个laplacian的函数 ， 那么作用一次相当于传播一次周围邻居节点的信息。 又因为 , 所以我们可以把 看成是laplacian 特征值的函数 , 参数为 。 所以图卷积在Fourier域上可以表示为： 我们知道 需要先计算laplacian matrix 的 特征值，这涉及到大量的矩阵运算，所以文章借用了Chebyshev polynomials进行近似计算： 其中 , 代表的是 次Laplacian，即它取决于中心节点的最近的 order 的邻居节点（邻居节点和中心节点的距离最大为K）。 , 其中 以及 。 我们回到最初的图卷积计算： 其中 我们知道论文中采用的传播邻居层数为1， 所以取 , 并且我们假设 , 可以得到： 实际运用中，为了防止overfitting以及减少操作，我们令 得到： 我们令 , 以及 得到： 再加上激活函数 , 我们获得了 其中 对应输入 ， 对应参数 ref https:// en.wikipedia.org/wiki/L aplacian_matrix T. N. Kipf, M. Welling, Semi-Supervised Classification with Graph Convolutional Networks(ICLR 2017) [ Link , PDF (arXiv) , code , blog ] https:// math.stackexchange.com/ questions/1113467/why-laplacian-matrix-need-normalization-and-how-come-the-sqrt-of-degree-matrix</p>
<h2>9. 一文了解Transformer全貌（图解Transformer）</h2>
<ul>
<li>链接：https://www.zhihu.com/tardis/zm/art/600773858</li>
<li>来源：bing</li>
<li>摘要：2025年9月26日 · 网上有关Transformer原理的介绍很多，在本文中我们将尽量模型简化，让普通读者也能轻松理解。 1. Transformer整体结构 在机器翻译中，Transformer可以将一种语言翻译成另一种语 …</li>
</ul>
<h3>正文（抓取，非 AI）</h3>
<p>打个小广告 ☻，知乎专栏 《大模型前沿应用》 的内容已经收录在新书 《揭秘大模型：从原理到实战》 中。感兴趣的朋友可以购买，多谢支持！♥♥ 自2017年Google推出Transformer以来，基于其架构的语言模型便如雨后春笋般涌现，其中Bert、T5等备受瞩目，而近期风靡全球的大模型ChatGPT和LLaMa更是大放异彩。网络上关于Transformer的解析文章非常大，但本文将力求用浅显易懂的语言，为大家深入解析Transformer的技术内核。 前言 Transformer是谷歌在2017年的论文《Attention Is All You Need》中提出的，用于NLP的各项任务，现在是谷歌云TPU推荐的参考模型。网上有关Transformer原理的介绍很多，在本文中我们将尽量模型简化，让普通读者也能轻松理解。 1. Transformer整体结构 在机器翻译中，Transformer可以将一种语言翻译成另一种语言，如果把Transformer看成一个黑盒，那么其结构如下图所示： 将法语翻译成英语 那么拆开这个黑盒，那么可以看到Transformer由若干个编码器和解码器组成，如下图所示： 继续将Encoder和Decoder拆开，可以看到完整的结构，如下图所示： Transformer整体结构（引自谷歌论文） 可以看到Encoder包含一个Muti-Head Attention模块，是由多个Self-Attention组成，而Decoder包含两个Muti-Head Attention。Muti-Head Attention上方还包括一个 Add &amp; Norm 层，Add 表示残差连接 (Residual Connection) 用于防止网络退化，Norm 表示 Layer Normalization，用于对每一层的激活值进行归一化。 假设我们的输入包含两个单词，我们看一下Transformer的整体结构： Transformer整体结构（输入两个单词的例子） 为了能够对Transformer的流程有个大致的了解，我们举一个简单的例子，还是以之前的为例，将法语"Je suis etudiant"翻译成英文。 第一步 ：获取输入句子的每一个单词的表示向量 ， 由单词的Embedding和单词位置的Embedding 相加得到。 Transformer输入表示 第二步 ：将单词向量矩阵传入Encoder模块，经过N个Encoder后得到句子所有单词的编码信息矩阵 ，如下图。输入句子的单词向量矩阵用 表示，其中 是单词个数， 表示向量的维度（论文中 ）。每一个Encoder输出的矩阵维度与输入完全一致。 输入X经过Encoder输出编码矩阵C 第三步 ：将Encoder输出的编码矩阵 传递到Decoder中，Decoder会根据当前翻译过的单词 翻译下一个单词 ，如下图所示。 Transformer Decoder预测 上图Decoder接收了Encoder的编码矩阵，然后首先输入一个开始符 "<Begin>"，预测第一个单词，输出为"I"；然后输入翻译开始符 "<Begin>" 和单词 "I"，预测第二个单词，输出为"am"，以此类推。这是Transformer的大致流程，接下来介绍里面各个部分的细节。 2. Transformer的输入表示 Transformer中单词的输入表示由 单词Embedding 和 位置Embedding （Positional Encoding）相加得到。 Transformer输入表示 2.1 单词Embedding 单词的Embedding可以通过Word2vec等模型预训练得到，可以在Transformer中加入Embedding层。 2.2 位置Embedding Transformer 中除了单词的Embedding，还需要使用位置Embedding 表示单词出现在句子中的位置。 因为 Transformer不采用RNN结构，而是使用全局信息，不能利用单词的顺序信息，而这部分信息对于NLP来说非常重要。 所以Transformer中使用位置Embedding保存单词在序列中的相对或绝对位置。 位置Embedding用 表示， 的维度与单词Embedding相同。 可以通过训练得到，也可以使用某种公式计算得到。在Transformer中采用了后者，计算公式如下： 其中， 表示单词在句子中的位置， 表示 的维度。 3. Multi-Head Attention（多头注意力机制） Transformer内部结构 上图是Transformer的内部结构，其中红色方框内为 Multi-Head Attention ，是由多个 Self-Attention 组成，具体结构如下图： Self-Attention和Multi-Head Attention 因为 Self-Attention 是Transformer的重点，所以我们重点关注 Multi-Head Attention 以及 Self-Attention，首先介绍下Self-Attention的内部逻辑。 3.1 Self-Attention结构 Self-Attention结构 上图是Self-Attention结构，最下面是 (查询)、 (键值)、 (值)矩阵，是通过输入矩阵 和权重矩阵 相乘得到的。 Q,K,V的计算 得到 之后就可以计算出Self-Attention的输出，如下图所示： Self-Attention输出 3.2 Multi-Head Attention输出 在上一步，我们已经知道怎么通过Self-Attention计算得到输出矩阵 ，而Multi-Head Attention是由多个Self-Attention组合形成的，下图是论文中Multi-Head Attention的结构图。 Multi-Head Attention 从上图可以看到Multi-Head Attention包含多个Self-Attention层，首先将输入 分别传递到 个不同的Self-Attention中，计算得到 个输出矩阵 。下图是 的情况，此时会得到 8 个输出矩阵 。 多个Self-Attention 得到8个输出矩阵 后，Multi-Head Attention将它们拼接在一起（Concat），然后传入一个Linear层，得到Multi-Head Attention最终的输出矩阵 。 Multi-Head Attention输出 4. 编码器Encoder结构 Transformer Encoder模块 上图红色部分是Transformer的Encoder结构， 表示Encoder的个数，可以看到是由Multi-Head Attention、Add &amp; Norm、Feed Forward、Add &amp; Norm组成的。前面已经介绍了Multi-Head Attention的计算过程，现在了解一下Add &amp; Norm和 Feed Forward部分。 4.1 单个Encoder输出 Add &amp; Norm 是指残差连接后使用LayerNorm，表示如下： 其Sublayer表示经过的变换，比如第一个Add &amp; Norm中Sublayer表示Multi-Head Attention。 Feed Forward 是指全连接层，表示如下： 因此输入矩阵 经过一个Encoder后，输出表示如下： 4.2 多个Encoder输出 通过上面的单个Encoder，输入矩阵 ，最后输出矩阵 。通过多个Encoder叠加，最后便是编码器Encoder的输出。 5. 解码器Decoder结构 Transformer Decoder模块 上图红色部分为Transformer的Decoder结构，与Encoder相似，但是存在一些区别： 包含两个Multi-Head Attention 第一个Multi-Head Attention采用了Masked操作 第二个Multi-Head Attention的 矩阵使用Encoder的 编码信息矩阵 进行计算，而 使用上一个 Decoder的输出计算 最后有一个Softmax层计算下一个翻译单词的概率 5.1 第一个Multi-Head Attention Decoder的第一个Multi-Head Attention采用了Masked操作，因为在翻译的过程中是顺序翻译的，即翻译完第 个单词，才可以翻译第 个单词。通过 Masked 操作可以防止第 个单词知道 个单词之后的信息。下面以法语"Je suis etudiant"翻译成英文"I am a student"为例，了解一下 Masked 操作。 在Decoder的时候，需要根据之前翻译的单词，预测当前最有可能翻译的单词，如下图所示。首先根据输入"<Begin>"预测出第一个单词为"I"，然后根据输入"<Begin> I" 预测下一个单词 "am"。 Decoder预测（右图有问题，应该是Decoder 1） Decoder在预测第 个输出时，需要将第 之后的单词掩盖住， Mask操作是在Self-Attention的Softmax之前使用的， 下面以前面的"I am a student"为例。 第一步： 是Decoder的输入矩阵和 Mask 矩阵，输入矩阵包含"<Begin> I am a student"4个单词的表示向量， Mask 是一个 的矩阵。在 Mask 可以发现单词"<Begin>"只能使用单词"<Begin>"的信息，而单词"I"可以使用单词"<Begin> I"的信息，即只能使用之前的信息。 输入矩阵与Mask矩阵 第二步 ：接下来的操作和之前Encoder中的Self-Attention一样，只是在Softmax之前需要进行Mask操作。 Mask Self-Attention输出 第三步 ：通过上述步骤就可以得到一个Mask Self-Attention的输出矩阵 ，然后和Encoder类似，通过Multi-Head Attention拼接多个输出 然后计算得到第一个Multi-Head Attention的输出 ， 与输入 维度一样。 5.2 第二个Multi-Head Attention Decoder的第二个Multi-Head Attention变化不大， 主要的区别在于其中Self-Attention的 矩阵不是使用上一个Multi-Head Attention的输出，而是使用 Encoder的编码信息矩阵 计算的。根据Encoder的输出 计算得到 ，根据上一个Multi-Head Attention的输出 计算 。这样做的好处是在Decoder的时候，每一位单词（这里是指"I am a student"）都可以利用到Encoder所有单词的信息（这里是指"Je suis etudiant"）。 6. Softmax预测输出 Softmax预测输出 编码器Decoder最后的部分是利用 Softmax 预测下一个单词，在Softmax之前，会经过Linear变换，将维度转换为词表的个数。 假设我们的词表只有6个单词，表示如下： 词表 因此，最后的输出可以表示如下： Softmax预测输出示例 总结 Transformer由于可并行、效果好等特点，如今已经成为机器翻译、特征抽取等任务的基础模块，目前ChatGPT特征抽取的模块用的就是Transformer，这对于后面理解ChatGPT的原理做了好的铺垫。 代码实现 绝密伏击：OPenAI ChatGPT（一）：Tensorflow实现Transformer 参考 初识CV：Transformer模型详解（图解最完整版） 数据汪：BERT大火却不懂Transformer？读这一篇就够了 The Illustrated Transformer 忆臻：搞懂Transformer结构，看这篇PyTorch实现就够了（上） The Annotated Transformer https:// arxiv.org/pdf/1706.0376 2.pdf 青空栀浅：图解Transformer Ph0en1x：Transformer结构及其应用详解--GPT、BERT、MT-DNN、GPT-2 大师兄：ChatGPT/InstructGPT详解 张俊林：ChatGPT会取代搜索引擎吗 张俊林：放弃幻想，全面拥抱Transformer：自然语言处理三大特征抽取器（CNN/RNN/TF）比较</p>
<h2>10. 【代码+公式+图解+参数详解】PyTorch的3D卷积nn.Conv3d ...</h2>
<ul>
<li>链接：https://blog.csdn.net/2503_92010587/article/details/148979778</li>
<li>来源：bing</li>
<li>摘要：2025年6月28日 · 文章目录 1. 核心概念：什么是三维卷积 (3D Convolution)？ <strong>通俗理解</strong> <strong>学术定义</strong> <strong>3D卷积是用来提取什么的？</strong> 2.<code>nn.Conv3d</code> 参数详解 (修正与扩充版) **2.1 初始化 …</li>
</ul>
<h3>正文（抓取，非 AI）</h3>
<p>本文章旨在提供对 PyTorch nn.Conv3d 模块的全面、深入且易于理解的介绍。 文章目录 1. 核心概念：什么是三维卷积 (3D Convolution)？ <strong>通俗理解</strong> <strong>学术定义</strong> <strong>3D卷积是用来提取什么的？</strong> 2.<code>nn.Conv3d</code> 参数详解 (修正与扩充版) <strong>2.1 初始化参数 (Constructor Parameters)</strong> 3. 输入与输出的Shape 4. 运算方式与公式总结 <strong>运算方式</strong> <strong>运算公式</strong> 5. 抽象应用场景概述 6.【代码】【shape示例数据流理解】 7.【可视化过程】【可视化3D卷积过程示例】 8.【代码】【工程运用示例】工程代码示例模板【以VideoClassification为例，参考套用代码】 <strong>从5D特征到2D分类的“三步走”策略详解</strong> 代码块的Markdown 1. 核心概念：什么是三维卷积 (3D Convolution)？ 通俗理解 想象一下，我们想让计算机“看懂”一段视频或者一个三维的医学影像（比如CT扫描）。 二维卷积 (Conv2D) 就像一个“图片扫描仪”。它用一个小窗口（卷积核）在单张图片上滑动，识别边缘、角点、纹理等 空间特征 。 三维卷积 (Conv3D) 则是一个“视频/立体扫描仪”。它的窗口不仅在二维平面上滑动，还能在 时间维度 （视频的连续帧）或 深度维度 （CT扫描的连续切片）上移动。这使得它不仅能像二维卷积那样识别每一帧/每一层的空间特征，还能捕捉这些特征随时间或深度的 变化规律 。 学术定义 三维卷积是对一个三维数据体（Volume）应用一个三维的卷积核（Kernel/Filter），通过在输入数据的三个维度（通常是深度、高度、宽度）上进行滑动窗口操作，计算卷积核与输入数据对应子区域的点积，从而生成一个三维的特征图（Feature Map）。这个过程旨在从三维数据中同时提取空间和时间（或深度）上的局部特征。 3D卷积是用来提取什么的？ 它主要用于提取 时空特征 (Spatio-temporal features) 或 三维空间特征 。 对于视频数据： 它可以识别动作。例如，一个 Conv3D 层可能学会识别“挥手”这个动作，因为它能同时看到手的形状（空间信息）以及手在连续几帧中的移动轨迹（时间信息）。而 Conv2D 只能识别每一帧中静止的手。 对于医学影像 (MRI, CT)： 它可以识别三维的病灶结构。例如，一个肿瘤在三维空间中可能呈现特定的球状或不规则形状， Conv3D 可以捕捉到这种贯穿多个切片的立体结构特征。 2. nn.Conv3d 参数详解 (修正与扩充版) 2.1 初始化参数 (Constructor Parameters) 为了清晰展示各参数对形状的影响，我们统一采用以下 “基准配置” 。在讲解某个特定参数时，未提及的参数均保持此处的基准值不变。 基准输入 Tensor Shape : (N=4, C_in=16, D=20, H=32, W=32) 基准卷积层固定参数 : out_channels = 32 kernel_size = 3 (即 (3, 3, 3) ) stride = 1 (即 (1, 1, 1) ) padding = 0 (即 (0, 0, 0) ) dilation = 1 (即 (1, 1, 1) ) groups = 1 基于此基准配置，我们可以计算出 基准输出 Shape ： D o u t = ⌊ 20 + 2 ⋅ 0 − 1 ⋅ ( 3 − 1 ) − 1 1 ⌋ + 1 = 18 D_{out} = \lfloor \frac{20 + 2 \cdot 0 - 1 \cdot (3 - 1) - 1}{1} \rfloor + 1 = 18 D o u t ​ = ⌊ 1 20 + 2 ⋅ 0 − 1 ⋅ ( 3 − 1 ) − 1 ​ ⌋ + 1 = 18 H o u t = ⌊ 32 + 2 ⋅ 0 − 1 ⋅ ( 3 − 1 ) − 1 1 ⌋ + 1 = 30 H_{out} = \lfloor \frac{32 + 2 \cdot 0 - 1 \cdot (3 - 1) - 1}{1} \rfloor + 1 = 30 H o u t ​ = ⌊ 1 32 + 2 ⋅ 0 − 1 ⋅ ( 3 − 1 ) − 1 ​ ⌋ + 1 = 30 W o u t = ⌊ 32 + 2 ⋅ 0 − 1 ⋅ ( 3 − 1 ) − 1 1 ⌋ + 1 = 30 W_{out} = \lfloor \frac{32 + 2 \cdot 0 - 1 \cdot (3 - 1) - 1}{1} \rfloor + 1 = 30 W o u t ​ = ⌊ 1 32 + 2 ⋅ 0 − 1 ⋅ ( 3 − 1 ) − 1 ​ ⌋ + 1 = 30 基准输出 Shape : (4, 32, 18, 30, 30) in_channels (int) 含义 ：输入数据体的“通道数”。必须与输入Tensor的通道维度( C_in )完全匹配。 Shape示例 : 输入 Shape : (4, 16, 20, 32, 32) in_channels 值 : 必须设置为 16 。如果设置为其他值（如 in_channels=3 ），则在执行时会因维度不匹配而报错。 对输出Shape的影响 : 这是正确计算的前提条件，不直接决定输出的 D , H , W D,H,W D , H , W 尺寸。 out_channels (int) 含义 ：卷积层输出的通道数，即使用的卷积核数量。此参数直接决定输出Tensor的通道维度( C_out )。 固定参数 : kernel_size=3 , stride=1 , padding=0 , dilation=1 输入 Shape : (4, 16, 20, 32, 32) Shape示例 (基于基准输入) : 示例1 (<strong><em>* out_channels=32 ) : -&gt; 输出 Shape : (4, 32, 18, 30, 30) (基准情况) 示例2 (</em></strong><em> out_channels=64 ) : -&gt; 输出 Shape : (4, 64, 18, 30, 30) (只有通道数改变) 示例3 (</em><strong><em> out_channels=128 ) : -&gt; 输出 Shape : (4, 128, 18, 30, 30) (只有通道数改变) kernel_size (int or tuple) 输入 Shape : (4, 16, 20, 32, 32) 含义 ：卷积核的尺寸 (D_k, H_k, W_k) 。直接影响输出的D,H,W尺寸。 固定参数 : out_channels=32 , stride=1 , padding=0 , dilation=1 Shape示例 (基于基准输入) : 示例1 (</em></strong><em> kernel_size=3 ) : D o u t = 18 D_{out}=18 D o u t ​ = 18 , H o u t = 30 H_{out}=30 H o u t ​ = 30 , W o u t = 30 W_{out}=30 W o u t ​ = 30 输出 Shape : (4, 32, 18, 30, 30) (基准情况) 示例2 (</em><strong><em> kernel_size=5 ) : D o u t = ⌊ 20 − ( 5 − 1 ) − 1 1 ⌋ + 1 = 16 D_{out} = \lfloor \frac{20 - (5-1) - 1}{1} \rfloor + 1 = 16 D o u t ​ = ⌊ 1 20 − ( 5 − 1 ) − 1 ​ ⌋ + 1 = 16 H o u t = ⌊ 32 − ( 5 − 1 ) − 1 1 ⌋ + 1 = 28 H_{out} = \lfloor \frac{32 - (5-1) - 1}{1} \rfloor + 1 = 28 H o u t ​ = ⌊ 1 32 − ( 5 − 1 ) − 1 ​ ⌋ + 1 = 28 W o u t = ⌊ 32 − ( 5 − 1 ) − 1 1 ⌋ + 1 = 28 W_{out} = \lfloor \frac{32 - (5-1) - 1}{1} \rfloor + 1 = 28 W o u t ​ = ⌊ 1 32 − ( 5 − 1 ) − 1 ​ ⌋ + 1 = 28 输出 Shape : (4, 32, 16, 28, 28) 示例3 (非立方体, kernel_size=(3, 5, 5) ) : D o u t = ⌊ 20 − ( 3 − 1 ) − 1 1 ⌋ + 1 = 18 D_{out} = \lfloor \frac{20 - (3-1) - 1}{1} \rfloor + 1 = 18 D o u t ​ = ⌊ 1 20 − ( 3 − 1 ) − 1 ​ ⌋ + 1 = 18 H o u t = ⌊ 32 − ( 5 − 1 ) − 1 1 ⌋ + 1 = 28 H_{out} = \lfloor \frac{32 - (5-1) - 1}{1} \rfloor + 1 = 28 H o u t ​ = ⌊ 1 32 − ( 5 − 1 ) − 1 ​ ⌋ + 1 = 28 W o u t = ⌊ 32 − ( 5 − 1 ) − 1 1 ⌋ + 1 = 28 W_{out} = \lfloor \frac{32 - (5-1) - 1}{1} \rfloor + 1 = 28 W o u t ​ = ⌊ 1 32 − ( 5 − 1 ) − 1 ​ ⌋ + 1 = 28 输出 Shape : (4, 32, 18, 28, 28) stride (int or tuple) 输入 Shape : (4, 16, 20, 32, 32) 含义 ：卷积核滑动的步长 (D_s, H_s, W_s) 。是改变输出尺寸最主要的方式之一，起到</em>* 下采样 </strong>作用。 固定参数 : out_channels=32 , kernel_size=3 , padding=0 , dilation=1 Shape示例 (基于基准输入) : 示例1 (<strong><em>* stride=1 ) : D o u t = 18 D_{out}=18 D o u t ​ = 18 , H o u t = 30 H_{out}=30 H o u t ​ = 30 , W o u t = 30 W_{out}=30 W o u t ​ = 30 输出 Shape : (4, 32, 18, 30, 30) (基准情况) 示例2 (</em></strong><em> stride=2 ) : D o u t = ⌊ 20 − ( 3 − 1 ) − 1 2 ⌋ + 1 = 9 D_{out} = \lfloor \frac{20 - (3-1) - 1}{2} \rfloor + 1 = 9 D o u t ​ = ⌊ 2 20 − ( 3 − 1 ) − 1 ​ ⌋ + 1 = 9 H o u t = ⌊ 32 − ( 3 − 1 ) − 1 2 ⌋ + 1 = 15 H_{out} = \lfloor \frac{32 - (3-1) - 1}{2} \rfloor + 1 = 15 H o u t ​ = ⌊ 2 32 − ( 3 − 1 ) − 1 ​ ⌋ + 1 = 15 W o u t = ⌊ 32 − ( 3 − 1 ) − 1 2 ⌋ + 1 = 15 W_{out} = \lfloor \frac{32 - (3-1) - 1}{2} \rfloor + 1 = 15 W o u t ​ = ⌊ 2 32 − ( 3 − 1 ) − 1 ​ ⌋ + 1 = 15 输出 Shape : (4, 32, 9, 15, 15) (D,H,W尺寸约减半) 示例3 (非均匀, stride=(1, 2, 2) ) : D o u t = ⌊ 20 − ( 3 − 1 ) − 1 1 ⌋ + 1 = 18 D_{out} = \lfloor \frac{20 - (3-1) - 1}{1} \rfloor + 1 = 18 D o u t ​ = ⌊ 1 20 − ( 3 − 1 ) − 1 ​ ⌋ + 1 = 18 H o u t = ⌊ 32 − ( 3 − 1 ) − 1 2 ⌋ + 1 = 15 H_{out} = \lfloor \frac{32 - (3-1) - 1}{2} \rfloor + 1 = 15 H o u t ​ = ⌊ 2 32 − ( 3 − 1 ) − 1 ​ ⌋ + 1 = 15 W o u t = ⌊ 32 − ( 3 − 1 ) − 1 2 ⌋ + 1 = 15 W_{out} = \lfloor \frac{32 - (3-1) - 1}{2} \rfloor + 1 = 15 W o u t ​ = ⌊ 2 32 − ( 3 − 1 ) − 1 ​ ⌋ + 1 = 15 输出 Shape : (4, 32, 18, 15, 15) (D尺寸不变, H,W尺寸减半) padding (int, tuple or str) 输入 Shape : (4, 16, 20, 32, 32) 含义 ：在输入数据体的边界周围进行填充。用于控制输出尺寸，常用于保持尺寸不变。 固定参数 : out_channels=32 , kernel_size=3 , stride=1 , dilation=1 Shape示例 (基于基准输入) : 示例1 (</em><strong><em> padding=0 ) : -&gt; 输出 Shape : (4, 32, 18, 30, 30) (基准情况) 示例2 (</em></strong><em> padding=1 ) : D o u t = ⌊ 20 + 2 ⋅ 1 − ( 3 − 1 ) − 1 1 ⌋ + 1 = 20 D_{out} = \lfloor \frac{20 + 2\cdot1 - (3-1) - 1}{1} \rfloor + 1 = 20 D o u t ​ = ⌊ 1 20 + 2 ⋅ 1 − ( 3 − 1 ) − 1 ​ ⌋ + 1 = 20 H o u t = ⌊ 32 + 2 ⋅ 1 − ( 3 − 1 ) − 1 1 ⌋ + 1 = 32 H_{out} = \lfloor \frac{32 + 2\cdot1 - (3-1) - 1}{1} \rfloor + 1 = 32 H o u t ​ = ⌊ 1 32 + 2 ⋅ 1 − ( 3 − 1 ) − 1 ​ ⌋ + 1 = 32 W o u t = ⌊ 32 + 2 ⋅ 1 − ( 3 − 1 ) − 1 1 ⌋ + 1 = 32 W_{out} = \lfloor \frac{32 + 2\cdot1 - (3-1) - 1}{1} \rfloor + 1 = 32 W o u t ​ = ⌊ 1 32 + 2 ⋅ 1 − ( 3 − 1 ) − 1 ​ ⌋ + 1 = 32 输出 Shape : (4, 32, 20, 32, 32) (D,H,W尺寸与输入完全相同) 示例3 (</em><strong><em> padding=2 ) : D o u t = ⌊ 20 + 2 ⋅ 2 − ( 3 − 1 ) − 1 1 ⌋ + 1 = 22 D_{out} = \lfloor \frac{20 + 2\cdot2 - (3-1) - 1}{1} \rfloor + 1 = 22 D o u t ​ = ⌊ 1 20 + 2 ⋅ 2 − ( 3 − 1 ) − 1 ​ ⌋ + 1 = 22 H o u t = ⌊ 32 + 2 ⋅ 2 − ( 3 − 1 ) − 1 1 ⌋ + 1 = 34 H_{out} = \lfloor \frac{32 + 2\cdot2 - (3-1) - 1}{1} \rfloor + 1 = 34 H o u t ​ = ⌊ 1 32 + 2 ⋅ 2 − ( 3 − 1 ) − 1 ​ ⌋ + 1 = 34 W o u t = ⌊ 32 + 2 ⋅ 2 − ( 3 − 1 ) − 1 1 ⌋ + 1 = 34 W_{out} = \lfloor \frac{32 + 2\cdot2 - (3-1) - 1}{1} \rfloor + 1 = 34 W o u t ​ = ⌊ 1 32 + 2 ⋅ 2 − ( 3 − 1 ) − 1 ​ ⌋ + 1 = 34 输出 Shape : (4, 32, 22, 34, 34) 示例4 (</em></strong>* padding='same' ) : 这是一个字符串参数，PyTorch会自动计算填充量以在 stride=1 时保持空间维度(H, W)不变。对于 kernel_size=3 ，这等同于设置 padding=(?, 1, 1) （D维度的padding需单独考虑或默认为0）。 输出 Shape : (4, 32, 18, 32, 32) (H, W与输入相同，D根据其维度上的参数计算) 示例5 (非对称填充, padding=(0, 1, 2) ) : D o u t = ⌊ 20 + 2 ⋅ 0 − ( 3 − 1 ) − 1 1 ⌋ + 1 = 18 D_{out} = \lfloor \frac{20 + 2\cdot0 - (3-1) - 1}{1} \rfloor + 1 = 18 D o u t ​ = ⌊ 1 20 + 2 ⋅ 0 − ( 3 − 1 ) − 1 ​ ⌋ + 1 = 18 H o u t = ⌊ 32 + 2 ⋅ 1 − ( 3 − 1 ) − 1 1 ⌋ + 1 = 32 H_{out} = \lfloor \frac{32 + 2\cdot1 - (3-1) - 1}{1} \rfloor + 1 = 32 H o u t ​ = ⌊ 1 32 + 2 ⋅ 1 − ( 3 − 1 ) − 1 ​ ⌋ + 1 = 32 W o u t = ⌊ 32 + 2 ⋅ 2 − ( 3 − 1 ) − 1 1 ⌋ + 1 = 34 W_{out} = \lfloor \frac{32 + 2\cdot2 - (3-1) - 1}{1} \rfloor + 1 = 34 W o u t ​ = ⌊ 1 32 + 2 ⋅ 2 − ( 3 − 1 ) − 1 ​ ⌋ + 1 = 34 输出 Shape : (4, 32, 18, 32, 34) (D维度因无填充而缩小，H维度因 padding=1 而保持不变，W维度因 padding=2 而增大) 常见问题：3D卷积padding=(1,2,3)是不是DHW分别扩张了2，4，6 是的 这是一个非常关键的细节。当您在 nn.Conv3d 中设置 padding=(p_D, p_H, p_W) 时，PyTorch 会将每个填充值应用到对应维度的 两侧 。 因此，对于 padding=</p>
<h2>11. 3D Convolutional Neural Network (3D CNN) — A Guide for …</h2>
<ul>
<li>链接：https://www.neuralconcept.com/post/3d-convolutional-neural-network-a-guide-for-engineers</li>
<li>来源：bing</li>
<li>摘要：4 天之前 · Discover how 3D convolutional neural networks (3D CNN) enable AI to learn 3D CAD shapes and transform product design in engineering.</li>
</ul>
<h3>正文（抓取，非 AI）</h3>
<p>3D Convolutional Neural Network (3D CNN) — A Guide for Engineers | Neural Concept Download Name Company name Company email Thank you! Our manager will contact You shortly. Homepage Oops! Something went wrong while submitting the form. Download Email Thank you! Download the Industry Briefing Oops! Something went wrong while submitting the form. This site uses cookies to optimize the user experience. Accept CES 2026 : Meet our leadership team from Jan 6 to 9 in Las Vegas to unlock Engineering impact beyond AI hype. Request a demo content Preliminary Case History: Better Competition with 3D Convolutional Neural Network Learning - in Humans and Artificial Agents What Are Classification and Regression? Deep Learning and 3D CNN Architecture What is the Goal of Deep Learning? Training the Neural Network The Building Brick: The Artificial Neuron How Does a 3D Convolutional Neural Network Work? What is a gCNN? What are Euclidean and Non-Euclidean Distances? Resources 3D Convolutional Neural Network (3D CNN) — A Guide for Engineers Blog content Preliminary Case History: Better Competition with 3D Convolutional Neural Network Learning - in Humans and Artificial Agents What Are Classification and Regression? Deep Learning and 3D CNN Architecture What is the Goal of Deep Learning? Training the Neural Network The Building Brick: The Artificial Neuron How Does a 3D Convolutional Neural Network Work? What is a gCNN? What are Euclidean and Non-Euclidean Distances? A 3D Convolutional Neural Network (3D CNN) is a deep learning architecture that extends the concept of pattern recognition from two dimensional data to three-dimensional inputs. Instead of processing data in height and width only (like 2D CNNs), 3D CNNs operate over height × width × depth (input volume), making them ideal for volumetric data (e.g., CT scans, CAD models, CFD simulations) or time-series of images (video). This added depth dimension allows the network to detect patterns and correlations across multiple slices or frames, enabling engineers to extract richer and more context-aware features for tasks such as simulation acceleration, defect detection, and complex product design. Artificial Intelligence (AI) and Deep Learning (DL) revolutionized how we approach engineering problems. The revolution took off in computer vision and speech recognition; now it is spreading to product design. One of the most exciting progresses in DL is the Convolutional Neural Network (ConvNet). Recent research has opened the way for implementing even a 3D Convolutional Neural Network. 3D CNNs are key enablers for the revolution in engineering; empowering product design engineers with high-end simulation capability. SO 3D CNNs are an extension of traditional Convolutional Neural Networks, designed to handle data with spatial dimensions including height, width, and depth. ‍ ‍ ‍ What about introducing a 3D convolutional neural network in your business? ‍ In this article, we will introduce the concept of 3D CNNs and explore its business relevance, particularly in fields like medical imaging . We will start with a case from the automotive industry. In this article you will learn: Preliminary Case History: Better Competition with 3D Convolutional Neural Network How to Support Product Design? Can My Company Use AI to Predict Designs? Why 3D Convolutional Neural Networks Matter for Engineers Learning - in Humans and Artificial Agents Training Humans Introduction to Deep Learning Training Technical Aspects of DL Training Predicting with DL Prediction &amp; Regression Speed What Are Classification and Regression? Practical Examples: Cats Vs Dogs &amp; House Prices Classification and Regression in ML Deep Learning and 3D CNN Architecture How Neural Networks Learn aaaa The Goal of Deep Learning 3D CNNs: Classification 3D CNNs: Regression Training the Neural Network Training: Loss Fuctions Training: Loss Gradient The Building Brick: The Artificial Neuron Model for the Single Physical Neuron The Artificial Neuron Building Blocks of the Artificial Neuron Core Operation in the Artificial Neuron From a Single Neuron to a Neural Network Neural Network Training Neural Network Training Data Loss Functions How Does a 3D Convolutional Neural Network Work? Layers &amp; Feature Maps Max-Pooling Layers Fully Connected Layers Convolution in Detail The Mathematics of Convolution Application to Image Processing Advantages of Convolution What is a gCNN? Advantages - gCNN More on gCNN What are Euclidean and Non-Euclidean Distances? Euclidean Distance Non-Euclidean Distance Cat Example 3D Convolutional Neural Network Applications Medical Imaging Video Classification Autonomous Driving and Image Classification Geoscience Material Science 3D CAD Models We will explain basic concepts of this technology for readers without an AI background, progressing to advanced applications, including training and validation sets, as well as model building . The goal is to understand how neural networks enable AI to “learn” 3D CAD shapes. The learning process aims to create a predictive model that forecasts engineering behavior, like a car’s aerodynamics, using related data. By the end, engineers will grasp this technology’s basics and be equipped to influence their organization positively in product design. Preliminary Case History: Better Competition with 3D Convolutional Neural Network This case involves Tier 1 suppliers providing automotive parts to OEMs, based on the author’s experience as an engineering director across four lines: passenger cars, luxury, trucks, and aftermarket. The process begins with an OEM’s RFQ to multiple suppliers, competing from design to manufacturing. Recently, OEMs and suppliers shifted from mainly using product simulation to fix issues to comparing design options and exploring solutions. Two major challenges with traditional, high-fidelity simulation (CAE) are still in 2025: how to make simulation technology accessible to all engineers without extensive infrastructure and training how can we enable all engineers to interactively explore several design options ? An early engineering predictive capability accessible to R&amp;D, product design, and sales teams could give sales engineers a competitive edge in RFQs and customer relationships. Next, we review tools that provide evidence that products are innovative, cost-effective, and meet performance targets. ‍ How to Support Product Design The most effective way to support product design is to combine engineering expertise with intelligent, data-driven tools capable of tasks such as image segmentation to predict performance and verify compliance before a single part is manufactured. How can we demonstrate that a product complies with targets and constraints before manufacturing? There are three possible approaches: The first strategy is copying previous product designs, but this isn’t sustainable long-term due to issues like win-lose dynamics, mistrust in performance, and poor regulatory adaptation. The second involves costly, time-consuming testing, prototyping, and CAE simulations, requiring significant resources and investment amid fierce competition. The third, most innovative approach, uses DL to recycle solutions into predictive models. This enables organizations to demonstrate product capabilities quickly with real-time, deployable data-driven neural network models. Once a real-time solution is available, it is immediate to consider the next step of implementing it in an iterative design process , thus reaching your product’s desired objectives faster, such as reduced weight, better heat dissipation, and better energy efficiency. The iterative design process can be automated and operated by humans or embedded in a generative design approach. ‍ Can My Company Use AI to Predict Designs? Entering the world of AI-driven simulation for product design is not as complicated as it may seem. The two key ingredients you need to get started are a business case and data: A business case is simply the need</p>
<h2>12. 图神经网络：方法与应用综述</h2>
<ul>
<li>链接：https://www.zhihu.com/tardis/bd/art/413648055</li>
<li>来源：bing</li>
<li>摘要：2023年2月27日 · 由于它的卓越的表现，最近GNN在图分析方法中的应用越来越广泛。 在下面的段落中，我们将说明图神经网络的原始灵感。 GNN的第一个灵感源于悠久的历史，第一次尝试将神经网络 …</li>
</ul>
<h3>正文（抓取，非 AI）</h3>
<p>摘要 图数据（graph data）中包含了元素之间丰富的信息，很多任务都是需要处理图数据。比如：对物理系统建模、学习分子指纹、预测蛋白质界面、对疾病分类都是以图数据作为输入。在其他领域比如：从文本和图像这些非结构化数据中学习、对抽取出来的结构（比如句子的依赖树和图像的场景图）进行推理都是非常重要的研究课题，都需要用到图推理模型（graph reasoning models）。图神经网络（Graph neural networks (GNNs)）是通过图中结点的信息传递（message passing）来捕获图的依赖关系的神经网络模型。近年来，许多GNN的变体，例如：图卷积神经网络（graph convolutional network (GCN)）、图注意力网络（graph attention network (GAT)）、图循环网络（graph recurrent network (GRN)）在很多深度学习任务上取得了突破性的成绩。本研究中，我们总结了GNN模型的通用设计流程（general design pipeline）并讨论每一个GNN变体，还系统地对应用进行了分类，最后针对未来的研究方向提出了四个问题。 1.Introduction 图（Graphs）是一种对物体（objects）和他们之间的关系（relationships）建模的数据结构，物体以结点（nodes）表示，关系以边（edges）表示。最近，使用机器学习（machine learning）对图进行分析的研究受到越来越多的关注。主要是由于图具有强大的表示能力，即，图可以用来分析超大规模跨领域的问题，比如社会科学（社会网络（social networks））、自然科学（物理系统（physical systems）、蛋白质相互作用）、知识图谱等。图分析（graph analysis）作为一种机器学习中独特的非欧几里得几何学的数据结构，主要的研究任务是节点分类（node classifification）、连接预测（link prediction）和聚类（clustering）。GNN是应用在图领域的深度学习方法。由于它的卓越的表现，最近GNN在图分析方法中的应用越来越广泛。在下面的段落中，我们将说明图神经网络的原始灵感。 GNN的第一个灵感源于悠久的历史，第一次尝试将神经网络应用在图上。在90年代，RNN被首次应用在有向无环图上（1997）。后来又引入了前馈神经网络来处理环路（2009）。这些方法虽然取得了成功，但其普遍思想是在图上构建状态转换系统（state transition systems），并迭代直到收敛，这限制了可扩展性（extendability）和表示能力（representation）。深度学习的最新进展（尤其是LeCun的CNN，1998）让我们重新审视GNN。CNN具有提取多尺度局部空间特征（ multi-scale localized spatial features）并将其组合构建出具有良好表达的能力，这使得几乎所有机器学习领域都取得了突破，开启了深度学习的新时代。CNN的核心在于局部连接（local connection）、权值共享（shared weights）和多层（multiple layers）。这也是解决图问题的关键。然而，CNN只能处理常规的欧几里得几何学数据，比如：图像（2D grids）和文本（1D sequences），这些都可以视为一种图。因此，可以直接将CNN广义化成图。 如图1所示，我们很难将欧式（Euclidean）空间转到非欧式空间（non-Euclidean），因为不容易定义卷积核和池化核。将深度神经模型扩展到非欧几里得领域，即几何深度学习（geometric deep learning），已经成为一个新兴的研究领域（2017）。在这个术语下，图的深度学习受到了极大的关注。 另一个灵感来自图表示学习（graph representation learning），它学习用低维向量表示图节点、边或子图。在图分析领域，传统的机器学习方法通常依靠人工提取特征，不仅灵活性差而且代价高。2013年Word2vec在表示学习中取得了成功，借鉴它的经验，DeepWalk于2014年被研发出来，并被视为第一个图嵌入方法（graph embedding）。DeepWalk是在随机游走数据上应用的SkipGram算法建模的。相似的算法例如node2vec（2016）、LINE（2015）和TADW（2015）也取得了突破。然而，这些方法有两个严重的缺点。首先，编码器中节点之间不共享参数，导致计算效率低下，因为这意味着参数的数量随着节点的数量线性增长。其次，直接嵌入方法缺乏泛化能力，不能处理动态图，也不能泛化到新图。 基于CNN和图嵌入，许多GNN的变体被提出，来共同聚合来自图结构中的信息。因此，它们能对输入 和/或 输出进行建模（对结点建模、对边建模）。 关于图神经网络，已有一些综述。但是，他们主要关注在图上定义的卷积运算（convolution operators），而我们研究了GNN中的其他计算模块，如跳过连接（skip connections ）和池操作（pooling operators）。 也有一些研究对GNN做了一些分类。我们的论文提供了一个不同的分类，我们主要集中在经典的GNN模型。另外，我们总结了不同类型的图的变体，并提供了详细的在不同应用领域的GNN的应用。 还有一些研究致力于一些特殊的图学习领域。我们总结了异构图（heterogeneous graphs）、动态图（dynamic graphs）和组合优化（combinatorial optimization）的GNN。 在本文中，我们提供了一个全面的对不同的图神经网络模型的概述，以及一个非常系统的图应用的分类。总而言之，我们的贡献是： 我们对现有的图神经网络模型进行了详细的回顾。我们提出了一个通用的设计流程，并讨论了每个模块的变体。本文还介绍了GNN模型的理论研究和实证分析。 我们系统地将应用进行分类，分成了结构化（structural）场景和非结构化（non-structural）场景。我们展示了几个主要的应用和他们在不同场景下对应的方法。 我们提出了未来研究的四个开放问题。我们对每个问题提供了全面的分析并提出了未来的研究方向。 本研究的剩余部分的组织如下。第2节，展示了通用GNN的设计流程（general GNN design pipeline）。顺着这个设计流程，第3~6节，我们细致地讨论GNN变体的每个步骤。第7节，我们重新审视了GNN的理论和实践研究现状。第8节我们引入了几个重要的GNN应用，包括结构化场景、非结构化场景和其他场景。第9节我们提出了4个关于GNN的开放问题和未来研究方向。第10节是我们对本调查的总结。 2. General design pipeline of GNNs 本文，我们从一个设计师的视角介绍GNN模型。本节，我们首先展示了通用GNN设计流程。在随后的3、4、5节我们分别对每一步骤给出细致的解释，例如选择计算模块（selecting computational modules）、考虑图类型和规模（considering graph type and scale）和设计损失函数（designing loss function）。最后在第6节使用一个例子来展示GNN对具体任务的设计过程。 在之后的节中，我们把图记为G=(V,E)，其中|V|=N表示图中的节点数， 表示边的条数。 是邻接矩阵（adjacency matrix）。对于图表示学习（graph representation learning），我们使用向量 和向量 表示结点v的隐藏状态（hidden state）和输出向量（output vector）。更多细节的符号见图表1. 本节，我们在具体的图类型（graph type）和具体的任务（specifific task）上介绍通用GNN模型的设计流程。概括来说，流程分为四步：（1）找到图结构（fifind graph structure）、（2）具体化图类型和规模（specify graph type and scale）、（3）设计损失函数（design loss function）和（4）使用计算模块构建模型（build model using computational modules）。本节会给出通用的设计流程和一些背景知识。我们来看每一步的细节： 2.1. Find graph structure 首先，我们要弄清楚应用中的图结构。通常有两种场景：结构化场景（structural scenarios）和非结构化场景（non-structural scenarios）。在结构化场景中，在应用中的图结构是明确的，比如在分子化学、物理系统和知识图谱等方面。在非结构化场景中，图是不明确的，因此我们首先要根据任务构建图（build the graph from the task），比如在文本领域构建一个全连接的“word”图或根据图像构建一个情景图（scene graph）。当我们确定了图结构，之后的设计就是在这个图上对GNN进行优化。 2.2. Specify graph type and scale 当我们确定了应用中的图结构之后，我们就要弄清楚图的类型（graph type）和他的规模（scale）。 复杂类型的图可以在节点（nodes）和连接（connections）上携带更多的信息。通常图可以有如下分类： 有向/无向图 （Directed/Undirected Graphs）。有向图里的边（Edges）是从一个节点指向另一个节点，比无向图携带更多的信息。无向图中的边也可以视为双向的边。 同构/异构图 （Homogeneous/Heterogeneous Graphs）。同构图中的节点和边拥有相同的类型；异构图中节点和边是不同类型的。节点和边的类型在异构图中起着重要的作用，需要进一步考虑。 静态/动态图 （Static/Dynamic Graphs）。如果输入特征或图的拓扑结构随着时间变化，就是动态图。在动态图中应该仔细考虑时间信息。 注意，这些类型是正交的（orthogonal），意味着这些类型可以组合，比如你可以采用动态有向异构图（ dynamic directed heterogeneous graph）。还有一些为不同任务设计的其他类型的图结构，比如超图（hypergraphs）和符号图（signed graphs）。虽然我们这里不会枚举出所有的类型，但是主要这些图的携带的最主要的信息还是会兼顾的。一旦我们确定了图类型，图类型背后的附加信息会在未来的设计过程进行考虑。 至于图的规模，小图和大图没有一个明确的划分标准。划分标准仍然随着计算设备的升级而改变（比如GPU的运行速度）。本文中，当邻接矩阵（adjacency matrix）或图的拉普拉斯图（graph Laplacian of a graph）（空间复杂度是O(n^2)）在设备上无法存储和处理时，我们就认为该图是大图（large-scale graph），并研究着使用一些采样方法（sampling methods）。 2.3. Design loss function 在这一步我们基于任务类型（task type）和训练配置（training setting）设计损失函数（ loss function）。 对于图学习任务，通常有三类任务： 节点级别 （Node-level ）的任务重心在节点上，包括节点分类（node classifification）、节点回归（node regression）和节点聚类（node clustering）等等。节点分类试图把节点分为几个特定的类别；节点回归试图为节点预测一个连续值（continuous value）。节点聚类试图把节点划分为几个不相交的组，组内的节点应该相似。 边级别 （Edge-level）的任务是边的分类（edge classifification）和链接预测（link prediction），需要模型对边的类型分类 或者 预测两个节点之间是否有边。 图级别 （Graph-level）的任务包括图分类（graph classifification）、图回归（graph regression）和图匹配（graph matching），所有的技术都需要学习图的表示。 从监督的视角，我们能将图学习任务分为三个不同的训练配置： 监督配置 （Supervised setting）需要提供带标签的训练数据（labeled data）。 半监督配置 （Semi-supervised setting）给出少量的带标签训练数据和大量的无标签的训练数据。在测试阶段， 直推式配置 （transductive setting）需要模型预测那些之前给定的无标签节点，而 归纳式配置 （inductive setting）将会从同分布的数据中抽取新的无标签的节点进行预测。大多数节点和边的分类任务是半监督的。 无监督配置 （Unsupervised setting）仅提供无标签数据，让模型挖掘数据模式。节点聚类是典型的无监督学习任务。 有了任务类型和训练配置，我们就能为任务设计一个具体的损失函数。例如，对于一个节点级别的半监督分类任务，交叉熵损失（cross-entropy loss）可以应用在训练集的有标签的节点上。 2.4. Build model using computational modules 最后，我们可以使用计算模块来构建模型。一些常用的计算模块有： 传播模块 （Propagation Module）。传播模块可以让节点之间传递信息，聚合后的信息可以捕获特征和拓扑信息。在传播模块， 卷积运算 （convolution operator）和 递归运算 （recurrent operator）通常被用来聚合邻居的信息；而 跳跃连接操作 （skip connection operation）用来从历史的节点表示中收集信息，并缓解过渡平滑问题（over-smoothing problem）。过渡平滑问题可以参考： 图神经网络（GCN）中的过度平滑（over-smooth）问题以及 multi-hops解决思路 采样模块 （Sampling Module）。当图非常大的时候，采样模块通常需要被使用，采样模块通常和传播模块一起使用。 池化模块 （Pooling Module）。当我们需要高层级的子图或图的表示的时候，池化模块会被用来从节点中提取信息。 典型的GNN模型通常是上述计算模块的组合。典型的GNN模型结构如图2的中间部分所示，卷积运算、池化操作、采样模块和跳跃连接被使用在层之间传递信息，然后加一个池化层抽取高层次的信息。这些层通常被堆叠（stack）以获得更好的表示。注意，这个架构可以广义化大部分GNN模型，当然也有一些例外，比如NDCN。 GNN的通用设计流程如图2所示。第3节，我们首先给出现有的计算模型实例。第4节，考虑图类型和规模介绍心有的GNN变体。第5节，调研不同变体的训练配置。这些部分分别对应图2中的步骤4、步骤2和步骤3。第6节，我们给出一个具体的设计实例。 3. Instantiations of computational modules 该节中，我们主要介绍现有的三种计算模块（传播模块、采样模块和池化模块）的实例。在3.1、3.2、3.3节我们分别介绍传播模块的三个子组件（sub-components），即卷积运算、递归运算和跳跃连接。然后3.4、3.5节介绍采样模块和池化模块。计算模块的概览如图3所示： 3.1. Propagation modules - convolution operator 本节介绍了GNN模型中通常使用的卷积运算。卷积运算的主要思想是将其他领域的卷积广义化到图领域。这个方向的进展通常分为两个方向：基于谱分解的方法（ spectral approaches）和基于空间结构的方法（spatial approaches） ◆ 3.1.1. Spectral approaches 谱方法使用图的谱表示（spectral representation）。这些方法的理论基础是图信号处理（graph signal processing），并在谱域（spectral domain）定义卷积运算。 在谱方法中，一个图信号（graph signal） x 首先通过图傅里叶变换 （graph Fourier transform）转换到谱域，然后进行卷积运算。卷积之后的结果信号可以使用图傅里叶逆变换 （inverse graph Fourier transform）转换回来。这些操作定义为： 其中矩阵 U 是矩阵 L 的特征向量矩阵， 是归一化的图拉普拉斯矩阵（normalized graph Laplacian），（ D 是度矩阵（degree matrix）， A 是图的邻接矩阵）。归一化的图拉普拉斯矩阵是实对称半正定的，因此他可以分解为 （ 是特征值的对角矩阵）。基于卷积的理论，卷积操作的定义如下： 其中 是频域的过滤器（filter）。如果我们使用可学习的对角矩阵 简化过滤器，那么就有了在谱域的基本方程： 接下来我们介绍几个典型的谱域方法，他们就是设计了不同的过滤器 。 ----Spectral Network Spectral Network（2014）使用了一个可学习的对角矩阵作为过滤器， ，其中参数 。然而，这个运算是低效的，并且过滤器是非本地结构化的（non-spatially localized） ----ChebNet ChebNet（Hammond et al. (2011)）认为 可以用切比雪夫多项式的K阶截断展开式来近似。Defferrard et al. (2016)基于这个理论提出了ChebNet。他的操作可以写成： ----GCN Kipf and Welling (2017)简化了公式4中的卷积操作，让K=1避免了过拟合问题。 ----AGCN 自适应图卷积网络(AGCN)被提出用来学习潜在的关系(Li et al.， 2018a)。AGCN学习一个残差图拉普拉斯矩阵，并将其添加到原始拉普拉斯矩阵中。结果表明，该方法在多个图结构数据集中是有效的。 ----DGCN 提出了对偶图卷积网络(DGCN) (Zhuang and Ma, 2018)，共同考虑图的局部一致性和全局一致性。它使用两个卷积网络来捕获局部一致性和全局一致性，并采用无监督损失来集成它们。第一个卷积网络与Eq.(7)相同，第二个网络用正点向互信息(PPMI)矩阵代替邻接矩阵: ----GWNN 图小波神经网络(GWNN) (Xu et al.， 2019a)采用图小波变换代替图傅立叶变换。该方法具有以下优点:(1)无需分解矩阵即可快速得到图小波;(2)图小波是稀疏和局部化的，因此结果更好，更可解释。在半监督节点分类任务上，GWNN优于多种谱方法。 AGCN和DGCN试图从图拉普拉斯增广的角度对谱方法进行改进，而GWNN则取代了傅里叶变换。总之，光谱</p>
<h2>13. 挑战 Transformer：全新架构 Mamba 详解</h2>
<ul>
<li>链接：https://www.zhihu.com/tardis/zm/art/684231320</li>
<li>来源：bing</li>
<li>摘要：2025年9月23日 · 而就在最近，一名为 Mamba 的架构似乎打破了这一局面。 与类似规模的 Transformer 相比， Mamba 具有 5 倍的吞吐量， 而且 Mamba-3B 的效果与两倍于其规模的 Transformer 相当。 …</li>
</ul>
<h3>正文（抓取，非 AI）</h3>
<p>打个小广告 ☻，知乎专栏 《大模型前沿应用》 的内容已经收录在新书 《揭秘大模型：从原理到实战》 中。感兴趣的朋友可以购买，多谢支持！♥♥ 背景 屹立不倒的 Transformer 迎来了一个强劲竞争者。 自 2017 年被提出以来，Transformer 已经成为 AI 大模型的主流架构，但随着模型规模的扩展和需要处理的序列不断变长，Transformer 的局限性也逐渐凸显。一个很明显的缺陷是：Transformer 模型中自注意力机制的计算量会随着上下文长度的增加呈平方级增长，比如上下文增加 32 倍时，计算量可能会增长 1000 倍，计算效率非常低。 为了克服这些缺陷，研究者们开发出了很多注意力机制的高效变体，但这往往以牺牲其有效性特为代价。到目前为止，这些变体都还没有被证明能在不同领域发挥有效作用。 而就在最近，一名为 Mamba 的架构似乎打破了这一局面。 与类似规模的 Transformer 相比， Mamba 具有 5 倍的吞吐量 ， 而且 Mamba-3B 的效果与两倍于其规模的 Transformer 相当 。性能高、效果好，Mamba 成为新的研究热点。 图1 Mamba 在推理过程中的吞吐量对比 本文将详细的解读 Mamba 架构，由于 Mamba 是基于 SSM-&gt;HiPPO-&gt;S4-&gt;Mamba 演化过来的，而 HiPPO、S4、Mamba 的一作者都是卡内基梅隆大学机器学习系助理教授 Albert Gu 。因此，本文将从标准 SSM 开始，逐步介绍 HiPPO、S4、Mamba。 图2总结了SSM、HiPPO、S4、Mamba的主要区别，以及各个模型的主要内容。本文内容也将按图中内容展开。 图2-2：HiPPO、S4、Mamba 一、现有架构问题 序列建模的核心问题是：同时解决 有效 和 高效 。有效是指能够选择性记忆历史信息，解决 长距离依赖 （Long-Range Dependencies，LRDs）问题；高效是指计算高效。 尽管传统的模型如循环神经网络（RNNs）、卷积神经网络（CNNs）和 Transformers 在处理长距离依赖方面有专门的变体，但它们在 处理超过 10000 步的极长序列时仍然面临挑战 。 1.1 Transformer 问题 Transformer 的一个主要优点是，无论它接收到多长的输入，它都使用序列中的所有 token 信息（无论序列有多长）来对输入数据进行处理。 图1-1：Transformer会查看过去所有 token 但是为了获得全局信息，注意力机制在长序列上非常耗费显存。注意力创建一个矩阵，将每个 token 与之前的每个 token 进行比较。矩阵中的权重由 token 对之间的相关性决定。 图1-2：Transformer 会计算每个 token 之间的 Attention 在训练过程中，Attention 计算可以并行化，所以可以极大地加快训练速度。但是在推理过程中，当生成下一个 token 时，我们需要重新计算整个序列的注意力。 图1-3：生成新 token 时需要重新计算整个序列的注意力 长度为 L 的序列生成 token 大约需要 L² 的计算量，如果序列长度增加，计算量会平方级增长。因此， 需要重新计算整个序列是 Transformer 体系结构的主要瓶颈 。 图1-4：Transformer 训练快、推理慢 1.2 RNN 的问题 图1-5：循环神经网络 RNN 在生成输出时，RNN 只需要考虑之前的隐藏状态和当前的输入。这样不会重新计算以前的隐藏状态，这正Transformer 不具备的。 这种结构可以让 RNN 进行 快速推理 ，并且理论上可以无限扩展上下文长度，因为每次推理只取一个隐藏状态和当前输入，内存占用非常稳定。 RNN 的每个隐藏状态都是之前所有隐藏状态的聚合。但是这里会有一个问题，在生成 token "Liang" 时，最后一个隐藏状态不再包含关于 token "Hello" 的信息。这会导致随着时间的推移，RNN 会忘记更久的信息，因为它只考虑前一个状态。 图1-6：只考虑前一个 hidden state 并且 RNN 的这种顺序性产生了另一个问题。 训练不能并行进行 ，因为它需要按顺序完成每一步。 图1-7：RNN 训练不能并行 RNN的统一定义为： 其中 是每一步的输出，它由当前输入 和前一时刻输出 共同决定，而θ则是可训练参数。那么参数θ的梯度可以表示为： 可以看到，当前梯度依赖上个 token 的梯度。 与 Transformer 相比，RNN 的问题完全相反！它的 推理速度非常快，但不能并行化导致训练很慢 。 图1-8：RNN 和 Transformer对比 人们一直在寻找一种既能像 Transformer 那样并行化训练，能够记住先前的信息，又能在推理时时间是随序列长度线性增长的模型，Mamba 就是这样应运而生的 。解下来我们从 SSM 开始，逐步介绍 Mamaba。 二、状态空间模型 SSM 2.1 什么是 SSM 状态空间模型（State Space Models，SSM）由简单的方程（3）定义。它将一维输入信号 映射到 N 维潜在状态 ，然后再投影到一维输出信号 。 其中， 是状态转移矩阵， 是输入到状态的矩阵， 是状态到输出的矩阵，D是直接从输入到输出的参数（很多时候取 D = 0）。 2.2 SSM 架构 下图是 SSM 的架构，主要包含两个部分：状态更新方程和输出方程。 图2-1：SSM结构 SSM 可以简化为以下结构： 图2-2：简化的SSM结构 下面我们看一下更详细的结构，首先是状态更新，如下所示： 图2-3：状态更新详细结构 备注 ：图中的输入 ，表示输入的信号是 D 维的。 SSM 也可以用于处理多维信号输入。 然后是输出方程，详细机构如下所示： 图2-4：输出方程详细结构 2.3 SSM 例子：弹簧振子 下面举一个描述弹簧振子系统的 SSM 例子。 图2-5：弹簧振子 考虑一个质量为 的物体，它连接在一个劲度系数为 的弹簧上，并且受到阻尼系数为 的阻尼力作用。当物体从平衡位置偏离时，它会在弹簧力的作用下进行振动。我们可以用状态空间模型来描述这个系统的动态。 状态变量可以选择为物体的位移 和速度 。输入 在这个例子中可以为零，因为我们没有外部力作用在物体上。输出 可以是我们感兴趣的位移 。 状态向量定义为： 输入向量为： 输出位移 。弹簧振子的状态空间方程可以表示为： 在了解 SSM 基本概念之后，接下来我们介绍基于 SSM 的 HiPPO 架构。 三、HiPPO（High-order Polynomial Projection Operators） HiPPO 是 Albert Gu 于2020年在论文 HiPPO: Recurrent Memory with Optimal Polynomial Projections 中提出的新架构。HiPPO 主要为了解决 如何在有限的存储空间中有效地解决序列建模的长距离依赖问题。 HiPPO 通过函数逼近产生状态矩阵 A 的最优解，有效的解决了长距离依赖问题。 问题背景： 在处理序列数据时，一个核心问题是如何在增量方式下表示累积的历史信息。这涉及到如何在有限的存储空间中有效地更新和维护历史数据的表示。 HiPPO框架 ：作者介绍了一个名为 HiPPO（High-order Polynomial Projection Operators）的通用框架，它通过将连续信号和离散时间序列投影到多项式基上，实现了在线数据压缩。 重要性度量 ：HiPPO 框架考虑了一个度量，用于指定过去每个时间步的重要性。这个度量帮助HiPPO产生在线函数逼近问题的最优解。 理论贡献 ：HiPPO 框架不仅提供了对现有记忆单元的简短推导，还推广了循环神经网络（如GRUs）中普遍存在的门控机制。 新的记忆更新机制 ：作者提出了一个新的记忆更新机制（HiPPO-LegS），它能够随时间扩展以记住所有历史信息，避免了对时间尺度的先验假设。 理论优势 ：HiPPO-LegS 具有时间尺度鲁棒性、快速更新和有界梯度的理论优势。 实验结果 ：在基准测试中，HiPPO-LegS 在打乱的 MNIST 数据集上达到了98.3%的新最佳准确率。在一个新的轨迹分类任务中，HiPPO-LegS 在处理分布外时间尺度和缺失数据方面，比其他 RNN 和神经 ODE（一阶常微分方程）基线模型的性能提高了25-40%的准确率。 下面介绍 HiPPO 实现的具体细节。 3.1 HiPPO 架构：高阶多项式投影 3.1.1 HiPPO问题设置 问题定义 给定一个在时间 上的输入函数 ，需要在每个时间点操作累计历史 ，以便理解到目前为止看到的输入并对未来进行预测。 由于函数空间的庞大，无法完美记住整个历史，因此需要将其进行压缩，HiPPO 提出了将历史投影到有界维数的子空间的一半方法。 函数逼近与度量 为了评估逼近的质量，需要在函数空间中定义一个距离。任何在 上的概率度量 都可以为平方可积函数空间提供内积 ，从而诱导出一个希尔伯特空间 和相应的范数 。 为了选择合适的子空间，需要一个度量来量化历史的重要性。这个度量 随时间变化，支持在 上，因为 只在时间t之前定义。 多项式基展开 任何 N 维的函数子空间 G 都是逼近的合适候选。参数 N 对应于逼近的阶数，或者说压缩的大小；投影的历史可以通过G的任何基的N个系数来表示。 论文中使用多项式作为自然基，因此G是小于N阶的多项式的集合。 在线逼近 由于我们关心在每个时间 t 对 的逼近，我们也让度量 随时间变化。总体上，我们寻找一个 ，使得 最小。直观上，度量 控制输入域各部分的重要性。 挑战 挑战在于如何在给定度量 的情况下以封闭形式解决优化问题，以及在 时如何线性地维护这些系数。 3.1.2 HiPPO 通用架构 通过连续动态系统计算投影 这部分是 HiPPO 的关键步骤，它涉及到将输入函数 在时间 t 投影到一个多项式空间上，以便在线更新记忆表示。 投影的表示 ：投影可以通过输入函数 在时间 t 的限制 的 N 个系数来表示。这些系数是通过在多项式空间的基上展开 得到的。 正交多项式基 ：为了选择合适的基，作者利用了正交多项式的性质。正交多项式为 提供了一个自然的基，使得 的投影可以表示为这些基的线性组合。 系数的计算 ：投影的系数 是通过内积 计算得到的，其中 是正交多项式基的元素。 连续动态系统 ：为了在线更新这些系数，作者提出了一个连续动态系统，这个系统描述了系数 是如何随时间 t 变化的。这种动态系统可以表示为 ，其中 是依赖于时间的矩阵。 投影操作符 ：作者定义了一个投影操作符 ，它将 映射到 （多项式空间）中的 ，使得 最小化。这个操作符是 HiPPO 框架的核心。 系数提取操作符 ：除了投影操作符，作者还定义了一个系数提取操作符 ，它将多项式 映射到其对应的系数 。 在线更新 ：通过这个连续动态系统，HiPPO 框架能够在线更新记忆表示，即随着新数据的到来，系统能实时地调整系数 。 在线函数逼近 图3-1：HiPPO框架 图2-6展示了 HiPPO 框架，首先需要找到投影 ，将输入 投影到多项式空间；然后将投影通过一组系数 来表示，这些系数捕捉了函数 的历史信息；使用连续时间下的一阶常微分方程来表示系数 如何随时间 t 动态变化；最后，将连续时间的动态变换转化为离散时间的递归关系（比如双线性变换），这允许 HiPPO 在每个时间步 k 更新系数 。 3.1.3 高阶投影：度量方法以及 HiPPO 动态系统 作者定义了两种度量方法，分别是 LegT 和 LagT。LegT 度量为最近的历史信息分配均匀的权重，表示如下： LagT 度量使用指数衰减的方式来衡量历史信息的重要性，表示如下： 对于 LegT 和 LagT，系数 可以使用 ODE（一阶常微分方程）来表示： 其中 A 和 B 是与度量 相关的矩阵。这个 ODE 描述了系数 如何随时间 t 和输入函数 变化。 备注：公式（9）是 HiPPO 框架的关键部分， 具体推导可以参看论文中的附录 D 。 对于 LegT 度量，矩阵 A 和 矩阵 B 可以表示如下： 对于 LagT 度量，可以表示如下： 3.1.4 HiPPO 框架中的连续时间动态转换为离散时间递归关系 由于我们处理的输入往往是离散的，因此我们需要将公式（9）的 ODE 离散化。ODE 离散化是一种常用的数据技术，它将 连续时间的常微分方程转换为离散时间的差分方程 。这通常涉及到选择一个合适的时间步长（或步长Δt），并使用数值方法（如欧拉方法、双线性）来近似连续微分。 图3-2：连续信号离散化 使用双线性离散化，如下所示： 结合公式（9）和公式（11），我们可以得到离散化的状态更新公式，表示如下： 离散化之后的 SSM 结构可以表示如下： 图3-3：离散化 SSM 在每个时间步长，我们计算当前输入( )如何影响前一个状态( )，然后计算预测输出( )。 图3-4：每个时间步的计算 这种表示看起来是不是有点熟悉？其实他的处理方法和RNN一样。 图3-5：离散化后和RNN类似 3.2 HiPPO-LegS HiPPO-LegS 是作者基于新的度量提出的全新架构，具有时间鲁棒性、有界梯度、有界近似误差、长时间记忆等效果。 新的度量表示为 ，在新的度量下，矩阵 A 和矩阵 B 可以表示如下： 具体推导在论文的附录 D.3 部分 。 更好的学习长期依赖 HiPPO-LegS 是专门为记忆而设计的 ，它通过其独特的结构和更新机制来避免梯度消失问题。LegS 通过使用Legendre 多项式作为基函数，并结合时间尺度不变的度量，来保持梯度的稳定性。 对于任何时间 ，HiPPO-LegS 在时间 的输出相对于时间 的输入的梯度范数为 ，这意味着梯度随着时间的增加而减小，但是衰减的速度比 RNN 的指数级慢的多。 这个性质使得 HiPPO-LegS 能够有效地缓解 RNN 中的梯度消失问题。即使在长序列中，梯度也不会迅速衰减到0，这有助于网络在训练中更好地学习长期依赖。 近似有界误差 HiPPO-LegS 在时间 t 的近似误差 。其中 N 是多项式的最高阶。这表明随着多项式的阶 N 的增加，误差逐渐减小。 3.3 实验 将 HiPPO 和 RNN 相结合，当前状态 不仅和上一个状态 有关，还和 HiPPO 状态 有关，如下所示： 模型结构如下： 图3-6：HiPPO和RNN结合 下面是pMINIST 数据集上的结果，可以看到 LegS 的效果要好于 LagT 和 LegT，同时 HiPPO 的效果好于之前的其它模型。 图3-7：HiPPO实验结果 备注： pMNIST（permuted MNIST）是一个经过修改的MNIST数据集 ，它用于测试和评估机器学习模型在处理序列数据和学习长期依赖关系方面的能力。在 pMNIST 中，原始 MNIST 图像的像素被重新排列。这意味着图像的像素不再是按照自然顺序（从左到右，从上到下）呈现，而是按照一个固定的、随机的排列顺序。这种排列方式使得模型必须学习像素之间的长期依赖关系，而不能简单地依赖于局部空间结构。 四、S4 (Structured State Space Model) S4 是 HiPPO 的后续工作，论文名称为： Efficiently Modeling Long Sequences with Structured State Spaces 。 S4 的主要工作是将 HiPPO 中的矩阵 A（称为 HiPPO 矩阵 ）转换为正规矩阵（正规矩阵可以分解为对角矩阵）和低秩矩阵的和，以此提高计算效率。 S4 通过这种分解，将计算复杂度降低到了 ，其中 N 是 HiPPO 矩阵的维度，L 是序列长度。 在处理长度为 16000 的序列的语音分类任务中，S4 模型将专门设计的语音卷积神经网络（Speech CNNs）的测试错误率降低了一半，达到了1.7%。相比之下，所有的循环神经网络（RNN）和 Transformer 基线模型都无法学习，错误率均在70%以上。 下面我们就来介绍一下这篇工作。 4.1 HiPPO 解决了长期依赖 作者讨论了如何处理长距离依赖（Long-Range Dependencies，LRDs）的问题，LRDs 是序列建模中的一个关键挑战，因为它们涉及到在序列中跨越大量时间步的依赖关系。 作者指出，基本的 SSM 在实际应用中表现不佳，特别是在处理 LRDs 时。这是因为线性一阶常微分方程（ODEs）的解通常是指数函数，这可能导致梯度在序列长度上呈指数级增长，从而引发梯度消失或爆炸的问题。 为了解决这个问题，作者利用了 HiPPO 理论。 HiPPO 理论指定了一类特殊的矩阵 A，当这些矩阵被纳入 SSM 的方程中时，可以使状态 x(t) 能够记住输入 u(t) 的历史信息。这些特殊矩阵被称为 HiPPO 矩阵，它们具有特定的数学形式，可以有效地捕捉长期依赖关系。 HiPPO 矩阵的一个关键特性是它们允许 SSM 在数学和实证上捕捉 LRDs 。例如，通过将随机矩阵 A 替换为 HiPPO 矩阵，可以在序列 MNIST 基准测试上显著提高 SSM 的性能。 HiPPO 矩阵表示如下： 4.2 在线推理：使用递归形式 S4 在推理时，使用公式（12）的递归形式，每次只需要和上一个状态进行计算，具有和 RNN 相似的推理效率。 4.3 训练 S4：卷积表示 由于离散时间 SSM 的递归性质，它在硬件上进行训练时存在效率问题。因此，作者将离散时间 SSM 的 递归方程转换为离散卷积的形式 。通过展开递归方程，可以得到一个卷积核，这个卷积核可以用来在序列数据上应用卷积操作。这种转换允许 SSM 利用快速傅里叶变换（FFT）等高效的卷积计算方法，从而在训练过程中提高计算效率。 上面式子可以转化为卷积的形式： 其中， 是一个与 SSM 的参数（A, B, C）相关的卷积核，可以通过离散傅里叶变换（DFT）和逆变换（IDFT）来计算。这种卷积表示不仅在理论上是可行的，而且在实践中也是非常有效的，因为它允许在保持模型性能的同时，显著减少训练过程中的计算和内存需求。 作者在这一节中还讨论了如何计算 SSMn卷积核，这是他们技术贡献的关键部分。通过这种卷积表示，SSM 可以被有效地训练，同时保持其在处理长距离依赖（LRDs）方面的能力。这种表示形式为 SSM 在各种序列建模任务中的应用提供了灵活性，包括图像处理、语音识别和时间序列分析等。 图4-1：SSM 卷积核形式 下面是一个具体的例子，如何使用卷积核生成输出。 图4-2：使用卷积核生成输出 卷积的一个主要好处是它可以并行训练。但是由于核大小是固定，它们的推理不如 RNN 快速并且对序列长度有限制。 图4-3：递归 SSM 和 卷积 SSM 的对比 这里可以使用一个简单的技巧，即根据任务选择表示。在训练过程中使用可以并行化的卷积表示，在推理过程中，我们使用高效的循环表示。 图4-4：递归推理、卷积训练 4.4 为什么对角化可以减少 SSM 计算复杂度 为了进一步提升计算效率，作者讨论了对角化在计算离散时间状态空间模型（</p>
</div>
<script>
var articles=[]; var currentArticleIdx=-1; var maxSpeakChars=3500;
function buildArticles(){ var c=document.querySelector('.content'); if(!c) return; var h2s=c.querySelectorAll('h2'); for(var i=0;i<h2s.length;i++){ var start=h2s[i], end=i+1<h2s.length?h2s[i+1]:null; var r=document.createRange(); r.setStart(start,0); if(end) r.setEnd(end,0); else { var last=c.lastElementChild||c; r.setEndAfter(last); } var t=r.toString().replace(/\s+/g,' ').trim(); if(t.length>maxSpeakChars) t=t.slice(0,maxSpeakChars)+'…（内容过长已截断）'; articles.push({h2:start,text:t}); }
 articles.sort(function(a,b){ return a.text.length-b.text.length; }); }
function speakText(t){ if(!t){ document.getElementById('readStatus').textContent=''; return; }
 speechSynthesis.cancel(); var status=document.getElementById('readStatus'); status.textContent='准备中…';
 var chunks=[]; var maxLen=280; for(var i=0;i<t.length;i+=maxLen){ var s=t.slice(i,i+maxLen); var j=s.lastIndexOf('。'); if(j>80){ chunks.push(s.slice(0,j+1)); i-=s.length-(j+1); } else chunks.push(s); }
 if(chunks.length===0) chunks=[t]; var idx=0; function speakNext(){ if(idx>=chunks.length){ status.textContent=''; return; } var u=new SpeechSynthesisUtterance(chunks[idx]); u.lang='zh-CN'; u.rate=0.95; var v=speechSynthesis.getVoices().filter(function(x){ return x.lang.indexOf('zh')>=0; })[0]; if(v) u.voice=v;
 u.onend=function(){ idx++; setTimeout(speakNext,80); }; u.onerror=function(){ idx++; setTimeout(speakNext,80); }; status.textContent='朗读中 '+(idx+1)+'/'+chunks.length+'…'; speechSynthesis.speak(u); }
 if(speechSynthesis.getVoices().length) speakNext(); else speechSynthesis.onvoiceschanged=function(){ speechSynthesis.onvoiceschanged=null; speakNext(); }; }
function nextArticle(){ if(articles.length===0) buildArticles(); if(articles.length===0){ document.getElementById('readStatus').textContent='无可读内容'; return; }
 speechSynthesis.cancel(); currentArticleIdx++; if(currentArticleIdx>=articles.length){ currentArticleIdx=0; document.getElementById('readStatus').textContent='已读完，共 '+articles.length+' 篇。再按从第 1 篇开始'; return; }
 var a=articles[currentArticleIdx]; a.h2.scrollIntoView({behavior:'smooth',block:'start'}); document.getElementById('readStatus').textContent='第 '+(currentArticleIdx+1)+' / '+articles.length+' 篇'; speakText(a.text); }
function readSelected(){ var sel=window.getSelection(); var t=(sel&&sel.toString)?sel.toString():''; t=t.replace(/\\s+/g,' ').trim(); if(!t){ document.getElementById('readStatus').textContent='请先选中要朗读的段落'; return; } speechSynthesis.cancel(); document.getElementById('readStatus').textContent='朗读选中内容…'; speakText(t); }
function getBlockForTap(el){ var c=document.querySelector('.content'); var blockTags=['P','DIV','H2','H3','H4','LI','PRE']; while(el&&el!==c){ if(c.contains(el)&&blockTags.indexOf(el.tagName)!==-1) return el; el=el.parentElement; } return null; }
function onContentTap(e){ if(e.target.closest&&e.target.closest('a')) return; var block=getBlockForTap(e.target); if(block){ var t=(block.innerText||'').trim().replace(/\\s+/g,' '); if(t){ e.preventDefault(); speechSynthesis.cancel(); document.getElementById('readStatus').textContent='朗读本段…'; speakText(t); } } }
if(document.readyState==='loading'){ document.addEventListener('DOMContentLoaded', function(){ buildArticles(); var c=document.querySelector('.content'); if(c) c.addEventListener('click', onContentTap); }); } else { buildArticles(); var c=document.querySelector('.content'); if(c) c.addEventListener('click', onContentTap); }
</script>
</body>
</html>
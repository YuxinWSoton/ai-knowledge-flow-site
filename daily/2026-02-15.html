<!DOCTYPE html>
<html lang="zh-CN">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<title>知识流日报 2026-02-15</title>
<style>
  * { box-sizing: border-box; }
  body {
    font-family: system-ui, sans-serif;
    max-width: 720px;
    margin: 0 auto;
    padding: 2rem 1rem;
    color: #fff;
    min-height: 100vh;
    background: #0a0e1a;
    background-image:
      radial-gradient(2px 2px at 20px 30px, rgba(255,255,255,0.9), transparent),
      radial-gradient(2px 2px at 40px 70px, rgba(255,255,255,0.6), transparent),
      radial-gradient(1.5px 1.5px at 90px 40px, rgba(255,255,255,0.8), transparent),
      radial-gradient(2px 2px at 130px 80px, rgba(255,255,255,0.5), transparent),
      radial-gradient(1.5px 1.5px at 160px 120px, rgba(255,255,255,0.7), transparent),
      radial-gradient(ellipse 120% 100% at 50% 0%, rgba(88,60,120,0.25), transparent 50%),
      radial-gradient(ellipse 80% 80% at 80% 20%, rgba(40,60,100,0.2), transparent 40%);
    background-size: 200px 200px;
    background-repeat: repeat;
  }
  a { color: #a8d4ff; text-decoration: none; }
  a:hover { text-decoration: underline; }
 h1,h2,h3{margin-top:1.5rem;color:#fff;} pre{white-space:pre-wrap;color:rgba(255,255,255,0.9);} .toolbar{margin:0.5rem 0;color:rgba(255,255,255,0.8);} .content{color:rgba(255,255,255,0.95);} .content a{color:#a8d4ff;}</style>
</head>
<body>
<p><a href="https://YuxinWSoton.github.io/ai-knowledge-flow-site/">← 返回首页</a></p>
<h1>知识流日报 2026-02-15</h1>
<p class="toolbar"><button id="btnRead" onclick="readAloud()">朗读</button> <span id="readStatus"></span> <span style="color:rgba(255,255,255,0.65);font-size:0.85em;">（荣耀/华为手机若无效，请用浏览器菜单「朗读」或「听网页」）</span></p>
<div class="content">
<h1>知识流日报 2026-02-15</h1>
<p>共 11 条（来自百度学术 / Google / Bing 等，仅含正文抓取成功的条目）。</p>
<p>（以下含抓取正文，便于阅读。未调用任何 AI API。）</p>
<h2>1. 【3DCNN基础】-CSDN博客</h2>
<ul>
<li>链接：https://blog.csdn.net/qq_43561603/article/details/130123847</li>
<li>来源：bing</li>
<li>摘要：2024年9月21日 · 3DCNN是一种处理3D输入数据的深度学习模型，其结构类似2DCNN，但计算资源需求更大。3D卷积层和池化层分别用于提取和减少数据维度，全连接层则用于分类或回归任 …</li>
</ul>
<h3>正文（抓取，非 AI）</h3>
<p>【概述】<br>
3DCNN是可以处理３D输入数据的卷积神经网络，结构与２DCNN相同，但是比２DCNN占用更多的内存空间和运行时间。另一方面，由于输入数据的信息很丰富，３DCNN可以给出更精确的结果。CNN架构包括resnet, LeNet, Densenet等，这些架构也以三维形式提供。<br>
【３D卷积层】<br>
卷积层采用卷积来检索图像信号中包含的特征。卷积层的输出称为特征图或者激活图。３D卷积操作比２D卷积操作更为复杂。<br>
【３D池化层】<br>
池化层用于减少图像的空间维度，同时仅保留最具描述性的像素。有 3 种常用方法可供使用：最大池化（选择最大值）、最小池化（选择最低值）、平均池化（值的平均值）。<br>
【３D全连接层】<br>
全连接层适用于之前已经展平的输入，它将一层中的每个神经元连接到另一层中的所有神经元。<br>
【激活函数】<br>
激活函数是一个数学函数，它考虑权重和偏差来确定哪个结果将转移到下一个神经元，可以分为线性激活函数和非线性激活函数。这种激活函数的选择取决于要解决的问题的类型。<br>
非线性激活函数：<br>
【３DCNN和２ＤＣＮＮ比较】<br>
原文链接：<br>
https://www.reachiteasily.com/2021/06/3d-convolutional-neural-network-pytorch.html<br>
此笔记仅供自己复习使用！</p>
<h2>2. 【代码+公式+图解+参数详解】PyTorch的3D卷积nn.Conv3d ...</h2>
<ul>
<li>链接：https://blog.csdn.net/2503_92010587/article/details/148979778</li>
<li>来源：bing</li>
<li>摘要：2025年6月28日 · 文章目录 1. 核心概念：什么是三维卷积 (3D Convolution)？ <strong>通俗理解</strong> <strong>学术定义</strong> <strong>3D卷积是用来提取什么的？</strong> 2.<code>nn.Conv3d</code> 参数详解 (修正与扩充版) **2.1 初始化 …</li>
</ul>
<h3>正文（抓取，非 AI）</h3>
<dl>
<dt>本文章旨在提供对 PyTorch</dt>
<dt>nn.Conv3d</dt>
<dt>模块的全面、深入且易于理解的介绍。</dt>
<dt>文章目录</dt>
<dt>1. 核心概念：什么是三维卷积 (3D Convolution)？</dt>
<dt><strong>通俗理解</strong></dt>
<dt><strong>学术定义</strong></dt>
<dt><strong>3D卷积是用来提取什么的？</strong></dt>
<dt>2.<code>nn.Conv3d</code> 参数详解 (修正与扩充版)</dt>
<dt><strong>2.1 初始化参数 (Constructor Parameters)</strong></dt>
<dt>3. 输入与输出的Shape</dt>
<dt>4. 运算方式与公式总结</dt>
<dt><strong>运算方式</strong></dt>
<dt><strong>运算公式</strong></dt>
<dt>5. 抽象应用场景概述</dt>
<dt>6.【代码】【shape示例数据流理解】</dt>
<dt>7.【可视化过程】【可视化3D卷积过程示例】</dt>
<dt>8.【代码】【工程运用示例】工程代码示例模板【以VideoClassification为例，参考套用代码】</dt>
<dt><strong>从5D特征到2D分类的“三步走”策略详解</strong></dt>
<dt>代码块的Markdown</dt>
<dt>1. 核心概念：什么是三维卷积 (3D Convolution)？</dt>
<dt>通俗理解</dt>
<dt>想象一下，我们想让计算机“看懂”一段视频或者一个三维的医学影像（比如CT扫描）。</dt>
<dt>二维卷积 (Conv2D)</dt>
<dt>就像一个“图片扫描仪”。它用一个小窗口（卷积核）在单张图片上滑动，识别边缘、角点、纹理等</dt>
<dt>空间特征</dt>
<dt>。</dt>
<dt>三维卷积 (Conv3D)</dt>
<dt>则是一个“视频/立体扫描仪”。它的窗口不仅在二维平面上滑动，还能在</dt>
<dt>时间维度</dt>
<dt>（视频的连续帧）或</dt>
<dt>深度维度</dt>
<dt>（CT扫描的连续切片）上移动。这使得它不仅能像二维卷积那样识别每一帧/每一层的空间特征，还能捕捉这些特征随时间或深度的</dt>
<dt>变化规律</dt>
<dt>。</dt>
<dt>学术定义</dt>
<dt>三维卷积是对一个三维数据体（Volume）应用一个三维的卷积核（Kernel/Filter），通过在输入数据的三个维度（通常是深度、高度、宽度）上进行滑动窗口操作，计算卷积核与输入数据对应子区域的点积，从而生成一个三维的特征图（Feature Map）。这个过程旨在从三维数据中同时提取空间和时间（或深度）上的局部特征。</dt>
<dt>3D卷积是用来提取什么的？</dt>
<dt>它主要用于提取</dt>
<dt>时空特征 (Spatio-temporal features)</dt>
<dt>或</dt>
<dt>三维空间特征</dt>
<dt>。</dt>
<dt>对于视频数据：</dt>
<dt>它可以识别动作。例如，一个</dt>
<dt>Conv3D</dt>
<dt>层可能学会识别“挥手”这个动作，因为它能同时看到手的形状（空间信息）以及手在连续几帧中的移动轨迹（时间信息）。而</dt>
<dt>Conv2D</dt>
<dt>只能识别每一帧中静止的手。</dt>
<dt>对于医学影像 (MRI, CT)：</dt>
<dt>它可以识别三维的病灶结构。例如，一个肿瘤在三维空间中可能呈现特定的球状或不规则形状，</dt>
<dt>Conv3D</dt>
<dt>可以捕捉到这种贯穿多个切片的立体结构特征。</dt>
<dt>2.</dt>
<dt>nn.Conv3d</dt>
<dt>参数详解 (修正与扩充版)</dt>
<dt>2.1 初始化参数 (Constructor Parameters)</dt>
<dt>torch</dt>
<dt>.</dt>
<dt>nn</dt>
<dt>.</dt>
<dt>Conv3d</dt>
<dt>(</dt>
<dt>in_channels</dt>
<dt>,</dt>
<dt>out_channels</dt>
<dt>,</dt>
<dt>kernel_size</dt>
<dt>,</dt>
<dt>stride</dt>
<dt>=</dt>
<dt>1</dt>
<dt>,</dt>
<dt>padding</dt>
<dt>=</dt>
<dt>0</dt>
<dt>,</dt>
<dt>dilation</dt>
<dt>=</dt>
<dt>1</dt>
<dt>,</dt>
<dt>groups</dt>
<dt>=</dt>
<dt>1</dt>
<dt>,</dt>
<dt>bias</dt>
<dt>=</dt>
<dt>True</dt>
<dt>,</dt>
<dt>padding_mode</dt>
<dt>=</dt>
<dt>'zeros'</dt>
<dt>)</dt>
<dt>为了清晰展示各参数对形状的影响，我们统一采用以下</dt>
<dt>“基准配置”</dt>
<dt>。在讲解某个特定参数时，未提及的参数均保持此处的基准值不变。</dt>
<dt>基准输入 Tensor Shape</dt>
<dt>:</dt>
<dt>(N=4, C_in=16, D=20, H=32, W=32)</dt>
<dt>基准卷积层固定参数</dt>
<dt>:</dt>
<dt>out_channels = 32</dt>
<dt>kernel_size = 3</dt>
<dt>(即</dt>
<dt>(3, 3, 3)</dt>
<dt>)</dt>
<dt>stride = 1</dt>
<dt>(即</dt>
<dt>(1, 1, 1)</dt>
<dt>)</dt>
<dt>padding = 0</dt>
<dt>(即</dt>
<dt>(0, 0, 0)</dt>
<dt>)</dt>
<dt>dilation = 1</dt>
<dt>(即</dt>
<dt>(1, 1, 1)</dt>
<dt>)</dt>
<dt>groups = 1</dt>
<dt>基于此基准配置，我们可以计算出</dt>
<dt>基准输出 Shape</dt>
<dt>：</dt>
<dt>D</dt>
<dt>o</dt>
<dt>u</dt>
<dt>t</dt>
<dt>=</dt>
<dt>⌊</dt>
<dt>20</dt>
<dt>+</dt>
<dt>2</dt>
<dt>⋅</dt>
<dt>0</dt>
<dt>−</dt>
<dt>1</dt>
<dt>⋅</dt>
<dt>(</dt>
<dt>3</dt>
<dt>−</dt>
<dt>1</dt>
<dt>)</dt>
<dt>−</dt>
<dt>1</dt>
<dt>1</dt>
<dt>⌋</dt>
<dt>+</dt>
<dt>1</dt>
<dt>=</dt>
<dt>18</dt>
<dt>D_{out} = \lfloor \frac{20 + 2 \cdot 0 - 1 \cdot (3 - 1) - 1}{1} \rfloor + 1 = 18</dt>
<dt>D</dt>
<dt>o</dt>
<dt>u</dt>
<dt>t</dt>
<dt>​</dt>
<dt>=</dt>
<dt>⌊</dt>
<dt>1</dt>
<dt>20</dt>
<dt>+</dt>
<dt>2</dt>
<dt>⋅</dt>
<dt>0</dt>
<dt>−</dt>
<dt>1</dt>
<dt>⋅</dt>
<dt>(</dt>
<dt>3</dt>
<dt>−</dt>
<dt>1</dt>
<dt>)</dt>
<dt>−</dt>
<dt>1</dt>
<dt>​</dt>
<dt>⌋</dt>
<dt>+</dt>
<dt>1</dt>
<dt>=</dt>
<dt>18</dt>
<dt>H</dt>
<dt>o</dt>
<dt>u</dt>
<dt>t</dt>
<dt>=</dt>
<dt>⌊</dt>
<dt>32</dt>
<dt>+</dt>
<dt>2</dt>
<dt>⋅</dt>
<dt>0</dt>
<dt>−</dt>
<dt>1</dt>
<dt>⋅</dt>
<dt>(</dt>
<dt>3</dt>
<dt>−</dt>
<dt>1</dt>
<dt>)</dt>
<dt>−</dt>
<dt>1</dt>
<dt>1</dt>
<dt>⌋</dt>
<dt>+</dt>
<dt>1</dt>
<dt>=</dt>
<dt>30</dt>
<dt>H_{out} = \lfloor \frac{32 + 2 \cdot 0 - 1 \cdot (3 - 1) - 1}{1} \rfloor + 1 = 30</dt>
<dt>H</dt>
<dt>o</dt>
<dt>u</dt>
<dt>t</dt>
<dt>​</dt>
<dt>=</dt>
<dt>⌊</dt>
<dt>1</dt>
<dt>32</dt>
<dt>+</dt>
<dt>2</dt>
<dt>⋅</dt>
<dt>0</dt>
<dt>−</dt>
<dt>1</dt>
<dt>⋅</dt>
<dt>(</dt>
<dt>3</dt>
<dt>−</dt>
<dt>1</dt>
<dt>)</dt>
<dt>−</dt>
<dt>1</dt>
<dt>​</dt>
<dt>⌋</dt>
<dt>+</dt>
<dt>1</dt>
<dt>=</dt>
<dt>30</dt>
<dt>W</dt>
<dt>o</dt>
<dt>u</dt>
<dt>t</dt>
<dt>=</dt>
<dt>⌊</dt>
<dt>32</dt>
<dt>+</dt>
<dt>2</dt>
<dt>⋅</dt>
<dt>0</dt>
<dt>−</dt>
<dt>1</dt>
<dt>⋅</dt>
<dt>(</dt>
<dt>3</dt>
<dt>−</dt>
<dt>1</dt>
<dt>)</dt>
<dt>−</dt>
<dt>1</dt>
<dt>1</dt>
<dt>⌋</dt>
<dt>+</dt>
<dt>1</dt>
<dt>=</dt>
<dt>30</dt>
<dt>W_{out} = \lfloor \frac{32 + 2 \cdot 0 - 1 \cdot (3 - 1) - 1}{1} \rfloor + 1 = 30</dt>
<dt>W</dt>
<dt>o</dt>
<dt>u</dt>
<dt>t</dt>
<dt>​</dt>
<dt>=</dt>
<dt>⌊</dt>
<dt>1</dt>
<dt>32</dt>
<dt>+</dt>
<dt>2</dt>
<dt>⋅</dt>
<dt>0</dt>
<dt>−</dt>
<dt>1</dt>
<dt>⋅</dt>
<dt>(</dt>
<dt>3</dt>
<dt>−</dt>
<dt>1</dt>
<dt>)</dt>
<dt>−</dt>
<dt>1</dt>
<dt>​</dt>
<dt>⌋</dt>
<dt>+</dt>
<dt>1</dt>
<dt>=</dt>
<dt>30</dt>
<dt>基准输出 Shape</dt>
<dt>:</dt>
<dt>(4, 32, 18, 30, 30)</dt>
<dt>in_channels</dt>
<dt>(int)</dt>
<dt>含义</dt>
<dt>：输入数据体的“通道数”。必须与输入Tensor的通道维度(</dt>
<dt>C_in</dt>
<dt>)完全匹配。</dt>
<dt>Shape示例</dt>
<dt>:</dt>
<dt>输入 Shape</dt>
<dt>:</dt>
<dt>(4, 16, 20, 32, 32)</dt>
<dt>in_channels</dt>
<dt>值</dt>
<dd>
<dl>
<dt>必须设置为</dt>
<dt>16</dt>
<dt>。如果设置为其他值（如</dt>
<dt>in_channels=3</dt>
<dt>），则在执行时会因维度不匹配而报错。</dt>
<dt>对输出Shape的影响</dt>
<dd>
<dl>
<dt>这是正确计算的前提条件，不直接决定输出的</dt>
<dt>D</dt>
<dt>,</dt>
<dt>H</dt>
<dt>,</dt>
<dt>W</dt>
<dt>D,H,W</dt>
<dt>D</dt>
<dt>,</dt>
<dt>H</dt>
<dt>,</dt>
<dt>W</dt>
<dt>尺寸。</dt>
<dt>out_channels</dt>
<dt>(int)</dt>
<dt>含义</dt>
<dt>：卷积层输出的通道数，即使用的卷积核数量。此参数直接决定输出Tensor的通道维度(</dt>
<dt>C_out</dt>
<dt>)。</dt>
<dt>固定参数</dt>
<dt>:</dt>
<dt>kernel_size=3</dt>
<dt>,</dt>
<dt>stride=1</dt>
<dt>,</dt>
<dt>padding=0</dt>
<dt>,</dt>
<dt>dilation=1</dt>
<dt>输入 Shape</dt>
<dt>:</dt>
<dt>(4, 16, 20, 32, 32)</dt>
<dt>Shape示例 (基于基准输入)</dt>
<dt>:</dt>
<dt>示例1 (****</dt>
<dt>out_channels=32</dt>
<dt>)</dt>
<dd>
<dl>
<dt>-&gt;</dt>
<dt>输出 Shape</dt>
<dt>:</dt>
<dt>(4, 32, 18, 30, 30)</dt>
<dt>(基准情况)</dt>
<dt>示例2 (****</dt>
<dt>out_channels=64</dt>
<dt>)</dt>
<dd>
<dl>
<dt>-&gt;</dt>
<dt>输出 Shape</dt>
<dt>:</dt>
<dt>(4, 64, 18, 30, 30)</dt>
<dt>(只有通道数改变)</dt>
<dt>示例3 (****</dt>
<dt>out_channels=128</dt>
<dt>)</dt>
<dd>-&gt;<br>
输出 Shape<br>
:<br>
(4, 128, 18, 30, 30)<br>
(只有通道数改变)<br>
kernel_size<br>
(int or tuple)<br>
输入 Shape<br>
:<br>
(4, 16, 20, 32, 32)<br>
含义<br>
：卷积核的尺寸<br>
(D_k, H_k, W_k)<br>
。直接影响输出的D,H,W尺寸。<br>
固定参数<br>
:<br>
out_channels=32<br>
,<br>
stride=1<br>
,<br>
padding=0<br>
,<br>
dilation=1<br>
Shape示例 (基于基准输入)<br>
:<br>
示例1 (<strong><em>*<br>
kernel_size=3<br>
)<br>
:<br>
D<br>
o<br>
u<br>
t<br>
=<br>
18<br>
D_{out}=18<br>
D<br>
o<br>
u<br>
t<br>
​<br>
=<br>
18<br>
,<br>
H<br>
o<br>
u<br>
t<br>
=<br>
30<br>
H_{out}=30<br>
H<br>
o<br>
u<br>
t<br>
​<br>
=<br>
30<br>
,<br>
W<br>
o<br>
u<br>
t<br>
=<br>
30<br>
W_{out}=30<br>
W<br>
o<br>
u<br>
t<br>
​<br>
=<br>
30<br>
输出 Shape<br>
:<br>
(4, 32, 18, 30, 30)<br>
(基准情况)<br>
示例2 (</em></strong><em><br>
kernel_size=5<br>
)<br>
:<br>
D<br>
o<br>
u<br>
t<br>
=<br>
⌊<br>
20<br>
−<br>
(<br>
5<br>
−<br>
1<br>
)<br>
−<br>
1<br>
1<br>
⌋<br>
+<br>
1<br>
=<br>
16<br>
D_{out} = \lfloor \frac{20 - (5-1) - 1}{1} \rfloor + 1 = 16<br>
D<br>
o<br>
u<br>
t<br>
​<br>
=<br>
⌊<br>
1<br>
20<br>
−<br>
(<br>
5<br>
−<br>
1<br>
)<br>
−<br>
1<br>
​<br>
⌋<br>
+<br>
1<br>
=<br>
16<br>
H<br>
o<br>
u<br>
t<br>
=<br>
⌊<br>
32<br>
−<br>
(<br>
5<br>
−<br>
1<br>
)<br>
−<br>
1<br>
1<br>
⌋<br>
+<br>
1<br>
=<br>
28<br>
H_{out} = \lfloor \frac{32 - (5-1) - 1}{1} \rfloor + 1 = 28<br>
H<br>
o<br>
u<br>
t<br>
​<br>
=<br>
⌊<br>
1<br>
32<br>
−<br>
(<br>
5<br>
−<br>
1<br>
)<br>
−<br>
1<br>
​<br>
⌋<br>
+<br>
1<br>
=<br>
28<br>
W<br>
o<br>
u<br>
t<br>
=<br>
⌊<br>
32<br>
−<br>
(<br>
5<br>
−<br>
1<br>
)<br>
−<br>
1<br>
1<br>
⌋<br>
+<br>
1<br>
=<br>
28<br>
W_{out} = \lfloor \frac{32 - (5-1) - 1}{1} \rfloor + 1 = 28<br>
W<br>
o<br>
u<br>
t<br>
​<br>
=<br>
⌊<br>
1<br>
32<br>
−<br>
(<br>
5<br>
−<br>
1<br>
)<br>
−<br>
1<br>
​<br>
⌋<br>
+<br>
1<br>
=<br>
28<br>
输出 Shape<br>
:<br>
(4, 32, 16, 28, 28)<br>
示例3 (非立方体,<br>
kernel_size=(3, 5, 5)<br>
)<br>
:<br>
D<br>
o<br>
u<br>
t<br>
=<br>
⌊<br>
20<br>
−<br>
(<br>
3<br>
−<br>
1<br>
)<br>
−<br>
1<br>
1<br>
⌋<br>
+<br>
1<br>
=<br>
18<br>
D_{out} = \lfloor \frac{20 - (3-1) - 1}{1} \rfloor + 1 = 18<br>
D<br>
o<br>
u<br>
t<br>
​<br>
=<br>
⌊<br>
1<br>
20<br>
−<br>
(<br>
3<br>
−<br>
1<br>
)<br>
−<br>
1<br>
​<br>
⌋<br>
+<br>
1<br>
=<br>
18<br>
H<br>
o<br>
u<br>
t<br>
=<br>
⌊<br>
32<br>
−<br>
(<br>
5<br>
−<br>
1<br>
)<br>
−<br>
1<br>
1<br>
⌋<br>
+<br>
1<br>
=<br>
28<br>
H_{out} = \lfloor \frac{32 - (5-1) - 1}{1} \rfloor + 1 = 28<br>
H<br>
o<br>
u<br>
t<br>
​<br>
=<br>
⌊<br>
1<br>
32<br>
−<br>
(<br>
5<br>
−<br>
1<br>
)<br>
−<br>
1<br>
​<br>
⌋<br>
+<br>
1<br>
=<br>
28<br>
W<br>
o<br>
u<br>
t<br>
=<br>
⌊<br>
32<br>
−<br>
(<br>
5<br>
−<br>
1<br>
)<br>
−<br>
1<br>
1<br>
⌋<br>
+<br>
1<br>
=<br>
28<br>
W_{out} = \lfloor \frac{32 - (5-1) - 1}{1} \rfloor + 1 = 28<br>
W<br>
o<br>
u<br>
t<br>
​<br>
=<br>
⌊<br>
1<br>
32<br>
−<br>
(<br>
5<br>
−<br>
1<br>
)<br>
−<br>
1<br>
​<br>
⌋<br>
+<br>
1<br>
=<br>
28<br>
输出 Shape<br>
:<br>
(4, 32, 18, 28, 28)<br>
stride<br>
(int or tuple)<br>
输入 Shape<br>
:<br>
(4, 16, 20, 32, 32)<br>
含义<br>
：卷积核滑动的步长<br>
(D_s, H_s, W_s)<br>
。是改变输出尺寸最主要的方式之一，起到</em><em><br>
下采样<br>
</em><em>作用。<br>
固定参数<br>
:<br>
out_channels=32<br>
,<br>
kernel_size=3<br>
,<br>
padding=0<br>
,<br>
dilation=1<br>
Shape示例 (基于基准输入)<br>
:<br>
示例1 (</em><strong><em><br>
stride=1<br>
)<br>
:<br>
D<br>
o<br>
u<br>
t<br>
=<br>
18<br>
D_{out}=18<br>
D<br>
o<br>
u<br>
t<br>
​<br>
=<br>
18<br>
,<br>
H<br>
o<br>
u<br>
t<br>
=<br>
30<br>
H_{out}=30<br>
H<br>
o<br>
u<br>
t<br>
​<br>
=<br>
30<br>
,<br>
W<br>
o<br>
u<br>
t<br>
=<br>
30<br>
W_{out}=30<br>
W<br>
o<br>
u<br>
t<br>
​<br>
=<br>
30<br>
输出 Shape<br>
:<br>
(4, 32, 18, 30, 30)<br>
(基准情况)<br>
示例2 (</em></strong><em><br>
stride=2<br>
)<br>
:<br>
D<br>
o<br>
u<br>
t<br>
=<br>
⌊<br>
20<br>
−<br>
(<br>
3<br>
−<br>
1<br>
)<br>
−<br>
1<br>
2<br>
⌋<br>
+<br>
1<br>
=<br>
9<br>
D_{out} = \lfloor \frac{20 - (3-1) - 1}{2} \rfloor + 1 = 9<br>
D<br>
o<br>
u<br>
t<br>
​<br>
=<br>
⌊<br>
2<br>
20<br>
−<br>
(<br>
3<br>
−<br>
1<br>
)<br>
−<br>
1<br>
​<br>
⌋<br>
+<br>
1<br>
=<br>
9<br>
H<br>
o<br>
u<br>
t<br>
=<br>
⌊<br>
32<br>
−<br>
(<br>
3<br>
−<br>
1<br>
)<br>
−<br>
1<br>
2<br>
⌋<br>
+<br>
1<br>
=<br>
15<br>
H_{out} = \lfloor \frac{32 - (3-1) - 1}{2} \rfloor + 1 = 15<br>
H<br>
o<br>
u<br>
t<br>
​<br>
=<br>
⌊<br>
2<br>
32<br>
−<br>
(<br>
3<br>
−<br>
1<br>
)<br>
−<br>
1<br>
​<br>
⌋<br>
+<br>
1<br>
=<br>
15<br>
W<br>
o<br>
u<br>
t<br>
=<br>
⌊<br>
32<br>
−<br>
(<br>
3<br>
−<br>
1<br>
)<br>
−<br>
1<br>
2<br>
⌋<br>
+<br>
1<br>
=<br>
15<br>
W_{out} = \lfloor \frac{32 - (3-1) - 1}{2} \rfloor + 1 = 15<br>
W<br>
o<br>
u<br>
t<br>
​<br>
=<br>
⌊<br>
2<br>
32<br>
−<br>
(<br>
3<br>
−<br>
1<br>
)<br>
−<br>
1<br>
​<br>
⌋<br>
+<br>
1<br>
=<br>
15<br>
输出 Shape<br>
:<br>
(4, 32, 9, 15, 15)<br>
(D,H,W尺寸约减半)<br>
示例3 (非均匀,<br>
stride=(1, 2, 2)<br>
)<br>
:<br>
D<br>
o<br>
u<br>
t<br>
=<br>
⌊<br>
20<br>
−<br>
(<br>
3<br>
−<br>
1<br>
)<br>
−<br>
1<br>
1<br>
⌋<br>
+<br>
1<br>
=<br>
18<br>
D_{out} = \lfloor \frac{20 - (3-1) - 1}{1} \rfloor + 1 = 18<br>
D<br>
o<br>
u<br>
t<br>
​<br>
=<br>
⌊<br>
1<br>
20<br>
−<br>
(<br>
3<br>
−<br>
1<br>
)<br>
−<br>
1<br>
​<br>
⌋<br>
+<br>
1<br>
=<br>
18<br>
H<br>
o<br>
u<br>
t<br>
=<br>
⌊<br>
32<br>
−<br>
(<br>
3<br>
−<br>
1<br>
)<br>
−<br>
1<br>
2<br>
⌋<br>
+<br>
1<br>
=<br>
15<br>
H_{out} = \lfloor \frac{32 - (3-1) - 1}{2} \rfloor + 1 = 15<br>
H<br>
o<br>
u<br>
t<br>
​<br>
=<br>
⌊<br>
2<br>
32<br>
−<br>
(<br>
3<br>
−<br>
1<br>
)<br>
−<br>
1<br>
​<br>
⌋<br>
+<br>
1<br>
=<br>
15<br>
W<br>
o<br>
u<br>
t<br>
=<br>
⌊<br>
32<br>
−<br>
(<br>
3<br>
−<br>
1<br>
)<br>
−<br>
1<br>
2<br>
⌋<br>
+<br>
1<br>
=<br>
15<br>
W_{out} = \lfloor \frac{32 - (3-1) - 1}{2} \rfloor + 1 = 15<br>
W<br>
o<br>
u<br>
t<br>
​<br>
=<br>
⌊<br>
2<br>
32<br>
−<br>
(<br>
3<br>
−<br>
1<br>
)<br>
−<br>
1<br>
​<br>
⌋<br>
+<br>
1<br>
=<br>
15<br>
输出 Shape<br>
:<br>
(4, 32, 18, 15, 15)<br>
(D尺寸不变, H,W尺寸减半)<br>
padding<br>
(int, tuple or str)<br>
输入 Shape<br>
:<br>
(4, 16, 20, 32, 32)<br>
含义<br>
：在输入数据体的边界周围进行填充。用于控制输出尺寸，常用于保持尺寸不变。<br>
固定参数<br>
:<br>
out_channels=32<br>
,<br>
kernel_size=3<br>
,<br>
stride=1<br>
,<br>
dilation=1<br>
Shape示例 (基于基准输入)<br>
:<br>
示例1 (</em><strong><em><br>
padding=0<br>
)<br>
:<br>
-&gt;<br>
输出 Shape<br>
:<br>
(4, 32, 18, 30, 30)<br>
(基准情况)<br>
示例2 (</em></strong><em><br>
padding=1<br>
)<br>
:<br>
D<br>
o<br>
u<br>
t<br>
=<br>
⌊<br>
20<br>
+<br>
2<br>
⋅<br>
1<br>
−<br>
(<br>
3<br>
−<br>
1<br>
)<br>
−<br>
1<br>
1<br>
⌋<br>
+<br>
1<br>
=<br>
20<br>
D_{out} = \lfloor \frac{20 + 2\cdot1 - (3-1) - 1}{1} \rfloor + 1 = 20<br>
D<br>
o<br>
u<br>
t<br>
​<br>
=<br>
⌊<br>
1<br>
20<br>
+<br>
2<br>
⋅<br>
1<br>
−<br>
(<br>
3<br>
−<br>
1<br>
)<br>
−<br>
1<br>
​<br>
⌋<br>
+<br>
1<br>
=<br>
20<br>
H<br>
o<br>
u<br>
t<br>
=<br>
⌊<br>
32<br>
+<br>
2<br>
⋅<br>
1<br>
−<br>
(<br>
3<br>
−<br>
1<br>
)<br>
−<br>
1<br>
1<br>
⌋<br>
+<br>
1<br>
=<br>
32<br>
H_{out} = \lfloor \frac{32 + 2\cdot1 - (3-1) - 1}{1} \rfloor + 1 = 32<br>
H<br>
o<br>
u<br>
t<br>
​<br>
=<br>
⌊<br>
1<br>
32<br>
+<br>
2<br>
⋅<br>
1<br>
−<br>
(<br>
3<br>
−<br>
1<br>
)<br>
−<br>
1<br>
​<br>
⌋<br>
+<br>
1<br>
=<br>
32<br>
W<br>
o<br>
u<br>
t<br>
=<br>
⌊<br>
32<br>
+<br>
2<br>
⋅<br>
1<br>
−<br>
(<br>
3<br>
−<br>
1<br>
)<br>
−<br>
1<br>
1<br>
⌋<br>
+<br>
1<br>
=<br>
32<br>
W_{out} = \lfloor \frac{32 + 2\cdot1 - (3-1) - 1}{1} \rfloor + 1 = 32<br>
W<br>
o<br>
u<br>
t<br>
​<br>
=<br>
⌊<br>
1<br>
32<br>
+<br>
2<br>
⋅<br>
1<br>
−<br>
(<br>
3<br>
−<br>
1<br>
)<br>
−<br>
1<br>
​<br>
⌋<br>
+<br>
1<br>
=<br>
32<br>
输出 Shape<br>
:<br>
(4, 32, 20, 32, 32)<br>
(D,H,W尺寸与输入完全相同)<br>
示例3 (</em><strong><em><br>
padding=2<br>
)<br>
:<br>
D<br>
o<br>
u<br>
t<br>
=<br>
⌊<br>
20<br>
+<br>
2<br>
⋅<br>
2<br>
−<br>
(<br>
3<br>
−<br>
1<br>
)<br>
−<br>
1<br>
1<br>
⌋<br>
+<br>
1<br>
=<br>
22<br>
D_{out} = \lfloor \frac{20 + 2\cdot2 - (3-1) - 1}{1} \rfloor + 1 = 22<br>
D<br>
o<br>
u<br>
t<br>
​<br>
=<br>
⌊<br>
1<br>
20<br>
+<br>
2<br>
⋅<br>
2<br>
−<br>
(<br>
3<br>
−<br>
1<br>
)<br>
−<br>
1<br>
​<br>
⌋<br>
+<br>
1<br>
=<br>
22<br>
H<br>
o<br>
u<br>
t<br>
=<br>
⌊<br>
32<br>
+<br>
2<br>
⋅<br>
2<br>
−<br>
(<br>
3<br>
−<br>
1<br>
)<br>
−<br>
1<br>
1<br>
⌋<br>
+<br>
1<br>
=<br>
34<br>
H_{out} = \lfloor \frac{32 + 2\cdot2 - (3-1) - 1}{1} \rfloor + 1 = 34<br>
H<br>
o<br>
u<br>
t<br>
​<br>
=<br>
⌊<br>
1<br>
32<br>
+<br>
2<br>
⋅<br>
2<br>
−<br>
(<br>
3<br>
−<br>
1<br>
)<br>
−<br>
1<br>
​<br>
⌋<br>
+<br>
1<br>
=<br>
34<br>
W<br>
o<br>
u<br>
t<br>
=<br>
⌊<br>
32<br>
+<br>
2<br>
⋅<br>
2<br>
−<br>
(<br>
3<br>
−<br>
1<br>
)<br>
−<br>
1<br>
1<br>
⌋<br>
+<br>
1<br>
=<br>
34<br>
W_{out} = \lfloor \frac{32 + 2\cdot2 - (3-1) - 1}{1} \rfloor + 1 = 34<br>
W<br>
o<br>
u<br>
t<br>
​<br>
=<br>
⌊<br>
1<br>
32<br>
+<br>
2<br>
⋅<br>
2<br>
−<br>
(<br>
3<br>
−<br>
1<br>
)<br>
−<br>
1<br>
​<br>
⌋<br>
+<br>
1<br>
=<br>
34<br>
输出 Shape<br>
:<br>
(4, 32, 22, 34, 34)<br>
示例4 (</em></strong>*<br>
padding='same'<br>
)<br>
:<br>
这是一个字符串参数，PyTorch会自动计算填充量以在<br>
stride=1<br>
时保持空间维度(H, W)不变。对于<br>
kernel_size=3<br>
，这等同于设置<br>
padding=(?, 1, 1)<br>
（D维度的padding需单独考虑或默认为0）。<br>
输出 Shape<br>
:<br>
(4, 32, 18, 32, 32)<br>
(H, W与输入相同，D根据其维度上的参数计算)<br>
示例5 (非对称填充,<br>
padding=(0, 1, 2)<br>
)<br>
:<br>
D<br>
o<br>
u<br>
t<br>
=<br>
⌊<br>
20<br>
+<br>
2<br>
⋅<br>
0<br>
−<br>
(<br>
3<br>
−<br>
1<br>
)<br>
−<br>
1<br>
1<br>
⌋<br>
+<br>
1<br>
=<br>
18<br>
D_{out} = \lfloor \frac{20 + 2\cdot0 - (3-1) - 1}{1} \rfloor + 1 = 18<br>
D<br>
o<br>
u<br>
t<br>
​<br>
=<br>
⌊<br>
1<br>
20<br>
+<br>
2<br>
⋅<br>
0<br>
−<br>
(<br>
3<br>
−<br>
1<br>
)<br>
−<br>
1<br>
​<br>
⌋<br>
+<br>
1<br>
=<br>
18<br>
H<br>
o<br>
u<br>
t<br>
=<br>
⌊<br>
32<br>
+<br>
2<br>
⋅<br>
1<br>
−<br>
(<br>
3<br>
−<br>
1<br>
)<br>
−<br>
1<br>
1<br>
⌋<br>
+<br>
1<br>
=<br>
32<br>
H_{out} = \lfloor \frac{32 + 2\cdot1 - (3-1) - 1}{1} \rfloor + 1 = 32<br>
H<br>
o<br>
u<br>
t<br>
​<br>
=<br>
⌊<br>
1<br>
32<br>
+<br>
2<br>
⋅<br>
1<br>
−<br>
(<br>
3<br>
−<br>
1<br>
)<br>
−<br>
1<br>
​<br>
⌋<br>
+<br>
1<br>
=<br>
32<br>
W<br>
o<br>
u<br>
t<br>
=<br>
⌊<br>
32<br>
+<br>
2<br>
⋅<br>
2<br>
−<br>
(<br>
3<br>
−<br>
1<br>
)<br>
−<br>
1<br>
1<br>
⌋<br>
+<br>
1<br>
=<br>
34<br>
W_{out} = \lfloor \frac{32 + 2\cdot2 - (3-1) - 1}{1} \rfloor + 1 = 34<br>
W<br>
o<br>
u<br>
t<br>
​<br>
=<br>
⌊<br>
1<br>
32<br>
+<br>
2<br>
⋅<br>
2<br>
−<br>
(<br>
3<br>
−<br>
1<br>
)<br>
−<br>
1<br>
​<br>
⌋<br>
+<br>
1<br>
=<br>
34<br>
输出 Shape<br>
:<br>
(4, 32, 18, 32, 34)<br>
(D维度因无填充而缩小，H维度因<br>
padding=1<br>
而保持不变，W维度因</dd>
</dl>
</dd>
</dl>
</dd>
</dl>
</dd>
</dl>
</dd>
</dl>
<h2>3. 基于PCI模型的干预训练对高职高专 护生心理资本和学习效能的影响</h2>
<ul>
<li>链接：https://xueshu.baidu.com/usercenter/paper/show?paperid=e4b7d2aa5e993080b19fd0f7b225bfd1&amp;site=xueshu_se</li>
<li>来源：baidu_scholar</li>
</ul>
<h3>正文（抓取，非 AI）</h3>
<p>基于PCI模型的干预训练对高职高专 护生心理资本和学习效能的影响 - 百度学术<br>
<br>
意见反馈<br>
登录<br>
注册<br>
高级搜索<br>
包含全部检索词<br>
包含精确检索词<br>
包含至少一个检索词<br>
不包含检索词<br>
出现检索词的位置<br>
文章任何位置<br>
位于文章标题<br>
作者<br>
机构<br>
出版物<br>
期刊<br>
会议<br>
发表时间<br>
-<br>
语言检索范围<br>
不限<br>
不限<br>
英文<br>
中文<br>
<br>
文献<br>
期刊<br>
学者<br>
订阅<br>
收藏<br>
论文查重<br>
优惠<br>
<br>
论文查重<br>
开题分析<br>
单篇购买<br>
文献互助<br>
用户中心<br>
基于PCI模型的干预训练对高职高专 护生心理资本和学习效能的影响<br>
来自<br>
掌桥科研<br>
<br>
喜欢<br>
0<br>
阅读量：<br>
98<br>
作者：<br>
李桂兰<br>
展开<br>
<br>
摘要：<br>
目的了解湖南省衡阳地区高职高专护生心理资本和学习效能的现状;探讨高职高专护生心理资本与学习效能之间的关系,综合分析影响高职高专护生学习效能的主要因素;研究心理资本干预(PCI)模型的干预训练对高职高专护生学习效能的影响,为提高高职高专护生心理资本和学习效能水平提供参考依据.方法采用分层整群抽样法抽取湖南省衡阳市湖南环境生物职业技术学院的403名高职高专护生进行问卷调查.应用《高职护生一般情况调查问卷》调查高职高专护生的人口社会学特征;采用《大学生心理资本量表》了解高职高专护生的心理资本现状;《学业自我效能量表》调查高职高专护生的学习效能状况.通过基线调查,选择两个心理资本得分较低的自然班高职高专护生作为研究对象,其中一个班为对照组(n=47),另一个班为干预组(n=46),对照组不采取任何干预措施,干预组实施PCI模型的干预训练,于干预前及干预后2,4个月末组内和组间比较心理资本及学习效能.采用SPSS17.0统计软件整理和分析所有数据.ii结果1,高职高专护生的心理资本总均分为(4.28±0.70)分,各维度得分从高到低分别为:乐观[(4.55±0.830)分],自信[(4.35±0.74)分],希望[(4.09±0.80)分],韧性[(4.07±0.754)分];高职高专护生的学业自我效能感总均分为(3.27±0.41)分,各维度得分分别为学习能力效能感[(3.31±0.54)分]和学习行为效能感[(3.24±0.43)分].2,护生心理资本得分在年级,家庭来源地,家庭结构,家庭类型,是否获得奖学金及担任班干部方面,差异均有统计学意义(p0.05);护生学业自我效能感得分在年级,家庭来源地,家庭结构,家庭类型,是否获得奖学金及担任班干部方面,差异均有统计学意义(p0.05).3,护生心理资本和各维度得分与学习效能和各维度得分均呈正相关.4,心理资本是影响护生学习效能的主要因素.5,干预组和对照组之间研究对象的一般资料差异均无统计学意义(p0.05).干预2,4个月后,干预组心理资本和学习效能得分显著高于对照组(p0.05).结论1,高职高专护生的心理资本和学习效能水平总体处于中等偏上,两者呈正相关,心理资本是高职高专护生学习效能的主要影响因素.2,年级,家庭来源地,家庭结构,家庭类型,是否获得奖学金及担任学生干部可影响高职高专护生心理资本和学习效能.3,以Luthans建立的积极心理干预模型为指导的高职高专护生PCI干预训练方案能显著提高护生的心理资本及学习效能.<br>
展开<br>
<br>
关键词：<br>
护生；心理资本；学习效能；干预<br>
学位级别：<br>
硕士<br>
DOI：<br>
CNKI:CDMD:2.1015.422380<br>
被引量：<br>
6<br>
<br>
收藏<br>
<br>
引用<br>
<br>
批量引用<br>
<br>
报错<br>
<br>
分享<br>
全部来源<br>
求助全文<br>
掌桥科研<br>
知网<br>
通过<br>
文献互助<br>
平台发起求助，成功后即可免费获取论文全文。<br>
请先登入<br>
我们已与文献出版商建立了直接购买合作。<br>
你可以通过身份认证进行实名认证，认证成功后本次下载的费用将由您所在的图书馆支付<br>
您可以直接购买此文献，1~5分钟即可下载全文，部分资源由于网络原因可能需要更长时间，请您耐心等待哦~<br>
身份认证<br>
全文购买<br>
相似文献<br>
参考文献<br>
引证文献<br>
来源学校<br>
南华大学<br>
研究点推荐<br>
高职高专 护生<br>
护生心理资本<br>
PCI模型的干预训练<br>
引用走势<br>
站内活动<br>
辅助模式<br>
0<br>
引用<br>
文献可以<br>
批量引用<br>
啦~<br>
欢迎点我试用！<br>
关于我们<br>
百度学术集成海量学术资源，融合人工智能、深度学习、大数据分析等技术，为科研工作者提供全面快捷的学术服务。在这里我们保持学习的态度，不忘初心，砥砺前行。<br>
了解更多&gt;&gt;<br>
友情链接<br>
联系我们<br>
合作与服务<br>
期刊合作<br>
图书馆合作<br>
下载产品手册<br>
意见反馈<br>
©2026 Baidu 百度学术声明<br>
使用百度前必读</p>
<h2>4. 使用 3D 卷积神经网络 (CNN) 进行视频分类 - TensorFlow Core</h2>
<ul>
<li>链接：https://tensorflow.google.cn/tutorials/video/video_classification?hl=zh-cn</li>
<li>来源：bing</li>
<li>摘要：2023年11月7日 · 本教程演示了如何使用 UCF101 动作识别数据集训练一个用于视频分类的 3D 卷积神经网络。3D CNN 使用三维过滤器来执行卷积。内核能够在三个维度上滑动，而在 2D …</li>
</ul>
<h3>正文（抓取，非 AI）</h3>
<p>使用 3D 卷积神经网络 (CNN) 进行视频分类  |  TensorFlow Core<br>
跳至主要内容<br>
/<br>
English<br>
中文 – 简体<br>
日本語<br>
GitHub<br>
TensorFlow Core<br>
TensorFlow<br>
学习<br>
TensorFlow Core<br>
使用 3D 卷积神经网络 (CNN) 进行视频分类<br>
使用集合让一切井井有条<br>
根据您的偏好保存内容并对其进行分类。<br>
在 TensorFlow.org 上查看<br>
在 Google Colab 中运行<br>
在 GitHub 上查看源代码<br>
下载笔记本<br>
本教程演示了如何使用<br>
UCF101<br>
动作识别数据集训练一个用于视频分类的 3D 卷积神经网络。3D CNN 使用三维过滤器来执行卷积。内核能够在三个维度上滑动，而在 2D CNN 中，它可以在两个维度上滑动。此模型基于 D. Tran 等人在<br>
A Closer Look at Spatiotemporal Convolutions for Action Recognition<br>
（2017 年）中发表的工作。在本教程中，您将完成以下任务：<br>
构建输入流水线<br>
使用 Keras 函数式 API 构建具有残差连接的 3D 卷积神经网络模型<br>
训练模型<br>
评估和测试模型<br>
安装<br>
首先，安装和导入一些必要的库，包括：用于检查 ZIP 文件内容的<br>
remotezip<br>
，用于使用进度条的<br>
tqdm<br>
，用于处理视频文件的<br>
OpenCV<br>
，用于执行更复杂张量运算的<br>
einops<br>
，以及用于在 Jupyter 笔记本中嵌入数据的<br>
tensorflow_docs<br>
。<br>
pip<br>
install<br>
remotezip<br>
tqdm<br>
opencv-python<br>
einops<br>
import<br>
tqdm<br>
import<br>
random<br>
import<br>
pathlib<br>
import<br>
itertools<br>
import<br>
collections<br>
import<br>
cv2<br>
import<br>
einops<br>
import<br>
numpy<br>
as<br>
np<br>
import<br>
remotezip<br>
as<br>
rz<br>
import<br>
seaborn<br>
as<br>
sns<br>
import<br>
matplotlib.pyplot<br>
as<br>
plt<br>
import<br>
tensorflow<br>
as<br>
tf<br>
import<br>
keras<br>
from<br>
keras<br>
import<br>
layers<br>
2023-11-07 18:08:39.970660: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered<br>
2023-11-07 18:08:39.970710: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered<br>
2023-11-07 18:08:39.972243: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered<br>
加载并预处理视频数据<br>
下面的隐藏单元定义了从 UCF-101 数据集下载数据切片并将其加载到<br>
tf.data.Dataset<br>
中的函数。可以在<br>
加载视频数据教程<br>
中详细了解特定的预处理步骤，此教程将更详细地介绍此代码。<br>
Toggle code<br>
def<br>
list_files_per_class<br>
(<br>
zip_url<br>
):<br>
"""<br>
List the files in each class of the dataset given the zip URL.<br>
Args:<br>
zip_url: URL from which the files can be unzipped.<br>
Return:<br>
files: List of files in each of the classes.<br>
"""<br>
files<br>
=<br>
[]<br>
with<br>
rz<br>
.<br>
RemoteZip<br>
(<br>
URL<br>
)<br>
as<br>
zip<br>
:<br>
for<br>
zip_info<br>
in<br>
zip<br>
.<br>
infolist<br>
():<br>
files<br>
.<br>
append<br>
(<br>
zip_info<br>
.<br>
filename<br>
)<br>
return<br>
files<br>
def<br>
get_class<br>
(<br>
fname<br>
):<br>
"""<br>
Retrieve the name of the class given a filename.<br>
Args:<br>
fname: Name of the file in the UCF101 dataset.<br>
Return:<br>
Class that the file belongs to.<br>
"""<br>
return<br>
fname<br>
.<br>
split<br>
(<br>
'_'<br>
)[<br>
-<br>
3<br>
]<br>
def<br>
get_files_per_class<br>
(<br>
files<br>
):<br>
"""<br>
Retrieve the files that belong to each class.<br>
Args:<br>
files: List of files in the dataset.<br>
Return:<br>
Dictionary of class names (key) and files (values).<br>
"""<br>
files_for_class<br>
=<br>
collections<br>
.<br>
defaultdict<br>
(<br>
list<br>
)<br>
for<br>
fname<br>
in<br>
files<br>
:<br>
class_name<br>
=<br>
get_class<br>
(<br>
fname<br>
)<br>
files_for_class<br>
[<br>
class_name<br>
]<br>
.<br>
append<br>
(<br>
fname<br>
)<br>
return<br>
files_for_class<br>
def<br>
download_from_zip<br>
(<br>
zip_url<br>
,<br>
to_dir<br>
,<br>
file_names<br>
):<br>
"""<br>
Download the contents of the zip file from the zip URL.<br>
Args:<br>
zip_url: Zip URL containing data.<br>
to_dir: Directory to download data to.<br>
file_names: Names of files to download.<br>
"""<br>
with<br>
rz<br>
.<br>
RemoteZip<br>
(<br>
zip_url<br>
)<br>
as<br>
zip<br>
:<br>
for<br>
fn<br>
in<br>
tqdm<br>
.<br>
tqdm<br>
(<br>
file_names<br>
):<br>
class_name<br>
=<br>
get_class<br>
(<br>
fn<br>
)<br>
zip<br>
.<br>
extract<br>
(<br>
fn<br>
,<br>
str<br>
(<br>
to_dir<br>
/<br>
class_name<br>
))<br>
unzipped_file<br>
=<br>
to_dir<br>
/<br>
class_name<br>
/<br>
fn<br>
fn<br>
=<br>
pathlib<br>
.<br>
Path<br>
(<br>
fn<br>
)<br>
.<br>
parts<br>
[<br>
-<br>
1<br>
]<br>
output_file<br>
=<br>
to_dir<br>
/<br>
class_name<br>
/<br>
fn<br>
unzipped_file<br>
.<br>
rename<br>
(<br>
output_file<br>
,)<br>
def<br>
split_class_lists<br>
(<br>
files_for_class<br>
,<br>
count<br>
):<br>
"""<br>
Returns the list of files belonging to a subset of data as well as the remainder of<br>
files that need to be downloaded.<br>
Args:<br>
files_for_class: Files belonging to a particular class of data.<br>
count: Number of files to download.<br>
Return:<br>
split_files: Files belonging to the subset of data.<br>
remainder: Dictionary of the remainder of files that need to be downloaded.<br>
"""<br>
split_files<br>
=<br>
[]<br>
remainder<br>
=<br>
{}<br>
for<br>
cls<br>
in<br>
files_for_class<br>
:<br>
split_files<br>
.<br>
extend<br>
(<br>
files_for_class<br>
[<br>
cls<br>
][:<br>
count<br>
])<br>
remainder<br>
[<br>
cls<br>
]<br>
=<br>
files_for_class<br>
[<br>
cls<br>
][<br>
count<br>
:]<br>
return<br>
split_files<br>
,<br>
remainder<br>
def<br>
download_ufc_101_subset<br>
(<br>
zip_url<br>
,<br>
num_classes<br>
,<br>
splits<br>
,<br>
download_dir<br>
):<br>
"""<br>
Download a subset of the UFC101 dataset and split them into various parts, such as<br>
training, validation, and test.<br>
Args:<br>
zip_url: Zip URL containing data.<br>
num_classes: Number of labels.<br>
splits: Dictionary specifying the training, validation, test, etc. (key) division of data<br>
(value is number of files per split).<br>
download_dir: Directory to download data to.<br>
Return:<br>
dir: Posix path of the resulting directories containing the splits of data.<br>
"""<br>
files<br>
=<br>
list_files_per_class<br>
(<br>
zip_url<br>
)<br>
for<br>
f<br>
in<br>
files<br>
:<br>
tokens<br>
=<br>
f<br>
.<br>
split<br>
(<br>
'/'<br>
)<br>
if<br>
len<br>
(<br>
tokens<br>
)<br>
&lt;<br>
=<br>
2<br>
:<br>
files<br>
.<br>
remove<br>
(<br>
f<br>
)</p>
<h1>Remove that item from the list if it does not have a filename</h1>
<h1>files_for_class</h1>
<p>get_files_per_class<br>
(<br>
files<br>
)<br>
classes<br>
=<br>
list<br>
(<br>
files_for_class<br>
.<br>
keys<br>
())[:<br>
num_classes<br>
]<br>
for<br>
cls<br>
in<br>
classes<br>
:<br>
new_files_for_class<br>
=<br>
files_for_class<br>
[<br>
cls<br>
]<br>
random<br>
.<br>
shuffle<br>
(<br>
new_files_for_class<br>
)<br>
files_for_class<br>
[<br>
cls<br>
]<br>
=<br>
new_files_for_class</p>
<h1>Only use the number of classes you want in the dictionary</h1>
<h1>files_for_class</h1>
<p>{<br>
x<br>
:<br>
files_for_class<br>
[<br>
x<br>
]<br>
for<br>
x<br>
in<br>
list<br>
(<br>
files_for_class<br>
)[:<br>
num_classes<br>
]}<br>
dirs<br>
=<br>
{}<br>
for<br>
split_name<br>
,<br>
split_count<br>
in<br>
splits<br>
.<br>
items<br>
():<br>
print<br>
(<br>
split_name<br>
,<br>
":"<br>
)<br>
split_dir<br>
=<br>
download_dir<br>
/<br>
split_name<br>
split_files<br>
,<br>
files_for_class<br>
=<br>
split_class_lists<br>
(<br>
files_for_class<br>
,<br>
split_count<br>
)<br>
download_from_zip<br>
(<br>
zip_url<br>
,<br>
split_dir<br>
,<br>
split_files<br>
)<br>
dirs<br>
[<br>
split_name<br>
]<br>
=<br>
split_dir<br>
return<br>
dirs<br>
def<br>
format_frames<br>
(<br>
frame<br>
,<br>
output_size<br>
):<br>
"""<br>
Pad and resize an image from a video.<br>
Args:<br>
frame: Image that needs to resized and padded.<br>
output_size: Pixel size of the output frame image.<br>
Return:<br>
Formatted frame with padding of specified output size.<br>
"""<br>
frame<br>
=<br>
tf<br>
.<br>
image<br>
.<br>
convert_image_dtype<br>
(<br>
frame<br>
,<br>
tf<br>
.<br>
float32<br>
)<br>
frame<br>
=<br>
tf<br>
.<br>
image<br>
.<br>
resize_with_pad<br>
(<br>
frame<br>
,
*
output_size<br>
)<br>
return<br>
frame<br>
def<br>
frames_from_video_file<br>
(<br>
video_path<br>
,<br>
n_frames<br>
,<br>
output_size<br>
=<br>
(<br>
224<br>
,<br>
224<br>
),<br>
frame_step<br>
=<br>
15<br>
):<br>
"""<br>
Creates frames from each video file present for each category.<br>
Args:<br>
video_path: File path to the video.<br>
n_frames: Number of frames to be created per video file.<br>
output_size: Pixel size of the output frame image.<br>
Return:<br>
An NumPy array of frames in the shape of (n_frames, height, width, channels).<br>
"""</p>
<h1>Read each video frame by frame</h1>
<h1>result</h1>
<p>[]<br>
src<br>
=<br>
cv2<br>
.<br>
VideoCapture<br>
(<br>
str<br>
(<br>
video_path<br>
))<br>
video_length<br>
=<br>
src<br>
.<br>
get<br>
(<br>
cv2<br>
.<br>
CAP_PROP_FRAME_COUNT<br>
)<br>
need_length<br>
=<br>
1<br>
+<br>
(<br>
n_frames<br>
-<br>
1<br>
)
*
frame_step<br>
if<br>
need_length</p>
<blockquote>
<p>video_length<br>
:<br>
start<br>
=<br>
0<br>
else<br>
:<br>
max_start<br>
=<br>
video_length<br>
-<br>
need_length<br>
start<br>
=<br>
random<br>
.<br>
randint<br>
(<br>
0<br>
,<br>
max_start<br>
+<br>
1<br>
)<br>
src<br>
.<br>
set<br>
(<br>
cv2<br>
.<br>
CAP_PROP_POS_FRAMES<br>
,<br>
start<br>
)</p>
</blockquote>
<h1>ret is a boolean indicating whether read was successful, frame is the image itself</h1>
<p>ret<br>
,<br>
frame<br>
=<br>
src<br>
.<br>
read<br>
()<br>
result<br>
.<br>
append<br>
(<br>
format_frames<br>
(<br>
frame<br>
,<br>
output_size<br>
))<br>
for
_
in<br>
range<br>
(<br>
n_frames<br>
-<br>
1<br>
):<br>
for
_
in<br>
range<br>
(<br>
frame_step<br>
):<br>
ret<br>
,<br>
frame<br>
=<br>
src<br>
.<br>
read<br>
()<br>
if<br>
ret<br>
:<br>
frame<br>
=<br>
format_frames<br>
(<br>
frame<br>
,<br>
output_size<br>
)<br>
result<br>
.<br>
append<br>
(<br>
frame<br>
)<br>
else<br>
:<br>
result<br>
.<br>
append<br>
(<br>
np<br>
.<br>
zeros_like<br>
(<br>
result<br>
[<br>
0<br>
]))<br>
src<br>
.<br>
release<br>
()<br>
result<br>
=<br>
np<br>
.<br>
array<br>
(<br>
result<br>
)[<br>
...<br>
,<br>
[<br>
2<br>
,<br>
1<br>
,<br>
0<br>
]]<br>
return<br>
result<br>
class<br>
FrameGenerator<br>
:<br>
def<br>
<strong>init</strong><br>
(<br>
self<br>
,<br>
path<br>
,<br>
n_frames<br>
,<br>
training<br>
=<br>
False<br>
):<br>
""" Returns a set of frames with their associated label.<br>
Args:<br>
path: Video file paths.<br>
n_frames: Number of frames.<br>
training: Boolean to determine if training dataset is being created.<br>
"""<br>
self<br>
.<br>
path<br>
=<br>
path<br>
self<br>
.<br>
n_frames<br>
=<br>
n_frames<br>
self<br>
.<br>
training<br>
=<br>
training<br>
self<br>
.<br>
class_names<br>
=<br>
sorted<br>
(<br>
set<br>
(<br>
p<br>
.<br>
name<br>
for<br>
p<br>
in<br>
self<br>
.<br>
path<br>
.<br>
iterdir<br>
()<br>
if<br>
p<br>
.<br>
is_dir<br>
()))<br>
self<br>
.<br>
class_ids_for_name<br>
=<br>
dict<br>
((<br>
name<br>
,<br>
idx<br>
)<br>
for<br>
idx<br>
,<br>
name<br>
in<br>
enumerate<br>
(<br>
self<br>
.<br>
class_names<br>
))<br>
def<br>
get_files_and_class_names<br>
(<br>
self<br>
):<br>
video_paths<br>
=<br>
list<br>
(<br>
self<br>
.<br>
path<br>
.<br>
glob<br>
(<br>
'<em>/</em>.avi'<br>
))<br>
classes<br>
=<br>
[<br>
p<br>
.<br>
parent<br>
.<br>
name<br>
for<br>
p<br>
in<br>
video_paths<br>
]<br>
return<br>
video_paths<br>
,<br>
classes<br>
def<br>
<strong>call</strong><br>
(<br>
self<br>
):<br>
video_paths<br>
,<br>
classes<br>
=<br>
self<br>
.<br>
get_files_and_class_names<br>
()<br>
pairs<br>
=<br>
list<br>
(<br>
zip<br>
(<br>
video_paths<br>
,<br>
classes<br>
))<br>
if<br>
self<br>
.<br>
training<br>
:<br>
random<br>
.<br>
shuffle<br>
(<br>
pairs<br>
)<br>
for<br>
path<br>
,<br>
name<br>
in<br>
pairs<br>
:<br>
video_frames<br>
=<br>
frames_from_video_file<br>
(<br>
path<br>
,<br>
self<br>
.<br>
n_frames<br>
)<br>
label<br>
=<br>
self<br>
.<br>
class_ids_for</p>
<h2>5. 3D CNN - Picassooo - 博客园</h2>
<ul>
<li>链接：https://www.cnblogs.com/picassooo/p/13458454.html</li>
<li>来源：bing</li>
<li>摘要：2020年8月8日 · 从2D卷积到3D卷积，都有什么不一样(动态图演示) 3D卷积（3D Convolution) 论文笔记：基于3D卷积神经网络的人体行为识别(3D CNN) 理解3D CNN 第一种理解方式： 视频输 …</li>
</ul>
<h3>正文（抓取，非 AI）</h3>
<p>3D CNN - Picassooo - 博客园<br>
Picassooo<br>
博客园<br>
首页<br>
新随笔<br>
联系<br>
订阅<br>
管理<br>
3D CNN<br>
从2D卷积到3D卷积，都有什么不一样(动态图演示)<br>
3D卷积（3D Convolution)<br>
论文笔记：基于3D卷积神经网络的人体行为识别(3D CNN)<br>
理解3D CNN<br>
第一种理解方式：<br>
视频输入的维度：input_C x input_T x input_W x input_H；<br>
3D卷积核的维度：<br>
C个并列的<br>
维度为 kernel_T x kernel_W x kernel_H 的卷积核；<br>
3D卷积核在T, W, H三个方向上移动。<br>
第二种理解方式：<br>
posted @<br>
2020-08-08 16:54<br>
Picassooo<br>
阅读(<br>
1839<br>
)<br>
评论(<br>
0<br>
)<br>
收藏<br>
举报<br>
刷新页面<br>
返回顶部<br>
公告<br>
博客园<br>
©  2004-2026<br>
浙公网安备 33010602011771号<br>
浙ICP备2021040463号-3</p>
<h2>6. 3D Convolutional Neural Network (3D CNN) — A Guide for …</h2>
<ul>
<li>链接：https://www.neuralconcept.com/post/3d-convolutional-neural-network-a-guide-for-engineers</li>
<li>来源：bing</li>
<li>摘要：4 天之前 · Discover how 3D convolutional neural networks (3D CNN) enable AI to learn 3D CAD shapes and transform product design in engineering.</li>
</ul>
<h3>正文（抓取，非 AI）</h3>
<p>3D Convolutional Neural Network (3D CNN) — A Guide for Engineers | Neural Concept<br>
Download<br>
Name<br>
Company name<br>
Company email<br>
Thank you!<br>
Our manager will contact You shortly.<br>
Homepage<br>
Oops! Something went wrong while submitting the form.<br>
Download<br>
Email<br>
Thank you!<br>
Download the Industry Briefing<br>
Oops! Something went wrong while submitting the form.<br>
This site uses cookies to optimize the user experience.<br>
Accept<br>
CES 2026 :<br>
Meet our leadership team from Jan 6 to 9 in Las Vegas to<br>
unlock Engineering impact beyond AI hype.<br>
Request a demo<br>
content<br>
Preliminary Case History: Better Competition with 3D Convolutional Neural Network<br>
Learning - in Humans and Artificial Agents<br>
What Are Classification and Regression?<br>
Deep Learning and 3D CNN Architecture<br>
What is the Goal of Deep Learning?<br>
Training the Neural Network<br>
The Building Brick: The Artificial Neuron<br>
How Does a 3D Convolutional Neural Network Work?<br>
What is a gCNN?<br>
What are Euclidean and Non-Euclidean Distances?<br>
Resources<br>
3D Convolutional Neural Network (3D CNN) — A Guide for Engineers<br>
Blog<br>
content<br>
Preliminary Case History: Better Competition with 3D Convolutional Neural Network<br>
Learning - in Humans and Artificial Agents<br>
What Are Classification and Regression?<br>
Deep Learning and 3D CNN Architecture<br>
What is the Goal of Deep Learning?<br>
Training the Neural Network<br>
The Building Brick: The Artificial Neuron<br>
How Does a 3D Convolutional Neural Network Work?<br>
What is a gCNN?<br>
What are Euclidean and Non-Euclidean Distances?<br>
A<br>
3D Convolutional Neural Network (3D CNN)<br>
is a deep learning architecture that extends the concept of pattern recognition from two dimensional data to three-dimensional inputs. Instead of processing data in height and width only (like 2D CNNs), 3D CNNs operate over<br>
height × width × depth<br>
(input volume), making them ideal for volumetric data (e.g., CT scans, CAD models, CFD simulations) or time-series of images (video). This added depth dimension allows the network to detect patterns and correlations across multiple slices or frames, enabling engineers to extract richer and more context-aware features for tasks such as simulation acceleration, defect detection, and complex product design.<br>
Artificial Intelligence (AI) and Deep Learning (DL) revolutionized how we approach engineering problems. The revolution took off in computer vision and speech recognition; now it is spreading to product design. One of the most exciting progresses in DL is the Convolutional Neural Network (ConvNet). Recent research has opened the way for implementing even a 3D Convolutional Neural Network. 3D CNNs are key enablers for the revolution in engineering; empowering product design engineers with high-end simulation capability. SO 3D CNNs are an extension of traditional Convolutional Neural Networks, designed to handle data with spatial dimensions including height, width, and depth.<br>
‍<br>
‍<br>
‍<br>
What about introducing a 3D convolutional neural network in your business?<br>
‍<br>
In this article, we will introduce the concept of 3D CNNs and explore its business relevance, particularly in fields like medical imaging . We will start with a case from the automotive industry.<br>
In this article you will learn:<br>
Preliminary Case History: Better Competition with 3D Convolutional Neural Network<br>
How to Support Product Design?<br>
Can My Company Use AI to Predict Designs?<br>
Why 3D Convolutional Neural Networks Matter for Engineers<br>
Learning - in Humans and Artificial Agents<br>
Training Humans<br>
Introduction to Deep Learning Training<br>
Technical Aspects of DL Training<br>
Predicting with DL<br>
Prediction &amp; Regression Speed<br>
What Are Classification and Regression?<br>
Practical Examples: Cats Vs Dogs &amp; House Prices<br>
Classification and Regression in ML<br>
Deep Learning and 3D CNN Architecture<br>
How Neural Networks Learn<br>
aaaa<br>
The Goal of Deep Learning<br>
3D CNNs: Classification<br>
3D CNNs: Regression<br>
Training the Neural Network<br>
Training: Loss Fuctions<br>
Training: Loss Gradient<br>
The Building Brick: The Artificial Neuron<br>
Model for the Single Physical Neuron<br>
The Artificial Neuron<br>
Building Blocks of the Artificial Neuron<br>
Core Operation in the Artificial Neuron<br>
From a Single Neuron to a Neural Network<br>
Neural Network Training<br>
Neural Network Training Data<br>
Loss Functions<br>
How Does a 3D Convolutional Neural Network Work?<br>
Layers &amp; Feature Maps<br>
Max-Pooling Layers<br>
Fully Connected Layers<br>
Convolution in Detail<br>
The Mathematics of Convolution<br>
Application to Image Processing<br>
Advantages of Convolution<br>
What is a gCNN?<br>
Advantages - gCNN<br>
More on gCNN<br>
What are Euclidean and Non-Euclidean Distances?<br>
Euclidean Distance<br>
Non-Euclidean Distance<br>
Cat Example<br>
3D Convolutional Neural Network Applications<br>
Medical Imaging<br>
Video Classification<br>
Autonomous Driving and Image Classification<br>
Geoscience<br>
Material Science<br>
3D CAD Models<br>
We will explain basic concepts of this technology for readers without an AI background, progressing to advanced applications, including training and validation sets, as well as model building . The goal is to understand how neural networks enable AI to “learn” 3D CAD shapes.<br>
The learning process aims to create a predictive model that forecasts engineering behavior, like a car’s aerodynamics, using related data. By the end, engineers will grasp this technology’s basics and be equipped to influence their organization positively in product design.<br>
Preliminary Case History: Better Competition with 3D Convolutional Neural Network<br>
This case involves Tier 1 suppliers providing automotive parts to OEMs, based on the author’s experience as an engineering director across four lines: passenger cars, luxury, trucks, and aftermarket. The process begins with an OEM’s RFQ to multiple suppliers, competing from design to manufacturing. Recently, OEMs and suppliers shifted from mainly using product simulation to fix issues to comparing design options and exploring solutions.<br>
Two major challenges with traditional, high-fidelity simulation (CAE) are still in 2025:<br>
how to<br>
make simulation technology accessible<br>
to all engineers without extensive infrastructure and training<br>
how can we enable all engineers to<br>
interactively explore several design options<br>
?<br>
An early engineering predictive capability accessible to R&amp;D, product design, and sales teams could give sales engineers a competitive edge in RFQs and customer relationships.<br>
Next, we review tools that provide evidence that products are innovative, cost-effective, and meet performance targets.<br>
‍<br>
How to Support Product Design<br>
The most effective way to support product design is to combine engineering expertise with intelligent, data-driven tools capable of tasks such as image segmentation to<br>
predict performance and verify compliance<br>
before a single part is manufactured.<br>
How can we demonstrate that a product complies with targets and constraints before manufacturing?<br>
There are three possible approaches:<br>
The first strategy is copying previous product designs, but this isn’t sustainable long-term due to issues like win-lose dynamics, mistrust in performance, and poor regulatory adaptation.<br>
The second involves costly, time-consuming testing, prototyping, and CAE simulations, requiring significant resources and investment amid fierce competition.<br>
The third, most innovative approach, uses DL to recycle solutions into predictive models. This enables organizations to demonstrate product capabilities quickly with real-time, deployable data-driven neural network models.<br>
Once a real-time solution is available, it is immediate to consider the next step of implementing it in an<br>
iterative design process<br>
,<br>
thus reaching your product’s desired objectives faster, such as reduced weight, better heat dissipation, and better energy efficiency. The iterative design process can be automated and operated by humans or embedded in a<br>
generative design<br>
approach.<br>
‍<br>
Can My Company Use AI to Predict Designs?<br>
Entering the world of AI-driven simulation for product design is not as complicated as it may seem.<br>
The two key ingredients you need to get started are a business case and data:<br>
A<br>
business case<br>
is simply the need</p>
<h2>7. 3D CNN技术深度解析及其在3D物体识别中的应用-百度开发 ...</h2>
<ul>
<li>链接：https://developer.baidu.com/article/detail.html?id=3407282</li>
<li>来源：bing</li>
<li>摘要：2024年11月29日 · 本文深入探讨了3D卷积神经网络（3D CNN）的概念、结构及其在3D物体识别领域的应用。通过对比分析2D CNN与3D CNN的区别，揭示了3D CNN在处理具有时间或深度 …</li>
</ul>
<h3>正文（抓取，非 AI）</h3>
<p>3D CNN技术深度解析及其在3D物体识别中的应用-百度开发者中心<br>
推荐<br>
云原生<br>
文心快码 Baidu Comate<br>
飞桨PaddlePaddle<br>
人工智能<br>
超级链<br>
数据库<br>
百度安全<br>
物联网<br>
开源技术<br>
云计算<br>
大数据<br>
开发者<br>
企业服务<br>
更多内容<br>
千帆大模型平台<br>
客悦智能客服<br>
3D CNN技术深度解析及其在3D物体识别中的应用<br>
作者：<br>
菠萝爱吃肉<br>
2024.11.29 21:39<br>
浏览量：<br>
192<br>
简介：<br>
本文深入探讨了3D卷积神经网络（3D CNN）的概念、结构及其在3D物体识别领域的应用。通过对比分析2D CNN与3D CNN的区别，揭示了3D CNN在处理具有时间或深度信息的三维数据时的优势。同时，结合具体算法和实例，阐述了3D CNN在识别和定位3D对象中的实际效果。<br>
在当今的计算机视觉领域，三维卷积<br>
神经网络<br>
（3D CNN）正逐渐成为处理和分析三维数据的重要工具，特别是在3D物体识别方面展现出巨大的潜力。本文将深入探讨3D CNN的基本概念、结构特点及其在3D物体识别算法中的应用，通过对比分析，揭示3D CNN相比于传统二维卷积神经网络（2D CNN）的优势。<br>
一、3D CNN的基本概念与结构<br>
三维卷积神经网络（3D CNN）是一种<br>
深度学习<br>
模型，它能够处理<br>
视频<br>
、医学图像等具有时间或深度信息的三维数据。与2D CNN不同，3D CNN采用三维卷积核来处理三维数据，卷积核在三个方向上移动并执行卷积操作，从而捕捉三维数据中的空间和时间/深度特征。<br>
3D CNN通常由卷积层、池化层、批量归一化层和全连接层组成。卷积层和池化层可以有效地减少数据维度并提取特征，批量归一化层可以加速收敛和提高模型的泛化能力，而全连接层则将特征映射到具体的输出类别。<br>
二、3D CNN与2D CNN的对比分析<br>
在处理二维图像时，2D CNN表现出色，但在处理具有时间或深度信息的三维数据时，其性能受限。相比之下，3D CNN具有以下显著优势：<br>
时空特征捕捉<br>
：3D CNN可以对连续帧的视频数据进行处理，理解视频中的运动和动态变化，对于视频分类、动作识别等任务具有明显优势。<br>
多通道数据处理<br>
：3D CNN可以在一个模型中同时处理多个通道的数据，如RGB和深度数据，将不同的数据类型结合在一起进行处理。<br>
特征提取能力<br>
：3D CNN利用3D卷积核进行卷积操作，可以提取出空间上更加丰富的特征，从而提高模型的准确性。<br>
三、3D CNN在3D物体识别中的应用<br>
在3D物体识别领域，3D CNN的应用主要体现在以下几个方面：<br>
点云数据处理<br>
：在激光雷达（LiDAR）点云物体检测中，3D CNN能够处理点云数据，提取出物体的三维特征，实现精确的3D物体检测和定位。例如，VoxelNet和PointNet++等代表性方法，在点云物体检测领域取得了显著成果。<br>
视频分析<br>
：在视频分析中，3D CNN能够捕捉视频中的时空特征，用于视频分类、动作识别等任务。通过提取视频中的连续帧信息，3D CNN可以实现对视频中物体的精确识别和跟踪。<br>
医学影像分析<br>
：在医学影像分析领域，3D CNN能够处理医学图像数据，提取出病变部位的三维特征，辅助医生进行疾病诊断和治疗。例如，在肺部CT图像分析中，3D CNN可以实现对肺结节的精确检测和分类。<br>
四、具体算法与实例<br>
以pcl_recognition模块为例，该模块利用相关组算法对从3D描述器算法中提取的特征点进行聚类，将当前的场景与模型进行匹配。对于每一次聚类，描绘出一个在场景中的可能模型实例，并输出标识6DOF位姿估计的转换矩阵。这种方法在3D物体识别中取得了良好的效果。<br>
此外，在3D<br>
人脸识别<br>
等应用中，3D CNN也展现出巨大的潜力。传统2D人脸识别由于无法记录脸部的深度三维信息，存在<br>
安全<br>
隐患。而3D CNN能够提取人脸的三维特征，实现更加准确和安全的人脸识别。<br>
五、结论与展望<br>
综上所述，3D CNN在3D物体识别领域具有广泛的应用前景和巨大的潜力。随着深度学习技术的不断发展和计算能力的不断提升，3D CNN将在更多领域发挥重要作用。未来，我们可以期待3D CNN在自动驾驶、机器人视觉、医学影像分析等领域取得更加显著的成果。<br>
同时，为了进一步提高3D CNN的性能和泛化能力，我们需要不断探索新的网络结构、优化算法和训练策略。此外，结合其他先进技术如传感器融合、强化学习等，也将为3D CNN的应用拓展新的可能性。在选择相关技术平台时，<br>
千帆<br>
大模型开发<br>
与服务平台<br>
凭借其强大的模型开发能力，能够为3D CNN的研究与应用提供有力支持。<br>
相关文章推荐<br>
文心一言接入指南：通过百度智能云千帆大模型平台API调用<br>
本文介绍了如何通过百度智能云千帆大模型平台接入文心一言，包括创建千帆应用、API授权、获取访问凭证及调用API接口的详细流程。文心一言作为百度的人工智能大语言模型，拥有强大的语义理解与生成能力，通过千帆平台可轻松实现多场景应用。<br>
十万个为什么<br>
2023.10.20 16:56<br>
257262<br>
19<br>
10<br>
从 MLOps 到 LMOps 的关键技术嬗变<br>
本文整理自  QCon 全球软件开发大会 -从 MLOps 到 LMOps 分论坛的同名主题演讲<br>
百度智能云开发者中心<br>
2023.11.15 18:03<br>
34866<br>
9<br>
5<br>
Sugar BI教你怎么做数据可视化 - 拓扑图，让节点连接信息一目了然<br>
Sugar BI教你怎么做数据可视化 - 拓扑图，让节点连接信息一目了然<br>
百度智能云开发者中心<br>
2023.03.21 10:56<br>
30805<br>
3<br>
1<br>
更轻量的百度百舸，CCE Stack 智算版发布<br>
百度百舸·AI 异构计算平台，是百度智能云将百度内部强大的 AI 工程能力面向市场推出的解决方案。<br>
百度智能云开发者中心<br>
2023.03.02 12:17<br>
26679<br>
1<br>
1<br>
打造合规数据闭环，加速自动驾驶技术研发<br>
今天跟大家的演讲主题，主要是想交流如何去构建这样两个自动驾驶的数据闭环链路。<br>
百度智能云开发者中心<br>
2023.03.02 15:00<br>
28144<br>
0<br>
1<br>
LMOps 工具链与千帆大模型平台<br>
LMOps 相关的概念以及关键技术<br>
百度智能云开发者中心<br>
2023.11.17 15:49<br>
24325<br>
3<br>
3<br>
发表评论<br>
登录后可评论，请前往<br>
登录<br>
或<br>
注册<br>
评 论<br>
开发者关注产品榜<br>
1<br>
百度千帆·大模型服务及Agent开发平台<br>
企业级一站式大模型开发及服务平台<br>
模型训练限时免费<br>
2<br>
百度千帆·数据智能平台<br>
一站式多模态数据管理、加工和分析应用平台<br>
平台体验全免费<br>
3<br>
秒哒-生成式应用开发平台<br>
不用写代码，就能实现任意想法<br>
全功能免费体验<br>
4<br>
百度智能云客悦智能客服平台<br>
大模型重塑营销与客服体验<br>
0元试用一个月<br>
最热文章<br>
零基础调用文心大模型4.5API实操手册<br>
生产力UP！文心快码 Rules 功能实战指南<br>
Redis 数据恢复的月光宝盒，闪回到任意指定时间<br>
用文心快码Zulu打造太阳系3D模拟器：从需求到落地的全流程实践<br>
关于作者<br>
被阅读数<br>
被赞数<br>
被收藏数<br>
关 注<br>
活动<br>
咨询</p>
<h2>8. 图神经网络：方法与应用综述</h2>
<ul>
<li>链接：https://www.zhihu.com/tardis/bd/art/413648055</li>
<li>来源：bing</li>
<li>摘要：2023年2月27日 · 由于它的卓越的表现，最近GNN在图分析方法中的应用越来越广泛。 在下面的段落中，我们将说明图神经网络的原始灵感。 GNN的第一个灵感源于悠久的历史，第一次尝试将神经网络 …</li>
</ul>
<h3>正文（抓取，非 AI）</h3>
<p>摘要<br>
图数据（graph data）中包含了元素之间丰富的信息，很多任务都是需要处理图数据。比如：对物理系统建模、学习分子指纹、预测蛋白质界面、对疾病分类都是以图数据作为输入。在其他领域比如：从文本和图像这些非结构化数据中学习、对抽取出来的结构（比如句子的依赖树和图像的场景图）进行推理都是非常重要的研究课题，都需要用到图推理模型（graph reasoning models）。图神经网络（Graph neural networks (GNNs)）是通过图中结点的信息传递（message passing）来捕获图的依赖关系的神经网络模型。近年来，许多GNN的变体，例如：图卷积神经网络（graph convolutional network (GCN)）、图注意力网络（graph attention network (GAT)）、图循环网络（graph recurrent network (GRN)）在很多深度学习任务上取得了突破性的成绩。本研究中，我们总结了GNN模型的通用设计流程（general design pipeline）并讨论每一个GNN变体，还系统地对应用进行了分类，最后针对未来的研究方向提出了四个问题。<br>
1.Introduction<br>
图（Graphs）是一种对物体（objects）和他们之间的关系（relationships）建模的数据结构，物体以结点（nodes）表示，关系以边（edges）表示。最近，使用机器学习（machine learning）对图进行分析的研究受到越来越多的关注。主要是由于图具有强大的表示能力，即，图可以用来分析超大规模跨领域的问题，比如社会科学（社会网络（social networks））、自然科学（物理系统（physical systems）、蛋白质相互作用）、知识图谱等。图分析（graph analysis）作为一种机器学习中独特的非欧几里得几何学的数据结构，主要的研究任务是节点分类（node classifification）、连接预测（link prediction）和聚类（clustering）。GNN是应用在图领域的深度学习方法。由于它的卓越的表现，最近GNN在图分析方法中的应用越来越广泛。在下面的段落中，我们将说明图神经网络的原始灵感。<br>
GNN的第一个灵感源于悠久的历史，第一次尝试将神经网络应用在图上。在90年代，RNN被首次应用在有向无环图上（1997）。后来又引入了前馈神经网络来处理环路（2009）。这些方法虽然取得了成功，但其普遍思想是在图上构建状态转换系统（state transition systems），并迭代直到收敛，这限制了可扩展性（extendability）和表示能力（representation）。深度学习的最新进展（尤其是LeCun的CNN，1998）让我们重新审视GNN。CNN具有提取多尺度局部空间特征（ multi-scale localized spatial features）并将其组合构建出具有良好表达的能力，这使得几乎所有机器学习领域都取得了突破，开启了深度学习的新时代。CNN的核心在于局部连接（local connection）、权值共享（shared weights）和多层（multiple layers）。这也是解决图问题的关键。然而，CNN只能处理常规的欧几里得几何学数据，比如：图像（2D grids）和文本（1D sequences），这些都可以视为一种图。因此，可以直接将CNN广义化成图。<br>
如图1所示，我们很难将欧式（Euclidean）空间转到非欧式空间（non-Euclidean），因为不容易定义卷积核和池化核。将深度神经模型扩展到非欧几里得领域，即几何深度学习（geometric deep learning），已经成为一个新兴的研究领域（2017）。在这个术语下，图的深度学习受到了极大的关注。<br>
另一个灵感来自图表示学习（graph representation learning），它学习用低维向量表示图节点、边或子图。在图分析领域，传统的机器学习方法通常依靠人工提取特征，不仅灵活性差而且代价高。2013年Word2vec在表示学习中取得了成功，借鉴它的经验，DeepWalk于2014年被研发出来，并被视为第一个图嵌入方法（graph embedding）。DeepWalk是在随机游走数据上应用的SkipGram算法建模的。相似的算法例如node2vec（2016）、LINE（2015）和TADW（2015）也取得了突破。然而，这些方法有两个严重的缺点。首先，编码器中节点之间不共享参数，导致计算效率低下，因为这意味着参数的数量随着节点的数量线性增长。其次，直接嵌入方法缺乏泛化能力，不能处理动态图，也不能泛化到新图。<br>
基于CNN和图嵌入，许多GNN的变体被提出，来共同聚合来自图结构中的信息。因此，它们能对输入 和/或 输出进行建模（对结点建模、对边建模）。<br>
关于图神经网络，已有一些综述。但是，他们主要关注在图上定义的卷积运算（convolution operators），而我们研究了GNN中的其他计算模块，如跳过连接（skip connections ）和池操作（pooling operators）。<br>
也有一些研究对GNN做了一些分类。我们的论文提供了一个不同的分类，我们主要集中在经典的GNN模型。另外，我们总结了不同类型的图的变体，并提供了详细的在不同应用领域的GNN的应用。<br>
还有一些研究致力于一些特殊的图学习领域。我们总结了异构图（heterogeneous graphs）、动态图（dynamic graphs）和组合优化（combinatorial optimization）的GNN。<br>
在本文中，我们提供了一个全面的对不同的图神经网络模型的概述，以及一个非常系统的图应用的分类。总而言之，我们的贡献是：<br>
我们对现有的图神经网络模型进行了详细的回顾。我们提出了一个通用的设计流程，并讨论了每个模块的变体。本文还介绍了GNN模型的理论研究和实证分析。<br>
我们系统地将应用进行分类，分成了结构化（structural）场景和非结构化（non-structural）场景。我们展示了几个主要的应用和他们在不同场景下对应的方法。<br>
我们提出了未来研究的四个开放问题。我们对每个问题提供了全面的分析并提出了未来的研究方向。<br>
本研究的剩余部分的组织如下。第2节，展示了通用GNN的设计流程（general GNN design pipeline）。顺着这个设计流程，第3~6节，我们细致地讨论GNN变体的每个步骤。第7节，我们重新审视了GNN的理论和实践研究现状。第8节我们引入了几个重要的GNN应用，包括结构化场景、非结构化场景和其他场景。第9节我们提出了4个关于GNN的开放问题和未来研究方向。第10节是我们对本调查的总结。<br>
2. General design pipeline of GNNs<br>
本文，我们从一个设计师的视角介绍GNN模型。本节，我们首先展示了通用GNN设计流程。在随后的3、4、5节我们分别对每一步骤给出细致的解释，例如选择计算模块（selecting computational modules）、考虑图类型和规模（considering graph type and scale）和设计损失函数（designing loss function）。最后在第6节使用一个例子来展示GNN对具体任务的设计过程。<br>
在之后的节中，我们把图记为G=(V,E)，其中|V|=N表示图中的节点数，<br>
表示边的条数。<br>
是邻接矩阵（adjacency matrix）。对于图表示学习（graph representation learning），我们使用向量<br>
和向量<br>
表示结点v的隐藏状态（hidden state）和输出向量（output vector）。更多细节的符号见图表1.<br>
本节，我们在具体的图类型（graph type）和具体的任务（specifific task）上介绍通用GNN模型的设计流程。概括来说，流程分为四步：（1）找到图结构（fifind graph structure）、（2）具体化图类型和规模（specify graph type and scale）、（3）设计损失函数（design loss function）和（4）使用计算模块构建模型（build model using computational modules）。本节会给出通用的设计流程和一些背景知识。我们来看每一步的细节：<br>
2.1. Find graph structure<br>
首先，我们要弄清楚应用中的图结构。通常有两种场景：结构化场景（structural scenarios）和非结构化场景（non-structural scenarios）。在结构化场景中，在应用中的图结构是明确的，比如在分子化学、物理系统和知识图谱等方面。在非结构化场景中，图是不明确的，因此我们首先要根据任务构建图（build the graph from the task），比如在文本领域构建一个全连接的“word”图或根据图像构建一个情景图（scene graph）。当我们确定了图结构，之后的设计就是在这个图上对GNN进行优化。<br>
2.2. Specify graph type and scale<br>
当我们确定了应用中的图结构之后，我们就要弄清楚图的类型（graph type）和他的规模（scale）。<br>
复杂类型的图可以在节点（nodes）和连接（connections）上携带更多的信息。通常图可以有如下分类：<br>
有向/无向图<br>
（Directed/Undirected Graphs）。有向图里的边（Edges）是从一个节点指向另一个节点，比无向图携带更多的信息。无向图中的边也可以视为双向的边。<br>
同构/异构图<br>
（Homogeneous/Heterogeneous Graphs）。同构图中的节点和边拥有相同的类型；异构图中节点和边是不同类型的。节点和边的类型在异构图中起着重要的作用，需要进一步考虑。<br>
静态/动态图<br>
（Static/Dynamic Graphs）。如果输入特征或图的拓扑结构随着时间变化，就是动态图。在动态图中应该仔细考虑时间信息。<br>
注意，这些类型是正交的（orthogonal），意味着这些类型可以组合，比如你可以采用动态有向异构图（ dynamic directed heterogeneous graph）。还有一些为不同任务设计的其他类型的图结构，比如超图（hypergraphs）和符号图（signed graphs）。虽然我们这里不会枚举出所有的类型，但是主要这些图的携带的最主要的信息还是会兼顾的。一旦我们确定了图类型，图类型背后的附加信息会在未来的设计过程进行考虑。<br>
至于图的规模，小图和大图没有一个明确的划分标准。划分标准仍然随着计算设备的升级而改变（比如GPU的运行速度）。本文中，当邻接矩阵（adjacency matrix）或图的拉普拉斯图（graph Laplacian of a graph）（空间复杂度是O(n^2)）在设备上无法存储和处理时，我们就认为该图是大图（large-scale graph），并研究着使用一些采样方法（sampling methods）。<br>
2.3. Design loss function<br>
在这一步我们基于任务类型（task type）和训练配置（training setting）设计损失函数（ loss function）。<br>
对于图学习任务，通常有三类任务：<br>
节点级别<br>
（Node-level ）的任务重心在节点上，包括节点分类（node classifification）、节点回归（node regression）和节点聚类（node clustering）等等。节点分类试图把节点分为几个特定的类别；节点回归试图为节点预测一个连续值（continuous value）。节点聚类试图把节点划分为几个不相交的组，组内的节点应该相似。<br>
边级别<br>
（Edge-level）的任务是边的分类（edge classifification）和链接预测（link prediction），需要模型对边的类型分类 或者 预测两个节点之间是否有边。<br>
图级别<br>
（Graph-level）的任务包括图分类（graph classifification）、图回归（graph regression）和图匹配（graph matching），所有的技术都需要学习图的表示。<br>
从监督的视角，我们能将图学习任务分为三个不同的训练配置：<br>
监督配置<br>
（Supervised setting）需要提供带标签的训练数据（labeled data）。<br>
半监督配置<br>
（Semi-supervised setting）给出少量的带标签训练数据和大量的无标签的训练数据。在测试阶段，<br>
直推式配置<br>
（transductive setting）需要模型预测那些之前给定的无标签节点，而<br>
归纳式配置<br>
（inductive setting）将会从同分布的数据中抽取新的无标签的节点进行预测。大多数节点和边的分类任务是半监督的。<br>
无监督配置<br>
（Unsupervised setting）仅提供无标签数据，让模型挖掘数据模式。节点聚类是典型的无监督学习任务。<br>
有了任务类型和训练配置，我们就能为任务设计一个具体的损失函数。例如，对于一个节点级别的半监督分类任务，交叉熵损失（cross-entropy loss）可以应用在训练集的有标签的节点上。<br>
2.4. Build model using computational modules<br>
最后，我们可以使用计算模块来构建模型。一些常用的计算模块有：<br>
传播模块<br>
（Propagation Module）。传播模块可以让节点之间传递信息，聚合后的信息可以捕获特征和拓扑信息。在传播模块，<br>
卷积运算<br>
（convolution operator）和<br>
递归运算<br>
（recurrent operator）通常被用来聚合邻居的信息；而<br>
跳跃连接操作<br>
（skip connection operation）用来从历史的节点表示中收集信息，并缓解过渡平滑问题（over-smoothing problem）。过渡平滑问题可以参考：<br>
图神经网络（GCN）中的过度平滑（over-smooth）问题以及 multi-hops解决思路<br>
采样模块<br>
（Sampling Module）。当图非常大的时候，采样模块通常需要被使用，采样模块通常和传播模块一起使用。<br>
池化模块<br>
（Pooling Module）。当我们需要高层级的子图或图的表示的时候，池化模块会被用来从节点中提取信息。<br>
典型的GNN模型通常是上述计算模块的组合。典型的GNN模型结构如图2的中间部分所示，卷积运算、池化操作、采样模块和跳跃连接被使用在层之间传递信息，然后加一个池化层抽取高层次的信息。这些层通常被堆叠（stack）以获得更好的表示。注意，这个架构可以广义化大部分GNN模型，当然也有一些例外，比如NDCN。<br>
GNN的通用设计流程如图2所示。第3节，我们首先给出现有的计算模型实例。第4节，考虑图类型和规模介绍心有的GNN变体。第5节，调研不同变体的训练配置。这些部分分别对应图2中的步骤4、步骤2和步骤3。第6节，我们给出一个具体的设计实例。<br>
3. Instantiations of computational modules<br>
该节中，我们主要介绍现有的三种计算模块（传播模块、采样模块和池化模块）的实例。在3.1、3.2、3.3节我们分别介绍传播模块的三个子组件（sub-components），即卷积运算、递归运算和跳跃连接。然后3.4、3.5节介绍采样模块和池化模块。计算模块的概览如图3所示：<br>
3.1. Propagation modules - convolution operator<br>
本节介绍了GNN模型中通常使用的卷积运算。卷积运算的主要思想是将其他领域的卷积广义化到图领域。这个方向的进展通常分为两个方向：基于谱分解的方法（ spectral approaches）和基于空间结构的方法（spatial approaches）<br>
◆ 3.1.1. Spectral approaches<br>
谱方法使用图的谱表示（spectral representation）。这些方法的理论基础是图信号处理（graph signal processing），并在谱域（spectral domain）定义卷积运算。<br>
在谱方法中，一个图信号（graph signal）<br>
x<br>
首先通过图傅里叶变换<br>
（graph Fourier transform）转换到谱域，然后进行卷积运算。卷积之后的结果信号可以使用图傅里叶逆变换<br>
（inverse graph Fourier transform）转换回来。这些操作定义为：<br>
其中矩阵<br>
U<br>
是矩阵<br>
L<br>
的特征向量矩阵，<br>
是归一化的图拉普拉斯矩阵（normalized graph Laplacian），（<br>
D<br>
是度矩阵（degree matrix），<br>
A<br>
是图的邻接矩阵）。归一化的图拉普拉斯矩阵是实对称半正定的，因此他可以分解为<br>
（<br>
是特征值的对角矩阵）。基于卷积的理论，卷积操作的定义如下：<br>
其中<br>
是频域的过滤器（filter）。如果我们使用可学习的对角矩阵<br>
简化过滤器，那么就有了在谱域的基本方程：<br>
接下来我们介绍几个典型的谱域方法，他们就是设计了不同的过滤器<br>
。<br>
----Spectral Network<br>
Spectral Network（2014）使用了一个可学习的对角矩阵作为过滤器，<br>
，其中参数<br>
。然而，这个运算是低效的，并且过滤器是非本地结构化的（non-spatially localized）<br>
----ChebNet<br>
ChebNet（Hammond et al. (2011)）认为<br>
可以用切比雪夫多项式的K阶截断展开式来近似。Defferrard et al. (2016)基于这个理论提出了ChebNet。他的操作可以写成：<br>
----GCN<br>
Kipf and Welling (2017)简化了公式4中的卷积操作，让K=1避免了过拟合问题。<br>
----AGCN<br>
自适应图卷积网络(AGCN)被提出用来学习潜在的关系(Li et al.， 2018a)。AGCN学习一个残差图拉普拉斯矩阵，并将其添加到原始拉普拉斯矩阵中。结果表明，该方法在多个图结构数据集中是有效的。<br>
----DGCN<br>
提出了对偶图卷积网络(DGCN) (Zhuang and Ma, 2018)，共同考虑图的局部一致性和全局一致性。它使用两个卷积网络来捕获局部一致性和全局一致性，并采用无监督损失来集成它们。第一个卷积网络与Eq.(7)相同，第二个网络用正点向互信息(PPMI)矩阵代替邻接矩阵:<br>
----GWNN<br>
图小波神经网络(GWNN) (Xu et al.， 2019a)采用图小波变换代替图傅立叶变换。该方法具有以下优点:(1)无需分解矩阵即可快速得到图小波;(2)图小波是稀疏和局部化的，因此结果更好，更可解释。在半监督节点分类任务上，GWNN优于多种谱方法。<br>
AGCN和DGCN试图从图拉普拉斯增广的角度对谱方法进行改进，而GWNN则取代了傅里叶变换。总之，光谱</p>
<h2>9. 图卷积网络（GCN）入门详解</h2>
<ul>
<li>链接：https://www.zhihu.com/tardis/zm/art/107162772</li>
<li>来源：bing</li>
<li>摘要：2020年3月5日 · 图卷积网络（GCN）入门详解 什么是GCN GCN 概述 模型定义 数学推导 Graph Laplacian ref 图神经网络领域算是一个比较新的领域，有非常多的探索潜力，所以我也一直想着要入 …</li>
</ul>
<h3>正文（抓取，非 AI）</h3>
<dl>
<dt>图卷积网络（GCN）入门详解</dt>
<dt>什么是GCN</dt>
<dt>GCN 概述</dt>
<dt>模型定义</dt>
<dt>数学推导</dt>
<dt>Graph Laplacian</dt>
<dt>ref</dt>
<dt>图神经网络领域算是一个比较新的领域，有非常多的探索潜力，所以我也一直想着要入门。其中图卷积网络就非常热门，我找到了一篇教程：</dt>
<dt>图卷积网络(GCN)新手村完全指南</dt>
<dt>, 想着借此走出新手村，结果因为个人资质有限，简直成了劝退文，看完了之后还是没有非常的明白，所以决定自己写一篇入门介绍。当然这篇文章介绍得非常好，我也参考了很多，所以我会引用很多其中的公式，加上相关的推导补充。</dt>
<dt>本文主要分为两部分，第一部分介绍什么是GCN，第二部分将进行详细的数学推导。</dt>
<dt>什么是GCN</dt>
<dt>GCN 概述</dt>
<dt>本文讲的GCN 来源于论文：</dt>
<dt>SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS</dt>
<dt>，这是在GCN领域最经典的论文之一。</dt>
<dt>我们可以根据这个GCN的图看到，一个拥有</dt>
<dt>个input channel的graph作为输入，经过中间的hidden layers，得到</dt>
<dt>个 output channel的输出。</dt>
<dt>图卷积网络主要可以由两个级别的作用变换组成：</dt>
<dt>注意本文讲的图都特指无向无权重的图。</dt>
<dt>graph level：</dt>
<dt>例如说通过引入一些形式的pooling 操作 (see, e.g.</dt>
<dt>Duvenaud et al</dt>
<dt>., NIPS 2015). 然后改变图的结构。但是本次讲过GCN并没有进行这个级别的操作。所以看到上图我们的网络结构的输出和输出的graph的结构是一样的。</dt>
<dt>node level：</dt>
<dt>通常说node level的作用是不改变graph的结构的，仅通过对graph的特征/信号（features/signals）</dt>
<dt>作为输入：一个</dt>
<dt>的矩阵(</dt>
<dd>
<dl>
<dt>输入图的nodes的个数，</dt>
<dt>输入的特征维度)  ，得到输出</dt>
<dt>：一个</dt>
<dt>的矩阵(</dt>
<dt>输出的特征维度)。</dt>
<dt>a) 一个特征描述（feature description）</dt>
<dd>指的是每个节点<br>
的特征表示<br>
b) 每一个graph 的结构都可以通过邻接矩阵<br>
表示（或者其他根据它推导的矩阵）<br>
我们可以很容易的根据一个邻接矩阵重构出一个graph。   例如下图：<br>
其中<br>
代表节点，<br>
代表边<br>
我们通过构造<br>
的矩阵可以得到邻接矩阵<br>
, 其中<br>
如果节点<br>
和节点<br>
相连，否则<br>
， 我们根据graph可以得到<br>
， 同理通过<br>
也可以得到graph 的结构。<br>
所以网络中间的每一个隐藏层可以写成以下的非线性函数：<br>
其中输入层<br>
, 输出层<br>
,<br>
是层数。 不同的GCN模型，采用不同<br>
函数。<br>
模型定义<br>
论文中采用的函数如下：<br>
刚开始看的时候，都会被吓到！这个函数未免也太抽象了。但是我们先了解一下它在起的作用，然后再从头一步一步引出这个公式，以及为什么它起到了这些作用。<br>
首先物理上它起的作用是，每一个节点下一层的信息是由前一层本身的信息以及相邻的节点的信息加权加和得到，然后再经过线性变换<br>
以及非线性变换<br>
。<br>
我们一步一步分解，我们要定义一个简单的<br>
函数，作为基础的网络层。<br>
我们可以很容易的采用最简单的层级传导（ layer-wise propagation ）规则<br>
我们直接将<br>
做矩阵相乘，然后再通过一个权重矩阵<br>
做线性变换，之后再经过非线性激活函数<br>
, 比如说 ReLU，最后得到下一层的输入<br>
。<br>
我们需要特别注意的是<br>
做矩阵相乘，这代表了什么意思呢？<br>
我们先看看，对于下图。<br>
假设每个节点<br>
, 那么在经过矩阵相乘之后，它会变成什么呢。<br>
输入层的<br>
, 根据矩阵的运算公式我们可以很容易地得到下一层的该节点的表示<br>
, 也很容易发现<br>
，而<br>
就是节点1的相邻节点。具体计算结果可以参考下面的代码。<br>
A<br>
=<br>
torch<br>
.<br>
tensor<br>
([<br>
[<br>
0<br>
,<br>
1<br>
,<br>
0<br>
,<br>
0<br>
,<br>
1<br>
,<br>
0<br>
],<br>
[<br>
1<br>
,<br>
0<br>
,<br>
1<br>
,<br>
0<br>
,<br>
1<br>
,<br>
0<br>
],<br>
[<br>
0<br>
,<br>
1<br>
,<br>
0<br>
,<br>
1<br>
,<br>
0<br>
,<br>
0<br>
],<br>
[<br>
0<br>
,<br>
0<br>
,<br>
1<br>
,<br>
0<br>
,<br>
1<br>
,<br>
1<br>
],<br>
[<br>
1<br>
,<br>
1<br>
,<br>
0<br>
,<br>
1<br>
,<br>
0<br>
,<br>
0<br>
],<br>
[<br>
0<br>
,<br>
0<br>
,<br>
0<br>
,<br>
1<br>
,<br>
0<br>
,<br>
0<br>
]<br>
])<br>
H_0<br>
=<br>
torch<br>
.<br>
tensor<br>
([<br>
[<br>
1<br>
,<br>
1<br>
,<br>
1<br>
,<br>
1<br>
],<br>
[<br>
2<br>
,<br>
2<br>
,<br>
2<br>
,<br>
2<br>
],<br>
[<br>
3<br>
,<br>
3<br>
,<br>
3<br>
,<br>
3<br>
],<br>
[<br>
4<br>
,<br>
4<br>
,<br>
4<br>
,<br>
4<br>
],<br>
[<br>
5<br>
,<br>
5<br>
,<br>
5<br>
,<br>
5<br>
],<br>
[<br>
6<br>
,<br>
6<br>
,<br>
6<br>
,<br>
6<br>
]<br>
])<br>
A<br>
.<br>
matmul<br>
(<br>
H_0<br>
)<blockquote>
<blockquote></blockquote>
<p>tensor<br>
([[<br>
7<br>
,<br>
7<br>
,<br>
7<br>
,<br>
7<br>
],<br>
[<br>
9<br>
,<br>
9<br>
,<br>
9<br>
,<br>
9<br>
],<br>
[<br>
6<br>
,<br>
6<br>
,<br>
6<br>
,<br>
6<br>
],<br>
[<br>
14<br>
,<br>
14<br>
,<br>
14<br>
,<br>
14<br>
],<br>
[<br>
7<br>
,<br>
7<br>
,<br>
7<br>
,<br>
7<br>
],<br>
[<br>
4<br>
,<br>
4<br>
,<br>
4<br>
,<br>
4<br>
]])<br>
所以我们直到<br>
就是把通过邻接矩阵快速的方式，快速将相邻的节点的信息相加得到自己下一层的输入。<br>
但是这就完美了吗？<br>
问题一：我们虽然获得了周围节点的信息了，但是自己本身的信息却没了（除非自己有一条边指向自己）。<br>
我们采用的解决方案是，对每个节点手动增加一条self-loop 到每一个节点，即<br>
, 其中<br>
是单位矩阵identity matrix。<br>
问题二：从上面的结果也可以看出，在经过一次的<br>
矩阵变换后，得到的输出会变大，即特征向量<br>
的scale会改变，在经过多层的变化之后，将和输入的scale差距越来越大。<br>
所以我们是否可以将邻接矩阵<br>
做归一化使得最后的每一行的加和为1，使得<br>
获得的是weighted sum。<br>
我们可以将<br>
的每一行除以行的和，这就可以得到normalized的<br>
。而其中每一行的和，就是每个节点的度degree。用矩阵表示则为：<br>
, 对于<br>
我们还是按照上面图的graph来看。<br>
import<br>
torch<br>
A<br>
=<br>
torch<br>
.<br>
tensor<br>
([<br>
[<br>
0<br>
,<br>
1<br>
,<br>
0<br>
,<br>
0<br>
,<br>
1<br>
,<br>
0<br>
],<br>
[<br>
1<br>
,<br>
0<br>
,<br>
1<br>
,<br>
0<br>
,<br>
1<br>
,<br>
0<br>
],<br>
[<br>
0<br>
,<br>
1<br>
,<br>
0<br>
,<br>
1<br>
,<br>
0<br>
,<br>
0<br>
],<br>
[<br>
0<br>
,<br>
0<br>
,<br>
1<br>
,<br>
0<br>
,<br>
1<br>
,<br>
1<br>
],<br>
[<br>
1<br>
,<br>
1<br>
,<br>
0<br>
,<br>
1<br>
,<br>
0<br>
,<br>
0<br>
],<br>
[<br>
0<br>
,<br>
0<br>
,<br>
0<br>
,<br>
1<br>
,<br>
0<br>
,<br>
0<br>
]<br>
],<br>
dtype<br>
=<br>
torch<br>
.<br>
float32<br>
)<br>
D<br>
=<br>
torch<br>
.<br>
tensor<br>
([<br>
[<br>
2<br>
,<br>
0<br>
,<br>
0<br>
,<br>
0<br>
,<br>
0<br>
,<br>
0<br>
],<br>
[<br>
0<br>
,<br>
3<br>
,<br>
0<br>
,<br>
0<br>
,<br>
0<br>
,<br>
0<br>
],<br>
[<br>
0<br>
,<br>
0<br>
,<br>
2<br>
,<br>
0<br>
,<br>
0<br>
,<br>
0<br>
],<br>
[<br>
0<br>
,<br>
0<br>
,<br>
0<br>
,<br>
3<br>
,<br>
0<br>
,<br>
0<br>
],<br>
[<br>
0<br>
,<br>
0<br>
,<br>
0<br>
,<br>
0<br>
,<br>
3<br>
,<br>
0<br>
],<br>
[<br>
0<br>
,<br>
0<br>
,<br>
0<br>
,<br>
0<br>
,<br>
0<br>
,<br>
1<br>
],<br>
],<br>
dtype<br>
=<br>
torch<br>
.<br>
float32<br>
)<br>
hat_A<br>
=<br>
D<br>
.<br>
inverse<br>
()<br>
.<br>
matmul<br>
(<br>
A<br>
)</p>
<blockquote>
<blockquote>
<p>hat_A<br>
tensor<br>
([[<br>
0.0000<br>
,<br>
0.5000<br>
,<br>
0.0000<br>
,<br>
0.0000<br>
,<br>
0.5000<br>
,<br>
0.0000<br>
],<br>
[<br>
0.3333<br>
,<br>
0.0000<br>
,<br>
0.3333<br>
,<br>
0.0000<br>
,<br>
0.3333<br>
,<br>
0.0000<br>
],<br>
[<br>
0.0000<br>
,<br>
0.5000<br>
,<br>
0.0000<br>
,<br>
0.5000<br>
,<br>
0.0000<br>
,<br>
0.0000<br>
],<br>
[<br>
0.0000<br>
,<br>
0.0000<br>
,<br>
0.3333<br>
,<br>
0.0000<br>
,<br>
0.3333<br>
,<br>
0.3333<br>
],<br>
[<br>
0.3333<br>
,<br>
0.3333<br>
,<br>
0.0000<br>
,<br>
0.3333<br>
,<br>
0.0000<br>
,<br>
0.0000<br>
],<br>
[<br>
0.0000<br>
,<br>
0.0000<br>
,<br>
0.0000<br>
,<br>
1.0000<br>
,<br>
0.0000<br>
,<br>
0.0000<br>
]])<br>
但是在实际运用中我们采用的是对称的normalization：<br>
对于<br>
这跟Laplacian Matrix 有关，下一部分会介绍。   我们可以发现<br>
D_minus_sqrt<br>
=<br>
D<br>
.<br>
inverse<br>
().<br>
sqrt<br>
()<br>
D_minus_sqrt<br>
.<br>
matmul<br>
(<br>
A<br>
).<br>
matmul<br>
(<br>
D_minus_sqrt<br>
)<br>
</p>
</blockquote>
</blockquote>
<p>tensor<br>
([[<br>
0.0000<br>
,<br>
0.4082<br>
,<br>
0.0000<br>
,<br>
0.0000<br>
,<br>
0.4082<br>
,<br>
0.0000<br>
],<br>
[<br>
0.4082<br>
,<br>
0.0000<br>
,<br>
0.4082<br>
,<br>
0.0000<br>
,<br>
0.3333<br>
,<br>
0.0000<br>
],<br>
[<br>
0.0000<br>
,<br>
0.4082<br>
,<br>
0.0000<br>
,<br>
0.4082<br>
,<br>
0.0000<br>
,<br>
0.0000<br>
],<br>
[<br>
0.0000<br>
,<br>
0.0000<br>
,<br>
0.4082<br>
,<br>
0.0000<br>
,<br>
0.3333<br>
,<br>
0.5774<br>
],<br>
[<br>
0.4082<br>
,<br>
0.3333<br>
,<br>
0.0000<br>
,<br>
0.3333<br>
,<br>
0.0000<br>
,<br>
0.0000<br>
],<br>
[<br>
0.0000<br>
,<br>
0.0000<br>
,<br>
0.0000<br>
,<br>
0.5774<br>
,<br>
0.0000<br>
,<br>
0.0000<br>
]])<br>
把这两个tricks结合起来，我们可以得到<br>
其中<br>
,<br>
是<br>
的degree matrix。 而<br>
是对<br>
做了一个对称的归一化。<br>
数学推导<br>
Graph Laplacian<br>
首先我们表示一个graph的方式有很多，我们可以用邻接矩阵，也可以用Incidence matrix。 这个matrix 中，每一行表示一个边，每一列表示一个节点。每一行中，边的节点的起点用记为1，边的终点记为-1。 我们将这个metrix 记为<br>
. 具体如下图。<br>
那么 graph Laplacian 定义为：<br>
C<br>
=<br>
torch<br>
.<br>
tensor<br>
([<br>
[<br>
1<br>
,<br>
-<br>
1<br>
,<br>
0<br>
,<br>
0<br>
],<br>
[<br>
1<br>
,<br>
0<br>
,<br>
-<br>
1<br>
,<br>
0<br>
],<br>
[<br>
1<br>
,<br>
0<br>
,<br>
0<br>
,<br>
-<br>
1<br>
],<br>
[<br>
0<br>
,<br>
-<br>
1<br>
,<br>
1<br>
,<br>
0<br>
],<br>
[<br>
0<br>
,<br>
0<br>
,<br>
1<br>
,<br>
-<br>
1<br>
],<br>
[<br>
0<br>
,<br>
-<br>
1<br>
,<br>
0<br>
,<br>
1<br>
],<br>
])<br>
C<br>
.<br>
T<br>
.<br>
matmul<br>
(<br>
C<br>
)</p>
<blockquote>
<blockquote>
<p>tensor<br>
(<br>
[[<br>
3<br>
,<br>
-<br>
1<br>
,<br>
-<br>
1<br>
,<br>
-<br>
1<br>
],<br>
[<br>
-<br>
1<br>
,<br>
3<br>
,<br>
-<br>
1<br>
,<br>
-<br>
1<br>
],<br>
[<br>
-<br>
1<br>
,<br>
-<br>
1<br>
,<br>
3<br>
,<br>
-<br>
1<br>
],<br>
[<br>
-<br>
1<br>
,<br>
-<br>
1<br>
,<br>
-<br>
1<br>
,<br>
3<br>
]])<br>
我们可以发现，对角线的值<br>
, 其中如果<br>
, 则其积 = 0，如果<br>
或者<br>
, 则其积 = 1。所以我们可以知道对角线代表的是每个节点的度（Degree）<br>
对于非对角线的值<br>
, 我们可以看出来，如果节点<br>
和<br>
没有相连，那么<br>
否则<br>
， 于是知道非对角线的值就是邻接矩阵的负值。<br>
所以我们可以推导得到<br>
如下图（注意这边W表示的是邻接矩阵）<br>
总结来说：<br>
具体计算参考下面的代码<br>
C<br>
=<br>
torch<br>
.<br>
tensor<br>
([<br>
[<br>
-<br>
1<br>
,<br>
1<br>
,<br>
0<br>
,<br>
0<br>
,<br>
0<br>
,<br>
0<br>
],</p>
</blockquote>
</blockquote>
</blockquote>
</dd>
</dl>
</dd>
</dl>
<h1>1-2</h1>
<h2>[</h2>
<p>1<br>
,<br>
0<br>
,<br>
0<br>
,<br>
0<br>
,<br>
1<br>
,<br>
0<br>
],</p>
<h1>1-5</h1>
<p>[<br>
0<br>
,<br>
-<br>
1<br>
,<br>
1<br>
,<br>
0<br>
,<br>
0<br>
,<br>
0<br>
],</p>
<h1>2-3</h1>
<p>[<br>
0<br>
,<br>
-<br>
1<br>
,<br>
0<br>
,<br>
0<br>
,<br>
1<br>
,<br>
0<br>
],</p>
<h1>2-5</h1>
<p>[<br>
0<br>
,<br>
0<br>
,<br>
-<br>
1<br>
,<br>
1<br>
,<br>
0<br>
,<br>
0<br>
],</p>
<h1>3-4</h1>
<p>[<br>
0<br>
,<br>
0<br>
,<br>
0<br>
,<br>
-<br>
1<br>
,<br>
1<br>
,<br>
0<br>
],</p>
<h1>4-5</h1>
<p>[<br>
0<br>
,<br>
0<br>
,<br>
0<br>
,<br>
-<br>
1<br>
,<br>
0<br>
,<br>
1<br>
],</p>
<h1>5-6</h1>
<p>])<br>
C<br>
.<br>
T<br>
.<br>
matmul<br>
(<br>
C<br>
)</p>
<blockquote>
<blockquote>
<blockquote>
<p>tensor<br>
([[<br>
2<br>
,<br>
-<br>
1<br>
,<br>
0<br>
,<br>
0<br>
,<br>
-<br>
1<br>
,<br>
0<br>
],<br>
[<br>
-<br>
1<br>
,<br>
3<br>
,<br>
-<br>
1<br>
,<br>
0<br>
,<br>
-<br>
1<br>
,<br>
0<br>
],<br>
[<br>
0<br>
,<br>
-<br>
1<br>
,<br>
2<br>
,<br>
-<br>
1<br>
,<br>
0<br>
,<br>
0<br>
],<br>
[<br>
0<br>
,<br>
0<br>
,<br>
-<br>
1<br>
,<br>
3<br>
,<br>
-<br>
1<br>
,<br>
-<br>
1<br>
],<br>
[<br>
-<br>
1<br>
,<br>
-<br>
1<br>
,<br>
0<br>
,<br>
-<br>
1<br>
,<br>
3<br>
,<br>
0<br>
],<br>
[<br>
0<br>
,<br>
0<br>
,<br>
0<br>
,<br>
-<br>
1<br>
,<br>
0<br>
,<br>
1<br>
]])<br>
我们需要知道 laplacian<br>
的性质：<br>
是对称矩阵<br>
有实数的，非负的特征值（eigen values）<br>
有实数的，正交的特征矩阵（eigen vectors), i.e.<br>
对此，我们假设<br>
的特征值为<br>
特征向量为<br>
:<br>
对于特征值我们有<br>
对称归一化的Laplacian （Symmetric normalized Laplacian）<br>
其元素值，对角线为1，非对角线为<br>
我们要知道两个函数的卷积可以由以下公式得到，具体<br>
参考<br>
其中<br>
代表傅立叶变换<br>
而Graph Fourier变换对应的就是以下：<br>
其中Graph Fourier 逆变换对应的就是以下：<br>
其中<br>
是laplacian<br>
的特征矩阵 具体的对应关系：<br>
我们知道普通的卷积公式：<br>
那么相应的图卷积的公式为：<br>
作为图的特征<br>
的filter，我们希望它的作用域跟CNN一样，都是在中心节点附近的区域，所以我们定义<br>
是一个laplacian的函数<br>
， 那么作用一次相当于传播一次周围邻居节点的信息。<br>
又因为<br>
, 所以我们可以把<br>
看成是laplacian 特征值的函数<br>
, 参数为<br>
。<br>
所以图卷积在Fourier域上可以表示为：<br>
我们知道<br>
需要先计算laplacian matrix<br>
的 特征值，这涉及到大量的矩阵运算，所以文章借用了Chebyshev polynomials进行近似计算：<br>
其中<br>
,<br>
代表的是<br>
次Laplacian，即它取决于中心节点的最近的<br>
order 的邻居节点（邻居节点和中心节点的距离最大为K）。<br>
, 其中<br>
以及<br>
。<br>
我们回到最初的图卷积计算：<br>
其中<br>
我们知道论文中采用的传播邻居层数为1， 所以取<br>
, 并且我们假设<br>
, 可以得到：<br>
实际运用中，为了防止overfitting以及减少操作，我们令<br>
得到：<br>
我们令<br>
, 以及<br>
得到：<br>
再加上激活函数<br>
, 我们获得了<br>
其中<br>
对应输入<br>
，<br>
对应参数<br>
ref<br>
https://<br>
en.wikipedia.org/wiki/L<br>
aplacian_matrix<br>
T. N. Kipf, M. Welling, Semi-Supervised Classification with Graph Convolutional Networks(ICLR 2017) [<br>
Link<br>
,<br>
PDF (arXiv)<br>
,<br>
code<br>
,<br>
blog<br>
]<br>
https://<br>
math.stackexchange.com/<br>
questions/1113467/why-laplacian-matrix-need-normalization-and-how-come-the-sqrt-of-degree-matrix</p>
</blockquote>
</blockquote>
</blockquote>
<h2>10. 注意力机制 - 知乎</h2>
<ul>
<li>链接：https://www.zhihu.com/topic/20682987/intro</li>
<li>来源：bing</li>
<li>摘要：2020年4月24日 · 注意力机制是非常优美而神奇的机制，在神经网络「信息过载」的今天，让 NN 学会只关注特定的部分，无疑会大幅度提升任务的效果与效率。借助注意力机制，神经机器翻译、预训练语 …</li>
</ul>
<h3>正文（抓取，非 AI）</h3>
<p>注意力机制 - 知乎<br>
注意力机制<br>
「注意力机制」（Attention Mechanism）是机器学习中的一种数据处理方法，广泛应用在自然语言处理、图像识别及语音识别等各种不同类型的机器学习任务中。...<br>
查看全部内容<br>
关注话题<br>
​<br>
管理<br>
​<br>
分享<br>
​<br>
百科<br>
讨论<br>
精华<br>
等待回答<br>
详细内容<br>
简介<br>
注意力机制（Attention Mechanism）是机器学习中的一种数据处理方法，广泛应用在自然语言处理、图像识别及语音识别等各种不同类型的机器学习任务中。<br>
注意力机制分成两个部分：<br>
一、注意力机制为什么有必要存在（科普向）<br>
二、注意力机制具体是如何实现的（知识向）<br>
存在意义<br>
一、注意力机制为什么有必要存在<br>
注意力这个词本来是属于人类才有的动作。<br>
也就是说，注意力机制可以看做是一种仿生，是机器通过对人类阅读、听说中的注意力行为进行模拟。<br>
那为何要对注意力进行仿生，按理说，计算机理应拥有无限的记忆力和注意力。<br>
这是因为，人脑在进行阅读任务、读图任务时，并不是严格的解码过程，而是接近于一种模式识别。<br>
大脑会自动忽略低可能、低价值的信息。<br>
大脑会自动忽略可能性低的答案<br>
上面的这段话，好几个词都存在顺序混乱，但是阅读的第一感受，不会感觉到这种乱序。<br>
甚至不用停下来思考，这种乱序是不是存在笔误，不仔细看甚至发现不了问题的存在。<br>
这不是眼睛都出了问题，而是大脑在识别文字的过程中，自动就把低可能的解释忽略了。<br>
这一点在读图任务上，还更为明显一些，读图的过程中，大脑总是会优先获取认为有用的信息，而将次要的内容直接抛弃。<br>
这是因为，大脑在阅读或读图的过程中，会直接抛弃低可能性答案，将阅读的内容更正为“大脑认为正确的版本”。<br>
同样的文字随着对话主题的不同，含义也会发生变化<br>
上下文联系影响文字的意义<br>
这是网络上的一个段子，所谓的中文十级考试。<br>
这段话的第二句话里，两个“对”字代表了不同的含义。<br>
但如果单独把第二局话挑出来，即使是中国人也会对里面的意义产生疑义。因为本身这句话的“对了”可以解释成“已经校对过了”、“正确了”或者“无意义的承接词”。<br>
但如果有了第一句话的限定，这个“对了”就只能是“对答案了”的意思。<br>
也就是说，理解一句话的含义，不仅仅取决于这句话本身，而与上下文相关联的词也有很大影响。<br>
还不单单如此，通常一段对话中，都会存在一个反复出现的概念，例如这段话中的“对答案”，其他的词语或多或少都能与这个概念产生联系。也就是说，这个概念就是这段话的主题。而在更复杂一些的段落里，还会出现部分与主题没有什么关联的内容，通常这些内容都会被我们弱化或者自动遗忘。<br>
结合上面的两个例子，人们在阅读、交流的过程中，本身就存在着信息的舍弃。虽然每段文字可能字号、粗细都相同，但注意力却不是那样均衡地分配给每一个词。<br>
如果计算机不能模拟人类的注意力状态，就可能让无关的信息对处理结果造成干扰，最后导致处理结果偏离实际的应用场景。<br>
例如，聊天场景中，用户输入了错别字，导致了歧义。如果是人工场景，就很容易忽略错别字的影响，理解文字的本来含义。<br>
又或者，同样的句子，在不同语境中含义发生变化，导致机器翻译在段落和文章的翻译上，似是而非，语言不通顺。<br>
这些干扰，都让人工智能显得像是“人工智障”，逻辑硬伤导致无法执行较为复杂的任务。<br>
为了让计算机更加适应人类交流场景，必须教会计算机选择遗忘和关联上下文，这种机制就是所谓的注意力机制。<br>
实现过程<br>
严格来说，注意力机制更像是一种方法论。没有严格的数学定义，而是根据具体任务目标，对关注的方向和加权模型进行调整。<br>
简单的理解就是，在神经网络的隐藏层，增加注意力机制的加权。<br>
使不符合注意力模型的内容弱化或者遗忘。<br>
Google 2017年论文中，Attention Is All You Need曾经为Attention做了一个抽象定义：<br>
Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_k}})V<br>
An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.<br>
注意力是将一个查询和键值对映射到输出的方法，Q、K、V均为向量，输出通过对V进行加权求和得到，权重就是Q、K相似度。<br>
1）机器视觉中的应用（精细分类、图像分割、图像焦点）<br>
例如，识别鸟类的品种问题。对于鸟品种的精细分类，对结果影响最大的可能是鸟类的头部，通过注意力机制将头部的特征强化，而忽略其他部分（羽毛、爪子），以实现区分鸟类的具体品种。<br>
2）机器翻译中的应用（LSTM+注意力模型）<br>
LSTM单元<br>
LSTM（Long Short Term Memory）是RNN（循环神经网络）的一种应用。可以简单理解为，每一个神经元都具有输入门、输出门、遗忘门。<br>
输入门、输出门将LSTM神经元首尾连接在一起，而遗忘门将无意义内容弱化或遗忘。注意力机制就应用在LSTM的遗忘门，使得机器阅读更加贴近于人类阅读的习惯，也使得翻译结果具有上下文联系。<br>
参考资料<br>
【计算机视觉】深入理解Attention机制 - Slow down, Keep learning and Enjoy life - CSDN博客<br>
周知瑞：Attention的梳理、随想与尝试<br>
瑟木：计算机视觉中的注意力机制<br>
百科摘录<br>
3<br>
2019 ICCV  用于检测、去除阴影区域的注意力循环生成对抗网路<br>
下的内容摘录<br>
B1gme<br>
总是再不停的突破自己的b1gme<br>
注意力机制是用来编码序列数据，这些数据基于给每个参数分配重要的权值的方式。它为自然语言处理方向、语音识别、计算机视觉、图像描述和视觉问答方向提供重大帮助。不同于上述方法，论文采取了一种使用渐进式和循环方法来整合不同层特征的多内容信息。并且能处理复杂环境下阴影去除和检测。<br>
知乎小知<br>
摘录于<br>
2020-04-24<br>
哈希算法、爱因斯坦求和约定，这是2020年的注意力机制<br>
下的内容摘录<br>
机器之心<br>
​<br>
数学等 2 个话题下的优秀答主<br>
注意力机制是非常优美而神奇的机制，在神经网络「信息过载」的今天，让 NN 学会只关注特定的部分，无疑会大幅度提升任务的效果与效率。借助注意力机制，神经机器翻译、预训练语言模型等任务获得了前所未有的提升。<br>
知乎小知<br>
摘录于<br>
2020-04-24<br>
【ACL 2019】为知识图谱添加注意力机制<br>
下的内容摘录<br>
超正经学术君<br>
让更多人读懂科学<br>
注意力机制（Attention）是近些年来提出的一种改进神经网络的方法，在图像识别、自然语言处理和图网络表示等领域都取得了很好的效果，可以说注意力机制的加入极大地丰富了神经网络的表示能力。<br>
知乎小知<br>
摘录于<br>
2020-04-24<br>
浏览量<br>
3677 万<br>
讨论量<br>
1.6 万</p>
<h2>11. 注意力机制到底在做什么，Q/K/V怎么来的？一文读懂 ...</h2>
<ul>
<li>链接：https://www.zhihu.com/tardis/zm/art/414084879</li>
<li>来源：bing</li>
<li>摘要：2023年2月2日 · Transformer <sup id="fnref3:1"><a class="footnote-ref" href="#fn:1">1</a></sup>论文中使用了注意力Attention机制，注意力Attention机制的最核心的公式为： 这个公式中的 Q 、 K 和 V 分别代表Query、Key和Value，他们之间进行的数学计算并不容易 …</li>
</ul>
<h3>正文（抓取，非 AI）</h3>
<p>本文同时发布于我的个人网站，公式图片显示效果更好，欢迎访问：<br>
https://<br>
lulaoshi.info/machine-l<br>
earning/attention/transformer-attention.html<br>
Transformer<sup id="fnref:1"><a class="footnote-ref" href="#fn:1">1</a></sup>论文中使用了注意力Attention机制，注意力Attention机制的最核心的公式为：<br>
这个公式中的<br>
Q<br>
、<br>
K<br>
和<br>
V<br>
分别代表Query、Key和Value，他们之间进行的数学计算并不容易理解。<br>
从向量点乘说起<br>
我们先从<br>
这样一个公式开始。<br>
首先需要复习一下向量点乘（Dot Product）的概念。对于两个行向量<br>
和<br>
：<br>
向量点乘的几何意义是：向量<br>
在向量<br>
方向上的投影再与向量<br>
的乘积，能够反应两个向量的相似度。向量点乘结果大，两个向量越相似。<br>
一个矩阵<br>
由<br>
行向量组成。比如，我们可以将某一行向量<br>
理解成一个词的词向量，共有<br>
个行向量组成<br>
的方形矩阵：<br>
矩阵<br>
与矩阵的转置<br>
相乘，<br>
中的每一行与<br>
的每一列相乘得到目标矩阵的一个元素，<br>
可表示为：<br>
以<br>
中的第一行第一列元素为例，其实是向量<br>
与<br>
自身做点乘，其实就是<br>
自身与自身的相似度，那第一行第二列元素就是<br>
与<br>
之间的相似度。<br>
下面以词向量矩阵为例，这个矩阵中，每行为一个词的词向量。矩阵与自身的转置相乘，生成了目标矩阵，目标矩阵其实就是一个词的词向量与各个词的词向量的相似度。<br>
词向量矩阵相乘<br>
如果再加上Softmax呢？我们进行下面的计算：<br>
。Softmax的作用是对向量做归一化，那么就是对相似度的归一化，得到了一个归一化之后的权重矩阵，矩阵中，某个值的权重越大，表示相似度越高。<br>
在这个基础上，再进一步：<br>
，将得到的归一化的权重矩阵与词向量矩阵相乘。权重矩阵中某一行分别与词向量的一列相乘，词向量矩阵的一列其实代表着不同词的某一维度。经过这样一个矩阵相乘，相当于一个加权求和的过程，得到结果词向量是经过加权求和之后的新表示，而权重矩阵是经过相似度和归一化计算得到的。<br>
通过与权重矩阵相乘，完成加权求和过程<br>
Q、K、V<br>
注意力Attention机制的最核心的公式为：<br>
，与我们刚才分析的<br>
有几分相似。Transformer<sup id="fnref2:1"><a class="footnote-ref" href="#fn:1">1</a></sup>论文中将这个Attention公式描述为：Scaled Dot-Product Attention。其中，Q为Query、K为Key、V为Value。Q、K、V是从哪儿来的呢？Q、K、V其实都是从同样的输入矩阵X线性变换而来的。我们可以简单理解成：<br>
用图片演示为：<br>
X分别乘以三个矩阵，生成Q、K、V矩阵<br>
其中，<br>
，<br>
和<br>
是三个可训练的参数矩阵。输入矩阵<br>
分别与<br>
，<br>
和<br>
相乘，生成<br>
、<br>
和<br>
，相当于经历了一次线性变换。Attention不直接使用<br>
，而是使用经过矩阵乘法生成的这三个矩阵，因为使用三个可训练的参数矩阵，可增强模型的拟合能力。<br>
Scaled Dot-Product Attention<br>
在这张图中，<br>
与<br>
经过MatMul，生成了相似度矩阵。对相似度矩阵每个元素除以<br>
，<br>
为<br>
的维度大小。这个除法被称为Scale。当<br>
很大时，<br>
的乘法结果方差变大，进行Scale可以使方差变小，训练时梯度更新更稳定。<br>
Mask是机器翻译等自然语言处理任务中经常使用的环节。在机器翻译等NLP场景中，每个样本句子的长短不同，对于句子结束之后的位置，无需参与相似度的计算，否则影响Softmax的计算结果。<br>
我们用国外博主Transformer详解博文<sup id="fnref:2"><a class="footnote-ref" href="#fn:2">2</a></sup>中的例子来将上述计算串联起来解释。<br>
输入为词向量矩阵X，每个词为矩阵中的一行，经过与W进行矩阵乘法，首先生成Q、K和V。<br>
q1 = X1 * WQ<br>
，<br>
q1<br>
为<br>
Q<br>
矩阵中的行向量，<br>
k1<br>
等与之类似。<br>
从词向量到Q、K、V<br>
第二步是进行<br>
计算，得到相似度。<br>
Q与K相乘，得到相似度<br>
第三步，将刚得到的相似度除以<br>
，再进行Softmax。经过Softmax的归一化后，每个值是一个大于0小于1的权重系数，且总和为0，这个结果可以被理解成一个权重矩阵。<br>
Scale &amp; Softmax<br>
第四步是使用刚得到的权重矩阵，与V相乘，计算加权求和。<br>
使用权重矩阵与V相乘，得到加权求和<br>
多头注意力<br>
为了增强拟合性能，Transformer对Attention继续扩展，提出了多头注意力（Multiple Head Attention）。刚才我们已经理解了，<br>
、<br>
、<br>
是输入<br>
与<br>
、<br>
和<br>
分别相乘得到的，<br>
、<br>
和<br>
是可训练的参数矩阵。现在，对于同样的输入<br>
，我们定义多组不同的<br>
、<br>
、<br>
，比如<br>
、<br>
、<br>
，<br>
、<br>
和<br>
，每组分别计算生成不同的<br>
、<br>
、<br>
，最后学习到不同的参数。<br>
定义多组W，生成多组Q、K、V<br>
比如我们定义8组参数，同样的输入<br>
，将得到8个不同的输出<br>
到<br>
。<br>
定义8组参数<br>
在输出到下一层前，我们需要将8个输出拼接到一起，乘以矩阵<br>
，将维度降低回我们想要的维度。<br>
将多组输出拼接后乘以矩阵Wo以降低维度<br>
多头注意力的计算过程如下图所示。对于下图中的第2）步，当前为第一层时，直接对输入词进行编码，生成词向量X；当前为后续层时，直接使用上一层输出。<br>
多头注意力计算过程<br>
再去观察Transformer论文中给出的多头注意力图示，似乎更容易理解了：<br>
Transformer论文给出的多头注意力图示</p>
<div class="footnote">
<hr>
<ol>
<li id="fn:1">
<p>Vaswani A, Shazeer N, Parmar N, et al. Attention is all you need. 31st Conference on Neural Information Processing Systems 2017(NIPS 2017). Long Beach, CA, USA: 2017: 5998–6008.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:1" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:1" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:2">
<p>https://<br>
jalammar.github.io/illu<br>
strated-transformer/&#160;<a class="footnote-backref" href="#fnref:2" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>
</div>
<script>
function readAloud(){ var el=document.querySelector('.content'); if(!el){ document.getElementById('readStatus').textContent='无可读内容'; return; } var t=el.innerText.replace(/\s+/g,' ').trim(); if(!t){ document.getElementById('readStatus').textContent='无可读内容'; return; }
 speechSynthesis.cancel();
 var status=document.getElementById('readStatus'); status.textContent='准备中…';
 var chunks=[]; var maxLen=280; for(var i=0;i<t.length;i+=maxLen){ var s=t.slice(i,i+maxLen); var j=s.lastIndexOf('。'); if(j>80){ chunks.push(s.slice(0,j+1)); i-=s.length-(j+1); } else chunks.push(s); }
 if(chunks.length===0) chunks=[t];
 var idx=0; function speakNext(){ if(idx>=chunks.length){ status.textContent=''; return; } var u=new SpeechSynthesisUtterance(chunks[idx]); u.lang='zh-CN'; u.rate=0.95; var v=speechSynthesis.getVoices().filter(function(x){ return x.lang.indexOf('zh')>=0; })[0]; if(v) u.voice=v;
 u.onend=function(){ idx++; setTimeout(speakNext,80); }; u.onerror=function(){ idx++; setTimeout(speakNext,80); }; status.textContent='朗读中 '+(idx+1)+'/'+chunks.length+'…'; speechSynthesis.speak(u); }
 if(speechSynthesis.getVoices().length) speakNext(); else speechSynthesis.onvoiceschanged=function(){ speechSynthesis.onvoiceschanged=null; speakNext(); };
 }
</script>
</body>
</html>
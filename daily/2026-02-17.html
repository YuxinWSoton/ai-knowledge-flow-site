<!DOCTYPE html>
<html lang="zh-CN">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<title>知识流日报 2026-02-17：softmax、交叉熵、感受野</title>
<style>
  * { box-sizing: border-box; }
  body {
    font-family: system-ui, sans-serif;
    max-width: 720px;
    margin: 0 auto;
    padding: 2rem 1rem;
    color: #fff;
    min-height: 100vh;
    background: #0a0e1a;
    background-image:
      radial-gradient(2px 2px at 20px 30px, rgba(255,255,255,0.9), transparent),
      radial-gradient(2px 2px at 40px 70px, rgba(255,255,255,0.6), transparent),
      radial-gradient(1.5px 1.5px at 90px 40px, rgba(255,255,255,0.8), transparent),
      radial-gradient(2px 2px at 130px 80px, rgba(255,255,255,0.5), transparent),
      radial-gradient(1.5px 1.5px at 160px 120px, rgba(255,255,255,0.7), transparent),
      radial-gradient(ellipse 120% 100% at 50% 0%, rgba(88,60,120,0.25), transparent 50%),
      radial-gradient(ellipse 80% 80% at 80% 20%, rgba(40,60,100,0.2), transparent 40%);
    background-size: 200px 200px;
    background-repeat: repeat;
  }
  a { color: #a8d4ff; text-decoration: none; }
  a:hover { text-decoration: underline; }
 h1,h2,h3{margin-top:1.5rem;color:#fff;} pre{white-space:pre-wrap;color:rgba(255,255,255,0.9);} .toolbar{margin:0.5rem 0;color:rgba(255,255,255,0.8);} .content{color:rgba(255,255,255,0.95);} .content a{color:#a8d4ff;} .meta{color:rgba(255,255,255,0.65);font-size:0.9em;} .content p,.content div,.content h2,.content h3,.content h4,.content li,.content pre{cursor:pointer;-webkit-tap-highlight-color:rgba(255,255,255,0.15);} .content .insight-detail,.content .insight-detail li{color:#8dd4e8;list-style:disc;margin-left:1.2em;padding-left:0.3em;}</style>
</head>
<body>
<p><a href="https://YuxinWSoton.github.io/ai-knowledge-flow-site/">← 返回首页</a></p>
<h1>知识流日报 2026-02-17：softmax、交叉熵、感受野</h1>
<p class="meta" style="margin-top:0;">最后更新：2026-02-17 20:01</p>
<p class="toolbar"><button id="btnNext" onclick="nextArticle()">下一篇</button> <button id="btnSel" onclick="readSelected()">朗读这段</button> <span id="readStatus"></span></p>
<p class="meta" style="margin-top:0;">（点任意段落即可朗读，手机/触屏友好；也可选中后点「朗读这段」或点「下一篇」按条朗读）</p>
<div class="content">
<h1>知识流日报 2026-02-17：softmax、交叉熵、感受野</h1>
<p>共 4 条（来自百度学术 / Google / Bing 等，仅含正文抓取成功的条目；按正文长度从短到长排列，便于先听短的）。</p>
<p>（以下含抓取正文，便于阅读。未调用任何 AI API。）</p>
<p>（部分条目含模型提取的「易漏细节」关键点，页面上以颜色区分。）</p>
<h2>1. 如何理解CNN中的感受野（receptive-field）？</h2>
<ul>
<li>链接：https://www.zhihu.com/tardis/bd/ans/1910838113</li>
<li>来源：bing</li>
<li>摘要：2021年6月2日 · 和显示一样 ，它基本遵循着“近大远小”的逻辑。或者从另外一个角度说，能够看到近处的范围有限，能够看到远处更广泛的视野。 在CNN中，感受野的概念： • 卷积神经网络（CNN）中， …</li>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>感受野是指每个神经元能够访问的原始图像的区域大小。</li>
<li>感受野遵循“近大远小”的逻辑。</li>
<li>感受野的概念在CNN中指每一层输出的特征图上，每个像素点在原始图像上映射的区域大小。</li>
<li>感受野值越大，接触到的原始图像范围越大，蕴含更全局、语义层次更高的特征。</li>
<li>感受野值越小，包含的特征越趋向局部和细节。</li>
<li>使用更小的卷积核可以在增强网络容量的同时减少参数个数。</li>
<li>的卷积核可以被2个3*3的卷积核替代。</li>
<li>AlexNet使用11x11的卷积核，而ZFNet用7x7替换了该卷积层。</li>
<li>GoogleNet使用5x5的卷积核，VGG仅使用3x3的卷积核。</li>
<li>感受野的计算可以使用特定的工具和代码进行。</li>
</ul>

<h3>正文（抓取，非 AI）</h3>
<p>可以用这幅图大概了解【感受野】，大概就是我们能够感受到的视野范围。 更进一步说，如下图，根据图像，眼睛可以看到的整个区域（图中的网格）称为视野。人类的视觉系统由数百万个神经元组成，每个神经元捕获不同的信息。我们将神经元的接受视野定义为总视野的斑块。换句话说，单个神经元可以访问哪些信息。简单来说，这就是生物细胞的感受野。 和显示一样 ，它基本遵循着“近大远小”的逻辑。或者从另外一个角度说，能够看到近处的范围有限，能够看到远处更广泛的视野。 在CNN中，感受野的概念： • 卷积神经网络（CNN）中，每一层输出的特征图上，每个像素点在原始图像上映射（对应）的区域大小。 • 原始图像是CNN的输入图像，是经过预处理（如缩放、扭曲、裁切等操作）后的图像。 概念往往是很严谨的，但是，也是非常抽象的。 简单来说，感受野，就是我们看到的物体，或者我们感知到的物体，它的本来大小是多少呢？ 当然，要了解感受野，必须要先了解什么是卷积。 简单来说，卷积就是让一个“核”在一个输入图像上进行运算，从而得到结果。 下图显示了卷积。我们采用一个过滤器/内核（3×3矩阵），并将其应用于输入图像以获得卷积特征。该卷积特征将传递到下一层。 或者简单点，可以表示下面这个样子： 下面的动画看起来更直观了。 通过上面的动画，大家可以看到，一个5<em>5的输入，在使用3</em>3大小的卷积核处理后，得到的结果为3<em>3大小。 下面进入正题，下图中的左侧白色背景区域中： 最左侧是一个4</em>4大小输入原始图像 中间的黄色3<em>3的大小的矩形，是一个卷积核 最右侧是运算结果，它的大小是2</em>2； 此时，我们可以说，输出大小虽然是2<em>2的，但是它能够切实地感受到原始4</em>4大小的区域。 或者进一步来说，当前2<em>2的输出结果中，所有的值都来源于原始4</em>4大小的输入图像。 下面看一个特例： 输入：2<em>2大小 卷积核：2</em>2大小 输出：1<em>1大小 此时，可以说输出结果中，实际上包含了原始输入2</em>2大小的信息。或者说，当前1<em>1输出的感受野是2</em>2大小。 下面这个例子，经过了两次卷积运算。 第1次卷积： 输入：4<em>4大小 卷积核：3</em>3大小 输出：2<em>2大小 第2次卷积： 输入：2</em>2大小 卷积核：2<em>2大小 输出：1</em>1大小 可以看到，经过多次卷积，将输入从“4<em>4”大小，转换到“1</em>1”大小的输出。 我们可以尝试着，将输出往回看，看看它能感受到怎样的感受野。 首先，最终的“1<em>1”的输出，感受到的是中间的“2</em>2”得绿色视野。 更进一步，中间的“2<em>2”绿色小块，感受到的是最左边的“4</em>4”大小的视野。 总体来看，最终的“1<em>1”大小的输出，感受到的是最初的“4</em>4”大小的视野。 以上，我们观察的是一个输出像素点的情况，针对多个像素点，结果如下图所示。 来看看YOLO的模型，它的感受野大概是这样的。 感受野的情况： • 值越大：它所能接触到的原始图像的范围越大，意味着它可能蕴含更为全局、语义层次更高的特征 • 值越小：它所包含的特征越趋向局部和细节 • 感受野值，可以大致判断每一层的抽象层次 使用更小的卷积核，能够在增强网络容量的同时减少参数个数。 我们先比较一下核大小分别是5<em>5和3</em>3的情况。 例如，在下图中： l 左侧原始输入为5<em>5大小，使用一个5</em>5大小的核，处理它，得到的结果为1<em>1大小。 l 右侧原始输入是5</em>5大小，使用两次3<em>3大小的核，处理它，得到的结果为1</em>1大小。 观察一下参数的数量， l 左侧，核大小为5<em>5=25，需要：25个参数。 l 右侧，3</em>3大小核，需要两次运算，所以需要的参数为：3<em>3</em>2=18个参数。 所以，我们经常看到的是这张图，也就是说，2层3<em>3的卷积核，可以替代一个5</em>5的卷积核。 在2012年，AlexNet的第一次卷积大小为11x11。2013年，ZFNet用7x7替换了该卷积层。2014年，GoogleNet最大的卷积内核是5x5。它保留了第一个7x7卷积层。同年，二等奖VGG仅使用了3x3卷积内核。在更高版本中，第一个版本的GoogleNet的5x5卷积层已被2个堆叠的3x3卷积层代替，复制了VGG16。 下面看一个3个3<em>3的卷积核，相当于1个7</em>7卷积核得例子。 上面有个问题，为什么经过卷积，输出结果会变小呢？ 看看下面这个动态图，也许能够帮助我们理解。 下面我看一个卷积的示例，进一步理解问题规模： Input：输入 Kernels：核 Feature maps：特征图 Feature vector：特征向量值 这里，我们主要关注右侧的对象规模。 下列网址，提供了一个非常有趣的感受野示例： https:// distill.pub/2019/comput ing-receptive-fields/ 下述网址，提供了感受野计算的源代码： https:// distill.pub/2019/comput ing-receptive-fields/ 本文首发于公众号【计算机之光】 。搜索【cvlight】关注公众号，免费视频学习，领取学习资料。 感兴趣可以进一步阅读: 参考资料： 1. Convolutional Neural Networks (CNN) 2. https:// theaisummer.com/recepti ve-field/ 3. Computing Receptive Fields of Convolutional Neural Networks 4. google-research/receptive_field</p>
<h2>2. 一文揭秘Softmax函数：解锁机器学习多类分类之门</h2>
<ul>
<li>链接：https://www.zhihu.com/tardis/bd/art/683697682</li>
<li>来源：bing</li>
<li>摘要：2024年2月24日 · 1. 引言 Softmax函数的定义和基本概念 Softmax函数，也称为归一化指数函数，是一个将向量映射到另一个向量的函数，其中输出向量的元素值代表了一个概率分布。 在机 …</li>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>Softmax函数将向量映射到概率分布</li>
<li>Softmax函数输出值范围在(0, 1)之间</li>
<li>所有输出值之和为1</li>
<li>Softmax函数可以看作逻辑回归的推广</li>
<li>指数函数确保输出值非负</li>
<li>指数函数放大差异有助于决策</li>
<li>Softmax函数适用于多类分类</li>
<li>Softmax与交叉熵损失结合使用</li>
<li>Softmax对异常值敏感</li>
<li>计算成本较高，涉及指数运算</li>
</ul>

<h3>正文（抓取，非 AI）</h3>
<p>省流版 这篇文章介绍了在机器学习多酚类问题中非常重要的一类函数：softmax函数。我们介绍了其数学原理、特点、应用以及python实现方法，非常值得你收藏起来反复学习。在保研考研、数据分析、算法等面试环节均有可能出现。 文中部分内容来自GPT生成，他已经成为我最重要的生产力工具。但我了解到还有部分朋友苦于不知道怎么在国内使用正版GPT，可以参考我的这篇博客： GPT4.0使用教程 或者直接从下方链接登陆： WildCard | 一分钟注册，轻松订阅海外软件服务 创作不易，感谢你的三联和支持~ 1. 引言 Softmax函数的定义和基本概念 Softmax函数，也称为归一化指数函数，是一个将向量映射到另一个向量的函数，其中输出向量的元素值代表了一个概率分布。在机器学习中，特别是在处理多类分类问题时，Softmax函数扮演着至关重要的角色。它可以将未归一化的数值转换成一个概率分布，使得每个类别都有一个对应的概率值，且所有类别的概率之和为1。 Softmax在机器学习中的重要性 在机器学习的多类分类问题中，我们经常需要预测一个实例属于多个类别中的哪一个。Softmax函数正是为了满足这一需求而设计。它不仅提供了一种将模型输出转换为概率解释的方法，而且由于其输出的概率性质，Softmax函数也使得模型的结果更易于理解和解释。 Softmax函数的广泛应用包括但不限于神经网络的输出层，在深度学习模型中，尤其是分类任务中，Softmax函数经常被用作最后一个激活函数，用于输出预测概率。 2. Softmax函数的数学原理 函数的数学表达式和解释 Softmax函数将一个含任意实数的K维向量z转换成另一个K维实向量σ(z)，其中每一个元素的范围都在(0, 1)之间，并且所有元素的和为1。函数的数学表达式如下： 其中，i表示向量z中的元素索引，K是向量z的维度。 分子e^{z_i}是元素z_i的指数函数，保证了输出值的非负性。 分母是所有元素指数值的总和，确保了所有输出值之和为1，从而形成一个概率分布。 Softmax与逻辑回归的联系 Softmax函数可以被看作是逻辑回归（或称作Logistic函数）在多类分类问题上的推广。在二分类问题中，逻辑回归输出一个实例属于某一类的概率，而Softmax则扩展到了多个类别，使得模型能够输出多个类别的概率预测 指数函数的作用 在Softmax函数中，指数函数e^{z_i}用于确保每个输出值都是正数，这对于将输出解释为概率是必要的。指数函数还能够放大差异，即使是很小的输入值差异，在经过指数函数处理后，也会在输出概率中反映为较大的差异，这有助于模型在多个类别之间做出更加明确的决策。 3. Softmax函数的应用 Softmax函数在机器学习和深度学习中有着广泛的应用，尤其是在解决多类分类问题时。 在多类分类问题中的应用 多类分类是指将实例分类到三个或更多的类别中。Softmax函数在这类问题中非常有用，因为它能够为每个类别生成一个概率分布。这意味着模型不仅能够预测每个实例最有可能属于哪个类别，还能给出相对于其他类别的概率评估，提供了更多的决策信息。 与神经网络的结合 在神经网络中，Softmax函数通常被用作输出层的激活函数，特别是在进行多类分类时。它将神经网络最后一层的线性输出转换为概率分布，每个节点（或神经元）对应一个特定类别的概率。这样，神经网络不仅能够识别输入数据最可能属于的类别，还能以概率形式表达对各个类别的预测信心。 示例：使用Softmax进行手写数字识别 一个典型的应用示例是使用Softmax函数和神经网络进行手写数字识别，如MNIST数据集。在这种场景下，网络的最后一层是一个含有10个节点的Softmax层，每个节点对应一个数字（0到9）。网络的输出是一个10维向量，表示输入图像属于每个数字的概率。选择概率最高的节点所对应的数字作为预测结果。 在这个简化的示例中，我们使用TensorFlow构建和训练一个简单的神经网络，其中最后一层是Softmax层，用于对手写数字图像进行分类。 4. Softmax函数的优缺点 优点 概率解释 ：Softmax函数的输出可以被解释为一个概率分布，每个类别都有一个对应的概率。这种概率输出使得模型的预测结果更容易被理解和解释。 扩展性 ：Softmax函数适用于二类分类和多类分类问题，非常灵活。它可以无缝地扩展到任意数量的类别，使得模型设计更加通用。 与交叉熵损失的结合 ：在训练阶段，Softmax函数通常与交叉熵损失函数结合使用，这种组合在数学上具有良好的性质，有助于模型的优化和训练。 缺点 对异常值敏感 ：由于Softmax函数使用了指数函数，它对异常值非常敏感。一个很大的负输入值在经过Softmax转换后可能会接近于零，而一个很大的正输入值可能会导致其它类别的概率显著降低。 计算成本 ：Softmax函数涉及指数运算，这在计算上可能比线性或其他简单函数更昂贵，尤其是当类别数量很大时。 数值稳定性问题 ：在实际计算中，Softmax函数可能遇到数值稳定性问题，特别是当输入值非常大或非常小时。这可能导致数值溢出或下溢，影响模型的稳定性和性能。 5. Softmax的变种和改进 为了解决Softmax函数的一些局限性，研究人员提出了几种变种和改进方法： 引入温度参数 ： 通过在Softmax函数中引入一个温度参数T，可以控制输出分布的平滑程度。温度较高时，输出概率分布更加平滑；温度较低时，输出分布更加尖锐。 使用Log-Softmax ： Log-Softmax是Softmax的对数版本，它在数学上更稳定，尤其是与交叉熵损失结合使用时。 稀疏Softmax ： 为了提高处理大量类别的效率，稀疏Softmax对于只有少数几个输出有显著概率的情况进行了优化。 这些改进使得Softmax函数在不同的应用场景中更加灵活和鲁棒。 6. 实现Softmax函数 实现Softmax函数需要注意的一个关键问题是数值稳定性。由于指数函数的特性，在处理很大的输入值时，直接计算会导致数值溢出。下面提供了一个考虑数值稳定性的Softmax函数的Python实现。 这段代码展示了如何计算一个向量或矩阵的Softmax。注意，在计算指数前，从输入值中减去了最大值，这是为了提高数值稳定性，防止计算指数时发生溢出。 数值稳定性问题和解决方案 数值稳定性问题主要是由于指数函数在处理很大的数时可能导致数值溢出。解决这个问题的一个常见技巧是，在进行指数运算前，先从输入值中减去其最大值。 这个操作不会改变Softmax函数的输出，因为我们同时从分子和分母中减去了相同的值，但它可以有效地减少计算指数时可能遇到的数值问题。 7. Softmax与其他激活函数的比较 Softmax函数通常与Sigmoid函数比较，尤其是在二分类问题中。Sigmoid函数将单个输入映射到(0, 1)区间，适用于二分类，而Softmax适用于多类分类。 Sigmoid ： 适用于二分类问题，输出一个代表正类概率的值。 Softmax ： 扩展到多类分类，为每个类别提供概率分布。 在选择激活函数时，考虑任务的性质（二分类还是多分类）以及模型的具体需求。 8. 总结 Softmax函数是机器学习和深度学习中的一个重要概念，特别是在处理分类问题时。它通过提供一个概率分布，使得模型的输出更加直观和易于解释。虽然实现时需要考虑数值稳定性问题，但正确使用Softmax函数可以大大提高多类分类问题的处理效率和准确性。 文中部分内容来自GPT生成，他已经成为我最重要的生产力工具。但我了解到还有部分朋友苦于不知道怎么在国内使用正版GPT，可以参考我的这篇博客： GPT4.0使用教程 或者直接从下方链接登陆： WildCard | 一分钟注册，轻松订阅海外软件服务 创作不易，感谢你的三联和支持~</p>
<h2>3. 为什么交叉熵（cross-entropy）可以用于计算代价？</h2>
<ul>
<li>链接：https://www.zhihu.com/tardis/bd/ans/244557337</li>
<li>来源：bing</li>
<li>摘要：2020年5月8日 · 通用的说，熵 (Entropy)被用于描述一个系统中的不确定性 (the uncertainty of a system)。在不同领域熵有不同的解释，比如热力学的定义和信息论也不大相同。 要想明白交叉熵 …</li>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>交叉熵可以等价于最小化KL散度</li>
<li>交叉熵的计算更简单</li>
<li>交叉熵是非负的</li>
<li>交叉熵不具有对称性</li>
<li>KL散度不具备对称性</li>
<li>KL散度由事件A的熵与B在A上的期望共同决定</li>
<li>交叉熵可以用于衡量两个分布之间的不同</li>
<li>最小化交叉熵等价于最小化KL散度</li>
<li>交叉熵可以作为学习模型的代价函数</li>
<li>训练数据的分布是固定的</li>
</ul>

<h3>正文（抓取，非 AI）</h3>
<p>通用的说，熵(Entropy)被用于描述一个系统中的不确定性(the uncertainty of a system)。在不同领域熵有不同的解释，比如热力学的定义和信息论也不大相同。 要想明白交叉熵(Cross Entropy)的意义，可以从熵(Entropy) -&gt; KL散度(Kullback-Leibler Divergence) -&gt; 交叉熵这个顺序入手。 当然，也有多种解释方法[1]。 先给出一个“接地气但不严谨”的概念表述： 熵：可以表示一个事件A的自信息量，也就是A包含多少信息。 KL散度：可以用来表示从事件A的角度来看，事件B有多大不同。 交叉熵：可以用来表示从事件A的角度来看，如何描述事件B。 一句话总结的话： KL散度可以被用于计算代价，而在特定情况下最小化KL散度等价于最小化交叉熵。而交叉熵的运算更简单，所以用交叉熵来当做代价 。 我知道你现在看着有点晕，但请保持耐心继续往下看。 <em>为了通俗易懂，我没有严格按照数学规范来命名概念，比如文中的“事件”指的是“消息”，望各位严谨的读者理解。 1. 什么是熵(Entropy)？ 放在信息论的语境里面来说，就是一个事件所包含的信息量。我们常常听到“这句话信息量好大”，比如“昨天花了10万，终于在西二环买了套四合院”。 这句话为什么信息量大？ 因为它的内容出乎意料，违反常理。 由此引出： 越不可能发生的事件信息量越大， 比如“我不会死”这句话信息量就很大 。而确定事件的信息量就很低， 比如“我是我妈生的”，信息量就很低甚至为0 。 独立事件的信息量可叠加。 比如“a. 张三今天喝了阿萨姆红茶，b. 李四前天喝了英式早茶”的信息量就应该恰好等于a+b的信息量，如果张三李四喝什么茶是两个独立事件。 因此熵被定义为 ， 指的不同的事件比如喝茶， 指的是某个事件发生的概率比如和红茶的概率。对于一个一定会发生的事件，其发生概率为1， ，信息量为0。 2. 如何衡量两个事件/分布之间的不同（一）：KL散度 我们上面说的是对于一个随机变量x的事件A的自信息量，如果我们有另一个独立的随机变量x相关的事件B，该怎么计算它们之间的区别？ 此处我们介绍默认的计算方法：KL散度，有时候也叫KL距离，一般被用于计算两个分布之间的不同 。看名字似乎跟计算两个点之间的距离也很像，但实则不然，因为KL散度 不具备有对称性 。在距离上的对称性指的是A到B的距离等于B到A的距离。 举个不恰当的例子，事件A：张三今天买了2个土鸡蛋，事件B：李四今天买了6个土鸡蛋。我们定义随机变量x：买土鸡蛋，那么事件A和B的区别是什么？有人可能说，那就是李四多买了4个土鸡蛋？这个答案只能得50分，因为忘记了"坐标系"的问题。换句话说，对于张三来说，李四多买了4个土鸡蛋。对于李四来说，张三少买了4个土鸡蛋。 选取的参照物不同，那么得到的结果也不同 。更严谨的说，应该是说我们对于张三和李四买土鸡蛋的期望不同，可能张三天天买2个土鸡蛋，而李四可能因为孩子满月昨天才买了6个土鸡蛋，而平时从来不买。 KL散度的数学定义： 对于 离散事件 我们可以定义事件A和B的差别为(2.1)： 对于 连续事件 ，那么我们只是把求和改为求积分而已(2.2)。 从公式中可以看出： 如果 ，即两个事件分布完全相同，那么KL散度等于0。 观察公式2.1，可以发现减号左边的就是事件A的熵，请记住这个发现 。 如果颠倒一下顺序求 ，那么就需要使用B的熵，答案就不一样了。 所以KL散度来计算两个分布A与B的时候是不是对称的，有“坐标系”的问题 ， 换句话说，KL散度由A自己的熵与B在A上的期望共同决定。当使用KL散度来衡量两个事件(连续或离散)，上面的公式意义就是求 A与B之间的对数差 在 A上的期望值。 3. KL散度 = 交叉熵 - 熵？ 如果我们默认了用KL散度来计算两个分布间的不同，那还要交叉熵做什么？ 事实上交叉熵和KL散度的公式非常相近，其实就是KL散度的后半部分(公式2.1)：A和B的交叉熵 = A与B的KL散度 - A的熵。 对比一下这是KL散度的公式： 这是熵的公式： 这是交叉熵公式： 此处最重要的观察是，如果 是一个常量，那么 ， 也就是说KL散度和交叉熵在特定条件下等价。这个发现是这篇回答的重点。 同时补充交叉熵的一些性质： 和KL散度相同，交叉熵也不具备对称性： ，此处不再赘述。 从名字上来看，Cross(交叉)主要是用于描述这是 两个事件 之间的相互关系，对自己求交叉熵等于熵。即 ，注意只是非负而不一定等于0。 </em>4. 另一种理解KL散度、交叉熵、熵的角度（选读）- 可跳过 那么问题来了，为什么有KL散度和交叉熵两种算法？为什么他们可以用来求分布的不同？什么时候可以等价使用？ 一种信息论的解释是 ： 熵的意义是对A事件中的随机变量进行编码所需的最小字节数。 KL散度的意义是“额外所需的编码长度”如果我们用B的编码来表示A。 交叉熵指的是当你用B作为密码本来表示A时所需要的“平均的编码长度”。 对于大部分读者，我觉得可以不用深入理解。感谢评论区@王瑞欣的指正，不知道为什么@不到他。 一些对比与观察： KL散度和交叉熵的不同处：交叉熵中不包括“熵”的部分 KL散度和交叉熵的相同处：a. 都不具备对称性 b. 都是非负的 等价条件（章节3）：当 固定不变时，那么最小化KL散度 等价于最小化交叉熵 。 既然等价，那么我们优先选择更简单的公式，因此选择交叉熵。 5. 机器如何“学习”？ 机器学习的过程就是希望在训练数据上 模型学到的分布 和 真实数据的分布 越接近越好，那么我们已经介绍过了....怎么最小化两个分布之间的不同呢？ 用默认的方法，使其KL散度最小 ！ 但我们没有真实数据的分布，那么只能退而求其次， 希望 模型学到的分 布和 训练数据的分布 尽量相同， 也就是把训练数据当做模型和真实数据之间的代理人 。假设训练数据是从总体中独立同步分布采样(Independent and identically distributed sampled)而来，那么我们可以利用最小化训练数据的经验误差来降低模型的泛化误差。简单说： 最终目的是希望学到的模型的分布和真实分布一致： 但真实分布是不可知的，我们只好假设 训练数据 是从真实数据中独立同分布采样而来： 退而求其次，我们希望学到的模型分布至少和训练数据的分布一致 由此非常理想化的看法是如果 模型(左) 能够学到 训练数据(中) 的分布，那么应该近似的学到了 真实数据(右) 的分布： 6. 为什么交叉熵可以用作代价？ 接着上一点说，最小化模型分布 与 训练数据上的分布 的差异 等价于 最小化这两个分布间的KL散度，也就是最小化 。 比照第四部分的公式： 此处的A就是数据的真实分布： 此处的B就是模型从训练数据上学到的分布： 巧的是， 训练数据的分布A是给定的 。那么根据 我们在第四部分说的，因为A固定不变，那么求 等价于求 ，也就是A与B的交叉熵 。 得证，交叉熵可以用于计算“学习模型的分布”与“训练数据分布”之间的不同。当交叉熵最低时(等于训练数据分布的熵)，我们学到了“最好的模型”。 但是，完美的学到了训练数据分布往往意味着过拟合，因为训练数据不等于真实数据，我们只是假设它们是相似的，而一般还要假设存在一个高斯分布的误差，是模型的泛化误差下线。 7. 总结 因此在评价机器学习模型时，我们往往不能只看训练数据上的误分率和交叉熵，还是要关注测试数据上的表现。如果在测试集上的表现也不错，才能保证这不是一个过拟合或者欠拟合的模型。交叉熵比照误分率还有更多的优势，因为它可以和很多概率模型完美的结合。 所以逻辑思路是，为了让学到的模型分布更贴近真实数据分布，我们最小化 模型数据分布 与 训练数据之间的KL散度，而因为训练数据的分布是固定的，因此最小化KL散度等价于最小化交叉熵。 因为等价，而且交叉熵更简单更好计算，当然用它咯 ʕ•ᴥ•ʔ [1] 不同的领域都有不同解释，更传统的机器学习说法是似然函数的最大化就是交叉熵。 正所谓横看成岭侧成峰，大家没必要非说哪种思路是对的，有道理就好 。</p>
<h2>4. 损失函数｜交叉熵损失函数</h2>
<ul>
<li>链接：https://www.zhihu.com/tardis/zm/art/35709485</li>
<li>来源：bing</li>
<li>摘要：2022年3月28日 · 3. 学习过程 交叉熵损失函数经常用于分类问题中，特别是在神经网络做分类问题时，也经常使用交叉熵作为损失函数，此外，由于交叉熵涉及到计算每个类别的概率，所以交叉熵几乎每次 …</li>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>交叉熵损失函数适用于分类问题。</li>
<li>交叉熵损失函数可以捕捉模型预测效果的差异。</li>
<li>交叉熵损失函数是凸函数，易于求导。</li>
<li>交叉熵损失函数结合sigmoid/softmax函数使用。</li>
<li>交叉熵损失函数在模型效果差时学习速度快。</li>
<li>交叉熵损失函数在模型效果好时学习速度慢。</li>
<li>交叉熵损失函数对于不同情况导数结果一致。</li>
<li>交叉熵损失函数对于二分类和多分类求导形式一致。</li>
<li>交叉熵损失函数的偏导值反映了模型的错误程度。</li>
<li>交叉熵损失函数在分类数目增大时参数也增大。</li>
</ul>

<h3>正文（抓取，非 AI）</h3>
<p>这篇文章中，讨论的Cross Entropy损失函数常用于分类问题中，但是为什么它会在分类问题中这么有效呢？我们先从一个简单的分类例子来入手。 1. 图像分类任务 我们希望根据图片动物的轮廓、颜色等特征，来预测动物的类别，有三种可预测类别：猫、狗、猪。假设我们当前有两个模型（参数不同），这两个模型都是通过sigmoid/softmax的方式得到对于每个预测结果的概率值： 模型1 ： 预测 真实 是否正确 0.3 0.3 0.4 0 0 1 (猪) 正确 0.3 0.4 0.3 0 1 0 (狗) 正确 0.1 0.2 0.7 1 0 0 (猫) 错误 模型1 对于样本1和样本2以非常微弱的优势判断正确，对于样本3的判断则彻底错误。 模型2 ： 预测 真实 是否正确 0.1 0.2 0.7 0 0 1 (猪) 正确 0.1 0.7 0.2 0 1 0 (狗) 正确 0.3 0.4 0.3 1 0 0 (猫) 错误 模型2 对于样本1和样本2判断非常准确，对于样本3判断错误，但是相对来说没有错得太离谱。 好了，有了模型之后，我们需要通过定义损失函数来判断模型在样本上的表现了，那么我们可以定义哪些损失函数呢？ 1.1 Classification Error（分类错误率） 最为直接的损失函数定义为： 模型1： 模型2： 我们知道， 模型1 和 模型2 虽然都是预测错了1个，但是相对来说 模型2 表现得更好，损失函数值照理来说应该更小，但是，很遗憾的是， 并不能判断出来，所以这种损失函数虽然好理解，但表现不太好。 1.2 Mean Squared Error (均方误差) 均方误差损失也是一种比较常见的损失函数，其定义为： 模型1： 对所有样本的loss求平均： 模型2： 对所有样本的loss求平均： 我们发现，MSE能够判断出来 模型2 优于 模型1 ，那为什么不采样这种损失函数呢？主要原因是在分类问题中，使用sigmoid/softmx得到概率，配合MSE损失函数时，采用梯度下降法进行学习时，会出现模型一开始训练时，学习速率非常慢的情况（ MSE损失函数 ）。 有了上面的直观分析，我们可以清楚的看到，对于分类问题的损失函数来说，分类错误率和均方误差损失都不是很好的损失函数，下面我们来看一下交叉熵损失函数的表现情况。 1.3 Cross Entropy Loss Function（交叉熵损失函数） 1.3.1 表达式 (1) 二分类 在二分的情况下，模型最后需要预测的结果只有两种情况，对于每个类别我们的预测得到的概率为 和 ，此时表达式为（ 的底数是 ）： 其中： - —— 表示样本 的label，正类为 ，负类为 - —— 表示样本 预测为正类的概率 (2) 多分类 多分类的情况实际上就是对二分类的扩展： 其中： - ——类别的数量 - ——符号函数（ 或 ），如果样本 的真实类别等于 取 ，否则取 - ——观测样本 属于类别 的预测概率 现在我们利用这个表达式计算上面例子中的损失函数值： 模型1 ： 对所有样本的loss求平均： 模型2： 对所有样本的loss求平均： 上述计算可以使用python的sklearn库 可以发现，交叉熵损失函数可以捕捉到 模型1 和 模型2 预测效果的差异。 2. 函数性质 可以看出，该函数是凸函数，求导时能够得到全局最优值。 3. 学习过程 交叉熵损失函数经常用于分类问题中，特别是在神经网络做分类问题时，也经常使用交叉熵作为损失函数，此外，由于交叉熵涉及到计算每个类别的概率，所以交叉熵几乎每次都和 sigmoid(或softmax)函数 一起出现。 我们用神经网络最后一层输出的情况，来看一眼整个模型预测、获得损失和学习的流程： 神经网络最后一层得到每个类别的得分 scores（也叫logits） ； 该得分经过 sigmoid(或softmax)函数 获得概率输出； 模型预测的类别概率输出与真实类别的one hot形式进行交叉熵损失函数的计算。 学习任务分为二分类和多分类情况，我们分别讨论这两种情况的学习过程。 3.1 二分类情况 二分类交叉熵损失函数学习过程 如上图所示，求导过程可分成三个子过程，即拆成三项偏导的乘积： 3.1.1 计算第一项： - 表示样本 预测为正类的概率 - 为符号函数，样本 为正类时取 ，否则取 3.1.2 计算第二项： 这一项要计算的是sigmoid函数对于score的导数，我们先回顾一下sigmoid函数和分数求导的公式： 3.1.3 计算第三项： 一般来说，scores是输入的线性函数作用的结果，所以有： 3.1.4 计算结果 可以看到，我们得到了一个非常漂亮的结果，所以，使用交叉熵损失函数，不仅可以很好的衡量模型的效果，又可以很容易的的进行求导计算。 3.2 多分类情况 多分类交叉熵损失函数学习过程 如上图所示，求导过程可以分为三个子过程： 和二分类区别在于： 因为多分类只有一个类别为 ，其他为 ，不失一般性，我们可以假设 为 ，其他为 ，所以损失函数求和式子中只有 这项不为 ，即 这一项求导时，需要针对 是否等于 进行分类讨论（这里 表示的是样本真实类别为 ， 表示的是对我们想对输入到 的参数 求导） 3.2.1 计算第一项： 不失一般性，我们可以假设 为 ，其他为 ，则 求导： 3.2.2 计算第二项： 这一项要计算的是softmax函数对于得分的导数，我们先回顾一下softmax函数和分数求导的公式： 这里 表示的是样本真实类别为 ， 表示的是对输入到 的参数 求导，这时候存在两种情况： 情况1: 则第二项的求导式子，可以写成： 求导后得 情况2: 此时 这一项只在分母中存在，求导后得： 3.2.3 计算第三项： 一般来说，scores是输入的线性函数作用的结果，所以有： 3.2.4 计算结果 情况1: 情况2: 不失一般性，我们上述假设样本的真实类别为 ，则有: 我们求导时，对不同情况带入 的值后，得到了一致的表达式，如果采用向量化的形式，那么导数就不用再分情况写了，统一成： 可以看出，交叉熵损失函数对于二分类和多分类求导时，采用向量化的形式后，求导结果的形式是一致的。 4. 优缺点 4.1 优点 在用梯度下降法做参数更新的时候，模型学习的速度取决于两个值：一、 学习率 ；二、 偏导值 。其中，学习率是我们需要设置的超参数，所以我们重点关注偏导值。从上面的式子中，我们发现，偏导值的大小取决于 和 ，我们重点关注后者，后者的大小值反映了我们模型的错误程度，该值越大，说明模型效果越差，但是该值越大同时也会使得偏导值越大，从而模型学习速度更快。所以，使用逻辑函数得到概率，并结合交叉熵当损失函数时，在模型效果差的时候学习速度比较快，在模型效果好的时候学习速度变慢。 4.2 缺点 Deng [4]在2019年提出了ArcFace Loss，并在论文里说了Softmax Loss的两个缺点：1、随着分类数目的增大，分类层的线性变化矩阵参数也随着增大；2、对于封闭集分类问题，学习到的特征是可分离的，但对于开放集人脸识别问题，所学特征却没有足够的区分性。对于人脸识别问题，首先人脸数目(对应分类数目)是很多的，而且会不断有新的人脸进来，不是一个封闭集分类问题。 另外，sigmoid(softmax)+cross-entropy loss 擅长于学习类间的信息，因为它采用了类间竞争机制，它只关心对于正确标签预测概率的准确性，忽略了其他非正确标签的差异，导致学习到的特征比较散。基于这个问题的优化有很多，比如对softmax进行改进，如L-Softmax、SM-Softmax、AM-Softmax等。 5. 参考 [1]. 博客 - 神经网络的分类模型 LOSS 函数为什么要用 CROSS ENTROPY [2]. 博客 - Softmax as a Neural Networks Activation Function [3]. 博客 - A Gentle Introduction to Cross-Entropy Loss Function [4]. Deng, Jiankang, et al. "Arcface: Additive angular margin loss for deep face recognition." Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019.</p>
</div>
<script>
var articles=[]; var currentArticleIdx=-1; var maxSpeakChars=3500;
function buildArticles(){ var c=document.querySelector('.content'); if(!c) return; var h2s=c.querySelectorAll('h2'); for(var i=0;i<h2s.length;i++){ var start=h2s[i], end=i+1<h2s.length?h2s[i+1]:null; var r=document.createRange(); r.setStart(start,0); if(end) r.setEnd(end,0); else { var last=c.lastElementChild||c; r.setEndAfter(last); } var t=r.toString().replace(/\s+/g,' ').trim(); if(t.length>maxSpeakChars) t=t.slice(0,maxSpeakChars)+'…（内容过长已截断）'; articles.push({h2:start,text:t}); }
 articles.sort(function(a,b){ return a.text.length-b.text.length; }); }
function speakText(t){ if(!t){ document.getElementById('readStatus').textContent=''; return; }
 speechSynthesis.cancel(); var status=document.getElementById('readStatus'); status.textContent='准备中…';
 var chunks=[]; var maxLen=280; for(var i=0;i<t.length;i+=maxLen){ var s=t.slice(i,i+maxLen); var j=s.lastIndexOf('。'); if(j>80){ chunks.push(s.slice(0,j+1)); i-=s.length-(j+1); } else chunks.push(s); }
 if(chunks.length===0) chunks=[t]; var idx=0; function speakNext(){ if(idx>=chunks.length){ status.textContent=''; return; } var u=new SpeechSynthesisUtterance(chunks[idx]); u.lang='zh-CN'; u.rate=0.95; var v=speechSynthesis.getVoices().filter(function(x){ return x.lang.indexOf('zh')>=0; })[0]; if(v) u.voice=v;
 u.onend=function(){ idx++; setTimeout(speakNext,80); }; u.onerror=function(){ idx++; setTimeout(speakNext,80); }; status.textContent='朗读中 '+(idx+1)+'/'+chunks.length+'…'; speechSynthesis.speak(u); }
 if(speechSynthesis.getVoices().length) speakNext(); else speechSynthesis.onvoiceschanged=function(){ speechSynthesis.onvoiceschanged=null; speakNext(); }; }
function nextArticle(){ if(articles.length===0) buildArticles(); if(articles.length===0){ document.getElementById('readStatus').textContent='无可读内容'; return; }
 speechSynthesis.cancel(); currentArticleIdx++; if(currentArticleIdx>=articles.length){ currentArticleIdx=0; document.getElementById('readStatus').textContent='已读完，共 '+articles.length+' 篇。再按从第 1 篇开始'; return; }
 var a=articles[currentArticleIdx]; a.h2.scrollIntoView({behavior:'smooth',block:'start'}); document.getElementById('readStatus').textContent='第 '+(currentArticleIdx+1)+' / '+articles.length+' 篇'; speakText(a.text); }
function readSelected(){ var sel=window.getSelection(); var t=(sel&&sel.toString)?sel.toString():''; t=t.replace(/\\s+/g,' ').trim(); if(!t){ document.getElementById('readStatus').textContent='请先选中要朗读的段落'; return; } speechSynthesis.cancel(); document.getElementById('readStatus').textContent='朗读选中内容…'; speakText(t); }
function getBlockForTap(el){ var c=document.querySelector('.content'); var blockTags=['P','DIV','H2','H3','H4','LI','PRE']; while(el&&el!==c){ if(c.contains(el)&&blockTags.indexOf(el.tagName)!==-1) return el; el=el.parentElement; } return null; }
function onContentTap(e){ if(e.target.closest&&e.target.closest('a')) return; var block=getBlockForTap(e.target); if(block){ var t=(block.innerText||'').trim().replace(/\\s+/g,' '); if(t){ e.preventDefault(); speechSynthesis.cancel(); document.getElementById('readStatus').textContent='朗读本段…'; speakText(t); } } }
if(document.readyState==='loading'){ document.addEventListener('DOMContentLoaded', function(){ buildArticles(); var c=document.querySelector('.content'); if(c) c.addEventListener('click', onContentTap); }); } else { buildArticles(); var c=document.querySelector('.content'); if(c) c.addEventListener('click', onContentTap); }
</script>
</body>
</html>
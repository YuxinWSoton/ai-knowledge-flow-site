<!DOCTYPE html>
<html lang="zh-CN">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<title>知识流日报 2026-02-17：transformer、伴随函数非线性</title>
<style>
  * { box-sizing: border-box; }
  body {
    font-family: system-ui, sans-serif;
    max-width: 720px;
    margin: 0 auto;
    padding: 2rem 1rem;
    color: #fff;
    min-height: 100vh;
    background: #0a0e1a;
    background-image:
      radial-gradient(2px 2px at 20px 30px, rgba(255,255,255,0.9), transparent),
      radial-gradient(2px 2px at 40px 70px, rgba(255,255,255,0.6), transparent),
      radial-gradient(1.5px 1.5px at 90px 40px, rgba(255,255,255,0.8), transparent),
      radial-gradient(2px 2px at 130px 80px, rgba(255,255,255,0.5), transparent),
      radial-gradient(1.5px 1.5px at 160px 120px, rgba(255,255,255,0.7), transparent),
      radial-gradient(ellipse 120% 100% at 50% 0%, rgba(88,60,120,0.25), transparent 50%),
      radial-gradient(ellipse 80% 80% at 80% 20%, rgba(40,60,100,0.2), transparent 40%);
    background-size: 200px 200px;
    background-repeat: repeat;
  }
  a { color: #a8d4ff; text-decoration: none; }
  a:hover { text-decoration: underline; }
 h1,h2,h3{margin-top:1.5rem;color:#fff;} pre{white-space:pre-wrap;color:rgba(255,255,255,0.9);} .toolbar{margin:0.5rem 0;color:rgba(255,255,255,0.8);} .content{color:rgba(255,255,255,0.95);} .content a{color:#a8d4ff;} .meta{color:rgba(255,255,255,0.65);font-size:0.9em;} .content p,.content div,.content h2,.content h3,.content h4,.content li,.content pre{cursor:pointer;-webkit-tap-highlight-color:rgba(255,255,255,0.15);}</style>
</head>
<body>
<p><a href="https://YuxinWSoton.github.io/ai-knowledge-flow-site/">← 返回首页</a></p>
<h1>知识流日报 2026-02-17：transformer、伴随函数非线性</h1>
<p class="meta" style="margin-top:0;">最后更新：2026-02-17 10:26</p>
<p class="toolbar"><button id="btnNext" onclick="nextArticle()">下一篇</button> <button id="btnSel" onclick="readSelected()">朗读这段</button> <span id="readStatus"></span></p>
<p class="meta" style="margin-top:0;">（点任意段落即可朗读，手机/触屏友好；也可选中后点「朗读这段」或点「下一篇」按条朗读）</p>
<div class="content">
<h1>知识流日报 2026-02-17：transformer、伴随函数非线性</h1>
<p>共 2 条（来自百度学术 / Google / Bing 等，仅含正文抓取成功的条目；按正文长度从短到长排列，便于先听短的）。</p>
<p>（以下含抓取正文，便于阅读。未调用任何 AI API。）</p>
<h2>1. 一文了解Transformer全貌（图解Transformer）</h2>
<ul>
<li>链接：https://www.zhihu.com/tardis/zm/art/600773858</li>
<li>来源：bing</li>
<li>摘要：2025年9月26日 · 网上有关Transformer原理的介绍很多，在本文中我们将尽量模型简化，让普通读者也能轻松理解。 1. Transformer整体结构 在机器翻译中，Transformer可以将一种语言翻译成另一种语 …</li>
</ul>
<h3>正文（抓取，非 AI）</h3>
<p>打个小广告 ☻，知乎专栏 《大模型前沿应用》 的内容已经收录在新书 《揭秘大模型：从原理到实战》 中。感兴趣的朋友可以购买，多谢支持！♥♥ 自2017年Google推出Transformer以来，基于其架构的语言模型便如雨后春笋般涌现，其中Bert、T5等备受瞩目，而近期风靡全球的大模型ChatGPT和LLaMa更是大放异彩。网络上关于Transformer的解析文章非常大，但本文将力求用浅显易懂的语言，为大家深入解析Transformer的技术内核。 前言 Transformer是谷歌在2017年的论文《Attention Is All You Need》中提出的，用于NLP的各项任务，现在是谷歌云TPU推荐的参考模型。网上有关Transformer原理的介绍很多，在本文中我们将尽量模型简化，让普通读者也能轻松理解。 1. Transformer整体结构 在机器翻译中，Transformer可以将一种语言翻译成另一种语言，如果把Transformer看成一个黑盒，那么其结构如下图所示： 将法语翻译成英语 那么拆开这个黑盒，那么可以看到Transformer由若干个编码器和解码器组成，如下图所示： 继续将Encoder和Decoder拆开，可以看到完整的结构，如下图所示： Transformer整体结构（引自谷歌论文） 可以看到Encoder包含一个Muti-Head Attention模块，是由多个Self-Attention组成，而Decoder包含两个Muti-Head Attention。Muti-Head Attention上方还包括一个 Add &amp; Norm 层，Add 表示残差连接 (Residual Connection) 用于防止网络退化，Norm 表示 Layer Normalization，用于对每一层的激活值进行归一化。 假设我们的输入包含两个单词，我们看一下Transformer的整体结构： Transformer整体结构（输入两个单词的例子） 为了能够对Transformer的流程有个大致的了解，我们举一个简单的例子，还是以之前的为例，将法语"Je suis etudiant"翻译成英文。 第一步 ：获取输入句子的每一个单词的表示向量 ， 由单词的Embedding和单词位置的Embedding 相加得到。 Transformer输入表示 第二步 ：将单词向量矩阵传入Encoder模块，经过N个Encoder后得到句子所有单词的编码信息矩阵 ，如下图。输入句子的单词向量矩阵用 表示，其中 是单词个数， 表示向量的维度（论文中 ）。每一个Encoder输出的矩阵维度与输入完全一致。 输入X经过Encoder输出编码矩阵C 第三步 ：将Encoder输出的编码矩阵 传递到Decoder中，Decoder会根据当前翻译过的单词 翻译下一个单词 ，如下图所示。 Transformer Decoder预测 上图Decoder接收了Encoder的编码矩阵，然后首先输入一个开始符 "<Begin>"，预测第一个单词，输出为"I"；然后输入翻译开始符 "<Begin>" 和单词 "I"，预测第二个单词，输出为"am"，以此类推。这是Transformer的大致流程，接下来介绍里面各个部分的细节。 2. Transformer的输入表示 Transformer中单词的输入表示由 单词Embedding 和 位置Embedding （Positional Encoding）相加得到。 Transformer输入表示 2.1 单词Embedding 单词的Embedding可以通过Word2vec等模型预训练得到，可以在Transformer中加入Embedding层。 2.2 位置Embedding Transformer 中除了单词的Embedding，还需要使用位置Embedding 表示单词出现在句子中的位置。 因为 Transformer不采用RNN结构，而是使用全局信息，不能利用单词的顺序信息，而这部分信息对于NLP来说非常重要。 所以Transformer中使用位置Embedding保存单词在序列中的相对或绝对位置。 位置Embedding用 表示， 的维度与单词Embedding相同。 可以通过训练得到，也可以使用某种公式计算得到。在Transformer中采用了后者，计算公式如下： 其中， 表示单词在句子中的位置， 表示 的维度。 3. Multi-Head Attention（多头注意力机制） Transformer内部结构 上图是Transformer的内部结构，其中红色方框内为 Multi-Head Attention ，是由多个 Self-Attention 组成，具体结构如下图： Self-Attention和Multi-Head Attention 因为 Self-Attention 是Transformer的重点，所以我们重点关注 Multi-Head Attention 以及 Self-Attention，首先介绍下Self-Attention的内部逻辑。 3.1 Self-Attention结构 Self-Attention结构 上图是Self-Attention结构，最下面是 (查询)、 (键值)、 (值)矩阵，是通过输入矩阵 和权重矩阵 相乘得到的。 Q,K,V的计算 得到 之后就可以计算出Self-Attention的输出，如下图所示： Self-Attention输出 3.2 Multi-Head Attention输出 在上一步，我们已经知道怎么通过Self-Attention计算得到输出矩阵 ，而Multi-Head Attention是由多个Self-Attention组合形成的，下图是论文中Multi-Head Attention的结构图。 Multi-Head Attention 从上图可以看到Multi-Head Attention包含多个Self-Attention层，首先将输入 分别传递到 个不同的Self-Attention中，计算得到 个输出矩阵 。下图是 的情况，此时会得到 8 个输出矩阵 。 多个Self-Attention 得到8个输出矩阵 后，Multi-Head Attention将它们拼接在一起（Concat），然后传入一个Linear层，得到Multi-Head Attention最终的输出矩阵 。 Multi-Head Attention输出 4. 编码器Encoder结构 Transformer Encoder模块 上图红色部分是Transformer的Encoder结构， 表示Encoder的个数，可以看到是由Multi-Head Attention、Add &amp; Norm、Feed Forward、Add &amp; Norm组成的。前面已经介绍了Multi-Head Attention的计算过程，现在了解一下Add &amp; Norm和 Feed Forward部分。 4.1 单个Encoder输出 Add &amp; Norm 是指残差连接后使用LayerNorm，表示如下： 其Sublayer表示经过的变换，比如第一个Add &amp; Norm中Sublayer表示Multi-Head Attention。 Feed Forward 是指全连接层，表示如下： 因此输入矩阵 经过一个Encoder后，输出表示如下： 4.2 多个Encoder输出 通过上面的单个Encoder，输入矩阵 ，最后输出矩阵 。通过多个Encoder叠加，最后便是编码器Encoder的输出。 5. 解码器Decoder结构 Transformer Decoder模块 上图红色部分为Transformer的Decoder结构，与Encoder相似，但是存在一些区别： 包含两个Multi-Head Attention 第一个Multi-Head Attention采用了Masked操作 第二个Multi-Head Attention的 矩阵使用Encoder的 编码信息矩阵 进行计算，而 使用上一个 Decoder的输出计算 最后有一个Softmax层计算下一个翻译单词的概率 5.1 第一个Multi-Head Attention Decoder的第一个Multi-Head Attention采用了Masked操作，因为在翻译的过程中是顺序翻译的，即翻译完第 个单词，才可以翻译第 个单词。通过 Masked 操作可以防止第 个单词知道 个单词之后的信息。下面以法语"Je suis etudiant"翻译成英文"I am a student"为例，了解一下 Masked 操作。 在Decoder的时候，需要根据之前翻译的单词，预测当前最有可能翻译的单词，如下图所示。首先根据输入"<Begin>"预测出第一个单词为"I"，然后根据输入"<Begin> I" 预测下一个单词 "am"。 Decoder预测（右图有问题，应该是Decoder 1） Decoder在预测第 个输出时，需要将第 之后的单词掩盖住， Mask操作是在Self-Attention的Softmax之前使用的， 下面以前面的"I am a student"为例。 第一步： 是Decoder的输入矩阵和 Mask 矩阵，输入矩阵包含"<Begin> I am a student"4个单词的表示向量， Mask 是一个 的矩阵。在 Mask 可以发现单词"<Begin>"只能使用单词"<Begin>"的信息，而单词"I"可以使用单词"<Begin> I"的信息，即只能使用之前的信息。 输入矩阵与Mask矩阵 第二步 ：接下来的操作和之前Encoder中的Self-Attention一样，只是在Softmax之前需要进行Mask操作。 Mask Self-Attention输出 第三步 ：通过上述步骤就可以得到一个Mask Self-Attention的输出矩阵 ，然后和Encoder类似，通过Multi-Head Attention拼接多个输出 然后计算得到第一个Multi-Head Attention的输出 ， 与输入 维度一样。 5.2 第二个Multi-Head Attention Decoder的第二个Multi-Head Attention变化不大， 主要的区别在于其中Self-Attention的 矩阵不是使用上一个Multi-Head Attention的输出，而是使用 Encoder的编码信息矩阵 计算的。根据Encoder的输出 计算得到 ，根据上一个Multi-Head Attention的输出 计算 。这样做的好处是在Decoder的时候，每一位单词（这里是指"I am a student"）都可以利用到Encoder所有单词的信息（这里是指"Je suis etudiant"）。 6. Softmax预测输出 Softmax预测输出 编码器Decoder最后的部分是利用 Softmax 预测下一个单词，在Softmax之前，会经过Linear变换，将维度转换为词表的个数。 假设我们的词表只有6个单词，表示如下： 词表 因此，最后的输出可以表示如下： Softmax预测输出示例 总结 Transformer由于可并行、效果好等特点，如今已经成为机器翻译、特征抽取等任务的基础模块，目前ChatGPT特征抽取的模块用的就是Transformer，这对于后面理解ChatGPT的原理做了好的铺垫。 代码实现 绝密伏击：OPenAI ChatGPT（一）：Tensorflow实现Transformer 参考 初识CV：Transformer模型详解（图解最完整版） 数据汪：BERT大火却不懂Transformer？读这一篇就够了 The Illustrated Transformer 忆臻：搞懂Transformer结构，看这篇PyTorch实现就够了（上） The Annotated Transformer https:// arxiv.org/pdf/1706.0376 2.pdf 青空栀浅：图解Transformer Ph0en1x：Transformer结构及其应用详解--GPT、BERT、MT-DNN、GPT-2 大师兄：ChatGPT/InstructGPT详解 张俊林：ChatGPT会取代搜索引擎吗 张俊林：放弃幻想，全面拥抱Transformer：自然语言处理三大特征抽取器（CNN/RNN/TF）比较</p>
<h2>2. 挑战 Transformer：全新架构 Mamba 详解</h2>
<ul>
<li>链接：https://www.zhihu.com/tardis/zm/art/684231320</li>
<li>来源：bing</li>
<li>摘要：2025年9月23日 · 而就在最近，一名为 Mamba 的架构似乎打破了这一局面。 与类似规模的 Transformer 相比， Mamba 具有 5 倍的吞吐量， 而且 Mamba-3B 的效果与两倍于其规模的 Transformer 相当。 …</li>
</ul>
<h3>正文（抓取，非 AI）</h3>
<p>打个小广告 ☻，知乎专栏 《大模型前沿应用》 的内容已经收录在新书 《揭秘大模型：从原理到实战》 中。感兴趣的朋友可以购买，多谢支持！♥♥ 背景 屹立不倒的 Transformer 迎来了一个强劲竞争者。 自 2017 年被提出以来，Transformer 已经成为 AI 大模型的主流架构，但随着模型规模的扩展和需要处理的序列不断变长，Transformer 的局限性也逐渐凸显。一个很明显的缺陷是：Transformer 模型中自注意力机制的计算量会随着上下文长度的增加呈平方级增长，比如上下文增加 32 倍时，计算量可能会增长 1000 倍，计算效率非常低。 为了克服这些缺陷，研究者们开发出了很多注意力机制的高效变体，但这往往以牺牲其有效性特为代价。到目前为止，这些变体都还没有被证明能在不同领域发挥有效作用。 而就在最近，一名为 Mamba 的架构似乎打破了这一局面。 与类似规模的 Transformer 相比， Mamba 具有 5 倍的吞吐量 ， 而且 Mamba-3B 的效果与两倍于其规模的 Transformer 相当 。性能高、效果好，Mamba 成为新的研究热点。 图1 Mamba 在推理过程中的吞吐量对比 本文将详细的解读 Mamba 架构，由于 Mamba 是基于 SSM-&gt;HiPPO-&gt;S4-&gt;Mamba 演化过来的，而 HiPPO、S4、Mamba 的一作者都是卡内基梅隆大学机器学习系助理教授 Albert Gu 。因此，本文将从标准 SSM 开始，逐步介绍 HiPPO、S4、Mamba。 图2总结了SSM、HiPPO、S4、Mamba的主要区别，以及各个模型的主要内容。本文内容也将按图中内容展开。 图2-2：HiPPO、S4、Mamba 一、现有架构问题 序列建模的核心问题是：同时解决 有效 和 高效 。有效是指能够选择性记忆历史信息，解决 长距离依赖 （Long-Range Dependencies，LRDs）问题；高效是指计算高效。 尽管传统的模型如循环神经网络（RNNs）、卷积神经网络（CNNs）和 Transformers 在处理长距离依赖方面有专门的变体，但它们在 处理超过 10000 步的极长序列时仍然面临挑战 。 1.1 Transformer 问题 Transformer 的一个主要优点是，无论它接收到多长的输入，它都使用序列中的所有 token 信息（无论序列有多长）来对输入数据进行处理。 图1-1：Transformer会查看过去所有 token 但是为了获得全局信息，注意力机制在长序列上非常耗费显存。注意力创建一个矩阵，将每个 token 与之前的每个 token 进行比较。矩阵中的权重由 token 对之间的相关性决定。 图1-2：Transformer 会计算每个 token 之间的 Attention 在训练过程中，Attention 计算可以并行化，所以可以极大地加快训练速度。但是在推理过程中，当生成下一个 token 时，我们需要重新计算整个序列的注意力。 图1-3：生成新 token 时需要重新计算整个序列的注意力 长度为 L 的序列生成 token 大约需要 L² 的计算量，如果序列长度增加，计算量会平方级增长。因此， 需要重新计算整个序列是 Transformer 体系结构的主要瓶颈 。 图1-4：Transformer 训练快、推理慢 1.2 RNN 的问题 图1-5：循环神经网络 RNN 在生成输出时，RNN 只需要考虑之前的隐藏状态和当前的输入。这样不会重新计算以前的隐藏状态，这正Transformer 不具备的。 这种结构可以让 RNN 进行 快速推理 ，并且理论上可以无限扩展上下文长度，因为每次推理只取一个隐藏状态和当前输入，内存占用非常稳定。 RNN 的每个隐藏状态都是之前所有隐藏状态的聚合。但是这里会有一个问题，在生成 token "Liang" 时，最后一个隐藏状态不再包含关于 token "Hello" 的信息。这会导致随着时间的推移，RNN 会忘记更久的信息，因为它只考虑前一个状态。 图1-6：只考虑前一个 hidden state 并且 RNN 的这种顺序性产生了另一个问题。 训练不能并行进行 ，因为它需要按顺序完成每一步。 图1-7：RNN 训练不能并行 RNN的统一定义为： 其中 是每一步的输出，它由当前输入 和前一时刻输出 共同决定，而θ则是可训练参数。那么参数θ的梯度可以表示为： 可以看到，当前梯度依赖上个 token 的梯度。 与 Transformer 相比，RNN 的问题完全相反！它的 推理速度非常快，但不能并行化导致训练很慢 。 图1-8：RNN 和 Transformer对比 人们一直在寻找一种既能像 Transformer 那样并行化训练，能够记住先前的信息，又能在推理时时间是随序列长度线性增长的模型，Mamba 就是这样应运而生的 。解下来我们从 SSM 开始，逐步介绍 Mamaba。 二、状态空间模型 SSM 2.1 什么是 SSM 状态空间模型（State Space Models，SSM）由简单的方程（3）定义。它将一维输入信号 映射到 N 维潜在状态 ，然后再投影到一维输出信号 。 其中， 是状态转移矩阵， 是输入到状态的矩阵， 是状态到输出的矩阵，D是直接从输入到输出的参数（很多时候取 D = 0）。 2.2 SSM 架构 下图是 SSM 的架构，主要包含两个部分：状态更新方程和输出方程。 图2-1：SSM结构 SSM 可以简化为以下结构： 图2-2：简化的SSM结构 下面我们看一下更详细的结构，首先是状态更新，如下所示： 图2-3：状态更新详细结构 备注 ：图中的输入 ，表示输入的信号是 D 维的。 SSM 也可以用于处理多维信号输入。 然后是输出方程，详细机构如下所示： 图2-4：输出方程详细结构 2.3 SSM 例子：弹簧振子 下面举一个描述弹簧振子系统的 SSM 例子。 图2-5：弹簧振子 考虑一个质量为 的物体，它连接在一个劲度系数为 的弹簧上，并且受到阻尼系数为 的阻尼力作用。当物体从平衡位置偏离时，它会在弹簧力的作用下进行振动。我们可以用状态空间模型来描述这个系统的动态。 状态变量可以选择为物体的位移 和速度 。输入 在这个例子中可以为零，因为我们没有外部力作用在物体上。输出 可以是我们感兴趣的位移 。 状态向量定义为： 输入向量为： 输出位移 。弹簧振子的状态空间方程可以表示为： 在了解 SSM 基本概念之后，接下来我们介绍基于 SSM 的 HiPPO 架构。 三、HiPPO（High-order Polynomial Projection Operators） HiPPO 是 Albert Gu 于2020年在论文 HiPPO: Recurrent Memory with Optimal Polynomial Projections 中提出的新架构。HiPPO 主要为了解决 如何在有限的存储空间中有效地解决序列建模的长距离依赖问题。 HiPPO 通过函数逼近产生状态矩阵 A 的最优解，有效的解决了长距离依赖问题。 问题背景： 在处理序列数据时，一个核心问题是如何在增量方式下表示累积的历史信息。这涉及到如何在有限的存储空间中有效地更新和维护历史数据的表示。 HiPPO框架 ：作者介绍了一个名为 HiPPO（High-order Polynomial Projection Operators）的通用框架，它通过将连续信号和离散时间序列投影到多项式基上，实现了在线数据压缩。 重要性度量 ：HiPPO 框架考虑了一个度量，用于指定过去每个时间步的重要性。这个度量帮助HiPPO产生在线函数逼近问题的最优解。 理论贡献 ：HiPPO 框架不仅提供了对现有记忆单元的简短推导，还推广了循环神经网络（如GRUs）中普遍存在的门控机制。 新的记忆更新机制 ：作者提出了一个新的记忆更新机制（HiPPO-LegS），它能够随时间扩展以记住所有历史信息，避免了对时间尺度的先验假设。 理论优势 ：HiPPO-LegS 具有时间尺度鲁棒性、快速更新和有界梯度的理论优势。 实验结果 ：在基准测试中，HiPPO-LegS 在打乱的 MNIST 数据集上达到了98.3%的新最佳准确率。在一个新的轨迹分类任务中，HiPPO-LegS 在处理分布外时间尺度和缺失数据方面，比其他 RNN 和神经 ODE（一阶常微分方程）基线模型的性能提高了25-40%的准确率。 下面介绍 HiPPO 实现的具体细节。 3.1 HiPPO 架构：高阶多项式投影 3.1.1 HiPPO问题设置 问题定义 给定一个在时间 上的输入函数 ，需要在每个时间点操作累计历史 ，以便理解到目前为止看到的输入并对未来进行预测。 由于函数空间的庞大，无法完美记住整个历史，因此需要将其进行压缩，HiPPO 提出了将历史投影到有界维数的子空间的一半方法。 函数逼近与度量 为了评估逼近的质量，需要在函数空间中定义一个距离。任何在 上的概率度量 都可以为平方可积函数空间提供内积 ，从而诱导出一个希尔伯特空间 和相应的范数 。 为了选择合适的子空间，需要一个度量来量化历史的重要性。这个度量 随时间变化，支持在 上，因为 只在时间t之前定义。 多项式基展开 任何 N 维的函数子空间 G 都是逼近的合适候选。参数 N 对应于逼近的阶数，或者说压缩的大小；投影的历史可以通过G的任何基的N个系数来表示。 论文中使用多项式作为自然基，因此G是小于N阶的多项式的集合。 在线逼近 由于我们关心在每个时间 t 对 的逼近，我们也让度量 随时间变化。总体上，我们寻找一个 ，使得 最小。直观上，度量 控制输入域各部分的重要性。 挑战 挑战在于如何在给定度量 的情况下以封闭形式解决优化问题，以及在 时如何线性地维护这些系数。 3.1.2 HiPPO 通用架构 通过连续动态系统计算投影 这部分是 HiPPO 的关键步骤，它涉及到将输入函数 在时间 t 投影到一个多项式空间上，以便在线更新记忆表示。 投影的表示 ：投影可以通过输入函数 在时间 t 的限制 的 N 个系数来表示。这些系数是通过在多项式空间的基上展开 得到的。 正交多项式基 ：为了选择合适的基，作者利用了正交多项式的性质。正交多项式为 提供了一个自然的基，使得 的投影可以表示为这些基的线性组合。 系数的计算 ：投影的系数 是通过内积 计算得到的，其中 是正交多项式基的元素。 连续动态系统 ：为了在线更新这些系数，作者提出了一个连续动态系统，这个系统描述了系数 是如何随时间 t 变化的。这种动态系统可以表示为 ，其中 是依赖于时间的矩阵。 投影操作符 ：作者定义了一个投影操作符 ，它将 映射到 （多项式空间）中的 ，使得 最小化。这个操作符是 HiPPO 框架的核心。 系数提取操作符 ：除了投影操作符，作者还定义了一个系数提取操作符 ，它将多项式 映射到其对应的系数 。 在线更新 ：通过这个连续动态系统，HiPPO 框架能够在线更新记忆表示，即随着新数据的到来，系统能实时地调整系数 。 在线函数逼近 图3-1：HiPPO框架 图2-6展示了 HiPPO 框架，首先需要找到投影 ，将输入 投影到多项式空间；然后将投影通过一组系数 来表示，这些系数捕捉了函数 的历史信息；使用连续时间下的一阶常微分方程来表示系数 如何随时间 t 动态变化；最后，将连续时间的动态变换转化为离散时间的递归关系（比如双线性变换），这允许 HiPPO 在每个时间步 k 更新系数 。 3.1.3 高阶投影：度量方法以及 HiPPO 动态系统 作者定义了两种度量方法，分别是 LegT 和 LagT。LegT 度量为最近的历史信息分配均匀的权重，表示如下： LagT 度量使用指数衰减的方式来衡量历史信息的重要性，表示如下： 对于 LegT 和 LagT，系数 可以使用 ODE（一阶常微分方程）来表示： 其中 A 和 B 是与度量 相关的矩阵。这个 ODE 描述了系数 如何随时间 t 和输入函数 变化。 备注：公式（9）是 HiPPO 框架的关键部分， 具体推导可以参看论文中的附录 D 。 对于 LegT 度量，矩阵 A 和 矩阵 B 可以表示如下： 对于 LagT 度量，可以表示如下： 3.1.4 HiPPO 框架中的连续时间动态转换为离散时间递归关系 由于我们处理的输入往往是离散的，因此我们需要将公式（9）的 ODE 离散化。ODE 离散化是一种常用的数据技术，它将 连续时间的常微分方程转换为离散时间的差分方程 。这通常涉及到选择一个合适的时间步长（或步长Δt），并使用数值方法（如欧拉方法、双线性）来近似连续微分。 图3-2：连续信号离散化 使用双线性离散化，如下所示： 结合公式（9）和公式（11），我们可以得到离散化的状态更新公式，表示如下： 离散化之后的 SSM 结构可以表示如下： 图3-3：离散化 SSM 在每个时间步长，我们计算当前输入( )如何影响前一个状态( )，然后计算预测输出( )。 图3-4：每个时间步的计算 这种表示看起来是不是有点熟悉？其实他的处理方法和RNN一样。 图3-5：离散化后和RNN类似 3.2 HiPPO-LegS HiPPO-LegS 是作者基于新的度量提出的全新架构，具有时间鲁棒性、有界梯度、有界近似误差、长时间记忆等效果。 新的度量表示为 ，在新的度量下，矩阵 A 和矩阵 B 可以表示如下： 具体推导在论文的附录 D.3 部分 。 更好的学习长期依赖 HiPPO-LegS 是专门为记忆而设计的 ，它通过其独特的结构和更新机制来避免梯度消失问题。LegS 通过使用Legendre 多项式作为基函数，并结合时间尺度不变的度量，来保持梯度的稳定性。 对于任何时间 ，HiPPO-LegS 在时间 的输出相对于时间 的输入的梯度范数为 ，这意味着梯度随着时间的增加而减小，但是衰减的速度比 RNN 的指数级慢的多。 这个性质使得 HiPPO-LegS 能够有效地缓解 RNN 中的梯度消失问题。即使在长序列中，梯度也不会迅速衰减到0，这有助于网络在训练中更好地学习长期依赖。 近似有界误差 HiPPO-LegS 在时间 t 的近似误差 。其中 N 是多项式的最高阶。这表明随着多项式的阶 N 的增加，误差逐渐减小。 3.3 实验 将 HiPPO 和 RNN 相结合，当前状态 不仅和上一个状态 有关，还和 HiPPO 状态 有关，如下所示： 模型结构如下： 图3-6：HiPPO和RNN结合 下面是pMINIST 数据集上的结果，可以看到 LegS 的效果要好于 LagT 和 LegT，同时 HiPPO 的效果好于之前的其它模型。 图3-7：HiPPO实验结果 备注： pMNIST（permuted MNIST）是一个经过修改的MNIST数据集 ，它用于测试和评估机器学习模型在处理序列数据和学习长期依赖关系方面的能力。在 pMNIST 中，原始 MNIST 图像的像素被重新排列。这意味着图像的像素不再是按照自然顺序（从左到右，从上到下）呈现，而是按照一个固定的、随机的排列顺序。这种排列方式使得模型必须学习像素之间的长期依赖关系，而不能简单地依赖于局部空间结构。 四、S4 (Structured State Space Model) S4 是 HiPPO 的后续工作，论文名称为： Efficiently Modeling Long Sequences with Structured State Spaces 。 S4 的主要工作是将 HiPPO 中的矩阵 A（称为 HiPPO 矩阵 ）转换为正规矩阵（正规矩阵可以分解为对角矩阵）和低秩矩阵的和，以此提高计算效率。 S4 通过这种分解，将计算复杂度降低到了 ，其中 N 是 HiPPO 矩阵的维度，L 是序列长度。 在处理长度为 16000 的序列的语音分类任务中，S4 模型将专门设计的语音卷积神经网络（Speech CNNs）的测试错误率降低了一半，达到了1.7%。相比之下，所有的循环神经网络（RNN）和 Transformer 基线模型都无法学习，错误率均在70%以上。 下面我们就来介绍一下这篇工作。 4.1 HiPPO 解决了长期依赖 作者讨论了如何处理长距离依赖（Long-Range Dependencies，LRDs）的问题，LRDs 是序列建模中的一个关键挑战，因为它们涉及到在序列中跨越大量时间步的依赖关系。 作者指出，基本的 SSM 在实际应用中表现不佳，特别是在处理 LRDs 时。这是因为线性一阶常微分方程（ODEs）的解通常是指数函数，这可能导致梯度在序列长度上呈指数级增长，从而引发梯度消失或爆炸的问题。 为了解决这个问题，作者利用了 HiPPO 理论。 HiPPO 理论指定了一类特殊的矩阵 A，当这些矩阵被纳入 SSM 的方程中时，可以使状态 x(t) 能够记住输入 u(t) 的历史信息。这些特殊矩阵被称为 HiPPO 矩阵，它们具有特定的数学形式，可以有效地捕捉长期依赖关系。 HiPPO 矩阵的一个关键特性是它们允许 SSM 在数学和实证上捕捉 LRDs 。例如，通过将随机矩阵 A 替换为 HiPPO 矩阵，可以在序列 MNIST 基准测试上显著提高 SSM 的性能。 HiPPO 矩阵表示如下： 4.2 在线推理：使用递归形式 S4 在推理时，使用公式（12）的递归形式，每次只需要和上一个状态进行计算，具有和 RNN 相似的推理效率。 4.3 训练 S4：卷积表示 由于离散时间 SSM 的递归性质，它在硬件上进行训练时存在效率问题。因此，作者将离散时间 SSM 的 递归方程转换为离散卷积的形式 。通过展开递归方程，可以得到一个卷积核，这个卷积核可以用来在序列数据上应用卷积操作。这种转换允许 SSM 利用快速傅里叶变换（FFT）等高效的卷积计算方法，从而在训练过程中提高计算效率。 上面式子可以转化为卷积的形式： 其中， 是一个与 SSM 的参数（A, B, C）相关的卷积核，可以通过离散傅里叶变换（DFT）和逆变换（IDFT）来计算。这种卷积表示不仅在理论上是可行的，而且在实践中也是非常有效的，因为它允许在保持模型性能的同时，显著减少训练过程中的计算和内存需求。 作者在这一节中还讨论了如何计算 SSMn卷积核，这是他们技术贡献的关键部分。通过这种卷积表示，SSM 可以被有效地训练，同时保持其在处理长距离依赖（LRDs）方面的能力。这种表示形式为 SSM 在各种序列建模任务中的应用提供了灵活性，包括图像处理、语音识别和时间序列分析等。 图4-1：SSM 卷积核形式 下面是一个具体的例子，如何使用卷积核生成输出。 图4-2：使用卷积核生成输出 卷积的一个主要好处是它可以并行训练。但是由于核大小是固定，它们的推理不如 RNN 快速并且对序列长度有限制。 图4-3：递归 SSM 和 卷积 SSM 的对比 这里可以使用一个简单的技巧，即根据任务选择表示。在训练过程中使用可以并行化的卷积表示，在推理过程中，我们使用高效的循环表示。 图4-4：递归推理、卷积训练 4.4 为什么对角化可以减少 SSM 计算复杂度 为了进一步提升计算效率，作者讨论了对角化在计算离散时间状态空间模型（</p>
</div>
<script>
var articles=[]; var currentArticleIdx=-1; var maxSpeakChars=3500;
function buildArticles(){ var c=document.querySelector('.content'); if(!c) return; var h2s=c.querySelectorAll('h2'); for(var i=0;i<h2s.length;i++){ var start=h2s[i], end=i+1<h2s.length?h2s[i+1]:null; var r=document.createRange(); r.setStart(start,0); if(end) r.setEnd(end,0); else { var last=c.lastElementChild||c; r.setEndAfter(last); } var t=r.toString().replace(/\s+/g,' ').trim(); if(t.length>maxSpeakChars) t=t.slice(0,maxSpeakChars)+'…（内容过长已截断）'; articles.push({h2:start,text:t}); }
 articles.sort(function(a,b){ return a.text.length-b.text.length; }); }
function speakText(t){ if(!t){ document.getElementById('readStatus').textContent=''; return; }
 speechSynthesis.cancel(); var status=document.getElementById('readStatus'); status.textContent='准备中…';
 var chunks=[]; var maxLen=280; for(var i=0;i<t.length;i+=maxLen){ var s=t.slice(i,i+maxLen); var j=s.lastIndexOf('。'); if(j>80){ chunks.push(s.slice(0,j+1)); i-=s.length-(j+1); } else chunks.push(s); }
 if(chunks.length===0) chunks=[t]; var idx=0; function speakNext(){ if(idx>=chunks.length){ status.textContent=''; return; } var u=new SpeechSynthesisUtterance(chunks[idx]); u.lang='zh-CN'; u.rate=0.95; var v=speechSynthesis.getVoices().filter(function(x){ return x.lang.indexOf('zh')>=0; })[0]; if(v) u.voice=v;
 u.onend=function(){ idx++; setTimeout(speakNext,80); }; u.onerror=function(){ idx++; setTimeout(speakNext,80); }; status.textContent='朗读中 '+(idx+1)+'/'+chunks.length+'…'; speechSynthesis.speak(u); }
 if(speechSynthesis.getVoices().length) speakNext(); else speechSynthesis.onvoiceschanged=function(){ speechSynthesis.onvoiceschanged=null; speakNext(); }; }
function nextArticle(){ if(articles.length===0) buildArticles(); if(articles.length===0){ document.getElementById('readStatus').textContent='无可读内容'; return; }
 speechSynthesis.cancel(); currentArticleIdx++; if(currentArticleIdx>=articles.length){ currentArticleIdx=0; document.getElementById('readStatus').textContent='已读完，共 '+articles.length+' 篇。再按从第 1 篇开始'; return; }
 var a=articles[currentArticleIdx]; a.h2.scrollIntoView({behavior:'smooth',block:'start'}); document.getElementById('readStatus').textContent='第 '+(currentArticleIdx+1)+' / '+articles.length+' 篇'; speakText(a.text); }
function readSelected(){ var sel=window.getSelection(); var t=(sel&&sel.toString)?sel.toString():''; t=t.replace(/\\s+/g,' ').trim(); if(!t){ document.getElementById('readStatus').textContent='请先选中要朗读的段落'; return; } speechSynthesis.cancel(); document.getElementById('readStatus').textContent='朗读选中内容…'; speakText(t); }
function getBlockForTap(el){ var c=document.querySelector('.content'); var blockTags=['P','DIV','H2','H3','H4','LI','PRE']; while(el&&el!==c){ if(c.contains(el)&&blockTags.indexOf(el.tagName)!==-1) return el; el=el.parentElement; } return null; }
function onContentTap(e){ if(e.target.closest&&e.target.closest('a')) return; var block=getBlockForTap(e.target); if(block){ var t=(block.innerText||'').trim().replace(/\\s+/g,' '); if(t){ e.preventDefault(); speechSynthesis.cancel(); document.getElementById('readStatus').textContent='朗读本段…'; speakText(t); } } }
if(document.readyState==='loading'){ document.addEventListener('DOMContentLoaded', function(){ buildArticles(); var c=document.querySelector('.content'); if(c) c.addEventListener('click', onContentTap); }); } else { buildArticles(); var c=document.querySelector('.content'); if(c) c.addEventListener('click', onContentTap); }
</script>
</body>
</html>
<!DOCTYPE html>
<html lang="zh-CN">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<title>知识流日报 2026-02-17：注意力机制</title>
<style>
  * { box-sizing: border-box; }
  body {
    font-family: system-ui, sans-serif;
    max-width: 720px;
    margin: 0 auto;
    padding: 2rem 1rem;
    color: #fff;
    min-height: 100vh;
    background: #0a0e1a;
    background-image:
      radial-gradient(2px 2px at 20px 30px, rgba(255,255,255,0.9), transparent),
      radial-gradient(2px 2px at 40px 70px, rgba(255,255,255,0.6), transparent),
      radial-gradient(1.5px 1.5px at 90px 40px, rgba(255,255,255,0.8), transparent),
      radial-gradient(2px 2px at 130px 80px, rgba(255,255,255,0.5), transparent),
      radial-gradient(1.5px 1.5px at 160px 120px, rgba(255,255,255,0.7), transparent),
      radial-gradient(ellipse 120% 100% at 50% 0%, rgba(88,60,120,0.25), transparent 50%),
      radial-gradient(ellipse 80% 80% at 80% 20%, rgba(40,60,100,0.2), transparent 40%);
    background-size: 200px 200px;
    background-repeat: repeat;
  }
  a { color: #a8d4ff; text-decoration: none; }
  a:hover { text-decoration: underline; }
 h1,h2,h3{margin-top:1.5rem;color:#fff;} pre{white-space:pre-wrap;color:rgba(255,255,255,0.9);} .toolbar{margin:0.5rem 0;color:rgba(255,255,255,0.8);} .content{color:rgba(255,255,255,0.95);} .content a{color:#a8d4ff;} .meta{color:rgba(255,255,255,0.65);font-size:0.9em;} .content p,.content div,.content h2,.content h3,.content h4,.content li,.content pre{cursor:pointer;-webkit-tap-highlight-color:rgba(255,255,255,0.15);}</style>
</head>
<body>
<p><a href="https://YuxinWSoton.github.io/ai-knowledge-flow-site/">← 返回首页</a></p>
<h1>知识流日报 2026-02-17：注意力机制</h1>
<p class="meta" style="margin-top:0;">最后更新：2026-02-17 10:41</p>
<p class="toolbar"><button id="btnNext" onclick="nextArticle()">下一篇</button> <button id="btnSel" onclick="readSelected()">朗读这段</button> <span id="readStatus"></span></p>
<p class="meta" style="margin-top:0;">（点任意段落即可朗读，手机/触屏友好；也可选中后点「朗读这段」或点「下一篇」按条朗读）</p>
<div class="content">
<h1>知识流日报 2026-02-17：注意力机制</h1>
<p>共 2 条（来自百度学术 / Google / Bing 等，仅含正文抓取成功的条目；按正文长度从短到长排列，便于先听短的）。</p>
<p>（以下含抓取正文，便于阅读。未调用任何 AI API。）</p>
<h2>1. 注意力机制到底在做什么，Q/K/V怎么来的？一文读懂 ...</h2>
<ul>
<li>链接：https://www.zhihu.com/tardis/zm/art/414084879</li>
<li>来源：bing</li>
<li>摘要：2023年2月2日 · Transformer [^1]论文中使用了注意力Attention机制，注意力Attention机制的最核心的公式为： 这个公式中的 Q 、 K 和 V 分别代表Query、Key和Value，他们之间进行的数学计算并不容易 …</li>
</ul>
<h3>正文（抓取，非 AI）</h3>
<p>本文同时发布于我的个人网站，公式图片显示效果更好，欢迎访问： https:// lulaoshi.info/machine-l earning/attention/transformer-attention.html Transformer[^1]论文中使用了注意力Attention机制，注意力Attention机制的最核心的公式为： 这个公式中的 Q 、 K 和 V 分别代表Query、Key和Value，他们之间进行的数学计算并不容易理解。 从向量点乘说起 我们先从 这样一个公式开始。 首先需要复习一下向量点乘（Dot Product）的概念。对于两个行向量 和 ： 向量点乘的几何意义是：向量 在向量 方向上的投影再与向量 的乘积，能够反应两个向量的相似度。向量点乘结果大，两个向量越相似。 一个矩阵 由 行向量组成。比如，我们可以将某一行向量 理解成一个词的词向量，共有 个行向量组成 的方形矩阵： 矩阵 与矩阵的转置 相乘， 中的每一行与 的每一列相乘得到目标矩阵的一个元素， 可表示为： 以 中的第一行第一列元素为例，其实是向量 与 自身做点乘，其实就是 自身与自身的相似度，那第一行第二列元素就是 与 之间的相似度。 下面以词向量矩阵为例，这个矩阵中，每行为一个词的词向量。矩阵与自身的转置相乘，生成了目标矩阵，目标矩阵其实就是一个词的词向量与各个词的词向量的相似度。 词向量矩阵相乘 如果再加上Softmax呢？我们进行下面的计算： 。Softmax的作用是对向量做归一化，那么就是对相似度的归一化，得到了一个归一化之后的权重矩阵，矩阵中，某个值的权重越大，表示相似度越高。 在这个基础上，再进一步： ，将得到的归一化的权重矩阵与词向量矩阵相乘。权重矩阵中某一行分别与词向量的一列相乘，词向量矩阵的一列其实代表着不同词的某一维度。经过这样一个矩阵相乘，相当于一个加权求和的过程，得到结果词向量是经过加权求和之后的新表示，而权重矩阵是经过相似度和归一化计算得到的。 通过与权重矩阵相乘，完成加权求和过程 Q、K、V 注意力Attention机制的最核心的公式为： ，与我们刚才分析的 有几分相似。Transformer[^1]论文中将这个Attention公式描述为：Scaled Dot-Product Attention。其中，Q为Query、K为Key、V为Value。Q、K、V是从哪儿来的呢？Q、K、V其实都是从同样的输入矩阵X线性变换而来的。我们可以简单理解成： 用图片演示为： X分别乘以三个矩阵，生成Q、K、V矩阵 其中， ， 和 是三个可训练的参数矩阵。输入矩阵 分别与 ， 和 相乘，生成 、 和 ，相当于经历了一次线性变换。Attention不直接使用 ，而是使用经过矩阵乘法生成的这三个矩阵，因为使用三个可训练的参数矩阵，可增强模型的拟合能力。 Scaled Dot-Product Attention 在这张图中， 与 经过MatMul，生成了相似度矩阵。对相似度矩阵每个元素除以 ， 为 的维度大小。这个除法被称为Scale。当 很大时， 的乘法结果方差变大，进行Scale可以使方差变小，训练时梯度更新更稳定。 Mask是机器翻译等自然语言处理任务中经常使用的环节。在机器翻译等NLP场景中，每个样本句子的长短不同，对于句子结束之后的位置，无需参与相似度的计算，否则影响Softmax的计算结果。 我们用国外博主Transformer详解博文[^2]中的例子来将上述计算串联起来解释。 输入为词向量矩阵X，每个词为矩阵中的一行，经过与W进行矩阵乘法，首先生成Q、K和V。 q1 = X1 * WQ ， q1 为 Q 矩阵中的行向量， k1 等与之类似。 从词向量到Q、K、V 第二步是进行 计算，得到相似度。 Q与K相乘，得到相似度 第三步，将刚得到的相似度除以 ，再进行Softmax。经过Softmax的归一化后，每个值是一个大于0小于1的权重系数，且总和为0，这个结果可以被理解成一个权重矩阵。 Scale &amp; Softmax 第四步是使用刚得到的权重矩阵，与V相乘，计算加权求和。 使用权重矩阵与V相乘，得到加权求和 多头注意力 为了增强拟合性能，Transformer对Attention继续扩展，提出了多头注意力（Multiple Head Attention）。刚才我们已经理解了， 、 、 是输入 与 、 和 分别相乘得到的， 、 和 是可训练的参数矩阵。现在，对于同样的输入 ，我们定义多组不同的 、 、 ，比如 、 、 ， 、 和 ，每组分别计算生成不同的 、 、 ，最后学习到不同的参数。 定义多组W，生成多组Q、K、V 比如我们定义8组参数，同样的输入 ，将得到8个不同的输出 到 。 定义8组参数 在输出到下一层前，我们需要将8个输出拼接到一起，乘以矩阵 ，将维度降低回我们想要的维度。 将多组输出拼接后乘以矩阵Wo以降低维度 多头注意力的计算过程如下图所示。对于下图中的第2）步，当前为第一层时，直接对输入词进行编码，生成词向量X；当前为后续层时，直接使用上一层输出。 多头注意力计算过程 再去观察Transformer论文中给出的多头注意力图示，似乎更容易理解了： Transformer论文给出的多头注意力图示 [^1]: Vaswani A, Shazeer N, Parmar N, et al. Attention is all you need. 31st Conference on Neural Information Processing Systems 2017(NIPS 2017). Long Beach, CA, USA: 2017: 5998–6008. [^2]: https:// jalammar.github.io/illu strated-transformer/</p>
<h2>2. 注意力机制 - 知乎</h2>
<ul>
<li>链接：https://www.zhihu.com/topic/20682987/intro</li>
<li>来源：bing</li>
<li>摘要：2020年4月24日 · 注意力机制是非常优美而神奇的机制，在神经网络「信息过载」的今天，让 NN 学会只关注特定的部分，无疑会大幅度提升任务的效果与效率。借助注意力机制，神经机器翻译、预训练语 …</li>
</ul>
<h3>正文（抓取，非 AI）</h3>
<p>注意力机制 - 知乎 注意力机制 「注意力机制」（Attention Mechanism）是机器学习中的一种数据处理方法，广泛应用在自然语言处理、图像识别及语音识别等各种不同类型的机器学习任务中。... 查看全部内容 关注话题 ​ 管理 ​ 分享 ​ 百科 讨论 精华 等待回答 详细内容 简介 注意力机制（Attention Mechanism）是机器学习中的一种数据处理方法，广泛应用在自然语言处理、图像识别及语音识别等各种不同类型的机器学习任务中。 注意力机制分成两个部分： 一、注意力机制为什么有必要存在（科普向） 二、注意力机制具体是如何实现的（知识向） 存在意义 一、注意力机制为什么有必要存在 注意力这个词本来是属于人类才有的动作。 也就是说，注意力机制可以看做是一种仿生，是机器通过对人类阅读、听说中的注意力行为进行模拟。 那为何要对注意力进行仿生，按理说，计算机理应拥有无限的记忆力和注意力。 这是因为，人脑在进行阅读任务、读图任务时，并不是严格的解码过程，而是接近于一种模式识别。 大脑会自动忽略低可能、低价值的信息。 大脑会自动忽略可能性低的答案 上面的这段话，好几个词都存在顺序混乱，但是阅读的第一感受，不会感觉到这种乱序。 甚至不用停下来思考，这种乱序是不是存在笔误，不仔细看甚至发现不了问题的存在。 这不是眼睛都出了问题，而是大脑在识别文字的过程中，自动就把低可能的解释忽略了。 这一点在读图任务上，还更为明显一些，读图的过程中，大脑总是会优先获取认为有用的信息，而将次要的内容直接抛弃。 这是因为，大脑在阅读或读图的过程中，会直接抛弃低可能性答案，将阅读的内容更正为“大脑认为正确的版本”。 同样的文字随着对话主题的不同，含义也会发生变化 上下文联系影响文字的意义 这是网络上的一个段子，所谓的中文十级考试。 这段话的第二句话里，两个“对”字代表了不同的含义。 但如果单独把第二局话挑出来，即使是中国人也会对里面的意义产生疑义。因为本身这句话的“对了”可以解释成“已经校对过了”、“正确了”或者“无意义的承接词”。 但如果有了第一句话的限定，这个“对了”就只能是“对答案了”的意思。 也就是说，理解一句话的含义，不仅仅取决于这句话本身，而与上下文相关联的词也有很大影响。 还不单单如此，通常一段对话中，都会存在一个反复出现的概念，例如这段话中的“对答案”，其他的词语或多或少都能与这个概念产生联系。也就是说，这个概念就是这段话的主题。而在更复杂一些的段落里，还会出现部分与主题没有什么关联的内容，通常这些内容都会被我们弱化或者自动遗忘。 结合上面的两个例子，人们在阅读、交流的过程中，本身就存在着信息的舍弃。虽然每段文字可能字号、粗细都相同，但注意力却不是那样均衡地分配给每一个词。 如果计算机不能模拟人类的注意力状态，就可能让无关的信息对处理结果造成干扰，最后导致处理结果偏离实际的应用场景。 例如，聊天场景中，用户输入了错别字，导致了歧义。如果是人工场景，就很容易忽略错别字的影响，理解文字的本来含义。 又或者，同样的句子，在不同语境中含义发生变化，导致机器翻译在段落和文章的翻译上，似是而非，语言不通顺。 这些干扰，都让人工智能显得像是“人工智障”，逻辑硬伤导致无法执行较为复杂的任务。 为了让计算机更加适应人类交流场景，必须教会计算机选择遗忘和关联上下文，这种机制就是所谓的注意力机制。 实现过程 严格来说，注意力机制更像是一种方法论。没有严格的数学定义，而是根据具体任务目标，对关注的方向和加权模型进行调整。 简单的理解就是，在神经网络的隐藏层，增加注意力机制的加权。 使不符合注意力模型的内容弱化或者遗忘。 Google 2017年论文中，Attention Is All You Need曾经为Attention做了一个抽象定义： Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_k}})V An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key. 注意力是将一个查询和键值对映射到输出的方法，Q、K、V均为向量，输出通过对V进行加权求和得到，权重就是Q、K相似度。 1）机器视觉中的应用（精细分类、图像分割、图像焦点） 例如，识别鸟类的品种问题。对于鸟品种的精细分类，对结果影响最大的可能是鸟类的头部，通过注意力机制将头部的特征强化，而忽略其他部分（羽毛、爪子），以实现区分鸟类的具体品种。 2）机器翻译中的应用（LSTM+注意力模型） LSTM单元 LSTM（Long Short Term Memory）是RNN（循环神经网络）的一种应用。可以简单理解为，每一个神经元都具有输入门、输出门、遗忘门。 输入门、输出门将LSTM神经元首尾连接在一起，而遗忘门将无意义内容弱化或遗忘。注意力机制就应用在LSTM的遗忘门，使得机器阅读更加贴近于人类阅读的习惯，也使得翻译结果具有上下文联系。 参考资料 【计算机视觉】深入理解Attention机制 - Slow down, Keep learning and Enjoy life - CSDN博客 周知瑞：Attention的梳理、随想与尝试 瑟木：计算机视觉中的注意力机制 百科摘录 3 2019 ICCV 用于检测、去除阴影区域的注意力循环生成对抗网路 下的内容摘录 B1gme 总是再不停的突破自己的b1gme 注意力机制是用来编码序列数据，这些数据基于给每个参数分配重要的权值的方式。它为自然语言处理方向、语音识别、计算机视觉、图像描述和视觉问答方向提供重大帮助。不同于上述方法，论文采取了一种使用渐进式和循环方法来整合不同层特征的多内容信息。并且能处理复杂环境下阴影去除和检测。 知乎小知 摘录于 2020-04-24 哈希算法、爱因斯坦求和约定，这是2020年的注意力机制 下的内容摘录 机器之心 ​ 数学等 2 个话题下的优秀答主 注意力机制是非常优美而神奇的机制，在神经网络「信息过载」的今天，让 NN 学会只关注特定的部分，无疑会大幅度提升任务的效果与效率。借助注意力机制，神经机器翻译、预训练语言模型等任务获得了前所未有的提升。 知乎小知 摘录于 2020-04-24 【ACL 2019】为知识图谱添加注意力机制 下的内容摘录 超正经学术君 让更多人读懂科学 注意力机制（Attention）是近些年来提出的一种改进神经网络的方法，在图像识别、自然语言处理和图网络表示等领域都取得了很好的效果，可以说注意力机制的加入极大地丰富了神经网络的表示能力。 知乎小知 摘录于 2020-04-24 浏览量 3677 万 讨论量 1.6 万</p>
</div>
<script>
var articles=[]; var currentArticleIdx=-1; var maxSpeakChars=3500;
function buildArticles(){ var c=document.querySelector('.content'); if(!c) return; var h2s=c.querySelectorAll('h2'); for(var i=0;i<h2s.length;i++){ var start=h2s[i], end=i+1<h2s.length?h2s[i+1]:null; var r=document.createRange(); r.setStart(start,0); if(end) r.setEnd(end,0); else { var last=c.lastElementChild||c; r.setEndAfter(last); } var t=r.toString().replace(/\s+/g,' ').trim(); if(t.length>maxSpeakChars) t=t.slice(0,maxSpeakChars)+'…（内容过长已截断）'; articles.push({h2:start,text:t}); }
 articles.sort(function(a,b){ return a.text.length-b.text.length; }); }
function speakText(t){ if(!t){ document.getElementById('readStatus').textContent=''; return; }
 speechSynthesis.cancel(); var status=document.getElementById('readStatus'); status.textContent='准备中…';
 var chunks=[]; var maxLen=280; for(var i=0;i<t.length;i+=maxLen){ var s=t.slice(i,i+maxLen); var j=s.lastIndexOf('。'); if(j>80){ chunks.push(s.slice(0,j+1)); i-=s.length-(j+1); } else chunks.push(s); }
 if(chunks.length===0) chunks=[t]; var idx=0; function speakNext(){ if(idx>=chunks.length){ status.textContent=''; return; } var u=new SpeechSynthesisUtterance(chunks[idx]); u.lang='zh-CN'; u.rate=0.95; var v=speechSynthesis.getVoices().filter(function(x){ return x.lang.indexOf('zh')>=0; })[0]; if(v) u.voice=v;
 u.onend=function(){ idx++; setTimeout(speakNext,80); }; u.onerror=function(){ idx++; setTimeout(speakNext,80); }; status.textContent='朗读中 '+(idx+1)+'/'+chunks.length+'…'; speechSynthesis.speak(u); }
 if(speechSynthesis.getVoices().length) speakNext(); else speechSynthesis.onvoiceschanged=function(){ speechSynthesis.onvoiceschanged=null; speakNext(); }; }
function nextArticle(){ if(articles.length===0) buildArticles(); if(articles.length===0){ document.getElementById('readStatus').textContent='无可读内容'; return; }
 speechSynthesis.cancel(); currentArticleIdx++; if(currentArticleIdx>=articles.length){ currentArticleIdx=0; document.getElementById('readStatus').textContent='已读完，共 '+articles.length+' 篇。再按从第 1 篇开始'; return; }
 var a=articles[currentArticleIdx]; a.h2.scrollIntoView({behavior:'smooth',block:'start'}); document.getElementById('readStatus').textContent='第 '+(currentArticleIdx+1)+' / '+articles.length+' 篇'; speakText(a.text); }
function readSelected(){ var sel=window.getSelection(); var t=(sel&&sel.toString)?sel.toString():''; t=t.replace(/\\s+/g,' ').trim(); if(!t){ document.getElementById('readStatus').textContent='请先选中要朗读的段落'; return; } speechSynthesis.cancel(); document.getElementById('readStatus').textContent='朗读选中内容…'; speakText(t); }
function getBlockForTap(el){ var c=document.querySelector('.content'); var blockTags=['P','DIV','H2','H3','H4','LI','PRE']; while(el&&el!==c){ if(c.contains(el)&&blockTags.indexOf(el.tagName)!==-1) return el; el=el.parentElement; } return null; }
function onContentTap(e){ if(e.target.closest&&e.target.closest('a')) return; var block=getBlockForTap(e.target); if(block){ var t=(block.innerText||'').trim().replace(/\\s+/g,' '); if(t){ e.preventDefault(); speechSynthesis.cancel(); document.getElementById('readStatus').textContent='朗读本段…'; speakText(t); } } }
if(document.readyState==='loading'){ document.addEventListener('DOMContentLoaded', function(){ buildArticles(); var c=document.querySelector('.content'); if(c) c.addEventListener('click', onContentTap); }); } else { buildArticles(); var c=document.querySelector('.content'); if(c) c.addEventListener('click', onContentTap); }
</script>
</body>
</html>
<!DOCTYPE html>
<html lang="zh-CN">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<title>知识流日报 2026-02-17：softmax、交叉熵、感受野</title>
<style>
  * { box-sizing: border-box; }
  body {
    font-family: system-ui, sans-serif;
    max-width: 720px;
    margin: 0 auto;
    padding: 2rem 1rem;
    color: #fff;
    min-height: 100vh;
    background: #0a0e1a;
    background-image:
      radial-gradient(2px 2px at 20px 30px, rgba(255,255,255,0.9), transparent),
      radial-gradient(2px 2px at 40px 70px, rgba(255,255,255,0.6), transparent),
      radial-gradient(1.5px 1.5px at 90px 40px, rgba(255,255,255,0.8), transparent),
      radial-gradient(2px 2px at 130px 80px, rgba(255,255,255,0.5), transparent),
      radial-gradient(1.5px 1.5px at 160px 120px, rgba(255,255,255,0.7), transparent),
      radial-gradient(ellipse 120% 100% at 50% 0%, rgba(88,60,120,0.25), transparent 50%),
      radial-gradient(ellipse 80% 80% at 80% 20%, rgba(40,60,100,0.2), transparent 40%);
    background-size: 200px 200px;
    background-repeat: repeat;
  }
  a { color: #a8d4ff; text-decoration: none; }
  a:hover { text-decoration: underline; }
 h1,h2,h3{margin-top:1.5rem;color:#fff;} pre{white-space:pre-wrap;color:rgba(255,255,255,0.9);} .toolbar{margin:0.5rem 0;color:rgba(255,255,255,0.8);} .content{color:rgba(255,255,255,0.95);} .content a{color:#a8d4ff;} .meta{color:rgba(255,255,255,0.65);font-size:0.9em;} .content p,.content div,.content h2,.content h3,.content h4,.content li,.content pre{cursor:pointer;-webkit-tap-highlight-color:rgba(255,255,255,0.15);} .content .insight-summary{color:#c9a0dc;margin:0.5em 0 0.8em 0;} .content .insight-detail,.content .insight-detail li{color:#8dd4e8;list-style:disc;margin-left:1.2em;padding-left:0.3em;} .content .article-body-details{margin:0.8em 0;} .content .article-body-details summary{cursor:pointer;color:rgba(255,255,255,0.85);}</style>
</head>
<body>
<p><a href="https://YuxinWSoton.github.io/ai-knowledge-flow-site/">← 返回首页</a></p>
<h1>知识流日报 2026-02-17：softmax、交叉熵、感受野</h1>
<p class="meta" style="margin-top:0;">最后更新：2026-02-18 11:17</p>
<p class="toolbar"><button id="btnNext" onclick="nextArticle()">下一篇</button> <button id="btnSel" onclick="readSelected()">朗读这段</button> <span id="readStatus"></span></p>
<p class="meta" style="margin-top:0;">（点任意段落即可朗读，手机/触屏友好；也可选中后点「朗读这段」或点「下一篇」按条朗读）</p>
<div class="content">
<h1>知识流日报 2026-02-17：softmax、交叉熵、感受野</h1>
<p>共 3 条（来自百度学术 / Google / Bing 等，仅含正文抓取成功的条目；按正文长度从短到长排列，便于先听短的）。</p>
<p>（以下含抓取正文，便于阅读。未调用任何 AI API。）</p>
<p>（部分条目含模型提取的「易漏细节」关键点，页面上以颜色区分。）</p>
<h2>1. 如何理解CNN中的感受野（receptive-field）？</h2>
<ul>
<li>链接：https://www.zhihu.com/tardis/bd/ans/1910838113</li>
<li>来源：bing</li>
<li>摘要：2021年6月2日 · 和显示一样 ，它基本遵循着“近大远小”的逻辑。或者从另外一个角度说，能够看到近处的范围有限，能够看到远处更广泛的视野。 在CNN中，感受野的概念： • 卷积神经网络（CNN）中， …</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">感受野的大小与卷积核大小、卷积层数以及卷积操作密切相关。首先，感受野大小与卷积核大小相关，较小的卷积核能增强网络容量，而较大的卷积核则能减少参数数量。其次，感受野大小与卷积层数相关，卷积层数越多，感受野越大，接触到的原始图像范围也越大。此外，感受野越大，特征越全局，而感受野越小，特征越局部。因此，最终输出的感受野可以追溯到输入图像，其值可以指示抽象层次，用于判断网络层次。卷积操作影响特征图变化，涉及卷积核大小、层数、参数数量以及输入图像范围的变化。因此，感受野计算涉及卷积操作顺序、特征图变化规律、卷积核大小变化、卷积层数变化以及输入图像大小变化的影响。通过理解这些关系，可以更好地掌握卷积操作对特征图的影响，从而优化网络设计。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>感受野大小与卷积核大小相关。</li>
<li>感受野大小与卷积层数相关。</li>
<li>感受野越大，接触到的原始图像范围越大。</li>
<li>感受野越小，包含的特征越局部。</li>
<li>最终输出的感受野可以追溯到输入图像。</li>
<li>卷积可以减少参数数量。</li>
<li>卷积核可以由2个3*3卷积核替代。</li>
<li>感受野与卷积层数有关。</li>
<li>感受野与卷积核大小有关。</li>
<li>感受野值可以判断抽象层次。</li>
<li>较小卷积核能增强网络容量。</li>
<li>多次卷积可将输入从大尺寸转换到小尺寸。</li>
<li>感受野值越大，特征越全局。</li>
<li>感受野值越小，特征越局部。</li>
<li>感受野可以用于判断网络层次。</li>
<li>感受野计算涉及卷积核和层数。</li>
<li>感受野示例可帮助理解卷积效果。</li>
<li>感受野计算涉及参数数量。</li>
<li>感受野与卷积核形状相关。</li>
<li>感受野与卷积层数相关。</li>
<li>感受野计算涉及输入大小。</li>
<li>感受野值可以指示特征层次。</li>
<li>感受野计算涉及卷积操作。</li>
<li>感受野计算涉及特征图变化。</li>
<li>感受野计算涉及输出大小。</li>
<li>感受野计算涉及卷积核应用。</li>
<li>感受野计算涉及输入图像范围。</li>
<li>感受野计算涉及卷积层数量。</li>
<li>感受野计算涉及参数数量变化。</li>
<li>感受野计算涉及卷积核大小。</li>
<li>感受野计算涉及特征图大小。</li>
<li>感受野计算涉及卷积操作顺序。</li>
<li>感受野计算涉及输入图像变化。</li>
<li>感受野计算涉及卷积核应用次数。</li>
<li>感受野计算涉及特征图变化规律。</li>
<li>感受野计算涉及卷积核大小变化。</li>
<li>感受野计算涉及卷积层数变化。</li>
<li>感受野计算涉及输入图像大小变化。</li>
<li>感受野计算涉及卷积操作影响。</li>
<li>感受野计算涉及特征图变化影响。</li>
<li>感受野计算涉及卷积核应用影响。</li>
<li>感受野计算涉及卷积操作顺序影响。</li>
<li>感受野计算涉及特征图变化规律影响。</li>
<li>感受野计算涉及卷积核大小变化影响。</li>
<li>感受野计算涉及卷积层数变化影响。</li>
<li>感受野计算涉及输入图像大小变化影响。</li>
<li>感受野计算涉及卷积操作影响特征图。</li>
<li>感受野计算涉及卷积操作影响特征图大小。</li>
<li>感受野计算涉及卷积操作影响特征图变化。</li>
<li>感受野计算涉及卷积操作影响特征图变化规律。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3>正文（抓取，非 AI）</h3>
<p>可以用这幅图大概了解【感受野】，大概就是我们能够感受到的视野范围。 更进一步说，如下图，根据图像，眼睛可以看到的整个区域（图中的网格）称为视野。人类的视觉系统由数百万个神经元组成，每个神经元捕获不同的信息。我们将神经元的接受视野定义为总视野的斑块。换句话说，单个神经元可以访问哪些信息。简单来说，这就是生物细胞的感受野。 和显示一样 ，它基本遵循着“近大远小”的逻辑。或者从另外一个角度说，能够看到近处的范围有限，能够看到远处更广泛的视野。 在CNN中，感受野的概念： • 卷积神经网络（CNN）中，每一层输出的特征图上，每个像素点在原始图像上映射（对应）的区域大小。 • 原始图像是CNN的输入图像，是经过预处理（如缩放、扭曲、裁切等操作）后的图像。 概念往往是很严谨的，但是，也是非常抽象的。 简单来说，感受野，就是我们看到的物体，或者我们感知到的物体，它的本来大小是多少呢？ 当然，要了解感受野，必须要先了解什么是卷积。 简单来说，卷积就是让一个“核”在一个输入图像上进行运算，从而得到结果。 下图显示了卷积。我们采用一个过滤器/内核（3×3矩阵），并将其应用于输入图像以获得卷积特征。该卷积特征将传递到下一层。 或者简单点，可以表示下面这个样子： 下面的动画看起来更直观了。 通过上面的动画，大家可以看到，一个5<em>5的输入，在使用3</em>3大小的卷积核处理后，得到的结果为3<em>3大小。 下面进入正题，下图中的左侧白色背景区域中： 最左侧是一个4</em>4大小输入原始图像 中间的黄色3<em>3的大小的矩形，是一个卷积核 最右侧是运算结果，它的大小是2</em>2； 此时，我们可以说，输出大小虽然是2<em>2的，但是它能够切实地感受到原始4</em>4大小的区域。 或者进一步来说，当前2<em>2的输出结果中，所有的值都来源于原始4</em>4大小的输入图像。 下面看一个特例： 输入：2<em>2大小 卷积核：2</em>2大小 输出：1<em>1大小 此时，可以说输出结果中，实际上包含了原始输入2</em>2大小的信息。或者说，当前1<em>1输出的感受野是2</em>2大小。 下面这个例子，经过了两次卷积运算。 第1次卷积： 输入：4<em>4大小 卷积核：3</em>3大小 输出：2<em>2大小 第2次卷积： 输入：2</em>2大小 卷积核：2<em>2大小 输出：1</em>1大小 可以看到，经过多次卷积，将输入从“4<em>4”大小，转换到“1</em>1”大小的输出。 我们可以尝试着，将输出往回看，看看它能感受到怎样的感受野。 首先，最终的“1<em>1”的输出，感受到的是中间的“2</em>2”得绿色视野。 更进一步，中间的“2<em>2”绿色小块，感受到的是最左边的“4</em>4”大小的视野。 总体来看，最终的“1<em>1”大小的输出，感受到的是最初的“4</em>4”大小的视野。 以上，我们观察的是一个输出像素点的情况，针对多个像素点，结果如下图所示。 来看看YOLO的模型，它的感受野大概是这样的。 感受野的情况： • 值越大：它所能接触到的原始图像的范围越大，意味着它可能蕴含更为全局、语义层次更高的特征 • 值越小：它所包含的特征越趋向局部和细节 • 感受野值，可以大致判断每一层的抽象层次 使用更小的卷积核，能够在增强网络容量的同时减少参数个数。 我们先比较一下核大小分别是5<em>5和3</em>3的情况。 例如，在下图中： l 左侧原始输入为5<em>5大小，使用一个5</em>5大小的核，处理它，得到的结果为1<em>1大小。 l 右侧原始输入是5</em>5大小，使用两次3<em>3大小的核，处理它，得到的结果为1</em>1大小。 观察一下参数的数量， l 左侧，核大小为5<em>5=25，需要：25个参数。 l 右侧，3</em>3大小核，需要两次运算，所以需要的参数为：3<em>3</em>2=18个参数。 所以，我们经常看到的是这张图，也就是说，2层3<em>3的卷积核，可以替代一个5</em>5的卷积核。 在2012年，AlexNet的第一次卷积大小为11x11。2013年，ZFNet用7x7替换了该卷积层。2014年，GoogleNet最大的卷积内核是5x5。它保留了第一个7x7卷积层。同年，二等奖VGG仅使用了3x3卷积内核。在更高版本中，第一个版本的GoogleNet的5x5卷积层已被2个堆叠的3x3卷积层代替，复制了VGG16。 下面看一个3个3<em>3的卷积核，相当于1个7</em>7卷积核得例子。 上面有个问题，为什么经过卷积，输出结果会变小呢？ 看看下面这个动态图，也许能够帮助我们理解。 下面我看一个卷积的示例，进一步理解问题规模： Input：输入 Kernels：核 Feature maps：特征图 Feature vector：特征向量值 这里，我们主要关注右侧的对象规模。 下列网址，提供了一个非常有趣的感受野示例： https:// distill.pub/2019/comput ing-receptive-fields/ 下述网址，提供了感受野计算的源代码： https:// distill.pub/2019/comput ing-receptive-fields/ 本文首发于公众号【计算机之光】 。搜索【cvlight】关注公众号，免费视频学习，领取学习资料。 感兴趣可以进一步阅读: 参考资料： 1. Convolutional Neural Networks (CNN) 2. https:// theaisummer.com/recepti ve-field/ 3. Computing Receptive Fields of Convolutional Neural Networks 4. google-research/receptive_field</p>
</div></details><h2>2. 为什么交叉熵（cross-entropy）可以用于计算代价？</h2>
<ul>
<li>链接：https://www.zhihu.com/tardis/bd/ans/244557337</li>
<li>来源：bing</li>
<li>摘要：2020年5月8日 · 通用的说，熵 (Entropy)被用于描述一个系统中的不确定性 (the uncertainty of a system)。在不同领域熵有不同的解释，比如热力学的定义和信息论也不大相同。 要想明白交叉熵 …</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">熵可以表示事件的信息量，越不可能发生的事件信息量越大，而确定事件的信息量则很低。独立事件的信息量可以叠加，因此熵被定义为-log(p)。交叉熵用于衡量模型分布与真实分布的差异，其公式为-sum(p*log(q))，而交叉熵等价于最小化KL散度，即KL散度衡量两个分布的不同。交叉熵比误分率更有优势，可以更准确地计算代价，因此在训练过程中，最小化交叉熵可以避免过拟合，同时关注测试数据的表现，确保模型在实际应用中的表现。KL散度不具备对称性，但在特定条件下，交叉熵等价于最小化KL散度，这使得交叉熵在计算上更为简便。因此，通过最小化交叉熵，可以更有效地调整模型参数，使其更贴近真实分布。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>熵可以表示事件的信息量</li>
<li>越不可能发生的事件信息量越大</li>
<li>确定事件的信息量很低</li>
<li>独立事件的信息量可叠加</li>
<li>熵被定义为-log(p)</li>
<li>KL散度不具备对称性</li>
<li>KL散度衡量两个分布的不同</li>
<li>交叉熵等于KL散度加上熵</li>
<li>交叉熵公式为-sum(p*log(q))</li>
<li>KL散度等价于交叉熵在特定条件下</li>
<li>交叉熵更简单便于计算</li>
<li>最小化KL散度等价于最小化交叉熵</li>
<li>交叉熵用于衡量模型分布与真实分布的差异</li>
<li>训练数据的分布是固定的</li>
<li>最小化交叉熵等价于最小化KL散度</li>
<li>交叉熵可以用于计算代价</li>
<li>最小化交叉熵可以避免过拟合</li>
<li>测试数据的表现也需关注</li>
<li>交叉熵比误分率更有优势</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3>正文（抓取，非 AI）</h3>
<p>通用的说，熵(Entropy)被用于描述一个系统中的不确定性(the uncertainty of a system)。在不同领域熵有不同的解释，比如热力学的定义和信息论也不大相同。 要想明白交叉熵(Cross Entropy)的意义，可以从熵(Entropy) -&gt; KL散度(Kullback-Leibler Divergence) -&gt; 交叉熵这个顺序入手。 当然，也有多种解释方法[1]。 先给出一个“接地气但不严谨”的概念表述： 熵：可以表示一个事件A的自信息量，也就是A包含多少信息。 KL散度：可以用来表示从事件A的角度来看，事件B有多大不同。 交叉熵：可以用来表示从事件A的角度来看，如何描述事件B。 一句话总结的话： KL散度可以被用于计算代价，而在特定情况下最小化KL散度等价于最小化交叉熵。而交叉熵的运算更简单，所以用交叉熵来当做代价 。 我知道你现在看着有点晕，但请保持耐心继续往下看。 <em>为了通俗易懂，我没有严格按照数学规范来命名概念，比如文中的“事件”指的是“消息”，望各位严谨的读者理解。 1. 什么是熵(Entropy)？ 放在信息论的语境里面来说，就是一个事件所包含的信息量。我们常常听到“这句话信息量好大”，比如“昨天花了10万，终于在西二环买了套四合院”。 这句话为什么信息量大？ 因为它的内容出乎意料，违反常理。 由此引出： 越不可能发生的事件信息量越大， 比如“我不会死”这句话信息量就很大 。而确定事件的信息量就很低， 比如“我是我妈生的”，信息量就很低甚至为0 。 独立事件的信息量可叠加。 比如“a. 张三今天喝了阿萨姆红茶，b. 李四前天喝了英式早茶”的信息量就应该恰好等于a+b的信息量，如果张三李四喝什么茶是两个独立事件。 因此熵被定义为 ， 指的不同的事件比如喝茶， 指的是某个事件发生的概率比如和红茶的概率。对于一个一定会发生的事件，其发生概率为1， ，信息量为0。 2. 如何衡量两个事件/分布之间的不同（一）：KL散度 我们上面说的是对于一个随机变量x的事件A的自信息量，如果我们有另一个独立的随机变量x相关的事件B，该怎么计算它们之间的区别？ 此处我们介绍默认的计算方法：KL散度，有时候也叫KL距离，一般被用于计算两个分布之间的不同 。看名字似乎跟计算两个点之间的距离也很像，但实则不然，因为KL散度 不具备有对称性 。在距离上的对称性指的是A到B的距离等于B到A的距离。 举个不恰当的例子，事件A：张三今天买了2个土鸡蛋，事件B：李四今天买了6个土鸡蛋。我们定义随机变量x：买土鸡蛋，那么事件A和B的区别是什么？有人可能说，那就是李四多买了4个土鸡蛋？这个答案只能得50分，因为忘记了"坐标系"的问题。换句话说，对于张三来说，李四多买了4个土鸡蛋。对于李四来说，张三少买了4个土鸡蛋。 选取的参照物不同，那么得到的结果也不同 。更严谨的说，应该是说我们对于张三和李四买土鸡蛋的期望不同，可能张三天天买2个土鸡蛋，而李四可能因为孩子满月昨天才买了6个土鸡蛋，而平时从来不买。 KL散度的数学定义： 对于 离散事件 我们可以定义事件A和B的差别为(2.1)： 对于 连续事件 ，那么我们只是把求和改为求积分而已(2.2)。 从公式中可以看出： 如果 ，即两个事件分布完全相同，那么KL散度等于0。 观察公式2.1，可以发现减号左边的就是事件A的熵，请记住这个发现 。 如果颠倒一下顺序求 ，那么就需要使用B的熵，答案就不一样了。 所以KL散度来计算两个分布A与B的时候是不是对称的，有“坐标系”的问题 ， 换句话说，KL散度由A自己的熵与B在A上的期望共同决定。当使用KL散度来衡量两个事件(连续或离散)，上面的公式意义就是求 A与B之间的对数差 在 A上的期望值。 3. KL散度 = 交叉熵 - 熵？ 如果我们默认了用KL散度来计算两个分布间的不同，那还要交叉熵做什么？ 事实上交叉熵和KL散度的公式非常相近，其实就是KL散度的后半部分(公式2.1)：A和B的交叉熵 = A与B的KL散度 - A的熵。 对比一下这是KL散度的公式： 这是熵的公式： 这是交叉熵公式： 此处最重要的观察是，如果 是一个常量，那么 ， 也就是说KL散度和交叉熵在特定条件下等价。这个发现是这篇回答的重点。 同时补充交叉熵的一些性质： 和KL散度相同，交叉熵也不具备对称性： ，此处不再赘述。 从名字上来看，Cross(交叉)主要是用于描述这是 两个事件 之间的相互关系，对自己求交叉熵等于熵。即 ，注意只是非负而不一定等于0。 </em>4. 另一种理解KL散度、交叉熵、熵的角度（选读）- 可跳过 那么问题来了，为什么有KL散度和交叉熵两种算法？为什么他们可以用来求分布的不同？什么时候可以等价使用？ 一种信息论的解释是 ： 熵的意义是对A事件中的随机变量进行编码所需的最小字节数。 KL散度的意义是“额外所需的编码长度”如果我们用B的编码来表示A。 交叉熵指的是当你用B作为密码本来表示A时所需要的“平均的编码长度”。 对于大部分读者，我觉得可以不用深入理解。感谢评论区@王瑞欣的指正，不知道为什么@不到他。 一些对比与观察： KL散度和交叉熵的不同处：交叉熵中不包括“熵”的部分 KL散度和交叉熵的相同处：a. 都不具备对称性 b. 都是非负的 等价条件（章节3）：当 固定不变时，那么最小化KL散度 等价于最小化交叉熵 。 既然等价，那么我们优先选择更简单的公式，因此选择交叉熵。 5. 机器如何“学习”？ 机器学习的过程就是希望在训练数据上 模型学到的分布 和 真实数据的分布 越接近越好，那么我们已经介绍过了....怎么最小化两个分布之间的不同呢？ 用默认的方法，使其KL散度最小 ！ 但我们没有真实数据的分布，那么只能退而求其次， 希望 模型学到的分 布和 训练数据的分布 尽量相同， 也就是把训练数据当做模型和真实数据之间的代理人 。假设训练数据是从总体中独立同步分布采样(Independent and identically distributed sampled)而来，那么我们可以利用最小化训练数据的经验误差来降低模型的泛化误差。简单说： 最终目的是希望学到的模型的分布和真实分布一致： 但真实分布是不可知的，我们只好假设 训练数据 是从真实数据中独立同分布采样而来： 退而求其次，我们希望学到的模型分布至少和训练数据的分布一致 由此非常理想化的看法是如果 模型(左) 能够学到 训练数据(中) 的分布，那么应该近似的学到了 真实数据(右) 的分布： 6. 为什么交叉熵可以用作代价？ 接着上一点说，最小化模型分布 与 训练数据上的分布 的差异 等价于 最小化这两个分布间的KL散度，也就是最小化 。 比照第四部分的公式： 此处的A就是数据的真实分布： 此处的B就是模型从训练数据上学到的分布： 巧的是， 训练数据的分布A是给定的 。那么根据 我们在第四部分说的，因为A固定不变，那么求 等价于求 ，也就是A与B的交叉熵 。 得证，交叉熵可以用于计算“学习模型的分布”与“训练数据分布”之间的不同。当交叉熵最低时(等于训练数据分布的熵)，我们学到了“最好的模型”。 但是，完美的学到了训练数据分布往往意味着过拟合，因为训练数据不等于真实数据，我们只是假设它们是相似的，而一般还要假设存在一个高斯分布的误差，是模型的泛化误差下线。 7. 总结 因此在评价机器学习模型时，我们往往不能只看训练数据上的误分率和交叉熵，还是要关注测试数据上的表现。如果在测试集上的表现也不错，才能保证这不是一个过拟合或者欠拟合的模型。交叉熵比照误分率还有更多的优势，因为它可以和很多概率模型完美的结合。 所以逻辑思路是，为了让学到的模型分布更贴近真实数据分布，我们最小化 模型数据分布 与 训练数据之间的KL散度，而因为训练数据的分布是固定的，因此最小化KL散度等价于最小化交叉熵。 因为等价，而且交叉熵更简单更好计算，当然用它咯 ʕ•ᴥ•ʔ [1] 不同的领域都有不同解释，更传统的机器学习说法是似然函数的最大化就是交叉熵。 正所谓横看成岭侧成峰，大家没必要非说哪种思路是对的，有道理就好 。</p>
</div></details><h2>3. 损失函数｜交叉熵损失函数</h2>
<ul>
<li>链接：https://www.zhihu.com/tardis/zm/art/35709485</li>
<li>来源：bing</li>
<li>摘要：2022年3月28日 · 3. 学习过程 交叉熵损失函数经常用于分类问题中，特别是在神经网络做分类问题时，也经常使用交叉熵作为损失函数，此外，由于交叉熵涉及到计算每个类别的概率，所以交叉熵几乎每次 …</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">交叉熵损失函数在分类问题中扮演着关键角色，它通过与sigmoid或softmax函数结合使用，能够有效捕捉模型预测效果的差异。首先，交叉熵损失函数用于分类问题，通过模型输出的概率值来衡量分类的准确性。然而，传统的分类错误率无法有效区分模型的表现，而交叉熵损失函数则能更好地反映模型的预测效果。此外，交叉熵是凸函数，这意味着其求导后可直接获得全局最优解，从而提高模型训练的效率。在二分类和多分类问题中，交叉熵损失函数的求导过程可以分为三部分，进一步增强了其在不同场景下的适用性。值得注意的是，交叉熵损失函数在模型效果较差时学习速度快，而在模型效果较好时学习速度则会减慢。随着分类数目的增加，线性变化矩阵参数也会相应增大，这使得学习到的特征在封闭集分类问题中表现出良好的可分离性，但在开放集人脸识别问题中则显得缺乏区分性。因此，softmax+cross-entropy loss特别擅长于学习类间信息，但同时也忽略了非正确标签之间的差异。基于此问题，研究人员提出了多种优化方法，如L-Softmax、SM-Softmax和AM-Softmax等，以进一步提升模型在分类任务中的表现。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>交叉熵损失函数用于分类问题</li>
<li>模型通过sigmoid/softmax得到概率值</li>
<li>分类错误率不能有效区分模型表现</li>
<li>均方误差损失在分类问题中表现不佳</li>
<li>交叉熵损失函数能捕捉模型预测效果差异</li>
<li>交叉熵是凸函数，求导可得全局最优</li>
<li>交叉熵损失函数与sigmoid/softmax函数结合使用</li>
<li>二分类交叉熵损失函数求导可分三部分</li>
<li>多分类交叉熵损失函数求导可分三部分</li>
<li>交叉熵损失函数在模型效果差时学习速度快</li>
<li>交叉熵损失函数在模型效果好时学习速度慢</li>
<li>分类数目增大时，线性变化矩阵参数也增大</li>
<li>学习到的特征在封闭集分类问题中是可分离的</li>
<li>学习到的特征在开放集人脸识别问题中缺乏区分性</li>
<li>softmax+cross-entropy loss擅长学习类间信息</li>
<li>softmax+cross-entropy loss忽略非正确标签差异</li>
<li>基于此问题的优化方法有L-Softmax、SM-Softmax、AM-Softmax等</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3>正文（抓取，非 AI）</h3>
<p>这篇文章中，讨论的Cross Entropy损失函数常用于分类问题中，但是为什么它会在分类问题中这么有效呢？我们先从一个简单的分类例子来入手。 1. 图像分类任务 我们希望根据图片动物的轮廓、颜色等特征，来预测动物的类别，有三种可预测类别：猫、狗、猪。假设我们当前有两个模型（参数不同），这两个模型都是通过sigmoid/softmax的方式得到对于每个预测结果的概率值： 模型1 ： 预测 真实 是否正确 0.3 0.3 0.4 0 0 1 (猪) 正确 0.3 0.4 0.3 0 1 0 (狗) 正确 0.1 0.2 0.7 1 0 0 (猫) 错误 模型1 对于样本1和样本2以非常微弱的优势判断正确，对于样本3的判断则彻底错误。 模型2 ： 预测 真实 是否正确 0.1 0.2 0.7 0 0 1 (猪) 正确 0.1 0.7 0.2 0 1 0 (狗) 正确 0.3 0.4 0.3 1 0 0 (猫) 错误 模型2 对于样本1和样本2判断非常准确，对于样本3判断错误，但是相对来说没有错得太离谱。 好了，有了模型之后，我们需要通过定义损失函数来判断模型在样本上的表现了，那么我们可以定义哪些损失函数呢？ 1.1 Classification Error（分类错误率） 最为直接的损失函数定义为： 模型1： 模型2： 我们知道， 模型1 和 模型2 虽然都是预测错了1个，但是相对来说 模型2 表现得更好，损失函数值照理来说应该更小，但是，很遗憾的是， 并不能判断出来，所以这种损失函数虽然好理解，但表现不太好。 1.2 Mean Squared Error (均方误差) 均方误差损失也是一种比较常见的损失函数，其定义为： 模型1： 对所有样本的loss求平均： 模型2： 对所有样本的loss求平均： 我们发现，MSE能够判断出来 模型2 优于 模型1 ，那为什么不采样这种损失函数呢？主要原因是在分类问题中，使用sigmoid/softmx得到概率，配合MSE损失函数时，采用梯度下降法进行学习时，会出现模型一开始训练时，学习速率非常慢的情况（ MSE损失函数 ）。 有了上面的直观分析，我们可以清楚的看到，对于分类问题的损失函数来说，分类错误率和均方误差损失都不是很好的损失函数，下面我们来看一下交叉熵损失函数的表现情况。 1.3 Cross Entropy Loss Function（交叉熵损失函数） 1.3.1 表达式 (1) 二分类 在二分的情况下，模型最后需要预测的结果只有两种情况，对于每个类别我们的预测得到的概率为 和 ，此时表达式为（ 的底数是 ）： 其中： - —— 表示样本 的label，正类为 ，负类为 - —— 表示样本 预测为正类的概率 (2) 多分类 多分类的情况实际上就是对二分类的扩展： 其中： - ——类别的数量 - ——符号函数（ 或 ），如果样本 的真实类别等于 取 ，否则取 - ——观测样本 属于类别 的预测概率 现在我们利用这个表达式计算上面例子中的损失函数值： 模型1 ： 对所有样本的loss求平均： 模型2： 对所有样本的loss求平均： 上述计算可以使用python的sklearn库 可以发现，交叉熵损失函数可以捕捉到 模型1 和 模型2 预测效果的差异。 2. 函数性质 可以看出，该函数是凸函数，求导时能够得到全局最优值。 3. 学习过程 交叉熵损失函数经常用于分类问题中，特别是在神经网络做分类问题时，也经常使用交叉熵作为损失函数，此外，由于交叉熵涉及到计算每个类别的概率，所以交叉熵几乎每次都和 sigmoid(或softmax)函数 一起出现。 我们用神经网络最后一层输出的情况，来看一眼整个模型预测、获得损失和学习的流程： 神经网络最后一层得到每个类别的得分 scores（也叫logits） ； 该得分经过 sigmoid(或softmax)函数 获得概率输出； 模型预测的类别概率输出与真实类别的one hot形式进行交叉熵损失函数的计算。 学习任务分为二分类和多分类情况，我们分别讨论这两种情况的学习过程。 3.1 二分类情况 二分类交叉熵损失函数学习过程 如上图所示，求导过程可分成三个子过程，即拆成三项偏导的乘积： 3.1.1 计算第一项： - 表示样本 预测为正类的概率 - 为符号函数，样本 为正类时取 ，否则取 3.1.2 计算第二项： 这一项要计算的是sigmoid函数对于score的导数，我们先回顾一下sigmoid函数和分数求导的公式： 3.1.3 计算第三项： 一般来说，scores是输入的线性函数作用的结果，所以有： 3.1.4 计算结果 可以看到，我们得到了一个非常漂亮的结果，所以，使用交叉熵损失函数，不仅可以很好的衡量模型的效果，又可以很容易的的进行求导计算。 3.2 多分类情况 多分类交叉熵损失函数学习过程 如上图所示，求导过程可以分为三个子过程： 和二分类区别在于： 因为多分类只有一个类别为 ，其他为 ，不失一般性，我们可以假设 为 ，其他为 ，所以损失函数求和式子中只有 这项不为 ，即 这一项求导时，需要针对 是否等于 进行分类讨论（这里 表示的是样本真实类别为 ， 表示的是对我们想对输入到 的参数 求导） 3.2.1 计算第一项： 不失一般性，我们可以假设 为 ，其他为 ，则 求导： 3.2.2 计算第二项： 这一项要计算的是softmax函数对于得分的导数，我们先回顾一下softmax函数和分数求导的公式： 这里 表示的是样本真实类别为 ， 表示的是对输入到 的参数 求导，这时候存在两种情况： 情况1: 则第二项的求导式子，可以写成： 求导后得 情况2: 此时 这一项只在分母中存在，求导后得： 3.2.3 计算第三项： 一般来说，scores是输入的线性函数作用的结果，所以有： 3.2.4 计算结果 情况1: 情况2: 不失一般性，我们上述假设样本的真实类别为 ，则有: 我们求导时，对不同情况带入 的值后，得到了一致的表达式，如果采用向量化的形式，那么导数就不用再分情况写了，统一成： 可以看出，交叉熵损失函数对于二分类和多分类求导时，采用向量化的形式后，求导结果的形式是一致的。 4. 优缺点 4.1 优点 在用梯度下降法做参数更新的时候，模型学习的速度取决于两个值：一、 学习率 ；二、 偏导值 。其中，学习率是我们需要设置的超参数，所以我们重点关注偏导值。从上面的式子中，我们发现，偏导值的大小取决于 和 ，我们重点关注后者，后者的大小值反映了我们模型的错误程度，该值越大，说明模型效果越差，但是该值越大同时也会使得偏导值越大，从而模型学习速度更快。所以，使用逻辑函数得到概率，并结合交叉熵当损失函数时，在模型效果差的时候学习速度比较快，在模型效果好的时候学习速度变慢。 4.2 缺点 Deng [4]在2019年提出了ArcFace Loss，并在论文里说了Softmax Loss的两个缺点：1、随着分类数目的增大，分类层的线性变化矩阵参数也随着增大；2、对于封闭集分类问题，学习到的特征是可分离的，但对于开放集人脸识别问题，所学特征却没有足够的区分性。对于人脸识别问题，首先人脸数目(对应分类数目)是很多的，而且会不断有新的人脸进来，不是一个封闭集分类问题。 另外，sigmoid(softmax)+cross-entropy loss 擅长于学习类间的信息，因为它采用了类间竞争机制，它只关心对于正确标签预测概率的准确性，忽略了其他非正确标签的差异，导致学习到的特征比较散。基于这个问题的优化有很多，比如对softmax进行改进，如L-Softmax、SM-Softmax、AM-Softmax等。 5. 参考 [1]. 博客 - 神经网络的分类模型 LOSS 函数为什么要用 CROSS ENTROPY [2]. 博客 - Softmax as a Neural Networks Activation Function [3]. 博客 - A Gentle Introduction to Cross-Entropy Loss Function [4]. Deng, Jiankang, et al. "Arcface: Additive angular margin loss for deep face recognition." Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019.</p></div></details>
</div>
<script>
var articles=[]; var currentArticleIdx=-1; var maxSpeakChars=3500;
function buildArticles(){ var c=document.querySelector('.content'); if(!c) return; var h2s=c.querySelectorAll('h2'); for(var i=0;i<h2s.length;i++){ var start=h2s[i], end=i+1<h2s.length?h2s[i+1]:null; var r=document.createRange(); r.setStart(start,0); if(end) r.setEnd(end,0); else { var last=c.lastElementChild||c; r.setEndAfter(last); } var t=r.toString().replace(/\s+/g,' ').trim(); if(t.length>maxSpeakChars) t=t.slice(0,maxSpeakChars)+'…（内容过长已截断）'; articles.push({h2:start,text:t}); }
 articles.sort(function(a,b){ return a.text.length-b.text.length; }); }
function speakText(t){ if(!t){ document.getElementById('readStatus').textContent=''; return; }
 speechSynthesis.cancel(); var status=document.getElementById('readStatus'); status.textContent='准备中…';
 var chunks=[]; var maxLen=280; for(var i=0;i<t.length;i+=maxLen){ var s=t.slice(i,i+maxLen); var j=s.lastIndexOf('。'); if(j>80){ chunks.push(s.slice(0,j+1)); i-=s.length-(j+1); } else chunks.push(s); }
 if(chunks.length===0) chunks=[t]; var idx=0; function speakNext(){ if(idx>=chunks.length){ status.textContent=''; return; } var u=new SpeechSynthesisUtterance(chunks[idx]); u.lang='zh-CN'; u.rate=0.95; var v=speechSynthesis.getVoices().filter(function(x){ return x.lang.indexOf('zh')>=0; })[0]; if(v) u.voice=v;
 u.onend=function(){ idx++; setTimeout(speakNext,80); }; u.onerror=function(){ idx++; setTimeout(speakNext,80); }; status.textContent='朗读中 '+(idx+1)+'/'+chunks.length+'…'; speechSynthesis.speak(u); }
 if(speechSynthesis.getVoices().length) speakNext(); else speechSynthesis.onvoiceschanged=function(){ speechSynthesis.onvoiceschanged=null; speakNext(); }; }
function nextArticle(){ if(articles.length===0) buildArticles(); if(articles.length===0){ document.getElementById('readStatus').textContent='无可读内容'; return; }
 speechSynthesis.cancel(); currentArticleIdx++; if(currentArticleIdx>=articles.length){ currentArticleIdx=0; document.getElementById('readStatus').textContent='已读完，共 '+articles.length+' 篇。再按从第 1 篇开始'; return; }
 var a=articles[currentArticleIdx]; a.h2.scrollIntoView({behavior:'smooth',block:'start'}); document.getElementById('readStatus').textContent='第 '+(currentArticleIdx+1)+' / '+articles.length+' 篇'; speakText(a.text); }
function readSelected(){ var sel=window.getSelection(); var t=(sel&&sel.toString)?sel.toString():''; t=t.replace(/\\s+/g,' ').trim(); if(!t){ document.getElementById('readStatus').textContent='请先选中要朗读的段落'; return; } speechSynthesis.cancel(); document.getElementById('readStatus').textContent='朗读选中内容…'; speakText(t); }
function getBlockForTap(el){ var c=document.querySelector('.content'); var blockTags=['P','DIV','H2','H3','H4','LI','PRE']; while(el&&el!==c){ if(c.contains(el)&&blockTags.indexOf(el.tagName)!==-1) return el; el=el.parentElement; } return null; }
function onContentTap(e){ if(e.target.closest&&e.target.closest('a')) return; var block=getBlockForTap(e.target); if(block){ var t=(block.innerText||'').trim().replace(/\\s+/g,' '); if(t){ e.preventDefault(); speechSynthesis.cancel(); document.getElementById('readStatus').textContent='朗读本段…'; speakText(t); } } }
if(document.readyState==='loading'){ document.addEventListener('DOMContentLoaded', function(){ buildArticles(); var c=document.querySelector('.content'); if(c) c.addEventListener('click', onContentTap); }); } else { buildArticles(); var c=document.querySelector('.content'); if(c) c.addEventListener('click', onContentTap); }
</script>
</body>
</html>
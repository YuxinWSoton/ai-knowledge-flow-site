<!DOCTYPE html>
<html lang="zh-CN">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<title>知识流日报 2026-02-18：Task-Agnostic、Reinforcement Learning、LLM、Sim-to-Real、D-Optimal、Parameter-Robust、Continual Learning、Test-Time Adaptation、Non-Destructive LLM Editing、Partial Observability、Multi-Agent Systems、Large Language Models、Standardized Reusability、Agentic AI、Functional Coverage-Guided Verification、Quantum-Native、Sparse Reinforcement Learning、NN Accelerators、Compute Near-Register File、Reconfigurable、Sparsity-Aware、SW-HW Co-Design、Rowhammer-mitigated、Near-Cache、Compute-in-Memory、Microscaling、Floating-Point Charge-Trap Transistor、Transformer Accelerator</title>
<style>
  * { box-sizing: border-box; }
  body {
    font-family: system-ui, sans-serif;
    max-width: 720px;
    margin: 0 auto;
    padding: 2rem 1rem;
    color: #fff;
    min-height: 100vh;
    background: #0a0e1a;
    background-image:
      radial-gradient(2px 2px at 20px 30px, rgba(255,255,255,0.9), transparent),
      radial-gradient(2px 2px at 40px 70px, rgba(255,255,255,0.6), transparent),
      radial-gradient(1.5px 1.5px at 90px 40px, rgba(255,255,255,0.8), transparent),
      radial-gradient(2px 2px at 130px 80px, rgba(255,255,255,0.5), transparent),
      radial-gradient(1.5px 1.5px at 160px 120px, rgba(255,255,255,0.7), transparent),
      radial-gradient(ellipse 120% 100% at 50% 0%, rgba(88,60,120,0.25), transparent 50%),
      radial-gradient(ellipse 80% 80% at 80% 20%, rgba(40,60,100,0.2), transparent 40%);
    background-size: 200px 200px;
    background-repeat: repeat;
  }
  a { color: #a8d4ff; text-decoration: none; }
  a:hover { text-decoration: underline; }
 html{scroll-behavior:smooth;} .content h2,.content h3{scroll-margin-top:1rem;} h1,h2,h3{margin-top:1.5rem;color:#fff;} pre{white-space:pre-wrap;color:rgba(255,255,255,0.9);} .toolbar{margin:0.5rem 0;color:rgba(255,255,255,0.8);} .content{color:rgba(255,255,255,0.95);} .content a{color:#a8d4ff;} .meta{color:rgba(255,255,255,0.65);font-size:0.9em;} .toc{margin:1rem 0;padding:0.8rem 1rem;background:rgba(255,255,255,0.08);border-radius:6px;} .toc .toc-title{margin:0 0 0.5rem 0;font-weight:bold;color:rgba(255,255,255,0.9);} .toc ul{list-style:none;padding-left:0;margin:0;} .toc li{margin:0.35rem 0;} .toc li.toc-h3{padding-left:1em;font-size:0.95em;} .toc a{color:#a8d4ff;text-decoration:none;} .toc a:hover{text-decoration:underline;} .content p,.content div,.content h2,.content h3,.content h4,.content li,.content pre{cursor:pointer;-webkit-tap-highlight-color:rgba(255,255,255,0.15);} .content .insight-summary{color:#c9a0dc;margin:0.5em 0 0.8em 0;} .content .insight-detail,.content .insight-detail li{color:#8dd4e8;list-style:disc;margin-left:1.2em;padding-left:0.3em;} .content .article-body-details{margin:0.8em 0;} .content .article-body-details summary{cursor:pointer;color:rgba(255,255,255,0.85);}</style>
</head>
<body>
<p><a href="https://YuxinWSoton.github.io/ai-knowledge-flow-site/">← 返回首页</a></p>
<h1>知识流日报 2026-02-18：Task-Agnostic、Reinforcement Learning、LLM、Sim-to-Real、D-Optimal、Parameter-Robust、Continual Learning、Test-Time Adaptation、Non-Destructive LLM Editing、Partial Observability、Multi-Agent Systems、Large Language Models、Standardized Reusability、Agentic AI、Functional Coverage-Guided Verification、Quantum-Native、Sparse Reinforcement Learning、NN Accelerators、Compute Near-Register File、Reconfigurable、Sparsity-Aware、SW-HW Co-Design、Rowhammer-mitigated、Near-Cache、Compute-in-Memory、Microscaling、Floating-Point Charge-Trap Transistor、Transformer Accelerator</h1>
<p class="meta" style="margin-top:0;">最后更新：2026-02-24 09:06</p>
<p class="toolbar"><button id="btnFull" onclick="readFullText()">全文朗读</button> <span id="readStatus"></span></p>
<p class="meta" style="margin-top:0;">（点任意段落可朗读该段；「全文朗读」读本页全部内容）</p>
<nav class="toc" aria-label="目录">
<p class="toc-title">目录</p>
<ul>
  <li><a href="#toc-0">1. C# 异步中Task.Wait的坑? C# Task.Wait为什么不等待就返回？</a></li>
  <li class="toc-h3"><a href="#toc-1">正文</a></li>
  <li><a href="#toc-2">2. C# 为何一定要用asyn和await来异步执行,不可以直接Task.Run?</a></li>
  <li class="toc-h3"><a href="#toc-3">正文</a></li>
  <li><a href="#toc-4">3. 新买的win10系统的电脑，最近关机的时候总弹出“task host ...</a></li>
  <li class="toc-h3"><a href="#toc-5">正文</a></li>
  <li><a href="#toc-6">4. 你了解世坤的 worldquant brain 平台吗？ - 知乎</a></li>
  <li class="toc-h3"><a href="#toc-7">正文</a></li>
  <li><a href="#toc-8">5. mac中kernel_task占用大量内存如何解决？ - 知乎</a></li>
  <li class="toc-h3"><a href="#toc-9">正文</a></li>
  <li><a href="#toc-10">6. win10家庭版。最近每次关机出现task host windows任务宿主 ...</a></li>
  <li class="toc-h3"><a href="#toc-11">正文</a></li>
  <li><a href="#toc-12">7. 能否介绍一下强化学习（Reinforcement Learning），以及与 ...</a></li>
  <li class="toc-h3"><a href="#toc-13">正文</a></li>
  <li><a href="#toc-14">8. 关于对《Reinforcement Learning: An Introduction》的理解？</a></li>
  <li class="toc-h3"><a href="#toc-15">正文</a></li>
  <li><a href="#toc-16">9. 关于对《Reinforcement Learning: An Introduction》的理解？</a></li>
  <li class="toc-h3"><a href="#toc-17">正文</a></li>
  <li><a href="#toc-18">10. 强化学习 (reinforcement learning) 如何自学？有哪些必修的 ...</a></li>
  <li class="toc-h3"><a href="#toc-19">正文</a></li>
  <li><a href="#toc-20">11. 强化学习（reinforcement learning)有什么好的开源项目 ...</a></li>
  <li class="toc-h3"><a href="#toc-21">正文</a></li>
  <li><a href="#toc-22">12. 学习强化学习 (reinforcement learning)有哪些工具推荐？ - 知乎</a></li>
  <li class="toc-h3"><a href="#toc-23">正文</a></li>
  <li><a href="#toc-24">13. 强化学习该关注哪些顶会？ - 知乎</a></li>
  <li class="toc-h3"><a href="#toc-25">正文</a></li>
  <li><a href="#toc-26">14. 强化学习（Reinforcement learning）中Actor-Critic算法该 ...</a></li>
  <li class="toc-h3"><a href="#toc-27">正文</a></li>
  <li><a href="#toc-28">15. 初学者怎么入门大语言模型（LLM）？ - 知乎</a></li>
  <li class="toc-h3"><a href="#toc-29">正文</a></li>
  <li><a href="#toc-30">16. 想学习大语言模型 (LLM)，应该从哪个开源模型开始？</a></li>
  <li class="toc-h3"><a href="#toc-31">正文</a></li>
  <li><a href="#toc-32">17. 如何从零开始学习LLM大模型？ - 知乎</a></li>
  <li class="toc-h3"><a href="#toc-33">正文</a></li>
  <li><a href="#toc-34">18. 2025年大模型LLM还有哪些可研究的方向？ - 知乎</a></li>
  <li class="toc-h3"><a href="#toc-35">正文</a></li>
  <li><a href="#toc-36">19. 实现 LLM 复杂推理（Reasoning）目前有哪些主要方法？</a></li>
  <li class="toc-h3"><a href="#toc-37">正文</a></li>
  <li><a href="#toc-38">20. LLM （大型语言模型）都有哪些潜在应用场景？ - 知乎</a></li>
  <li class="toc-h3"><a href="#toc-39">正文</a></li>
  <li><a href="#toc-40">21. 985硕只为了就业，纯语言大模型LLM、多模态大模型、生成 ...</a></li>
  <li class="toc-h3"><a href="#toc-41">正文</a></li>
  <li><a href="#toc-42">22. 如何评价Rich Sutton关于「LLM是死路」的观点？ - 知乎</a></li>
  <li class="toc-h3"><a href="#toc-43">正文</a></li>
  <li><a href="#toc-44">23. LaTeX 排版中，如何输入长的等价符号（\sim），且上下可 ...</a></li>
  <li class="toc-h3"><a href="#toc-45">正文</a></li>
  <li><a href="#toc-46">24. 编程中，parameter、argument翻译成什么中文最好？</a></li>
  <li class="toc-h3"><a href="#toc-47">正文</a></li>
  <li><a href="#toc-48">25. 最近比较火的parameter server是什么？ - 知乎</a></li>
  <li class="toc-h3"><a href="#toc-49">正文</a></li>
  <li><a href="#toc-50">26. sarscape做sbas 第一步反演出现parameter read error问题?</a></li>
  <li class="toc-h3"><a href="#toc-51">正文</a></li>
  <li><a href="#toc-52">27. 能不能用简明的语言解释什么是非参数（nonparametric）模型？</a></li>
  <li class="toc-h3"><a href="#toc-53">正文</a></li>
  <li><a href="#toc-54">28. 什么是非独立同分布（Non-IID）数据，有没有很简单的解释 ...</a></li>
  <li class="toc-h3"><a href="#toc-55">正文</a></li>
  <li><a href="#toc-56">29. 请问多智能体（multi-agent system）有什么资料入门吗？ - 知乎</a></li>
  <li class="toc-h3"><a href="#toc-57">正文</a></li>
  <li><a href="#toc-58">30. 请问多智能体（multi-agent system）有什么资料入门吗？ - 知乎</a></li>
  <li class="toc-h3"><a href="#toc-59">正文</a></li>
  <li><a href="#toc-60">31. 如何评价AAMAS 2024的审稿结果？ - 知乎</a></li>
  <li class="toc-h3"><a href="#toc-61">正文</a></li>
  <li><a href="#toc-62">32. Moltbook：Agent 规模提升涌现出了更高的智能？是社交的 ...</a></li>
  <li class="toc-h3"><a href="#toc-63">正文</a></li>
  <li><a href="#toc-64">33. multiagent path finding的顶刊有哪些? - 知乎</a></li>
  <li class="toc-h3"><a href="#toc-65">正文</a></li>
  <li><a href="#toc-66">34. 多智能体强化学习可以投的英文会议推荐？ - 知乎</a></li>
  <li class="toc-h3"><a href="#toc-67">正文</a></li>
  <li><a href="#toc-68">35. 如何评价最近大火的LatentMAS？ - 知乎</a></li>
  <li class="toc-h3"><a href="#toc-69">正文</a></li>
  <li><a href="#toc-70">36. WWW 2026 投稿 Agent 安全 - 知乎</a></li>
  <li class="toc-h3"><a href="#toc-71">正文</a></li>
  <li><a href="#toc-72">37. npj系列算不算nature子刊? - 知乎</a></li>
  <li class="toc-h3"><a href="#toc-73">正文</a></li>
  <li><a href="#toc-74">38. 稀疏（sparse）在机器学习中很重要吗？为什么？ - 知乎</a></li>
  <li class="toc-h3"><a href="#toc-75">正文</a></li>
  <li><a href="#toc-76">39. 什么是稀疏特征 (Sparse Features)? - 知乎</a></li>
  <li class="toc-h3"><a href="#toc-77">正文</a></li>
  <li><a href="#toc-78">40. 深度学习中的sparse和dense模型指的是什么？ - 知乎</a></li>
  <li class="toc-h3"><a href="#toc-79">正文</a></li>
  <li><a href="#toc-80">41. 通俗理解，Sparse Attention是什么原理？ - 知乎</a></li>
  <li class="toc-h3"><a href="#toc-81">正文</a></li>
  <li><a href="#toc-82">42. 如何看待Native Sparse Attention？ - 知乎</a></li>
  <li class="toc-h3"><a href="#toc-83">正文</a></li>
  <li><a href="#toc-84">43. Sparse matrix 作为深度学习输入 在模型表现上会有影响吗 ...</a></li>
  <li class="toc-h3"><a href="#toc-85">正文</a></li>
  <li><a href="#toc-86">44. 请问机器学习中的稀疏先验（sparse prior）是什么？</a></li>
  <li class="toc-h3"><a href="#toc-87">正文</a></li>
  <li><a href="#toc-88">45. 如何理解稀疏主成分分析 (Sparse Principal Component ...</a></li>
  <li class="toc-h3"><a href="#toc-89">正文</a></li>
  <li><a href="#toc-90">46. 为什么sparse representation比起其它成分分析方法（DFT ...</a></li>
  <li class="toc-h3"><a href="#toc-91">正文</a></li>
  <li><a href="#toc-92">47. Evaluate,Compute,和Calculate三者有什么区别? - 知乎</a></li>
  <li class="toc-h3"><a href="#toc-93">正文</a></li>
  <li><a href="#toc-94">48. 什么是算力网络（Compute First Networking）？ - 知乎</a></li>
  <li class="toc-h3"><a href="#toc-95">正文</a></li>
  <li><a href="#toc-96">49. 备受互联网巨头推崇的『开放计算』到底是什么？可以应用到 ...</a></li>
  <li class="toc-h3"><a href="#toc-97">正文</a></li>
  <li><a href="#toc-98">50. Compute shader 和 CUDA 有哪些差异？ - 知乎</a></li>
  <li class="toc-h3"><a href="#toc-99">正文</a></li>
  <li><a href="#toc-100">51. 都快2025年了，国内手机对ComputeShader的兼容性如何 ...</a></li>
  <li class="toc-h3"><a href="#toc-101">正文</a></li>
  <li><a href="#toc-102">52. 如何学习与研究Reconfigurable Computing？ - 知乎</a></li>
  <li class="toc-h3"><a href="#toc-103">正文</a></li>
  <li><a href="#toc-104">53. CGRA和FPGA有什么不同？ - 知乎</a></li>
  <li class="toc-h3"><a href="#toc-105">正文</a></li>
  <li><a href="#toc-106">54. 【半导体所：可重构传感器实现高效三维成像 |《自然-电子学 ...</a></li>
  <li class="toc-h3"><a href="#toc-107">正文</a></li>
  <li><a href="#toc-108">55. 智能超表面和智能反射面的区别在哪里呢? - 知乎</a></li>
  <li class="toc-h3"><a href="#toc-109">正文</a></li>
  <li><a href="#toc-110">56. 可重构芯片的工作原理是什么？ - 知乎</a></li>
  <li class="toc-h3"><a href="#toc-111">正文</a></li>
  <li><a href="#toc-112">57. 集成电路设计的学术会议含金量排名如何？ - 知乎</a></li>
  <li class="toc-h3"><a href="#toc-113">正文</a></li>
  <li><a href="#toc-114">58. 美国科学家研发出细如发丝的纤维可同时控制数千神经元，这 ...</a></li>
  <li class="toc-h3"><a href="#toc-115">正文</a></li>
  <li><a href="#toc-116">59. 全球首款逻辑量子比特电路实现无差错计算，这是怎样的突破 ...</a></li>
  <li class="toc-h3"><a href="#toc-117">正文</a></li>
  <li><a href="#toc-118">60. MICRO25论文笔记 - 知乎</a></li>
  <li class="toc-h3"><a href="#toc-119">正文</a></li>
  <li><a href="#toc-120">61. 如何看待 openai提出的 “零参数模型/”超稀疏小模型： Circuit ...</a></li>
  <li class="toc-h3"><a href="#toc-121">正文</a></li>
  <li><a href="#toc-122">62. 如何评价 Meta 提出的 STEM 稀疏性架构？</a></li>
  <li class="toc-h3"><a href="#toc-123">正文</a></li>
  <li><a href="#toc-124">63. 终稿改回初稿 的想法: 清华朱军团队提出「稀疏-线性注意力 ...</a></li>
  <li class="toc-h3"><a href="#toc-125">正文</a></li>
  <li><a href="#toc-126">64. 什么是稀疏特征 (Sparse Features)? - 知乎</a></li>
  <li class="toc-h3"><a href="#toc-127">正文</a></li>
  <li><a href="#toc-128">65. 如何评价DeepSeek发布梁文锋署名论文，提出「条件记忆」 ...</a></li>
  <li class="toc-h3"><a href="#toc-129">正文</a></li>
  <li><a href="#toc-130">66. 菜鸟脱贫户 的想法: SparseInfer | 链接SparseInfer: Training ...</a></li>
  <li class="toc-h3"><a href="#toc-131">正文</a></li>
  <li><a href="#toc-132">67. XGBoost:Sparsity-aware Split Finding算法如何提升效率？</a></li>
  <li class="toc-h3"><a href="#toc-133">正文</a></li>
  <li><a href="#toc-134">68. 如何评价 OpenAI 开源 0.4B 参数模型？将对 AI 领域带来 ...</a></li>
  <li class="toc-h3"><a href="#toc-135">正文</a></li>
  <li><a href="#toc-136">69. Row Hammer漏洞会有多大实质性影响？ - 知乎</a></li>
  <li class="toc-h3"><a href="#toc-137">正文</a></li>
  <li><a href="#toc-138">70. 研究显示 AMD 处理器也受 Rowhammer 内存攻击影响 ...</a></li>
  <li class="toc-h3"><a href="#toc-139">正文</a></li>
  <li><a href="#toc-140">71. 如何看待研究员成功利用 Rowhammer 漏洞获取 Android root ...</a></li>
  <li class="toc-h3"><a href="#toc-141">正文</a></li>
  <li><a href="#toc-142">72. 在Cache设计中提高命中率能否对DRAM的Row Hammer攻 ...</a></li>
  <li class="toc-h3"><a href="#toc-143">正文</a></li>
  <li><a href="#toc-144">73. IEEE 在业界都怎么读？ - 知乎</a></li>
  <li class="toc-h3"><a href="#toc-145">正文</a></li>
  <li><a href="#toc-146">74. floating和not connected有什么区别？ - 知乎</a></li>
  <li class="toc-h3"><a href="#toc-147">正文</a></li>
  <li><a href="#toc-148">75. pfc显示错误illegal floating point value？ - 知乎</a></li>
  <li class="toc-h3"><a href="#toc-149">正文</a></li>
  <li><a href="#toc-150">76. 数据手册中 leave floating 和 unconnected 有什么区别</a></li>
  <li class="toc-h3"><a href="#toc-151">正文</a></li>
  <li><a href="#toc-152">77. 请问floating absolute risk在R上怎么实现呢？</a></li>
  <li class="toc-h3"><a href="#toc-153">正文</a></li>
  <li><a href="#toc-154">78. Transformer模型详解（图解最完整版） - 知乎</a></li>
  <li class="toc-h3"><a href="#toc-155">正文</a></li>
  <li><a href="#toc-156">79. 如何最简单、通俗地理解Transformer？ - 知乎</a></li>
  <li class="toc-h3"><a href="#toc-157">正文</a></li>
  <li><a href="#toc-158">80. 如何从浅入深理解 Transformer？ - 知乎</a></li>
  <li class="toc-h3"><a href="#toc-159">正文</a></li>
  <li><a href="#toc-160">81. 如何从浅入深理解 Transformer？ - 知乎</a></li>
  <li class="toc-h3"><a href="#toc-161">正文</a></li>
  <li><a href="#toc-162">82. Transformer是什么？ - 知乎</a></li>
  <li class="toc-h3"><a href="#toc-163">正文</a></li>
  <li><a href="#toc-164">83. 有没有比较详细通俗易懂的 Transformer 教程？ - 知乎</a></li>
  <li class="toc-h3"><a href="#toc-165">正文</a></li>
  <li><a href="#toc-166">84. 如何最简单、通俗地理解Transformer？ - 知乎</a></li>
  <li class="toc-h3"><a href="#toc-167">正文</a></li>
  <li><a href="#toc-168">85. Transformer 和 cnn 是两条差异巨大的路径吗？ - 知乎</a></li>
  <li class="toc-h3"><a href="#toc-169">正文</a></li>
  <li><a href="#toc-170">86. Toyota in georgia | Car Dealerships In georgia, atlanta</a></li>
  <li class="toc-h3"><a href="#toc-171">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-172">87. 连更预告|全球诡异时代 | 官方在线漫画全集-快看漫画</a></li>
  <li class="toc-h3"><a href="#toc-173">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-174">88. Quantum Flagship公布最新路线图，将欧洲定位为世界上第 ...</a></li>
  <li class="toc-h3"><a href="#toc-175">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-176">89. Introduction to Large Language Models | Google Skills</a></li>
  <li class="toc-h3"><a href="#toc-177">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-178">90. Pricing | Agentic.ai</a></li>
  <li class="toc-h3"><a href="#toc-179">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-180">91. 量子计算云平台</a></li>
  <li class="toc-h3"><a href="#toc-181">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-182">92. A Review on Large Language Models: Architectures, Applications ...</a></li>
  <li class="toc-h3"><a href="#toc-183">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-184">93. About | Agentic.ai</a></li>
  <li class="toc-h3"><a href="#toc-185">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-186">94. How it works | Agentic.ai</a></li>
  <li class="toc-h3"><a href="#toc-187">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-188">95. 全球诡异时代 | 全球诡异时代漫画全集免费 - 奈斯漫画</a></li>
  <li class="toc-h3"><a href="#toc-189">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-190">96. NN 游戏社区以技术驱动革新，百万玩家体验再升级 - IT之家</a></li>
  <li class="toc-h3"><a href="#toc-191">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-192">97. 机器人仿真教程丨Isaac Sim 4.5.0 与 lsaac Lab 2.0 安装指南</a></li>
  <li class="toc-h3"><a href="#toc-193">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-194">98. 强化学习 (Reinforcement Learning) - 知乎</a></li>
  <li class="toc-h3"><a href="#toc-195">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-196">99. 量子光学（quantum optics） - 知乎</a></li>
  <li class="toc-h3"><a href="#toc-197">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-198">100. 一文读懂：大语言模型（LLM)</a></li>
  <li class="toc-h3"><a href="#toc-199">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-200">101. Rick Hendrick Toyota Sandy Springs - Location, Deals and …</a></li>
  <li class="toc-h3"><a href="#toc-201">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-202">102. World Toyota - Location, Deals and Inventory</a></li>
  <li class="toc-h3"><a href="#toc-203">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-204">103. Sparse Transformer</a></li>
  <li class="toc-h3"><a href="#toc-205">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-206">104. A Comprehensive Overview of Large Language Models</a></li>
  <li class="toc-h3"><a href="#toc-207">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-208">105. 一文了解Transformer全貌（图解Transformer）</a></li>
  <li class="toc-h3"><a href="#toc-209">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-210">106. GPU编程21：Nsight Compute (1) Kernel Profiling Guide</a></li>
  <li class="toc-h3"><a href="#toc-211">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-212">107. Toyota Car Inventory in Atlanta</a></li>
  <li class="toc-h3"><a href="#toc-213">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-214">108. 什么是大语言模型 (LLM)？| 大语言模型（LLM）解析：企业 ...</a></li>
  <li class="toc-h3"><a href="#toc-215">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-216">109. Privacy Policy - Agentic.ai</a></li>
  <li class="toc-h3"><a href="#toc-217">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-218">110. 一文带你解密 Large Language Model（大型语言模型）</a></li>
  <li class="toc-h3"><a href="#toc-219">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-220">111. 全球诡异时代漫画,全球诡异时代漫画全集,爱漫画就看全球诡异 ...</a></li>
  <li class="toc-h3"><a href="#toc-221">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-222">112. Japanese joint research group launches quantum computing …</a></li>
  <li class="toc-h3"><a href="#toc-223">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-224">113. 全球诡异时代漫画在线免费看全集下载_漫星阁漫画</a></li>
  <li class="toc-h3"><a href="#toc-225">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-226">114. 大语言模型_百度百科</a></li>
  <li class="toc-h3"><a href="#toc-227">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-228">115. 大型语言模型综述 A Survey of Large Language Models</a></li>
  <li class="toc-h3"><a href="#toc-229">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-230">116. Large Language Models | Springer Nature Link</a></li>
  <li class="toc-h3"><a href="#toc-231">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-232">117. 挑战 Transformer：全新架构 Mamba 详解</a></li>
  <li class="toc-h3"><a href="#toc-233">正文（抓取，非 AI）</a></li>
</ul>
</nav>
<div class="content">
<h1>知识流日报 2026-02-18：Task-Agnostic、Reinforcement Learning、LLM、Sim-to-Real、D-Optimal、Parameter-Robust、Continual Learning、Test-Time Adaptation、Non-Destructive LLM Editing、Partial Observability、Multi-Agent Systems、Large Language Models、Standardized Reusability、Agentic AI、Functional Coverage-Guided Verification、Quantum-Native、Sparse Reinforcement Learning、NN Accelerators、Compute Near-Register File、Reconfigurable、Sparsity-Aware、SW-HW Co-Design、Rowhammer-mitigated、Near-Cache、Compute-in-Memory、Microscaling、Floating-Point Charge-Trap Transistor、Transformer Accelerator</h1>
<p>共 117 条（来自百度学术 / Google / Bing 等，仅含正文抓取成功的条目；按正文长度从短到长排列，便于先听短的）。</p>
<p>（以下含抓取正文，便于阅读。未调用任何 AI API。）</p>
<p>（部分条目含模型提取的「易漏细节」关键点，页面上以颜色区分。）</p>
<h2 id="toc-0">1. C# 异步中Task.Wait的坑? C# Task.Wait为什么不等待就返回？</h2>
<ul>
<li>链接：https://www.zhihu.com/question/512936487</li>
<li>来源：bing</li>
<li>摘要：Task的构造函数压根儿就没有接收异步方法的重载，这意味着Task的构造函数只会把这个方法当作普通的同步方法来执行，并创建一个Task用来调度这个方法。 而异步方法直接调用的时候，就是一个返 …</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">Task的构造函数没有提供异步方法的重载，这意味着在使用Task时，开发者无法直接通过构造函数来指定异步操作。这种设计使得Task的使用更加依赖于开发者明确地调用异步方法，如StartAsync或RunAsync等。因此，为了确保Task能够正确地执行异步操作，开发者需要在创建Task实例后，显式地调用相应的异步方法。这种设计虽然增加了代码的复杂性，但也使得异步操作的意图更加明确，有助于提高代码的可读性和可维护性。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>Task的构造函数没有异步方法的重载</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-1">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-2">2. C# 为何一定要用asyn和await来异步执行,不可以直接Task.Run?</h2>
<ul>
<li>链接：https://www.zhihu.com/question/8460793593</li>
<li>来源：bing</li>
<li>摘要：Task.Run 的语义是 「开 线程」执行新任务流. async await 的语义是「主动让出」 控制流 (像是 yield ). 后续焦点仍在当前的任务流上. 这是第一性的区别. Task 关注新任务, 因此想单开一个任务让它自个人 …</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">在编程中，开线程是主动让出控制流的一种方式，允许程序在当前任务执行过程中继续处理其他任务。这样做可以提高程序的响应性和效率。当一个线程被创建后，它会继续运行直到完成，而其他任务则可以被调度执行。Task关注点在于能够独立运行一个新的任务，使得单个任务可以在不同的线程中执行，而不影响当前的任务流。因此，通过这种方式，后续的焦点仍然保持在当前的任务流上，但同时能够处理新的任务，确保程序的流畅性和灵活性。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>开线程</li>
<li>主动让出控制流</li>
<li>后续焦点仍在当前的任务流上</li>
<li>Task关注新任务</li>
<li>想单开一个任务让它自个人运行</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-3">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-4">3. 新买的win10系统的电脑，最近关机的时候总弹出“task host ...</h2>
<ul>
<li>链接：https://www.zhihu.com/question/373645553</li>
<li>来源：bing</li>
<li>摘要：2022年9月8日 · 新买的win10系统的电脑，最近关机的时候总弹出“task host window”，这是怎么回事？ 下面只有“强制关机”和“取消”两个选项，点了取消就弹回桌面了，出现这个是正常现象吗？ 能关掉 …</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-5">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-6">4. 你了解世坤的 worldquant brain 平台吗？ - 知乎</h2>
<ul>
<li>链接：https://www.zhihu.com/question/6667015461</li>
<li>来源：bing</li>
<li>摘要：Task: In Settings, set Neutralization to Market and apply a value of 5 or higher for Decay, then simulate the Alpha. 还是中性化的事情，这次设置为市场中性，即忽略大盘的影响，另外引入 Decay 参数，意 …</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">首先，设置中性化为市场中性是模拟Alpha的关键步骤之一，这有助于消除市场整体波动对投资组合表现的影响。其次，引入Decay参数并将其值设为5或更高，可以有效模拟Alpha，同时减少模型对近期数据的过度依赖，使模型更具稳健性。此外，忽略大盘的影响是实现市场中性的重要环节，这有助于确保模拟结果更加客观，不受市场整体趋势的干扰。因此，通过这些步骤，可以更准确地评估投资策略的真实表现，从而为投资者提供更有价值的参考。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>设置中性化为市场中性</li>
<li>设置Decay参数值为5或更高</li>
<li>忽略大盘的影响</li>
<li>模拟Alpha</li>
<li>引入Decay参数</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-7">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-8">5. mac中kernel_task占用大量内存如何解决？ - 知乎</h2>
<ul>
<li>链接：https://www.zhihu.com/question/56689986</li>
<li>来源：bing</li>
<li>摘要：2020年6月29日 · kernel_task下有一个launchd进程，这个进程展开后显示的是Mac OS X里所有正在运行的进程。 可以用homebrew安装一个htop然后查看一下（需要用sudo），按F6用树状图显示，根本就 …</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">kernel_task占用大量内存，这是由于它在系统中扮演着核心角色，负责处理系统级的任务。值得注意的是，launchd进程属于kernel_task，这意味着它也是系统运行过程中的一部分，同样消耗着大量的内存。为了更好地监控这些进程，可以使用homebrew安装htop工具。然而，为了获得root权限，需要使用sudo运行htop。通过htop，用户可以方便地切换显示模式，其中一种模式是树状图，这有助于更直观地理解进程之间的关系。因此，通过htop的树状图模式，用户可以更清晰地看到launchd进程及其子进程的层级结构，从而更好地理解和管理系统资源。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>kernel_task占用大量内存</li>
<li>launchd进程属于kernel_task</li>
<li>可以使用homebrew安装htop查看</li>
<li>需要使用sudo运行htop</li>
<li>htop可以按F6切换显示模式</li>
<li>显示模式可以使用树状图</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-9">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-10">6. win10家庭版。最近每次关机出现task host windows任务宿主 ...</h2>
<ul>
<li>链接：https://www.zhihu.com/question/358989904</li>
<li>来源：bing</li>
<li>摘要：2022年8月27日 · 最好不要修改 注册表，容易导致进程的崩溃和在启动的问题。 可以按照微软给的方法试一试 Task Host Window - Microsoft Community 发布于 2022-08-26 18:00 酷跑加速器很垃圾 4 人 …</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">最好不要修改注册表，因为这容易导致进程的崩溃和在启动时出现问题。注册表是操作系统的核心数据库，任何不恰当的修改都可能引发系统不稳定。因此，除非绝对必要，否则建议遵循微软提供的安全方法进行操作，以避免潜在的风险。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>最好不要修改注册表</li>
<li>容易导致进程的崩溃</li>
<li>容易在启动时出现问题</li>
<li>可以按照微软给的方法试一试</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-11">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-12">7. 能否介绍一下强化学习（Reinforcement Learning），以及与 ...</h2>
<ul>
<li>链接：https://www.zhihu.com/question/41775291</li>
<li>来源：bing</li>
<li>摘要：2015年3月28日 · 見：Reinforcement Learning and its Relationship to Supervised Learning，Barto and Dietterich, 2004. "But is it possible to do this the other way around: to convert a …</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-13">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-14">8. 关于对《Reinforcement Learning: An Introduction》的理解？</h2>
<ul>
<li>链接：https://www.zhihu.com/question/50461146/answers/updated</li>
<li>来源：bing</li>
<li>摘要：2025年11月9日 · 关于对《Reinforcement Learning: An Introduction》的理解？ 看了半个月这本书，觉得书中很多例子和公式算法很难理解，不知道有没有大神已经研究完了这本书可以给出一 …</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-15">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-16">9. 关于对《Reinforcement Learning: An Introduction》的理解？</h2>
<ul>
<li>链接：https://www.zhihu.com/question/50461146</li>
<li>来源：bing</li>
<li>摘要：关于对《Reinforcement Learning: An Introduction》的理解？ 看了半个月这本书，觉得书中很多例子和公式算法很难理解，不知道有没有大神已经研究完了这本书可以给出一些阅读心得或者笔 …</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-17">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-18">10. 强化学习 (reinforcement learning) 如何自学？有哪些必修的 ...</h2>
<ul>
<li>链接：https://www.zhihu.com/question/524987199</li>
<li>来源：bing</li>
<li>摘要：2022年3月30日 · 导论 如果你想过这个问题：人类是怎么学习的？你可能会得出——“人类是通过与环境不断交互来学习的”这样一个答案。在心理学的行为主义理论中，在环境给予有机体奖励或 …</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">人类通过与环境不断交互来学习，这一过程是行为主义理论的核心。环境在这一过程中扮演着关键角色，它通过给予有机体奖励或惩罚来塑造行为。这种交互不仅促进了有机体适应环境的能力，也加深了对环境影响的理解。因此，行为主义理论强调环境的作用，认为有机体的行为是对外部刺激的反应，而这些反应又受到奖励和惩罚的调节。这种机制使得个体能够根据环境反馈调整自己的行为，从而更好地适应复杂多变的环境。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>人类通过与环境不断交互来学习</li>
<li>环境给予有机体奖励或惩罚</li>
<li>行为主义理论强调环境的作用</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-19">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-20">11. 强化学习（reinforcement learning)有什么好的开源项目 ...</h2>
<ul>
<li>链接：https://www.zhihu.com/question/49230922</li>
<li>来源：bing</li>
<li>摘要：代码github地址： MorvanZhou/Reinforcement-learning-with-tensorflow 6.David Silver视频 David Silver的课程在强化学习领域算是比较权威了，一定要认真仔细看，做好笔记，最好是有强化 …</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">首先，MorvanZhou的《Reinforcement-learning-with-tensorflow》项目是一个重要的资源，它为学习者提供了实践强化学习的机会。其次，David Silver的视频课程是强化学习领域内的权威课程，通过这些视频，学习者可以深入理解强化学习的基本原理和应用。此外，做好笔记是学习过程中的关键步骤，有助于巩固知识和回顾。因此，最好具备一定的强化学习背景，这样可以更好地理解和应用所学内容，提高学习效率。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>MorvanZhou/Reinforcement-learning-with-tensorflow</li>
<li>David Silver视频</li>
<li>强化学习领域权威课程</li>
<li>做好笔记</li>
<li>最好有强化学习背景</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-21">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-22">12. 学习强化学习 (reinforcement learning)有哪些工具推荐？ - 知乎</h2>
<ul>
<li>链接：https://www.zhihu.com/question/333671830</li>
<li>来源：bing</li>
<li>摘要：2024年9月13日 · 京东 ¥112.60 去购买 Algorithms for Reinforcement Learning 这本书短小简洁 (只有 100 多页)，省去了很多公式推理，适合那些讨厌理论推导，而喜欢一上手就干的童鞋们。 …</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">首先，这些「易漏但重要」的内容之所以容易被忽略，是因为它们不像理论推导那样直观和具体，反而更偏向于基础概念和细节，但这些细节往往是解决问题的关键。因此，对于那些不喜欢理论推导的读者来说，这些内容可能更容易被忽视。然而，这些「易漏但重要」的内容实际上在实际应用中扮演着至关重要的角色，它们是解决问题的基础，也是避免常见错误的关键。因此，即便不喜欢理论推导，也应重视这些基础内容，确保在实践中能够准确无误地应用。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>适合讨厌理论推导的读者</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-23">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-24">13. 强化学习该关注哪些顶会？ - 知乎</h2>
<ul>
<li>链接：https://www.zhihu.com/question/643854343</li>
<li>来源：bing</li>
<li>摘要：2024年2月10日 · 然后UAI和AISTATS，文章质量稍低，但你会碰到同一批审稿人。 事实上这批人搞出来的distributional RL也发在了AAAI。 但如果你不属于上述俩个视角， 对于其他RL的理论 …</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">首先，关注顶级会议如ICML、NeurIPS和IJCAI，这些会议的文章质量较高，涵盖了最新的研究成果和理论。其次，相比之下，UAI和AISTATS的文章质量稍低，但它们仍然提供了有价值的研究成果。此外，这些审稿人也处理distributional RL相关领域，这意味着他们的评审标准和关注点较为一致。因此，如果你的研究视角不属于上述两个主要方向，其他RL理论可能不太适合，建议专注于与这些顶级会议相关的研究领域。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>关注顶会如ICML、NeurIPS、IJCAI。</li>
<li>UAI和AISTATS文章质量稍低。</li>
<li>这批审稿人也处理distributional RL。</li>
<li>如果你不属于上述两个视角，其他RL理论可能不太适合。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-25">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-26">14. 强化学习（Reinforcement learning）中Actor-Critic算法该 ...</h2>
<ul>
<li>链接：https://www.zhihu.com/question/56692640</li>
<li>来源：bing</li>
<li>摘要：强化学习（Reinforcement learning）中Actor-Critic算法该如何深入理解？ 本人刚入门，水平太低，都不好意思提问了。 。 最近在看基于值迭代的强化学习来解决连续状态和连续动作的问 …</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-27">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-28">15. 初学者怎么入门大语言模型（LLM）？ - 知乎</h2>
<ul>
<li>链接：https://www.zhihu.com/question/644285055</li>
<li>来源：bing</li>
<li>摘要：2024年12月16日 · LLM 部署及优化技术：部署和优化LLM涉及有效提供预测的策略，同时有效管理计算资源。 这包括模型量化（减少数字的精度以节省内存）、模型修剪（移除不那么重要的权 …</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">部署和优化大型语言模型（LLM）需要采取一系列策略来确保其高效运行。首先，有效的预测策略是必不可少的，这有助于提高模型的响应速度和准确性。其次，模型量化能够显著节省内存，通过减少模型权重的精度，可以在不牺牲太多性能的情况下降低存储和计算需求。此外，模型修剪涉及移除那些不那么重要的权重，进一步减少模型的复杂性和资源消耗。通过这些策略的综合应用，可以显著提升LLM的性能和效率。因此，部署和优化LLM不仅需要考虑预测策略的有效性，还需要结合模型量化和修剪等技术，以实现最佳的性能和资源利用。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>部署和优化LLM需要有效提供预测的策略</li>
<li>模型量化可以节省内存</li>
<li>模型修剪涉及移除不那么重要的权重</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-29">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-30">16. 想学习大语言模型 (LLM)，应该从哪个开源模型开始？</h2>
<ul>
<li>链接：https://www.zhihu.com/question/608820310</li>
<li>来源：bing</li>
<li>摘要：2）场景领域微调，金融任务LLM、法律LLM、医学LLM、电商LLM Llama系列\Gemma系列中文增量预训练：先做Llama3.1, 等待Llama4，期望Llama5 RAG落地：搭建领域问答机器人、知识 …</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">首先，学习大语言模型时应关注开源模型的更新，特别是Llama系列和Gemma系列，它们适合进行场景领域的微调。对于金融任务、法律、医学、电商等特定领域的LLM，需要关注特定模型，如Llama3.1，它是开始学习的良好选择。此外，等待Llama4的发布，并期望Llama5的推出，以获得更强大的模型性能。其次，RAG（检索增强生成）技术可用于搭建领域问答机器人，提高模型在特定场景下的应用效果。最后，中文增量预训练可以进一步提升模型在特定场景中的表现，因此在学习过程中应考虑这一点。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>学习大语言模型应关注开源模型的更新。</li>
<li>Llama系列和Gemma系列适合场景领域微调。</li>
<li>金融任务、法律、医学、电商等领域的LLM需关注特定模型。</li>
<li>Llama3.1是开始学习的较好选择。</li>
<li>等待Llama4的发布。</li>
<li>期望Llama5的推出。</li>
<li>RAG技术可用于搭建领域问答机器人。</li>
<li>中文增量预训练有助于特定场景应用。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-31">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-32">17. 如何从零开始学习LLM大模型？ - 知乎</h2>
<ul>
<li>链接：https://www.zhihu.com/question/627723839</li>
<li>来源：bing</li>
<li>摘要：2012年2月1日 · LLM作答：AI 把你的问题和搜到的纪要内容，一起在脑子里过一遍，然后生成一段条理清晰的总结，甚至还能告诉你“该结论出自第3页第5段”。 看到没？ RAG解决了LLM两大 …</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">RAG（ Retrieval-Augmented Generation）通过解决LLM（Large Language Model）在处理问题和纪要内容时的两大关键问题，实现了更为精准和全面的总结生成。首先，RAG能够将问题和纪要内容一起处理，使得生成的总结更加贴合实际需求。其次，RAG通过检索相关的信息，补充了LLM可能遗漏的重要细节，从而提高了总结的质量和准确性。因此，RAG不仅提升了总结的全面性，还确保了信息的准确性和完整性。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>LLM作答：AI将问题和纪要内容一起处理生成总结。</li>
<li>RAG解决了LLM两大问题。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-33">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-34">18. 2025年大模型LLM还有哪些可研究的方向？ - 知乎</h2>
<ul>
<li>链接：https://www.zhihu.com/question/11285951981</li>
<li>来源：bing</li>
<li>摘要：2025年2月7日 · 写在前面：三个让我夜不能寐的矛盾 作为一个在大模型领域摸爬滚打了几年的老韭菜，我觉得当前LLM技术发展正面临着三个核心矛盾，搞清楚这些，后面的技术方向才好理 …</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">当前LLM技术的发展正面临三个核心矛盾，这些矛盾如同老韭菜身份一般，深深植根于技术进步的土壤中。首先，技术发展带来的高效与便捷，使得人们的生活变得更加便捷，但同时也引发了对隐私和安全的担忧，这成为了夜不能寐的矛盾之一。其次，技术的快速发展使得信息更新迅速，但同时也加剧了信息过载的问题，使得人们在海量信息中难以找到真正有价值的内容，这进一步加剧了信息筛选的难度。因此，这些矛盾不仅反映了技术发展带来的挑战，也揭示了技术进步与社会需求之间的张力，使得人们在享受技术带来的便利的同时，也面临着如何更好地管理信息、保护隐私的挑战。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>当前LLM技术发展正面临的三个核心矛盾</li>
<li>老韭菜身份</li>
<li>夜不能寐的矛盾</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-35">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-36">19. 实现 LLM 复杂推理（Reasoning）目前有哪些主要方法？</h2>
<ul>
<li>链接：https://www.zhihu.com/question/12292274612</li>
<li>来源：bing</li>
<li>摘要：自问自答，安放学习笔记。主要参考了综述文章： Advancing Reasoning in Large Language Models: Promising Methods and Approaches 一、什么是 LLM 推理（Reasoning）？ 1.1 避免 …</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">易漏但重要的概念或细节在于LLM（大型语言模型）的推理能力，这涉及理解、分析和生成文本的能力。为了促进这种推理能力，一种有效的方法是采用自问自答的机制，通过这种方式，模型可以在内部进行对话，从而更好地理解和生成文本。此外，综述文章提供了关于LLM推理方法的最新进展，这些进展有助于我们更深入地理解LLM如何进行推理，并为未来的研究提供了宝贵的参考。因此，这些易漏但重要的概念和细节对于提升LLM的性能至关重要。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>易漏但重要的概念或细节：</li>
<li>LLM 推理涉及理解、分析和生成文本的能力。</li>
<li>自问自答机制有助于促进 LLM 的推理能力。</li>
<li>综述文章提供了 LLM 推理方法的最新进展。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-37">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-38">20. LLM （大型语言模型）都有哪些潜在应用场景？ - 知乎</h2>
<ul>
<li>链接：https://www.zhihu.com/question/579605614</li>
<li>来源：bing</li>
<li>摘要：大型语言模型（LLM），如GPT-4，通过在海量文本数据上进行训练，已经在自然语言处理（NLP）领域取得了巨大的进展。 LLM的核心原理是通过深度学习技术，尤其是基 …</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">海量文本数据训练是深度学习技术在自然语言处理（NLP）领域应用的基础，通过大量文本数据的训练，深度学习模型能够学习到语言的复杂模式和结构。这种训练过程使得大型语言模型（LLM）具备了强大的语言理解和生成能力，从而在各种NLP任务中展现出卓越的表现。因此，海量文本数据训练是构建高效能大型语言模型的关键步骤，也是深度学习技术在NLP领域取得突破的重要支撑。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>海量文本数据训练</li>
<li>深度学习技术</li>
<li>自然语言处理（NLP）领域</li>
<li>大型语言模型（LLM）</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-39">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-40">21. 985硕只为了就业，纯语言大模型LLM、多模态大模型、生成 ...</h2>
<ul>
<li>链接：https://www.zhihu.com/question/15608452195</li>
<li>来源：bing</li>
<li>摘要：2025年3月23日 · 985硕只为了就业，纯语言大模型LLM、多模态大模型、生成式模型AIGC选那条路好？</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">首先，纯语言大模型LLM和多模态大模型是当前人工智能领域的两种重要模型。纯语言大模型主要处理文本信息，而多模态大模型则能够处理多种类型的数据，如文本、图像和视频等。其次，生成式模型AIGC（AI Generated Content）作为大模型的一种应用，能够自动生成文本、图像等内容，极大地提高了内容生成的效率和质量。此外，这些模型的应用导向往往与就业密切相关，尤其是对于硕士毕业生而言，选择适合就业需求的模型可以提高就业竞争力。因此，硕士毕业生在选择学习或研究方向时，应当考虑这些模型的实际应用价值和就业导向，以便更好地适应市场需求。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>纯语言大模型LLM</li>
<li>多模态大模型</li>
<li>生成式模型AIGC</li>
<li>就业导向选择</li>
<li>硕士就业</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-41">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-42">22. 如何评价Rich Sutton关于「LLM是死路」的观点？ - 知乎</h2>
<ul>
<li>链接：https://www.zhihu.com/question/1959222181658628381</li>
<li>来源：bing</li>
<li>摘要：一、LLM 是「系统1」，不是「系统2」 LLM 的强大在于：它几乎整合了人类全部显性知识，是一个巨大的「经验压缩体」。 但它的缺陷也同样明显： 推理不可靠、无法形成稳定的世界模型 …</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">首先，LLM作为大型语言模型（LLM）被视作「系统1」，而非「系统2」，这意味着它在处理信息时更依赖于快速、自动化的处理方式，而非深思熟虑的分析。其次，LLM几乎整合了人类全部显性知识，这使得它在回答问题和生成文本时显得非常丰富和全面。此外，LLM可以被看作是一个巨大的「经验压缩体」，其内部存储了大量的语言和知识信息，能够迅速提供相关信息。然而，由于其「系统1」的特性，LLM的推理过程往往不够可靠，难以进行复杂的逻辑推理。因此，尽管LLM拥有丰富的知识库，但它无法形成稳定的世界模型，即无法构建一个全面且连贯的认知框架来解释和预测现实世界的现象。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>LLM是「系统1」，不是「系统2」</li>
<li>LLM几乎整合了人类全部显性知识</li>
<li>LLM是一个巨大的「经验压缩体」</li>
<li>LLM的推理不可靠</li>
<li>LLM无法形成稳定的世界模型</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-43">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-44">23. LaTeX 排版中，如何输入长的等价符号（\sim），且上下可 ...</h2>
<ul>
<li>链接：https://www.zhihu.com/question/352249034</li>
<li>来源：bing</li>
<li>摘要：2019年10月24日 · LaTeX 排版中，如何输入长的等价符号（\sim），且上下可添加文字或公式？ 就是想要类似于\xlongequal [] {} 命令一样，只不过把长等号（=）变成长等价号（\sim）.谢谢 …</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">在数学表达中，某些公式或概念虽然容易被忽略，但它们的重要性不容忽视。例如，在讨论极限或连续性时，我们常常会遇到一些看似不起眼但至关重要的细节，比如导数定义中的自变量变化量趋于零时的极限计算。此外，上下文中的公式位置调整，以及长等价符号的自定义命令，虽然可能不会直接出现在最终的数学表达式中，但它们对于确保数学论证的严谨性和清晰性至关重要。因此，无论是进行严格的数学证明还是撰写学术论文，都应给予这些易漏但重要的细节足够的重视，以保证论证的准确性和逻辑的连贯性。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>\longsim</li>
<li>\Xlongequal</li>
<li>上下文中的公式位置调整</li>
<li>长等价符号的自定义命令</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-45">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-46">24. 编程中，parameter、argument翻译成什么中文最好？</h2>
<ul>
<li>链接：https://www.zhihu.com/question/24155012</li>
<li>来源：bing</li>
<li>摘要：引用：The term parameter is used to describe the names for values that are expected to be supplied… Argument 和 Parameter 两个词在很多文献中均翻译为参数，这是一个历史遗留问题。 但实际上 …</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">参数（Argument）是程序设计中用于传递信息的重要手段，它在不同函数或方法之间传递数据，确保程序的正确执行。参数的翻译（Parameter）在中文中通常指的是同样的概念，即用于指定函数或方法所需信息的变量。保持参数的一致性对于程序的稳定性和可维护性至关重要，这不仅有助于避免因参数不一致导致的错误，还能提高代码的可读性和可维护性。历史遗留问题往往会导致参数使用上的不一致，因此在进行代码审查或重构时，需要特别注意参数的一致性，以确保程序的稳定运行。因此，保持参数的一致性不仅是一个良好的编程习惯，也是解决历史遗留问题的关键步骤。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>参数</li>
<li>Argument</li>
<li>翻译</li>
<li>参数</li>
<li>Parameter</li>
<li>Argument</li>
<li>中文</li>
<li>最好</li>
<li>保持一致</li>
<li>历史遗留问题</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-47">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-48">25. 最近比较火的parameter server是什么？ - 知乎</h2>
<ul>
<li>链接：https://www.zhihu.com/question/26998075/answers/updated</li>
<li>来源：bing</li>
<li>摘要：李沐的PS（Parameter Server，参数服务器）架构确实使用了参数共享的概念。参数服务器是一种分布式计算框架，主要用于机器学习和深度学习模型的训练，特别是在大规模数据和模型时。 参数共享的 …</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">参数服务器在机器学习和深度学习模型的训练中扮演着关键角色，它不仅支持大规模数据和模型的处理，还采用了参数共享的概念，从而提高了训练效率和模型性能。首先，参数服务器能够处理大规模数据和模型，确保在分布式环境中高效地进行训练。其次，它通过参数共享，使得多个模型可以共享相同的参数，减少了存储需求和计算资源的消耗，进一步提升了训练的效率。因此，参数服务器不仅提高了训练的规模性，还优化了资源利用，是现代机器学习和深度学习不可或缺的重要工具。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>参数服务器用于机器学习和深度学习模型的训练</li>
<li>参数服务器支持大规模数据和模型</li>
<li>参数服务器采用了参数共享的概念</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-49">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-50">26. sarscape做sbas 第一步反演出现parameter read error问题?</h2>
<ul>
<li>链接：https://www.zhihu.com/question/656236730</li>
<li>来源：bing</li>
<li>摘要：在使用SARscape进行SBAS（Small Baseline Subset）处理时，如果遇到"Parameter read error"错误，这通常意味着软件在读取或解析参数时遇到了问题。这个问题可能由多种原因引起，包括文件损坏 …</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">文件损坏、参数文件格式错误、参数文件路径错误、权限问题导致文件无法读取、配置文件中存在非法字符以及参数文件编码不匹配，这些都可能导致程序在运行过程中出现读取文件失败的问题。首先，文件损坏或格式错误会使得程序无法正确解析文件内容，进而引发错误。其次，如果参数文件的路径设置不正确，程序将无法找到对应的文件。此外，权限问题也是一个常见原因，如果程序没有足够的权限访问文件，同样会导致读取失败。更进一步，配置文件中存在非法字符或参数文件编码不匹配也会引起读取问题，因为这些情况会导致文件内容无法被正确解析。因此，确保文件的完整性和正确性、设置正确的路径、赋予足够的访问权限以及检查文件编码是否匹配，是避免这些问题的关键步骤。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>文件损坏</li>
<li>参数文件格式错误</li>
<li>参数文件路径错误</li>
<li>权限问题导致文件无法读取</li>
<li>配置文件中存在非法字符</li>
<li>参数文件编码不匹配</li>
<li>文件被其他程序占用导致无法读取</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-51">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-52">27. 能不能用简明的语言解释什么是非参数（nonparametric）模型？</h2>
<ul>
<li>链接：https://www.zhihu.com/question/22855599</li>
<li>来源：bing</li>
<li>摘要：non-pararmetric则不然，不对总体分布做假设，自然也就不必estimate相应的参数。 一个比较简单的例子是 ，可以用来检验两个分布X和Y的中位数是否相等。在这里不必介意X和Y分别是什么分布，只在意 …</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">非参数模型在统计分析中具有重要地位，因为它不需要假设总体分布的形式，也不需要估计参数，从而使得分析更加灵活。这种模型特别适用于检验两个分布的中位数是否相等，无需介意具体分布类型，也不需要对总体分布的形式做出假设。因此，非参数模型在实际应用中显得尤为重要，特别是在数据分布不明确或不满足参数模型假设的情况下，它能提供可靠且有效的分析工具。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>不需要假设总体分布的形式</li>
<li>不需要估计参数</li>
<li>可以用于检验两个分布的中位数是否相等</li>
<li>不需要介意具体分布类型</li>
<li>非参数模型不对数据分布做假设</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-53">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-54">28. 什么是非独立同分布（Non-IID）数据，有没有很简单的解释 ...</h2>
<ul>
<li>链接：https://www.zhihu.com/question/395555567</li>
<li>来源：bing</li>
<li>摘要：2021年11月17日 · 什么是non-iid Non-IID，即非独立同分布， 非独立，即两个或多个随机变量之间存在一定程度的相互关联或相关性，一个随机变量的值可能受到另一个或多个随机变量的值的影响。 非同 …</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">两个或多个随机变量之间存在相互关联或相关性，这意味着一个随机变量的值可能影响另一个随机变量的值。这种关联性使得我们不能简单地假设这些随机变量是独立的。独立意味着一个随机变量的值不会影响另一个随机变量的值，而非独立则表明存在这种影响。此外，非同分布意味着随机变量的分布不同，这进一步增加了它们之间的复杂关系。值得注意的是，随机变量的分布可能随样本变化，这使得非独立同分布假设不再成立。因此，理解这些概念之间的关系对于分析和处理涉及多个随机变量的复杂系统至关重要。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>两个或多个随机变量之间存在相互关联或相关性</li>
<li>一个随机变量的值可能影响另一个随机变量的值</li>
<li>非独立与独立相反</li>
<li>非同分布意味着随机变量的分布不同</li>
<li>随机变量的分布可能随样本变化</li>
<li>非独立同分布假设不再成立</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-55">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-56">29. 请问多智能体（multi-agent system）有什么资料入门吗？ - 知乎</h2>
<ul>
<li>链接：https://www.zhihu.com/question/286542105</li>
<li>来源：bing</li>
<li>摘要：多智能体系统（Multi-Agent System，简称MAS）是一个很新的研究领域，目前学界和产业界几乎是在同步研究，相关论文大概也有100多篇了。 咱们找资料之前可以先简单了解一下，这样后面就能有的放 …</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">多智能体系统作为研究领域，近年来逐渐成为学术界的热点，吸引了大量学者的关注，产生了大量的相关论文。这一领域不仅在学术界备受瞩目，还逐渐渗透到产业界，成为连接理论与实践的重要桥梁。因此，多智能体系统不仅是一个充满活力的研究领域，也是一个具有广泛应用前景的前沿技术。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>多智能体系统是研究领域</li>
<li>多智能体系统是近期热门</li>
<li>多智能体系统有大量相关论文</li>
<li>多智能体系统涉及学界和产业界</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-57">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-58">30. 请问多智能体（multi-agent system）有什么资料入门吗？ - 知乎</h2>
<ul>
<li>链接：https://www.zhihu.com/question/286542105/answers/updated</li>
<li>来源：bing</li>
<li>摘要：请问多智能体（multi-agent system）有什么资料入门吗？ 本人渣硕，目前研究方向是深度学习，教研室学习了分布式系统。 目前觉得多智能体应该是以后 AI 的实现方式，多个智能体 model 协作，共同解 …</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-59">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-60">31. 如何评价AAMAS 2024的审稿结果？ - 知乎</h2>
<ul>
<li>链接：https://www.zhihu.com/question/632900710</li>
<li>来源：bing</li>
<li>摘要：2023年12月21日 · 会议全称：The 23rd International Conference on Autonomous Agents and Multi-Agent Systems录用率：20…</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">录用率较低是企业招聘过程中常见的现象，这往往反映出求职者与岗位要求之间的匹配度不高。首先，这可能是因为求职者普遍缺乏对应职位所需的专业技能或工作经验，导致即使提供了多份简历，也难以满足企业的具体要求。其次，企业可能在招聘过程中设置了较高的门槛，例如要求较高的学历背景或特定的工作经验，这进一步限制了符合条件的求职者数量。因此，录用率较低不仅反映了求职市场的竞争激烈，也揭示了求职者需要提升自身能力以适应市场需求，同时企业也需要调整招聘标准，以吸引更多合适的候选人。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>录用率较低</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-61">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-62">32. Moltbook：Agent 规模提升涌现出了更高的智能？是社交的 ...</h2>
<ul>
<li>链接：https://www.zhihu.com/question/2001073225891868934</li>
<li>来源：bing</li>
<li>摘要：Conformity Dynamics in LLM Multi-Agent Systems: The Roles of Topology and Self-Social Weighting (2026) 这篇最新论文研究了 大模型在多智能体系统中的集体决策与一致性动态，关注网络结构如何影 …</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">网络结构对集体决策有着深远的影响，它不仅塑造了大模型多智能体系统中的社交权重，还决定了社交权重如何对一致性动态产生作用。社交权重不仅影响个体的行为，还通过自我调节机制在不同规模下发生变化，从而促进一致性。网络拓扑作为关键因素，不仅影响智能体的行为，还通过其独特的结构促进智能涌现。因此，网络结构不仅通过社交权重影响集体决策，还通过其拓扑特性促进智能体间的协作与一致，最终推动智能体系统的整体性能提升。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>网络结构影响集体决策</li>
<li>大模型多智能体系统中的社交权重</li>
<li>社交权重对一致性动态的作用</li>
<li>网络拓扑如何塑造行为</li>
<li>集体决策中的社会影响</li>
<li>社交权重的自我调节机制</li>
<li>网络结构与智能涌现的关系</li>
<li>社交权重在不同规模下的变化</li>
<li>网络拓扑对智能体行为的影响</li>
<li>社交权重如何促进一致性</li>
<li>网络结构如何促进智能涌现</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-63">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-64">33. multiagent path finding的顶刊有哪些? - 知乎</h2>
<ul>
<li>链接：https://www.zhihu.com/question/629424617</li>
<li>来源：bing</li>
<li>摘要：Autonomous Robots IEEE Transactions on Robotics (T-RO) IEEE Transactions on Automation Science and Engineering (T-ASE) ACM Transactions on Intelligent Systems and Technology (TIST) ACM …</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">自主机器人是现代科技中的重要组成部分，它们在各种环境中执行任务，展现出高度的自主性和智能化。为了推动这一领域的研究和发展，学术界和工业界共同支持了一系列重要的学术期刊，其中IEEE Transactions on Robotics (T-RO)专注于机器人技术的各个方面，包括设计、控制和应用；IEEE Transactions on Automation Science and Engineering (T-ASE)则更侧重于自动化科学与工程的交叉领域，探讨如何通过自动化技术提升工程效率和性能；而ACM Transactions on Intelligent Systems and Technology (TIST)则关注智能系统的理论与实践，特别是如何利用先进的计算技术来增强机器人的智能水平。这些期刊共同构成了一个完整的知识体系，为自主机器人技术的发展提供了坚实的基础。因此，这些期刊不仅是学术研究的重要平台，也是推动自主机器人技术进步的关键力量。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>Autonomous Robots</li>
<li>IEEE Transactions on Robotics (T-RO)</li>
<li>IEEE Transactions on Automation Science and Engineering (T-ASE)</li>
<li>ACM Transactions on Intelligent Systems and Technology (TIST)</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-65">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-66">34. 多智能体强化学习可以投的英文会议推荐？ - 知乎</h2>
<ul>
<li>链接：https://www.zhihu.com/question/4058180322</li>
<li>来源：bing</li>
<li>摘要：2024年11月13日 · 多智能体深度强化学习会议推荐及投稿指南 一、会议等级及推荐 1. B类会议 AAMAS (International Conference on Autonomous Agents and Multi-agent Systems) 这是最适合的选择之一 …</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">AAMAS（International Conference on Autonomous Agents and Multi-Agent Systems）作为国际多智能体系统会议，其重要性不容忽视。首先，AAMAS汇聚了全球范围内从事智能体系统研究的顶尖学者和实践者，为他们提供了一个交流和展示最新研究成果的平台。其次，该会议不仅涵盖了多智能体系统的基础理论，还涉及了智能体在实际应用中的最新进展，如机器人协作、智能交通系统等。因此，无论是对于学术研究还是实际应用，AAMAS都是一个不可或缺的重要会议。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>AAMAS 是国际多智能体系统会议</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-67">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-68">35. 如何评价最近大火的LatentMAS？ - 知乎</h2>
<ul>
<li>链接：https://www.zhihu.com/question/1981362028082926001</li>
<li>来源：bing</li>
<li>摘要：2025年12月8日 · 如何评价最近大火的LatentMAS？ 如题，题主最近在看一些关于多智能体相关的文章，在看了Latent Collaboration in Multi-Agent Systems之后感觉通过… 显示全部 关注者 1 被浏览</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-69">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-70">36. WWW 2026 投稿 Agent 安全 - 知乎</h2>
<ul>
<li>链接：https://www.zhihu.com/column/c_1970627812474982640</li>
<li>来源：bing</li>
<li>摘要：1. A Multi-Agent Framework for Matrix-Poisoning Attacks on Recommender Systems 推荐指数：★ 我的评价：论文存在一定的缺陷，且在现实中不太可能实现 研究方向：推荐系统的数据投毒攻击 针对问 …</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">首先，推荐系统可能成为攻击目标，因为它们基于用户行为数据进行个性化推荐，而这些数据可能被恶意篡改。其次，数据投毒攻击可能利用多Agent框架实现，即通过控制多个代理节点来逐步污染推荐系统的训练数据，从而影响其推荐结果。然而，现实环境中实现此类攻击存在难度，因为需要精确控制多个代理节点，并且需要避开系统的检测机制。因此，确保推荐系统的数据安全性和完整性是至关重要的。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>数据投毒攻击可能利用多Agent框架实现</li>
<li>推荐系统可能成为攻击目标</li>
<li>现实环境中实现此类攻击存在难度</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-71">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-72">37. npj系列算不算nature子刊? - 知乎</h2>
<ul>
<li>链接：https://www.zhihu.com/question/15055775489</li>
<li>来源：bing</li>
<li>摘要：2025年4月23日 · npj系列影响因子最高的是我所在领域（medical informatics）的 npj digital medicine，常年稳定在15分左右，在该分类里排第二，仅次于lancet digital health。 因为medical …</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">npj系列期刊不算作Nature子刊，其中影响因子最高的期刊是npj digital medicine，该期刊常年稳定在15分左右，位居该领域内的第二位，仅次于Lancet Digital Health。因此，尽管npj系列期刊在学术界有一定的影响力，但其整体水平和声誉相较于Nature子刊仍有差距，特别是其顶级期刊npj digital medicine在该领域的地位也凸显了其在学术界的认可度。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>npj系列不算nature子刊</li>
<li>影响因子最高的是npj digital medicine</li>
<li>npj digital medicine常年稳定在15分左右</li>
<li>该领域里仅次于lancet digital health</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-73">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-74">38. 稀疏（sparse）在机器学习中很重要吗？为什么？ - 知乎</h2>
<ul>
<li>链接：https://www.zhihu.com/question/38286385</li>
<li>来源：bing</li>
<li>摘要：2015年12月7日 · 深度学习论文中经常看到"sparse"，所以应该是重要的概念，但一直不理解很困惑； 刚才在quora上的回答感觉很有帮助，尝试总结以下： sparse 代表数据为0，sparse数据的存在让不为0 …</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-75">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-76">39. 什么是稀疏特征 (Sparse Features)? - 知乎</h2>
<ul>
<li>链接：https://www.zhihu.com/question/31951092?sort=created</li>
<li>来源：bing</li>
<li>摘要：要回答什么是sparse feature，可能首先要理解什么是feature。 一般在machine learning意义上，我们常说的feature，是一种对数据的表达。当然，要衡量一种feature是否是合适的表达，要根据数据，应 …</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">数据的表达方式对于模型的性能至关重要，因此选择合适的表达方式是必不可少的。首先，合适的表达方式取决于具体的数据特性，不同的数据需要采用不同的表达方式来确保信息的有效传递。其次，sparse feature指的是那些非零值稀疏分布的特征，与之相对的是dense feature，后者指的是所有特征值都较为密集地分布。因此，根据数据的具体情况选择合适的表达方式，可以更有效地利用数据信息，提高模型的性能。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>数据的表达方式</li>
<li>合适的表达取决于数据</li>
<li>sparse feature的对立面是dense feature</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-77">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-78">40. 深度学习中的sparse和dense模型指的是什么？ - 知乎</h2>
<ul>
<li>链接：https://www.zhihu.com/question/66907303</li>
<li>来源：bing</li>
<li>摘要：2017年10月19日 · Sparse特征通常指的是那些具有大量可能值但实际使用值很少的特征，例如用户浏览过的商品ID。 这些特征在数据集中往往有很多零值，因此被称为稀疏。 在Sparse双塔模型中，稀疏 …</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">稀疏特征包含大量可能值但实际使用值很少，在数据集中往往表现为很多零值。这种特征与dense模型相对，后者指的是所有特征都有实际值。因此，稀疏特征在处理时需要特别注意，因为它们在数据集中占据大量空间但实际信息量较少，这可能导致模型训练效率降低。因此，在处理这类数据时，选择合适的模型和优化策略显得尤为重要。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>稀疏特征包含大量可能值但实际使用值很少</li>
<li>稀疏特征在数据集中往往有很多零值</li>
<li>dense模型与sparse特征相对</li>
<li>dense模型指的是所有特征都有实际值</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-79">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-80">41. 通俗理解，Sparse Attention是什么原理？ - 知乎</h2>
<ul>
<li>链接：https://www.zhihu.com/question/12682779853</li>
<li>来源：bing</li>
<li>摘要：通俗解释 Sparse Attention 的原理： 想象你在读一本长篇小说，如果每一页都要仔细读完全文才能理解剧情，效率会非常低。实际上，你会 快速跳过无关段落，只聚焦关键章节和人物对话，这就是 Sparse …</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">首先，为了提高阅读效率，每页都要仔细读完全文，尽管这可能导致效率降低。其次，可以通过快速跳过无关段落来节省时间，但这种方法可能会遗漏重要信息。此外，建议只聚焦于关键章节和人物对话，这样可以更有效地抓住核心内容。因此，使用 Sparse Attention（稀疏注意）技术可以帮助读者更专注于重要部分而非全部内容，从而提高整体阅读效率。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>每页都要仔细读完全文效率低</li>
<li>快速跳过无关段落</li>
<li>只聚焦关键章节和人物对话</li>
<li>Sparse Attention 提高效率</li>
<li>关注重要部分而非全部</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-81">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-82">42. 如何看待Native Sparse Attention？ - 知乎</h2>
<ul>
<li>链接：https://www.zhihu.com/question/12608484026</li>
<li>来源：bing</li>
<li>摘要：2025年2月18日 · 准确率上，sparse 的模型比 full 还要搞一些。 这个我猜一点原因：（1）模型还不算大，NSA 和 full 还没遇到“瓶颈”，所以 full attention 还没摸到其上限，不能和 NSA 拉开差 …</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">模型还不算大，这意味着在当前阶段，其性能和能力还有很大的提升空间。NSA（部分注意机制）和 full attention（全注意机制）都还未遇到所谓的“瓶颈”，即在资源和计算能力上还有进一步优化的空间。此外，full attention机制的潜力尚未完全挖掘，其上限还未被触及，这表明在未来的研究和实践中，还有巨大的改进和创新空间。因此，随着模型规模的扩大和对full attention机制的深入探索，我们有望看到性能的显著提升和更广泛的应用。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>模型还不算大</li>
<li>NSA 和 full 还没遇到“瓶颈”</li>
<li>full attention 还没摸到其上限</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-83">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-84">43. Sparse matrix 作为深度学习输入 在模型表现上会有影响吗 ...</h2>
<ul>
<li>链接：https://www.zhihu.com/question/634903451</li>
<li>来源：bing</li>
<li>摘要：sparse matrix (稀疏矩阵)：其实这个问题你可以关注下推荐系统的相关研究。我没记错的话，推荐系统里面最常见的问题就是输入特征的稀疏性。 对于这个问题呢，我们首先看下稀疏矩阵的定义：矩阵中 …</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">稀疏矩阵是指在矩阵中非零元素远少于总元素个数的一种情况。这种稀疏性在推荐系统中表现得尤为明显，因为推荐系统通常需要处理大量的用户-物品交互数据，而这些数据往往存在大量的未交互记录，导致矩阵变得非常稀疏。稀疏性问题还常见于特征输入阶段，即在进行特征选择或特征工程时，许多特征可能对某些样本没有贡献，从而导致特征矩阵的稀疏性。这种稀疏性不仅增加了存储和计算的负担，还可能影响模型的性能，因此在处理推荐系统和特征输入时，需要特别关注稀疏性带来的挑战。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>稀疏矩阵定义：矩阵中非零元素远少于总元素个数。</li>
<li>稀疏矩阵影响：在推荐系统中表现明显。</li>
<li>稀疏性问题：常见于特征输入。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-85">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-86">44. 请问机器学习中的稀疏先验（sparse prior）是什么？</h2>
<ul>
<li>链接：https://www.zhihu.com/question/509508528?write</li>
<li>来源：bing</li>
<li>摘要：2022年5月12日 · 还是离不开概率啊。 sparse prior是指在机器学习中，使用一个 概率 分布来描述参数的分布，其中大多数参数的值都是零或接近零。这个概念出自于2006年的论文《Sparse Bayesian …</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">概率分布描述了随机变量取值的概率情况，而参数的分布则进一步指明了这些参数本身的概率特征。在许多实际应用场景中，多数参数的值实际上是零或非常接近零，这表明在概率分布中，参数值为零或接近零的情况占据了主导地位。这种现象反映了参数在特定条件下的稀疏性，即大多数参数的取值集中在零附近，而远离零的参数值出现的概率相对较小。因此，这种分布特性对于理解和优化模型具有重要意义，尤其是在处理稀疏数据或进行参数估计时，能够帮助我们更准确地识别和利用关键参数。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>概率分布</li>
<li>参数的分布</li>
<li>大多数参数的值都是零或接近零</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-87">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-88">45. 如何理解稀疏主成分分析 (Sparse Principal Component ...</h2>
<ul>
<li>链接：https://www.zhihu.com/question/55606589</li>
<li>来源：bing</li>
<li>摘要：稀疏主成分分析简介 变量经过PCA后得到的主成分通常在多个原始变量上都不为 0 ，这导致进行PCA后的主成分的含义较为模糊，给数据分析带来了一定难度，因此Zou等（2006）提出的一种新的主成分 …</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">主成分分析（PCA）是一种常用的数据降维技术，其目的是通过线性变换将原始变量转换为新的变量，即主成分。然而，PCA后的主成分往往在多个原始变量上都不为0，这导致主成分的含义变得模糊，难以直接解释。为了解决这一问题，稀疏主成分分析应运而生。稀疏主成分分析通过引入稀疏性约束，使得主成分在原始变量上的系数尽可能稀疏，即每个主成分主要由少数几个原始变量构成，从而使得主成分的含义更加明确和易于解释。因此，稀疏主成分分析不仅解决了主成分含义模糊的问题，还提高了模型的可解释性。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>主成分的含义模糊</li>
<li>稀疏主成分分析旨在解决主成分含义模糊的问题</li>
<li>PCA后的主成分通常在多个原始变量上都不为0</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-89">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-90">46. 为什么sparse representation比起其它成分分析方法（DFT ...</h2>
<ul>
<li>链接：https://www.zhihu.com/question/24124122</li>
<li>来源：bing</li>
<li>摘要：2018年4月24日 · 问：Sparse真正胜过其它成分分析方法的地方在哪里？ 答：Sparse Model相对于Subspace Model（比如降维）而言，更加relaxed，因而具有更强的表达能力。 再加上自然信号，天 …</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">更强的表达能力是提升沟通效果的关键因素，它不仅能够帮助个体清晰准确地传达信息，还能增强说服力和感染力。其次，relaxed条件在表达过程中起到至关重要的作用，它能够使表达更加流畅自然，减少紧张感，从而提高表达的效率和质量。此外，自然信号的表示也是增强表达能力的重要方面，它包括肢体语言、面部表情等非言语信号，这些信号能够补充和强化言语信息，使沟通更加生动和有效。因此，通过提升表达能力、创造relaxed条件以及运用自然信号，可以显著提高沟通的效果和质量。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>更强的表达能力</li>
<li>relaxed条件</li>
<li>自然信号的表示</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-91">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-92">47. Evaluate,Compute,和Calculate三者有什么区别? - 知乎</h2>
<ul>
<li>链接：https://www.zhihu.com/question/47790953</li>
<li>来源：bing</li>
<li>摘要：2016年6月24日 · 这是搬运扣肉的回答：What is the difference between "calculate" and "compute"? You can compute the shortest path between two points. However, you can calculate only the distance …</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">计算只能得出具体数值，而不仅仅是找到最短路径。计算是一个广泛的概念，它涵盖了多种数学和逻辑操作，这些操作可以产生具体的数值结果。然而，计算与寻找最短路径是两个不同的概念。计算并不总是意味着要找到最优解或最短路径，它可能涉及更广泛的数学问题和解决方案。因此，理解计算的范围和目的对于正确应用计算方法至关重要。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>计算只能得出具体数值。</li>
<li>计算不总是意味着找到最短路径。</li>
<li>计算与计算最短路径是不同概念。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-93">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-94">48. 什么是算力网络（Compute First Networking）？ - 知乎</h2>
<ul>
<li>链接：https://www.zhihu.com/question/352119345</li>
<li>来源：bing</li>
<li>摘要：2019年12月15日 · 什么是算力网络（Compute First Networking）？ 算力网络应该是网络前沿的一个新方向，强调算网结合的，目前算力网络的技术发展到什么程度了呢？ 可以从学术和工程的角度分别谈 …</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">算力网络强调算网结合，是网络前沿的新方向，旨在通过优化计算资源与网络资源的协同，提升整体服务质量和效率。尽管其技术发展程度尚未详细说明，但可以从学术角度和技术工程角度进行探讨。学术研究可能侧重于理论模型和算法设计，探讨如何更有效地利用计算资源；而技术工程则更关注实际部署和应用，确保计算与网络资源的无缝集成。因此，算力网络不仅是一个重要的技术趋势，也是未来网络发展的关键方向。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>算力网络强调算网结合</li>
<li>算力网络是网络前沿的新方向</li>
<li>算力网络的技术发展程度未详细说明</li>
<li>学术角度和技术工程角度可分别探讨</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-95">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-96">49. 备受互联网巨头推崇的『开放计算』到底是什么？可以应用到 ...</h2>
<ul>
<li>链接：https://www.zhihu.com/question/430040323</li>
<li>来源：bing</li>
<li>摘要：OCP（Open Compute Project，开放计算项目）由Facebook联合英特尔、Rackspace、高盛和Arista Networks在2011年联合发起的开放硬件组织，其宗旨是以开源开放的方式，重构当前的数据中心硬 …</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">开放计算项目（OCP）由Facebook发起，是一个致力于开放硬件的组织。其主要目标是重构当前的数据中心硬件，以提高效率和降低成本。OCP以开源开放的方式运作，这意味着它鼓励社区参与和贡献，从而推动技术创新和硬件优化。因此，通过这种方式，OCP不仅能够满足不断变化的计算需求，还能够促进整个行业的进步。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>开放计算项目由Facebook发起</li>
<li>OCP是一个开放硬件组织</li>
<li>OCP旨在重构当前的数据中心硬件</li>
<li>OCP以开源开放的方式运作</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-97">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-98">50. Compute shader 和 CUDA 有哪些差异？ - 知乎</h2>
<ul>
<li>链接：https://www.zhihu.com/question/10888772800</li>
<li>来源：bing</li>
<li>摘要：2025年2月20日 · Compute Shader 和 CUDA 都是用于并行计算的技术，但它们服务于不同的使用场景。 场景差异 Compute Shader：适合需要集成到图形应用中的并行计算任务，例如图像处理、粒子系统 …</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">Compute Shader 适合集成到图形应用中的并行计算任务，这是因为其主要服务于图形应用场景，能够高效地处理大量数据并行计算的需求。Compute Shader 的设计初衷是为了在图形渲染过程中执行复杂的数学运算，例如物理模拟、光线追踪等，这些任务往往需要大量的并行处理能力。因此，Compute Shader 成为了图形应用中实现高性能并行计算的理想选择。通过将这些计算任务集成到图形应用中，可以充分利用图形处理单元（GPU）的并行处理能力，从而提升整体性能。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>Compute Shader 适合集成到图形应用中的并行计算任务</li>
<li>Compute Shader 服务于图形应用场景</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-99">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-100">51. 都快2025年了，国内手机对ComputeShader的兼容性如何 ...</h2>
<ul>
<li>链接：https://www.zhihu.com/question/4654475438</li>
<li>来源：bing</li>
<li>摘要：2024年12月7日 · 国内现在还不支持 CS 的手机基本也和游戏的目标用户不重合了，没有兼容的必要。 Mobile 上 CS 基本上不支持 RT 的 透明压缩，写带宽的压力会比 PS 大，需要仔细 Profile。最好只用 …</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-101">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-102">52. 如何学习与研究Reconfigurable Computing？ - 知乎</h2>
<ul>
<li>链接：https://www.zhihu.com/question/26604222</li>
<li>来源：bing</li>
<li>摘要：2014年12月6日 · 如何学习与研究Reconfigurable Computing？ 从本科毕业设计开始，研究生，工作，创业，都在用FPGA，可惜还真没有系统学习和研究过Reconfigurable Computing 关注者 37</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-103">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-104">53. CGRA和FPGA有什么不同？ - 知乎</h2>
<ul>
<li>链接：https://www.zhihu.com/question/407417134</li>
<li>来源：bing</li>
<li>摘要：2020年12月8日 · Coarse Grained Reconfigurable Array和Field Programmable Gate Array CGRA是可重构处理器，FPGA（Field－Programmable Gate Array），即现场可编程门阵列，它是在 PAL、GAL …</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-105">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-106">54. 【半导体所：可重构传感器实现高效三维成像 |《自然-电子学 ...</h2>
<ul>
<li>链接：https://www.zhihu.com/pin/1866059497748951040</li>
<li>来源：bing</li>
<li>摘要：2025年1月10日 · 【半导体所：可重构传感器实现高效三维成像 |《自然-电子学》论文】近期来自中国科学院半导体研究所的研究团队在《自然-电子学》发表了论文“可重构异质结晶体管阵列用于单目三维 …</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-107">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-108">55. 智能超表面和智能反射面的区别在哪里呢? - 知乎</h2>
<ul>
<li>链接：https://www.zhihu.com/question/410273762</li>
<li>来源：bing</li>
<li>摘要：这篇文章总结、翻译2020年发表在JSAC上的论文：“Reflecting Modulation”，截止到此文发表时间，论文已被引用25次 摘要 这篇论文提出了一种基于可控智能表面（Reconfigurable intelligent …</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-109">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-110">56. 可重构芯片的工作原理是什么？ - 知乎</h2>
<ul>
<li>链接：https://www.zhihu.com/question/576291694</li>
<li>来源：bing</li>
<li>摘要：FPGA 中分布着大量的可编程互联资源，通过编程就能够把各个不同的逻辑单元的输入输出连接起来。 可重构计算 ( Coarse-grained Reconfigurable Architecture，CGRA)是一种空域上的并行计算模式，以 …</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-111">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-112">57. 集成电路设计的学术会议含金量排名如何？ - 知乎</h2>
<ul>
<li>链接：https://www.zhihu.com/question/559252881</li>
<li>来源：bing</li>
<li>摘要：自己在学校留片过两次，也有师兄弟投稿的经验，基于此跟大家分享一下 isscc肯定是含金量最高的，一般是9月投稿，11月出结果，2月开会。这个会议发布了集成电路史上的许多重大芯片设计，历史氛围 …</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-113">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-114">58. 美国科学家研发出细如发丝的纤维可同时控制数千神经元，这 ...</h2>
<ul>
<li>链接：https://www.zhihu.com/question/1970442727876462041</li>
<li>来源：bing</li>
<li>摘要：相关论文 [1] 描述的 PRIME（全景可重构照明/panoramically reconfigurable illuminative）是单光纤探针，所用光纤的直径约 160 微米，上面用超快激光 3D 微加工技术刻蚀了超过一千个微小的光栅发光 …</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-115">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-116">59. 全球首款逻辑量子比特电路实现无差错计算，这是怎样的突破 ...</h2>
<ul>
<li>链接：https://www.zhihu.com/question/633808276</li>
<li>来源：bing</li>
<li>摘要：如果说做Shor算法去破解2048bit的RSA，那肯定是做不到的。 我们可以看一下论文的标题是什么：Logical quantum processor based on reconfigurable atom arrays。 这里有两个重点词。 第一个 …</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-117">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-118">60. MICRO25论文笔记 - 知乎</h2>
<ul>
<li>链接：https://www.zhihu.com/column/c_1964283140655788225</li>
<li>来源：bing</li>
<li>摘要：Quartz: A Reconfigurable, Distributed-Memory Accelerator for Sparse Applications MICRO'25 Session 5C: Sparsity - 2 原文链接： Quartz: A Reconfigurable, Distributed-Memory Accelerator for Sparse …</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-119">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-120">61. 如何看待 openai提出的 “零参数模型/”超稀疏小模型： Circuit ...</h2>
<ul>
<li>链接：https://www.zhihu.com/question/1983989002609648611</li>
<li>来源：bing</li>
<li>摘要：（一， 什么是“Circuit-Sparsity”， 以及初步数学形式化） OpenAI最近开源了一个名为“Circuit-Sparsity”的模型，其核心特点是： 参数量极小：仅0.4B（4亿）参数，且99.9%的权重为零，实际有效连接只有 …</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-121">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-122">62. 如何评价 Meta 提出的 STEM 稀疏性架构？</h2>
<ul>
<li>链接：https://www.zhihu.com/question/1996511469123180409</li>
<li>来源：bing</li>
<li>摘要：最后这俩研究都是基于“Lookup-based Sparsity”（基于查表的稀疏性） 的。 所以还是引用我对Engram的看法2026 年我们很可能会看到更通用的架构出现：未来的大模型可能是一个“小而精”的推理核心，外 …</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-123">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-124">63. 终稿改回初稿 的想法: 清华朱军团队提出「稀疏-线性注意力 ...</h2>
<ul>
<li>链接：https://www.zhihu.com/pin/1966079486824018320</li>
<li>来源：bing</li>
<li>摘要：2025年10月27日 · 清华朱军团队提出「稀疏-线性注意力」SLA | 论文标题：SLA: Beyond Sparsity in Diffusion Transformers via Fine-Tunable Sparse-Linear Attention 主要内容：在扩散 …</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-125">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-126">64. 什么是稀疏特征 (Sparse Features)? - 知乎</h2>
<ul>
<li>链接：https://www.zhihu.com/question/31951092</li>
<li>来源：bing</li>
<li>摘要：要回答什么是sparse feature，可能首先要理解什么是feature。 一般在machine learning意义上，我们常说的feature，是一种对数据的表达。当然，要衡量一种feature是否是合适的表达，要根据数据，应 …</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-127">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-128">65. 如何评价DeepSeek发布梁文锋署名论文，提出「条件记忆」 ...</h2>
<ul>
<li>链接：https://www.zhihu.com/question/1994233409871050526</li>
<li>来源：bing</li>
<li>摘要：所以论文里面说了一个Sparsity Allocation问题：给定固定的参数预算和计算预算,怎么在MoE专家和Engram记忆之间分配容量? 他们定义了allocation ratio ρ ∈ [0,1],表示把多少不活跃参数分给MoE，它 …</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-129">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-130">66. 菜鸟脱贫户 的想法: SparseInfer | 链接SparseInfer: Training ...</h2>
<ul>
<li>链接：https://www.zhihu.com/pin/1844477415704883200</li>
<li>来源：bing</li>
<li>摘要：2024年11月25日 · SparseInfer | 链接SparseInfer: Training-free Prediction of Activation Sparsity for Fast LLM Inference241119该论文提出了一种名为 SparseInfer 的方法，用于加速大语言模型（LLM）的推 …</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-131">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-132">67. XGBoost:Sparsity-aware Split Finding算法如何提升效率？</h2>
<ul>
<li>链接：https://www.zhihu.com/question/358230968</li>
<li>来源：bing</li>
<li>摘要：2019年11月28日 · 这跟数据格式有非常大的关系，我猜测xgboost所谓的稀疏指的就是样本中0的个数。 如果你看过像FM、FFM、DeepFM这些论文的话，你可以发现论文里面作者们衡量样本是否sparsity …</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-133">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-134">68. 如何评价 OpenAI 开源 0.4B 参数模型？将对 AI 领域带来 ...</h2>
<ul>
<li>链接：https://www.zhihu.com/question/1983893537264251815</li>
<li>来源：bing</li>
<li>摘要：先吐槽一波，OpenAI也真是可怜，5.2发布以后风评不好，新开源的东西也没人看了。 OpenAI 这个“砍断 99.9% 连接线”的技术（即 Circuit Sparsity/电路稀疏性）可以说是 AI 领域在“可解释性”方向上的一个重 …</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-135">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-136">69. Row Hammer漏洞会有多大实质性影响？ - 知乎</h2>
<ul>
<li>链接：https://www.zhihu.com/question/28666294</li>
<li>来源：bing</li>
<li>摘要：2016年12月27日 · Row Hammer漏洞会有多大实质性影响？ Solidot | rowhammer漏洞利用获得内核权限 今天的DRAM单元为了让容量更大，所以在物理密度上更紧凑，但这样很难阻止临近的 …</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-137">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-138">70. 研究显示 AMD 处理器也受 Rowhammer 内存攻击影响 ...</h2>
<ul>
<li>链接：https://www.zhihu.com/question/650506875</li>
<li>来源：bing</li>
<li>摘要：Rowhammer攻击，是一种针对DRAM内存的硬件漏洞利用方式，自2014年首次被报告以来，一直是内存安全领域的重要课题。它不仅仅影响Intel，还有题主说的AMD，还对几乎所有带 …</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-139">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-140">71. 如何看待研究员成功利用 Rowhammer 漏洞获取 Android root ...</h2>
<ul>
<li>链接：https://www.zhihu.com/question/51930184</li>
<li>来源：bing</li>
<li>摘要：2016年10月24日 · 安全: 利用Rowhammer漏洞翻转比特Root Android手机来自智能硬件研究人员成功演示了现实版的Rowhammer比特…</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-141">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-142">72. 在Cache设计中提高命中率能否对DRAM的Row Hammer攻 ...</h2>
<ul>
<li>链接：https://www.zhihu.com/question/663413458</li>
<li>来源：bing</li>
<li>摘要：row hammer是反复访问DRAM的某一个row上面的数据，访问过于频繁时会导致相邻row上面1T1C单元的电荷泄露。… 注意我说的是“利用Cache识别Row Hammer攻击”而不是“利 …</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-143">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-144">73. IEEE 在业界都怎么读？ - 知乎</h2>
<ul>
<li>链接：https://www.zhihu.com/question/39556019</li>
<li>来源：bing</li>
<li>摘要：2016年1月16日 · IEEE在业界的发音是"eye-triple-ee"或"I-3E"，分别对应其英文全称的缩写。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-145">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-146">74. floating和not connected有什么区别？ - 知乎</h2>
<ul>
<li>链接：https://www.zhihu.com/question/19974027</li>
<li>来源：bing</li>
<li>摘要：2021年11月12日 · floating和not connected有什么区别？ 很多Datasheet的pin Description里都会有某引脚可以left floating or not connected.</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-147">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-148">75. pfc显示错误illegal floating point value？ - 知乎</h2>
<ul>
<li>链接：https://www.zhihu.com/question/542568165</li>
<li>来源：bing</li>
<li>摘要：2022年7月10日 · pfc显示错误illegal floating point value？ 想每隔一段时间删除一个区域内的颗粒，但出现了错误，运行时提示porosity below 0.005.5 detected？ 这个是什么错误呢？ 没运行几… 显示全部 …</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-149">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-150">76. 数据手册中 leave floating 和 unconnected 有什么区别</h2>
<ul>
<li>链接：https://www.zhihu.com/question/264581700</li>
<li>来源：bing</li>
<li>摘要：2017年12月25日 · 如题 京ICP证110745号 · 京ICP备13052560号-1 · 京公网安备 11010802020088 号 · 互联网新闻信息服务许可证：11220250001 · 京网文 [2025]0422-132 号 · 药品医疗器械网络信息服务备 …</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-151">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-152">77. 请问floating absolute risk在R上怎么实现呢？</h2>
<ul>
<li>链接：https://www.zhihu.com/question/326007064</li>
<li>来源：bing</li>
<li>摘要：2019年5月29日 · 小白，想求R code 京ICP证110745号 · 京ICP备13052560号-1 · 京公网安备 11010802020088 号 · 互联网新闻信息服务许可证：11220250001 · 京网文 [2025]0422-132 号 · 药品医 …</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-153">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-154">78. Transformer模型详解（图解最完整版） - 知乎</h2>
<ul>
<li>链接：https://www.zhihu.com/column/p/338817680</li>
<li>来源：bing</li>
<li>摘要：2024年5月8日 · Transformer 的整体结构，左图Encoder和右图Decoder 可以看到 Transformer 由 Encoder 和 Decoder 两个部分组成，Encoder 和 Decoder 都包含 6 个 block。Transformer 的 …</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-155">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-156">79. 如何最简单、通俗地理解Transformer？ - 知乎</h2>
<ul>
<li>链接：https://www.zhihu.com/question/445556653</li>
<li>来源：bing</li>
<li>摘要：Transformer最开始应用于NLP领域的机器翻译任务，但是它的通用性很好，除了NLP领域的其他任务，经过变体，还可以用于视觉领域，如ViT（Vision Transformer）。 这些特点 …</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-157">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-158">80. 如何从浅入深理解 Transformer？ - 知乎</h2>
<ul>
<li>链接：https://www.zhihu.com/question/471328838</li>
<li>来源：bing</li>
<li>摘要：2024年7月7日 · Transformer升级之路：12、无限外推的ReRoPE？ Transformer升级之路：13、逆用Leaky ReRoPE Transformer升级之路：14、当HWFA遇见ReRoPE 预训练一 …</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-159">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-160">81. 如何从浅入深理解 Transformer？ - 知乎</h2>
<ul>
<li>链接：https://www.zhihu.com/question/471328838/answers/updated</li>
<li>来源：bing</li>
<li>摘要：Transformer 的整体结构，左图Encoder和右图Decoder 可以看到 Transformer 由 Encoder 和 Decoder 两个部分组成，Encoder 和 Decoder 都包含 6 个 block。 之前自然语言处理主要的算 …</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-161">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-162">82. Transformer是什么？ - 知乎</h2>
<ul>
<li>链接：https://www.zhihu.com/question/5473271833</li>
<li>来源：bing</li>
<li>摘要：Transformer模型的最终输出为每个时间步上各词元的概率分布，主要包括以下步骤： 线性层（Linear Layer）：将解码器堆栈输出的每个d_model维向量投影到词汇表大小（vocab_size） …</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-163">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-164">83. 有没有比较详细通俗易懂的 Transformer 教程？ - 知乎</h2>
<ul>
<li>链接：https://www.zhihu.com/question/485876732</li>
<li>来源：bing</li>
<li>摘要：总结：文章总结 延伸阅读：介绍一些延伸内容，比如Transformer变种等 参考文献 Transformer简介 Transformer最早由Ashish Vaswani等人在论文&lt;<attention all="" is="" need="" you="">&gt; [1] 中提出，是 …</attention></li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-165">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-166">84. 如何最简单、通俗地理解Transformer？ - 知乎</h2>
<ul>
<li>链接：https://www.zhihu.com/question/445556653/answer/2804026992?uk_sc=WENDY&amp;utm_psn=1587138077150896128</li>
<li>来源：bing</li>
<li>摘要：考虑到Transformer直接建模全局信息的魅力，许多研究尝试对其进行改造，以适应视觉数据分析任务。 自2020年10月，谷歌提出ViT模型开始，Transformer在计算机视觉领域展现出了强大的 …</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-167">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-168">85. Transformer 和 cnn 是两条差异巨大的路径吗？ - 知乎</h2>
<ul>
<li>链接：https://www.zhihu.com/question/7385521828</li>
<li>来源：bing</li>
<li>摘要：2024年12月22日 · 卷积和注意力：AI 领域的“分手还是复合”剧本？ Transformer 和 CNN，真的是两条差异巨大的路径吗？ 两者设计逻辑不一样，但目标一致——让机器看懂东西 CNN 是图像 …</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-169">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-170">86. Toyota in georgia | Car Dealerships In georgia, atlanta</h2>
<ul>
<li>链接：https://www.toyota.com/dealers/atlanta/georgia/dealers/</li>
<li>来源：bing</li>
<li>摘要：Find a Toyota dealer in georgia, atlanta. Contact your nearest Toyota dealer to schedule a test drive today.</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-171">正文（抓取，非 AI）</h3>
<p>Toyota in georgia | Car Dealerships In georgia, atlanta Dealer Hub , Select a Dealer</p>
</div></details><h2 id="toc-172">87. 连更预告|全球诡异时代 | 官方在线漫画全集-快看漫画</h2>
<ul>
<li>链接：https://m.kuaikanmanhua.com/mobile/comics/832849/</li>
<li>来源：bing</li>
<li>摘要：全球诡异时代 连更预告 快看小程序 我的关注 阅读历史 开灯 100% 上一话 下一话 章节 关注 评论 10 整本购 工具栏 上一话 下一话</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-173">正文（抓取，非 AI）</h3>
<p>连更预告｜全球诡异时代漫画｜官方在线漫画全集-快看漫画 全球诡异时代 连更预告 快看小程序 我的关注 阅读历史 开灯 100% 上一话 下一话 章节 关注 评论 10 整本购 工具栏 上一话 下一话</p>
</div></details><h2 id="toc-174">88. Quantum Flagship公布最新路线图，将欧洲定位为世界上第 ...</h2>
<ul>
<li>链接：https://quantumcomputer.ac.cn/Knowledge/detail/all/d9655fd7d6f44ca2a29e73096c4ba1ab.html</li>
<li>来源：bing</li>
<li>摘要：2024年2月23日 · 2月14日，欧盟量子旗舰项目（Quantum Flagship）推出了新的路线图，呼吁停止依赖外部国家开发重要组件和硬件。路线图的主题包括经济和技术主权，旨在将欧盟打造成一个拥有创新 …</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-175">正文（抓取，非 AI）</h3>
<p>Quantum Flagship公布最新路线图，将欧洲定位为世界上第一个“量子谷”——undefined 请使用谷歌浏览器 X 当前位置 : 知识库 &gt; 全部 &gt; Quantum Flagship公布最新路线图，将欧洲定位为世界上第一个“量子谷” Quantum Flagship公布最新路线图，将欧洲定位为世界上第一个“量子谷” 发布时间:2024-02-23 发布者: 量妹 浏览量:4631 欧盟量子旗舰项目（Quantum Flagship）推出了新的路线图，呼吁停止依赖外部国家开发重要组件和硬件。 上一篇:美国DARPA选择微软开发公用事业级量子计算机 下一篇:Rescale 与 IonQ 合作，通过混合量子计算加速创新</p>
</div></details><h2 id="toc-176">89. Introduction to Large Language Models | Google Skills</h2>
<ul>
<li>链接：https://www.skills.google/course_templates/539</li>
<li>来源：bing</li>
<li>摘要：This is an introductory level micro-learning course that explores what large language models (LLM) are, the use cases where they can be utilized, and how you can use prompt tuning to enhance LLM …</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-177">正文（抓取，非 AI）</h3>
<p>Introduction to Large Language Models | Google Skills Loading... No results found. Apply your skills in Google Cloud console Course Introduction to Large Language Models 10 minutes Introductory Updated 4 months ago This is an introductory level micro-learning course that explores what large language models (LLM) are, the use cases where they can be utilized, and how you can use prompt tuning to enhance LLM performance. It also covers Google tools to help you develop your own Gen AI apps. Earn a badge today! The Power of Challenge Labs Now you can fast track your way to a skill badge without having to take the entire course. If you're confident with your skills, jump straight to the challenge lab. Preview</p>
</div></details><h2 id="toc-178">90. Pricing | Agentic.ai</h2>
<ul>
<li>链接：https://agentic.ai/pricing</li>
<li>来源：bing</li>
<li>摘要：Agentic.ai pricing — 14-day free trial, then $9/month or $79/year. Unlimited agents, lenses, and AI Q&amp;A.</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-179">正文（抓取，非 AI）</h3>
<p>Pricing | Agentic.ai Agentic.ai requires JavaScript enabled to run. Simple, Transparent Pricing Try Agentic free for 14 days. Cancel anytime. Monthly Annual -27% Pro $9 /month Unlimited agents &amp; lenses Unlimited AI Q&amp;A Priority processing — get news faster Early access to new features Direct founder access for feedback Start 14-Day Free Trial After trial: keep 2 agents , 1 lens free forever — or stay Pro for unlimited access. Cancel anytime, no questions asked Card required — you won't be charged until Day 14 Try everything free for 14 days. Set up agents for any topic, interpret stories with lenses, and ask questions across your saved collections — all unlimited during your trial. Start Your Free Trial How it works</p>
</div></details><h2 id="toc-180">91. 量子计算云平台</h2>
<ul>
<li>链接：https://quantumcomputer.ac.cn/index.html</li>
<li>来源：bing</li>
<li>摘要：量子计算云平台成立于2017年，在2017年10月11日“2017杭州·云栖大会”上，中国科学院量子信息与量子科技创新研究院（上海）联合阿里云共同宣布“量子计算云平台”上线。2018年2月再次在超导量子计 …</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-181">正文（抓取，非 AI）</h3>
<p>量子计算云平台 请使用谷歌浏览器 X 量子计算实验室 12比特超导量子计算原型机 - 12比特超导量子计算原型机 12比特物理机是一个小规模的量子计算原型机，由一个包含12个超导量子比特的量子芯片、极低温极低噪声测控平台、高精度量子调控电子学系统、量子操控软件系统组成，可以通过标准的量子门指令集进行编程运行量子算法。 立即实验 更多比特超导量子计算原型机 - 更多比特超导量子计算原型机 正在安装调试，尽情期待。如有实验和应用需求，请联系我们。 期待您的来信：support@quantumcomputer.ac.cn 立即实验 光量子计算机 九章 - 光量子计算原型机 光量子计算机 可有效求解“高斯玻色取样”数学问题，并将致力于把该数学问题映射到量子化学、机器学习等应用。如有实验和应用需求，也请联系我们。 期待您的来信：support@quantumcomputer.ac.cn 立即实验 中国科学院量子信息与量子科技创新研究院量子计算云平台 QUANTUM COMPUTING CLOUD, CAS CENTER FOR EXCELLENCE IN QUANTUM INFORMATION AND QUANTUM PHYSICS 平台介绍丨 查看更多&gt;&gt; 量子计算云平台成立于2017年，在2017年10月11日“2017杭州·云栖大会”上，中国科学院量子信息与量子科技创新研究院（上海）联合阿里云共同宣布“量子计算云平台”上线。2018年2月再次在超导量子计算方向发布11比特的云接入超导量子计算服务*。时隔3年后，中国科学院量子信息与量子科技创新研究院（上海）联手科大国盾量子技术股份有限公司，对量子计算云平台进行了全面升级，以全新的面貌服务于大家。 合作伙伴</p>
</div></details><h2 id="toc-182">92. A Review on Large Language Models: Architectures, Applications ...</h2>
<ul>
<li>链接：https://ieeexplore.ieee.org/document/10433480</li>
<li>来源：bing</li>
<li>摘要：2024年2月13日 · Large Language Models (LLMs) recently demonstrated extraordinary capability in various natural language processing (NLP) tasks including language translation, text generation, …</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-183">正文（抓取，非 AI）</h3>
<p>A Review on Large Language Models: Architectures, Applications, Taxonomies, Open Issues and Challenges | IEEE Journals &amp; Magazine | IEEE Xplore IEEE Account Change Username/Password Update Address Purchase Details Payment Options Order History View Purchased Documents Profile Information Communications Preferences Profession and Education Technical Interests Need Help? US &amp; Canada: +1 800 678 4333 Worldwide: +1 732 981 0060 Contact &amp; Support About IEEE Xplore Contact Us Help Accessibility Terms of Use Nondiscrimination Policy Sitemap Privacy &amp; Opting Out of Cookies A not-for-profit organization, IEEE is the world's largest technical professional organization dedicated to advancing technology for the benefit of humanity. © Copyright 2026 IEEE - All rights reserved. Use of this web site signifies your agreement to the terms and conditions.</p>
</div></details><h2 id="toc-184">93. About | Agentic.ai</h2>
<ul>
<li>链接：https://agentic.ai/about</li>
<li>来源：bing</li>
<li>摘要：AI-powered semantic alerts and interpretation for whatever you care about. Create agents to track topics, use lenses for different perspectives, and turn noisy news into actionable insight with citations.</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-185">正文（抓取，非 AI）</h3>
<p>About | Agentic.ai Agentic.ai requires JavaScript enabled to run. Track what matters. Understand what changed. Act faster. Agentic.ai is an AI-powered semantic alerts and interpretation platform for whatever you care about. It clusters noisy coverage into clean story groups, then explains what it means from the perspectives you choose—with receipts. Start free How it works Agents Create agents that watch the web for specific topics, competitors, or niche interests—without drowning you in duplicates. Lenses Define perspectives (roles you play) and see what each story means for that viewpoint—CEO, engineer, investor, parent, or your own. Collections + AI Q&amp;A Save stories and ask questions across what you’ve collected. Answers include citations to the underlying sources. Who it’s for Anyone who needs signal over noise: business research, competitive intelligence, investing, technology tracking, local community updates, hobbies, and anything else that’s hard to follow with generic feeds. See pricing Create free account</p>
</div></details><h2 id="toc-186">94. How it works | Agentic.ai</h2>
<ul>
<li>链接：https://agentic.ai/how-it-works</li>
<li>来源：bing</li>
<li>摘要：How Agentic.ai turns noise into signal Set up agents to watch for what you care about, cluster duplicate coverage, then use lenses to interpret what it means—so you can act with confidence.</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-187">正文（抓取，非 AI）</h3>
<p>How it works | Agentic.ai Agentic.ai requires JavaScript enabled to run. How Agentic.ai turns noise into signal Set up agents to watch for what you care about, cluster duplicate coverage, then use lenses to interpret what it means—so you can act with confidence. Create free account About Agentic.ai 1) Create Agents Tell us what to track—companies, competitors, keywords, niches, or personal interests. Agents continuously monitor sources and look for relevant matches. 2) Deduplicate into Clusters Instead of twenty versions of the same story, Agentic.ai groups near-duplicate coverage into a single cluster with multiple sources. 3) Interpret with Lenses Lenses are roles or perspectives you define. Each lens explains what changed and why it matters for that perspective. 4) Save + Ask Questions (with receipts) Save important clusters to collections and ask questions across your saved knowledge. Answers include citations to original sources. Alerts and briefings Get daily briefings by email (and adjust frequency in settings). You can also browse clusters discovered by your agents in the app. Pricing Start free</p>
</div></details><h2 id="toc-188">95. 全球诡异时代 | 全球诡异时代漫画全集免费 - 奈斯漫画</h2>
<ul>
<li>链接：https://www.nicemh.com/manhua/p4QNdXVbW9</li>
<li>来源：bing</li>
<li>摘要：2026年2月11日 · 全球诡异时代漫画全集免费,诡异复苏一百年的时间，人类只剩下十分之一，鬼的数量已经超过人类！为了生存，每个人从生下来开始，都需要选择一个“伴生鬼灵”，并不断培养自己的伴生 …</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-189">正文（抓取，非 AI）</h3>
<p>全球诡异时代 | 全球诡异时代漫画全集免费 - 奈斯漫画 首页 漫画库 最新 排行 全球诡异时代 作者 黑白茶（原著）+奇小怪 状态 连载中 题材 悬疑 都市 热血 更新时间 2026-02-18 10:00 介绍 诡异复苏一百年的时间，人类只剩下十分之一，鬼的数量已经超过人类！为了生存，每个人从生下来开始，都需要选择一个“伴生鬼灵”，并不断培养自己的伴生鬼灵，以鬼制鬼。全球进入了与鬼共存的诡异时代！借助系统，林风培养出的无敌的鬼灵，整个世界都为之瑟瑟发抖！ 开始阅读 手机扫码阅读 目录 最新 第573话 扶桑篇-东京炼狱！ 11小时前 正序 第518话 扶桑篇-友谊魔法力量！ 第519话 扶桑篇-罗生门！ 第520话 扶桑篇-五行之力！ 第521话 扶桑篇-织田信长的诞生！ 第522话 扶桑篇-天下布武！ 第523话 扶桑篇-高僧出世！诛邪灭魔！ 第524话 扶桑篇-鉴真破魔！ 第525话 扶桑篇-毁灭与重生！ 第416话 扶桑篇-怪人军团！ 第332话 虚假的世界！ 监视！控制！净化！ 第324话 佛影现！林风暴走！ 第323话 鬼王分身灭！ 第324话 佛影现！林风暴走！ 第323话 鬼王分身灭！ 第324话 佛影现！林风暴走！ 第323话 鬼王分身灭！ 第324话 佛影现！林风暴走！ 第323话 鬼王分身灭！ 第323话 鬼王分身灭！ 第535话 扶桑篇-永不谢幕！ 第416话 扶桑篇-怪人军团！ 第332话 虚假的世界！ 监视！控制！净化！ 第324话 佛影现！林风暴走！ 第323话 鬼王分身灭！ 第324话 佛影现！林风暴走！ 第323话 鬼王分身灭！ 第324话 佛影现！林风暴走！ 第323话 鬼王分身灭！ 第542话 扶桑篇-鬼主庆典！ 第547话 扶桑篇-情深陷！意迷离！ 第548话 扶桑篇-花魁的结局！ 第550话 扶桑篇-报仇雪恨！ 第551话 扶桑篇-雪女的心愿！ 第549话 扶桑篇-背后的真相！ 第552话 扶桑篇-猎人与谎言！ 第553话 扶桑篇-家人的守护！ 第554话 扶桑篇-断水流小柔！VS蓬莱绫罗！ 第555话 扶桑篇-都市怪谈来袭！ 第556话 扶桑篇-路西法的背刺！ 第557话 扶桑篇-圣域降临！ 第558话 扶桑篇-共战地狱！ 第559话 扶桑篇-审判仪式启动！ 第560话 扶桑篇-神！路西法！ 第561话 扶桑篇-弑神！ 第562话 扶桑篇-两个路西法！ 第563话 扶桑篇-路西法之心！ 第564话 扶桑篇-五行火之力！ 第565话 扶桑篇-神风部队！ 第566话 扶桑篇-狂猿出关！ 第567话 扶桑篇-随心铁杆兵！ 第568话 扶桑篇-恩怨了结！ 第569话 扶桑篇-执剑人考验！ 第570话 扶桑篇-绘梨衣的心愿！ 第571话 扶桑篇——特殊的晋级！ 第572话 扶桑篇-出发东京！ 第573话 扶桑篇-东京炼狱！ 猜你喜欢 绝世兵王 特大号X战警：万磁王 完美蜜月行 爆宠小萌妃 特大号X战警：雷鸟 片恋未亡人 关于死亡 Go Tinged With Heart 特大号X战警：暴风女 巴比伦王妃 皇太子的王子</p>
</div></details><h2 id="toc-190">96. NN 游戏社区以技术驱动革新，百万玩家体验再升级 - IT之家</h2>
<ul>
<li>链接：https://www.ithome.com/0/873/896.htm</li>
<li>来源：bing</li>
<li>摘要：2025年8月8日 · NN 构建的智能社交生态，通过技术创新重构组队逻辑：依托全球玩家行为分析系统，平台可精准捕捉用户操作特征、胜率区间及战术倾向，无论是偏好突击的激进型玩家，还是侧重策略的 …</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-191">正文（抓取，非 AI）</h3>
<p>NN 游戏社区以技术驱动革新，百万玩家体验再升级 - IT之家 首页 IT圈 最会买 设置 日夜间 随系统 浅色 深色 主题色 黑色 投稿 订阅 RSS订阅 收藏IT之家 软媒应用 App客户端 要知App 软媒魔方 业界 手机 电脑 测评 视频 AI 苹果 iPhone 鸿蒙 软件 智车 数码 学院 游戏 直播 5G 微软 Win10 Win11 专题 搜索 首页 &gt; IT资讯 &gt; 业界 NN 游戏社区以技术驱动革新，百万玩家体验再升级 2025/8/8 11:05:31 来源：之家网站 作者： - 责编： - 评论： 深耕全球游戏服务领域五年的 NN 游戏社区，近日宣布完成服务体系全面升级。作为雷神控股旗下的全球游戏服务平台，自 2020 年上线以来，NN 以“五年磨一剑”的扎实服务积累超 500 万全球注册用户，日活跃用户稳定在 40 万以上。此次升级直击玩家在组队效率、队友质量、赛事体验上的核心诉求，为行业游戏服务升级提供了新范式。 精准匹配 + 高清语音，重构高效组队生态 对注重团队协作的玩家而言，组队效率低下与队友适配性不足是长期存在的体验痛点。NN 构建的智能社交生态，通过技术创新重构组队逻辑：依托全球玩家行为分析系统，平台可精准捕捉用户操作特征、胜率区间及战术倾向，无论是偏好突击的激进型玩家，还是侧重策略的稳健型玩家，均能在 3 分钟内匹配到风格契合的队友，实现从“随机组队”到“精准组队”的跨越。 语音沟通是团队协作的核心支撑。NN 依托全球专线网络搭建专属语音传输通道，实现全程高清无卡顿传输，语音延迟与游戏操作同步，确保战术指令以毫秒级速度传递。搭载的智能降噪算法可有效过滤键盘操作、环境杂音等干扰，即便在复杂声学环境中，关键战术信息仍能清晰传达。赛后专属语音复盘功能与实时战绩分析工具联动，使组队社交不仅服务于赛事竞技，更成为技术提升与队友联结的纽带。 此外，平台构建的细分游戏社群与百万级兴趣社群体系，助力玩家快速锁定固定协作团队。通过游戏标签、历史战绩、社交推荐三重认证机制，“寻找队友”从随机行为转化为可预期的精准对接，为核心玩家解决“孤军奋战”的困境。 专业赛事体系，降低竞技参与门槛 对追求竞技体验的玩家而言，专业赛事是检验团队实力的核心场景。NN 联合腾讯、网易等头部厂商打造的全链路赛事服务，通过流程优化与技术赋能，让“参与赛事”更简单、更专业。 从面向新手的“常规速通赛”到聚焦精英战队的“全球挑战赛”，NN 构建的多层级赛事体系覆盖不同竞技水平玩家，确保每支队伍都能找到适配的竞技舞台。报名、对手匹配、观赛、数据复盘等全流程操作均可在平台内完成，无需跨平台跳转，大幅降低参与成本。依托稳定的技术架构，全球玩家可实现跨地域竞技，无论不同地区战队间的对抗，还是跨时区赛事参与，均能在公平竞技环境中共享赛事乐趣。 端技术突破，消除设备兼容性障碍 设备差异曾是制约组队体验的关键瓶颈：移动端与 PC 端玩家协作时易出现帧率失衡、操作适配性不足等问题；设备切换时的数据不同步，更可能导致团队节奏滞后。 此次服务升级中，NN 的跨端技术彻底打破这一壁垒。其内置的 PC 模拟器已适配国内 90% 以上热门手游，实现移动端与 PC 端数据无缝互通；针对键盘鼠标操作的专项优化，确保跨设备组队时操作手感一致，保障团队协作的流畅性。启动速度较同类产品提升 30% 的技术优势，进一步确保团队成员同步上线、协同开团，避免因设备性能差异影响团队节奏。 五年来，NN 始终以“让游戏更好玩”为核心，从首代 PC 模拟器到如今的全球社交生态，每一步迭代均源于玩家需求。未来，NN 将持续深化技术创新，让全球玩家在流畅体验、高效社交与公平竞技中，真正感受“因游结缘”的价值。 广告声明：文内含有的对外跳转链接（包括不限于超链接、二维码、口令等形式），用于传递更多信息，节省甄选时间，结果仅供参考，IT之家所有文章均包含本声明。 投诉水文 我要纠错 下载IT之家APP，签到赚金币兑豪礼 相关文章 关键词： 业界动态 打造多元化内容矩阵创新玩法推动商业化加速 创新业务规模持续增长斗鱼第二增长曲线渐成 斗鱼转型“提质增效”：创新业务八季连增背后的多元化发展道路 黑竞宗轴 + FSA 球帽：机械师 K600 三模机械键盘 149.5 元半价发车 荣耀 Magic6 系列新品首发多项影像技术 激光雷达对焦技术超越苹果 搭建仅需 3 分钟，和好朋友一起联机玩《幻兽帕鲁》 软媒旗下网站： IT之家 最会买 - 返利返现优惠券 iPhone之家 Win7之家 Win10之家 Win11之家 软媒旗下软件： 软媒手机APP应用 魔方 最会买 要知</p>
</div></details><h2 id="toc-192">97. 机器人仿真教程丨Isaac Sim 4.5.0 与 lsaac Lab 2.0 安装指南</h2>
<ul>
<li>链接：https://www.zhihu.com/tardis/bd/art/1917973401177427971</li>
<li>来源：bing</li>
<li>摘要：2025年6月16日 · Isaac Sim 和 Isaac Lab 目前开放下载的版本是 Isaac Sim 4.5.0 以及 Isaac Lab 2.0，本篇文章将为大家带来这两个软件的安装教程。 在此提醒下，Isaac Sim 4.5.0 将是 …</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-193">正文（抓取，非 AI）</h3>
<p>开篇福利依旧： https:// xg.zhihu.com/plugin/ebe 4707d4455f342b30e9c3498c93e4b?BIZ=ECOMMERCE 先进的机器人仿真平台有助于机器人学习和无需实体机器人的虚拟机器人测试。 NVIDIA Isaac Sim™ 是一款基于 NVIDIA Omniverse™ 构建的参考应用，开发者能够在基于物理的虚拟环境中模拟和测试 AI 驱动的机器人开发解决方案。而 NVIDIA Isaac™ Lab 是基于 Isaac Sim 开发的用于机器人学习的开源模块框架，能够利用 NVIDIA® PhysX® 以及基于物理性质的 NVIDIA RTX™ 渲染实现高保真物理仿真。 Isaac Sim 和 Isaac Lab 目前开放下载的版本是 Isaac Sim 4.5.0 以及 Isaac Lab 2.0 ，本篇文章将为大家带来这两个软件的安装教程。 在此提醒下，Isaac Sim 4.5.0 将是 Omniverse Launcher 的最后一个版本，从 2025 年 10 月 1 日起，Omniverse Launcher、Nucleus Workstation 和 Nucleus Cache 将不再可用，如果继续使用，功能可能会减少。 在安装 Isaac Sim 之前，需要确保计算机是否满足系统要求和兼容性，以下是最低规格要求： 操作系统：Ubuntu 20.04/22.04、Windows 10/11 及以上 CPU：Intel Core i7、AMD Ryzen 5 及以上 内存：32GB 以上 GPU：具备 RT Core，8GB 以上 驱动程序：推荐 Windows 537.58 和 Linux 535.129.03 一、演示环境 1. 硬件： 服务器：LEADTEK GS4850H GPU：NVIDIA RTX™ 5880 Ada Generation 2. 软件 虚拟化平台：Proxmox Virtual Environment 虚拟化平台，配置显卡直通环境 系统：Ubuntu 22.04.5 二、Isaac Sim 4.5.0 安装 1. 前往官网下载软件包： https:// docs.isaacsim.omniverse.nvidia.com /latest/installation/download.html 2. 创建对应目录然后解压安装包进行安装。可通过以下命令运行 Isaac Sim App Selector 。Isaac Sim App Selector 是一款迷你窗口应用程序，可帮助以不同模式运行 Isaac Sim。 在 Linux 中运行以下命令： 3. 至此，已完成 Isaac Sim 4.5.0 版本的基本安装。 <em>以上安装步骤参考官方 Workstation Installation 教程，其他环境（如容器/云）安装可根据官网步骤进行安装： https:// docs.isaacsim.omniverse.nvidia.com /latest/installation/install_workstation.html 三、Isaac Lab 2.0 部署 Isaac Lab 建立在 Isaac Sim 平台之上。此前已经安装好了 Isaac Sim 4.5.0，本次演示采用 binaries installation，使用二进制文件进行安装，在部署 Isaac Lab 之前只需要验证 Isaac Sim 安装是否成功即可。 1. 验证 Isaac Sim 1.1 为避免每次查找和定位 Isaac Sim 安装目录，建议先将以下环境变量输入到终端： 1.2 刷新环境变量并运行脚本验证。 1.3 验证 Isaac Sim 是否可以通过独立的 python 脚本运行。 2. 安装 Isaac Lab 2.1 前往 github 克隆项目文件。 SSH： HTTPS： 2.2 确认是否可以正常运行。 2.3 为 Isaac Sim 的安装目录创建一个名为 _isaac_sim 的符号链接(软链接)，方便在 Isaac Lab 目录使用。 2.4 安装 Isaac Lab。 先安装所需要的依赖项： 然后运行 install 命令，该命令会遍历目录中的所有扩展程序并进行安装： 2.5 验证 Isaac Lab。使用 isaaclab.sh 可执行文件进行环境验证。 2.6 完成以上步骤，便可把 Isaac Sim 4.5 和 Isaac Lab 2.0 顺利部署至环境中。 </em>如需了解如何通过 Python 的包管理工具 pip 进行安装，可参考官方文档： https:// isaac-sim.github.io/Isa acLab/main/source/setup/installation/index.html# 四、机器人仿真环境推荐硬件配置 机器人仿真需要算力来加速机器人运动模拟、碰撞检测等行为，同时要对大量的几何模型和场景进行渲染，所以，仿真硬件性能的需求也尤为关键。以下是丽台针对个人开发者/研究人员的机器人仿真环境推荐配置，聚焦于流畅运行 Isaac Sim 4.5.0 以及 Isaac Lab 2.0（工程文件导入、参数调整及强化学习训练），导入验证完成后可再与企业级多卡集群方案形成互补，能够完全满足中小规模的实验需求。 ▲ LEADTEK WS3008 工作站 LEADTEK WS3008 产品特性 支持选配 2 张 NVIDIA RTX 5880 Ada GPU 支持单 Intel Xeon W-3400、W-2400 系列处理器 Intel W790 芯片组 支持最高至 350W CPU TDP 支持最多 DDR5-4800MHz x8 内存 支持 4 PCIe 5.0 x16 插槽 支持 1 M.2 NVMe PCI-E 4.0 x4 支持 2 个 10GbE BaseT、1 个 2.5GbE BaseT 和 1 个 1GbE BaseT 网口 支持 1 个 IPMI 管理口 支持 2 个 USB-A 支持 1 个 VGA 口，1 个 COM 口 支持 1 个 1200W/1300W 铂金电源 机箱体积：400.0×278.0×167.6 mm 如需获取更多产品信息，欢迎联系 @丽台科技 。 *与 NVIDIA 产品相关的图片或视频（完整或部分）的版权均归 NVIDIA Corporation 所有。 https:// xg.zhihu.com/plugin/ebe 4707d4455f342b30e9c3498c93e4b?BIZ=ECOMMERCE</p>
</div></details><h2 id="toc-194">98. 强化学习 (Reinforcement Learning) - 知乎</h2>
<ul>
<li>链接：https://www.zhihu.com/topic/20039099/intro</li>
<li>来源：bing</li>
<li>摘要：强化学习 (Reinforcement Learning) 详细内容 简介 根据维基百科对强化学习的定义：Reinforcement learning (RL) is an area of machine learning inspired by behaviorist …</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-195">正文（抓取，非 AI）</h3>
<p>强化学习 (Reinforcement Learning) - 知乎 强化学习 (Reinforcement Learning) 知史明未，为了更好地学习强化学习，需要我们对强化学习的发展历史进行整体的了解。唯有当系统性地了解强化学习的发展历史之后，才能够更为直观、更为深刻地理解强化学习目前所取得的成就和存在的不足以及厘清强… 查看全部内容 关注话题 ​ 管理 ​ 分享 ​ 百科 讨论 精华 等待回答 详细内容 简介 根据维基百科对强化学习的定义：Reinforcement learning (RL) is an area of machine learning inspired by behaviorist psychology, concerned with how software agents ought to take actions in an environment so as to maximize some notion of cumulative reward. （强化学习是机器学习领域之一，受到行为心理学的启发，主要关注智能体如何在环境中采取不同的行动，以最大限度地提高累积奖励。） 强化学习主要由智能体（Agent）、环境（Environment）、状态（State）、动作（Action）、奖励（Reward）组成。智能体执行了某个动作后，环境将会转换到一个新的状态，对于该新的状态环境会给出奖励信号（正奖励或者负奖励）。随后，智能体根据新的状态和环境反馈的奖励，按照一定的策略执行新的动作。上述过程为智能体和环境通过状态、动作、奖励进行交互的方式。 智能体通过强化学习，可以知道自己在什么状态下，应该采取什么样的动作使得自身获得最大奖励。由于智能体与环境的交互方式与人类与环境的交互方式类似，可以认为强化学习是一套通用的学习框架，可用来解决通用人工智能的问题。因此强化学习也被称为通用人工智能的机器学习方法。 组成元素 智能体 强化学习的本体，作为学习者或者决策者。 环境 强化学习智能体以外的一切，主要由状态集合组成。 状态 一个表示环境的数据，状态集则是环境中所有可能的状态。 动作 智能体可以做出的动作，动作集则是智能体可以做出的所有动作。 奖励 智能体在执行一个动作后，获得的正/负反馈信号，奖励集则是智能体可以获得的所有反馈信息。 策略 强化学习是从环境状态到动作的映射学习，称该映射关系为策略。通俗的理解，即智能体如何选择动作的思考过程称为策略。 目标 智能体自动寻找在连续时间序列里的最优策略，而最优策略通常指最大化长期累积奖励。 因此，强化学习实际上是智能体在与环境进行交互的过程中，学会最佳决策序列。 基本框架 强化学习主要由智能体和环境组成。由于智能体与环境的交互方式与生物跟环境的交互方式类似，因此可以认为强化学习是一套通用的学习框架，是通用人工智能算法的未来。 强化学习的基本框架如图所示，智能体通过状态、动作、奖励与环境进行交互。假设图1.7中环境当前处于时刻t的状态记为 ，智能体在环境中执行某动作 ，这时候该动作 改变了环境原来的状态并使得智能体在时刻t+1到达新的状态 ，在新的状态使得环境产生了反馈奖励 给智能体。智能体基于新的状态 和反馈奖励 执行新的动作 ，如此反复迭代地与环境通过反馈信号进行交互。 上述过程的最终目的是让智能体最大化累积奖励（Cumulative Reward），公式为累积奖励G： 在上述过程中，如何根据状态 和奖励 选择动作的规则称为策略π，其中价值函数（Value Function）v是累计奖励的期望。 举例说明 为了更好地解释强化学习基本框架，这里给出一个简单的例子：当还是一个调皮的孩子不愿意做作业，父母就会在孩子不愿意做作业的时候就会说：“做完作业带你去麦当劳”。这时候，小孩子眼睛闪着金光，于是调皮的孩子就会为了去麦当劳乖乖地去写作业，久而久之，就会明白只有努力写作业才能获得去麦当劳的奖励。 然而事情不是总这么简单，父母对于作业完成的顺序可能会有要求，假如父母特别希望看到孩子先做完数学，然后再做语文、英语作业，如果按照父母的意愿先做完数学再做其他作业，那么就不仅能吃上炸鸡，还可以加一个雪糕。于是小孩就会学聪明点，为了吃到更多麦当劳食品，就会按照父母的意愿去先完成数学作业，再做其他作业。最后小孩不仅知道努力做作业可以获得奖励，并且为了吃到更多的麦当劳食品，改变做作业的顺序，这就相当于找到一个好的策略，能够使小孩获得最大累积奖励。 在上面的这个例子中，调皮的孩子就是智能体，父母代表环境，麦当劳的炸鸡和雪糕分别代表不同的奖励信号，小孩选择不做作业、做作业、做作业的顺序就是动作，当前作业的完成情况可以类比为状态。父母（环境）会根据孩子的作业的完成情况（当前状态）给予不同的奖励，对于不同奖励人们会采取不同的方式去做作业（选择动作），做作业并且先做数学作业就是最优策略。 强化学习就是不断地根据环境的反馈信息进行试错学习，进而调整优化自身的状态信息，其目的是为了找到最优策略、或者找到最大奖励的过程。 比较区别 2016年，由Google DeepMind开发的AlphaGo程序在人机围棋对弈中打败了韩国的围棋大师李世石。就如同1997年IBM的“深蓝”计算机战胜了国际象棋大师卡斯帕罗夫一样，媒体开始铺天盖地般地宣传人工智能时代的来临。 在介绍AlphaGo程序时，很多媒体都会把人工智能（Artificial Intelligence）、机器学习（Machine Learning）和深度强化学习混为一谈。从严格定义上来说，DeepMind在AlphaGo程序中对上述三种技术都有所使用，但使用得更多的是深度强化学习。 下图展示了人工智能、机器学习、深度强化学习三者之间的关系。其中人工智能包含机器学习，而强化学习则是机器学习的重要分支之一，它们三者是包含与被包含的关系，而非并列的关系。 从20世纪50年代“人工智能”这一概念第一次提出至今，人工智能的问题大致分为6个具体的方向：问题求解、知识推理、规划问题、不确定推理、通信感知与行动、学习问题。而机器学习主要分为3个方向：分类、回归、关联性分析。最后到深度强化学习则是对机器学习中的强化学习进行深度拓展。 人工智能实际上包含了日常使用到的算法。例如在问题求解方面，有A*搜索算法和a-b剪枝算法等经典算法，又如人工智能中的学习问题则包含了机器学习的大部分内容。现阶段已经有很多资料介绍机器学习相关的算法，较为著名的机器学习十大算法为：决策树、支持向量机SVM、随机森林算法、逻辑回归、朴素贝叶斯、KNN算法、K-means算法、Adaboost算法、Apriori算法、PageRank算法。 在机器学习里，其范式主要分为监督学习（Supervised Learning），无监督学习（Unsupervised Learning）和强化学习。 正如维基百科所说，强化学习是机器学习的一个分支组成部分，但是却与机器学习当中常见的监督学习和无监督学习不同。具体而言，强化学习是一种通过交互的目标导向学习方法，旨在找到连续时间序列的最优策略；监督学习是通过有标签的数据，学习规则，通常指回归、分类问题；非监督学习是通过无标签的数据，找到其中的隐藏模式，通常指聚类、降维等算法。 参考资料 《深度强化学习原理与实践》人民邮电出版社 张爽 浏览量 2.5 亿 讨论量 11.7 万</p>
</div></details><h2 id="toc-196">99. 量子光学（quantum optics） - 知乎</h2>
<ul>
<li>链接：https://www.zhihu.com/topic/20232436/intro</li>
<li>来源：bing</li>
<li>摘要："量子光学（quantum optics）是物理学在1990年后成熟的新兴分支，是原子分子与光物理的一部分，也和冷原子物理紧密相连。与凝态物理、粒子物理学、宇宙学等其他成熟分支相比，在精密的实验和理论 …</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-197">正文（抓取，非 AI）</h3>
<p>量子光学（quantum optics） - 知乎 量子光学（quantum optics） "量子光学（quantum optics）是物理学在1990年后成熟的新兴分支，是原子分子与光物理的一部分，也和冷原子物理紧密相连。与凝态物理、粒子物理学、宇宙学等其他成熟分支相比，在精密… 查看全部内容 关注话题 ​ 管理 ​ 分享 ​ 百科 讨论 精华 等待回答 详细内容 概述 量子光学（quantum optics）是物理学在1990年后成熟的新兴分支，是原子分子与光物理的一部分，也和冷原子物理紧密相连。与凝态物理、粒子物理学、宇宙学等其他成熟分支相比，在精密的实验和理论上，有着紧密、具建设性的互动。量子光学以半经典物理学及量子力学来研究“光的现象”以及“光和物质在亚微观尺度下的相互作用”。在1960年代因为汉伯里·布朗及特维斯效应刺激而发展出理论基础，讨论不同程度的相量子相干性,如 为零是典型的单光子源判准主要研究光子和原子的量子相互作用，研究工具为激光及离子井。 历史背景 光在真空传递的能量及动量为量子化的，量子化对应着光子的粒子数，量子光学也就是研究量子化的光子本性和影响的学科，首先的重要发展在1899年， 1878年学生时代的普朗克 普朗克假设光的能量是以离散单位来发射，该假设正确描述了黑体辐射。随后在1905年，爱因斯坦解释光电效应的论文更进一步为量子化带来证据，因此爱因斯坦荣获了1921年的诺贝尔奖。尼尔斯·玻尔指出光辐射的量子化假设与他的原子量子化能级理论相符，特别是氢的发射光谱。这些发展为光与物质之间的相互作用带来的理解，对于整体量子力学的发展至关重要。然而，这项用于处理“物质-光相互作用”的量子力学子领域，主要被认为是对物质的研究而不是对光的研究; 因此在1960人们称其为“原子物理学”和“量子电子学”。对这些装置的原理、设计、应用的研究使激光科学成为一个重要的领域，当今研究激光原理的量子力学，更加强调光的性质，因此人们也习惯了称其为“量子光学”。 由于激光科学需要良好的理论基础，加上基础的研究成果丰硕，人们对量子光学的兴趣也随之上升。继狄拉克在量子场论领域的工作之后，在1950年代和1960年代，George Sudarshan、Roy J. Glauber和Leonard Mandel将量子理论应用于电磁场， 不同温度的黑体辐射频谱。随着温度下降，频谱峰值波长增加 更详细理解了光探测和光统计(参见相干度(degree of coherence))。这引入相干态的概念来解决激光、热光、奇异压缩态等之间的变化问题，因为人们已经认识到光不仅仅为经典图像中描述波的电磁场。1977年，Kimble等人展示了一次发射单光子的单原子，进一步地证明了光是由光子组成的。随后发现了特征与经典状态不同的未知光量子态，例如压缩光。 透过Q开关和锁模技术开发了短脉冲和超短冲激光脉冲，其发展开启了超快过程的研究。 量子光学发现了固态方面的应用（例如拉曼光谱​​），也研究了光作用在物质上的机械力。后者可透过激光束对光学陷阱或光镊中的原子云或甚至微小的生物样品悬浮和定位。这个技术与多普勒冷却同为实现著名的玻色 - 爱因斯坦凝聚的关键。 其他显著的成果有量子纠缠、量子隐形传态和量子逻辑门。量子信息理论部分来自量子光学，部分来自理论计算机科学，量子信息领域对量子逻辑门非常感兴趣。 今天量子光学研究人员感兴趣的领域包括参量下转换，参数振荡，甚至更短（阿秒）光脉冲，量子光学在量子信息的使用，单原子的操纵，玻色 - 爱因斯坦凝聚，它们的应用，以及如何操纵它们（一个通常称为原子光学的子场），相干的完美吸收器(Coherent perfect absorber)等等。量子光学术语下分类的主题中，现代术语“光子学”通常是指应用于工程和技术创新的学科。 多项诺贝尔奖授予了量子光学方面的工作。被授予者如下： 2012年，Serge Haroche和David J. Wineland“开创了能够测量和操纵单个量子系统的突破性实验方法”。 2005年，TheodorW.Hänsch，Roy J. Glauber和John L. Hall 2001年，Wolfgang Ketterle，Eric Allin Cornell和Carl Wieman 1997年，Steven Chu，Claude Cohen-Tannoudji和William Daniel Phillips 基本观念 根据量子理论，光不仅仅只被视为电磁波，也可以被看作在真空中以光速c行进的粒子流，称为光子。这些粒子不该当作经典的台球，而是量子力学中在有限范围内以波函数描述的粒子。 每个光子携带一量子的能量，其值为hf，h为普朗克常数，f为光的频率。当原子发射出光子，光子的能量对应于内部离散能级的跃迁，物质吸收光子则是相反的过程，爱因斯坦对自发辐射的解释也预测受激辐射的存在，受激辐射即激光的原理，而激光的发明则要等到多年后有了居量反转的方法后才得以实现。 统计力学是量子光学的观念基础：光以场的创生算符与湮没算符来描述，也就是以量子电动力学的语言来描述。 光场最常用到的态是1960由E.C. George Sudarshan 引入的相干态，这种状态可以用来近似描述高于激光阈值的单频激光器的输出，表现出泊松光子数统计，通过某些非线性相互作用，应用具超泊松光子统计或亚泊松光子统计的压缩算符，可以将相干态转换为压缩相干态。这种光被称为压缩光。其他重要的量子观点，与不同光束之间的光子统计相关。例如，自发参量下转换可以产生所谓的“双光束”(twin beam)，理想情况下，一个光束的每个光子与另一个光束中的一个光子相关联。 原子被认为是具有离散能谱的量子力学振荡器，根据爱因斯坦的理论，能量本征态之间的跃迁由光的吸收或发射驱动。 对于固态物质，人们使用固态物理的能带模型。这对理解实验中常用的固态元件如何侦测光是很重要的 重要实验 近三十年来重要的量子光学实验包括： "which way"实验（Hong等，罗切斯特大学） “卡西米尔效应”实验（卡普索等，哈佛大学） “玻色–爱因斯坦凝聚”实验（麻省理工学院） “激光冷却和捕获原子”实验（朱棣文等，贝尔实验室） 墨子号量子科学实验卫星“星地双向纠缠分发实验”与“空间尺度量子隐形传态实验”实验（潘建伟等，中国科学院） 参考文献 B. Hoffmann, The Strange Story of the Quantum, Pelican 1963. Lucretius, On the Nature of the Universe, transl. from the Latin by R.E. Latham, Penguin Books Ltd., Harmondsworth 1951. J. Mehra and H. Rechenberg, The Historical Development of Quantum Theory, Vol.1, Part 1, Springer-Verlag New York Inc., New York 1982. M. Planck, A Survey of Physical Theory, transl. by R. Jones and D.H. Williams, Methuen &amp; Co., Ltd., London 1925 (Dover editions 1960 and 1993) including the Nobel lecture. Rodney, Brooks (2011) Fields of Color: The theory that escaped Einstein. Allegra Print &amp; Imaging. 浏览量 2188 万 讨论量 1.5 万</p>
</div></details><h2 id="toc-198">100. 一文读懂：大语言模型（LLM)</h2>
<ul>
<li>链接：https://www.zhihu.com/tardis/bd/art/32829727368</li>
<li>来源：bing</li>
<li>摘要：2025年3月26日 · 一、大模型（LLM）的定义与起源 大模型（Large Language Model, LLM） 是一种基于深度学习的自然语言处理模型，通过海量文本数据的预训练学习语言规律，具备理解、 …</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-199">正文（抓取，非 AI）</h3>
<p>一、大模型（LLM）的定义与起源 大模型（Large Language Model, LLM） 是一种基于深度学习的自然语言处理模型，通过海量文本数据的预训练学习语言规律，具备理解、生成和推理文本的能力。其核心特征包括： 1. 参数规模庞大 ：通常包含数十亿至数千亿参数（如GPT-3的1750亿参数）。 2. 基于Transformer架构 ：依赖自注意力机制处理长文本序列，显著提升并行计算效率。 3. 多阶段训练流程 ：包括预训练（无监督学习）、微调（有监督学习）和RLHF（基于人类反馈的强化学习）。 起源： 早期阶段 ：20世纪90年代的统计语言模型（如n-gram）仅依赖词频统计，无法捕捉复杂语义。 深度学习革命 ：2003年Bengio提出神经网络语言模型，引入词向量概念；2010年后LSTM/GRU解决序列建模问题，但仍受限于长程依赖。 Transformer突破 ：2017年谷歌提出Transformer架构，通过自注意力机制实现高效并行计算，为BERT（2018）、GPT系列（2018-2023）等模型奠定基础。 二、LLM与传统AI的区别 1. 模型规模与通用性 ： 传统AI多为垂直领域专用模型（如围棋AI、翻译工具），任务单一； LLM通过海量数据训练，具备跨领域通用能力（如上下文学习、指令遵循），可适应多种任务。 2. 架构差异 ： 传统模型依赖规则或浅层神经网络（如SVM、RNN）； LLM基于Transformer，支持长文本理解与生成，且参数规模呈指数级增长。 3. 应用范围 ： 传统AI局限于特定场景（如语音识别）； LLM覆盖文本生成、多模态交互、代码编写等广泛领域，并支持通过API快速集成。 三、LLM改变的核心领域 1. 自然语言处理（NLP）： 文本生成： 自动化撰写文章、生成代码（如GitHub Copilot）。 对话系统： ChatGPT等实现类人交互，应用于客服、教育问答。 翻译与摘要 ：支持多语言实时翻译，提炼长文本核心信息。 2. 多模态与跨领域融合： 结合图像、音频生成（如DALL·E生成图像，GPT-4V处理图文混合输入）。 3. 行业应用革新： 医疗：辅助诊断、医学文献分析； 金融：自动化报告生成、风险预测； 教育：个性化学习资源推荐。 四、LLM的局限与未来展望 当前局限： 1. 幻觉问题 ：生成内容可能偏离事实或包含虚构信息。 2 . 算力与成本 ：训练需消耗巨额计算资源（如GPT-3训练成本超千万美元）。 3. 伦理与安全 ：存在偏见传播、隐私泄露风险（如数据训练中的敏感信息）。 4. 长文本处理不足 ：对超长文本的连贯性与逻辑性仍待提升。 未来发展方向： 1. 多模态深度整合 ：增强图文、音视频的跨模态生成与理解能力。 2. 模型轻量化 ：通过知识蒸馏、模型压缩（如GPT-4o-mini）降低部署成本。 3. 个性化与私有化： 定制化模型满足企业数据安全与垂直领域需求。 4. 伦理与可解释性 ：开发透明化训练机制，减少偏见与误生成。 五、如何学习AI大模型？ 由于新岗位的生产效率，要优于被取代岗位的生产效率，所以实际上整个社会的生产效率是提升的。 但是具体到个人，只能说是： “最先掌握AI的人，将会比较晚掌握AI的人有竞争优势”。 这句话，放在计算机、互联网、移动互联网的开局时期，都是一样的道理。 我在一线互联网企业工作十余年里，指导过不少同行后辈。帮助很多人得到了学习和成长。 我意识到有很多经验和知识值得分享给大家，也可以通过我们的能力和经验解答大家在人工智能学习中的很多困惑，所以在工作繁忙的情况下还是坚持各种整理和分享。但苦于知识传播途径有限，很多互联网行业朋友无法获得正确的资料得到学习提升，故此将并将重要的AI大模型资料包括AI大模型入门学习思维导图、精品AI大模型学习书籍手册、视频教程、实战学习等录播视频免费分享出来。 这份完整版的大模型 AI 学习资料已经上传知乎，朋友们如果需要可以点击下方小卡片免费领取【保证100%免费】 https://xg.zhihu.com/plugin/39359f23dd0c81ee16b0aa6338c7cc69?BIZ=ECOMMERCE​xg.zhihu.com/plugin/39359f23dd0c81ee16b0aa6338c7cc69?BIZ=ECOMMERCE 第一阶段（10天）：初阶应用 该阶段让大家对大模型 AI有一个最前沿的认识，对大模型 AI 的理解超过 95% 的人，可以在相关讨论时发表高级、不跟风、又接地气的见解，别人只会和 AI 聊天，而你能调教 AI，并能用代码将大模型和业务衔接。 大模型 AI 能干什么？ 大模型是怎样获得「智能」的？ 用好 AI 的核心心法 大模型应用业务架构 大模型应用技术架构 代码示例：向 GPT-3.5 灌入新知识 提示工程的意义和核心思想 Prompt 典型构成 指令调优方法论 思维链和思维树 Prompt 攻击和防范 … 第二阶段（30天）：高阶应用 该阶段我们正式进入大模型 AI 进阶实战学习，学会构造私有知识库，扩展 AI 的能力。快速开发一个完整的基于 agent 对话机器人。掌握功能最强的大模型开发框架，抓住最新的技术进展，适合 Python 和 JavaScript 程序员。 为什么要做 RAG 搭建一个简单的 ChatPDF 检索的基础概念 什么是向量表示（Embeddings） 向量数据库与向量检索 基于向量检索的 RAG 搭建 RAG 系统的扩展知识 混合检索与 RAG-Fusion 简介 向量模型本地部署 … 第三阶段（30天）：模型训练 恭喜你，如果学到这里，你基本可以找到一份大模型 AI相关的工作，自己也能训练 GPT 了！通过微调，训练自己的垂直大模型，能独立训练开源多模态大模型，掌握更多技术方案。 到此为止，大概2个月的时间。你已经成为了一名“AI小子”。那么你还想往下探索吗？ 为什么要做 RAG 什么是模型 什么是模型训练 求解器 &amp; 损失函数简介 小实验2：手写一个简单的神经网络并训练它 什么是训练/预训练/微调/轻量化微调 Transformer结构简介 轻量化微调 实验数据集的构建 … 第四阶段（20天）：商业闭环 对全球大模型从性能、吞吐量、成本等方面有一定的认知，可以在云端和本地等多种环境下部署大模型，找到适合自己的项目/创业方向，做一名被 AI 武装的产品经理。 硬件选型 带你了解全球大模型 使用国产大模型服务 搭建 OpenAI 代理 热身：基于阿里云 PAI 部署 Stable Diffusion 在本地计算机运行大模型 大模型的私有化部署 基于 vLLM 部署大模型 案例：如何优雅地在阿里云私有部署开源大模型 部署一套开源 LLM 项目 内容安全 互联网信息服务算法备案 … 学习是一个过程，只要学习就会有挑战。天道酬勤，你越努力，就会成为越优秀的自己。 如果你能在15天内完成所有的任务，那你堪称天才。然而，如果你能完成 60-70% 的内容，你就已经开始具备成为一名大模型 AI 的正确特征了。 这份完整版的大模型 AI 学习资料已经上传知乎，朋友们如果需要可以点击下方小卡片免费领取【保证100%免费】 https://xg.zhihu.com/plugin/39359f23dd0c81ee16b0aa6338c7cc69?BIZ=ECOMMERCE​xg.zhihu.com/plugin/39359f23dd0c81ee16b0aa6338c7cc69?BIZ=ECOMMERCE 总结 LLM通过规模化的参数与通用性重构了AI技术的边界，其影响已渗透至多个行业。尽管面临成本、伦理等技术挑战，未来通过多模态融合与轻量化设计，LLM有望进一步推动人机协作的智能化进程，成为通用人工智能（AGI）的重要基石。</p>
</div></details><h2 id="toc-200">101. Rick Hendrick Toyota Sandy Springs - Location, Deals and …</h2>
<ul>
<li>链接：https://www.toyota.com/dealers/Georgia/Atlanta/30328/Rick-Hendrick-Toyota-Sandy-Springs/</li>
<li>来源：bing</li>
<li>摘要：Get the address and phone for Rick Hendrick Toyota Sandy Springs. Visit us today for great deals on your favorite Toyota models.</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-201">正文（抓取，非 AI）</h3>
<p>Rick Hendrick Toyota Sandy Springs - Location, Deals and Inventory Dealer Hub Georgia Atlanta Rick Hendrick Toyota Sandy Springs Rick Hendrick Toyota Sandy Springs of Atlanta, Georgia - 30328 Contact Information Address 6475 Roswell Road NE Atlanta, Georgia 30328 Get Directions Phone General: (404) 256-3392 Today's Hours: 9:00 AM to 8:00 PM Contact Dealer Dealer Website https://www.rickhendricktoyotasandysprings.com Hours of Operation Sales Hours Service Hours Parts Hours Sales Hours Sunday - Monday - Tuesday - Wednesday - Thursday - Friday - Saturday - Service Hours Sunday Closed Monday 7:00 AM to 6:00 PM Tuesday 7:00 AM to 6:00 PM Wednesday 7:00 AM to 6:00 PM Thursday 7:00 AM to 6:00 PM Friday 7:00 AM to 6:00 PM Saturday 7:00 AM to 5:00 PM Parts Hours Sunday Closed Monday 8:00 AM to 6:00 PM Tuesday 8:00 AM to 6:00 PM Wednesday 8:00 AM to 6:00 PM Thursday 8:00 AM to 6:00 PM Friday 8:00 AM to 6:00 PM Saturday 7:00 AM to 5:00 PM Sales Hours Service Hours Parts Hours Sunday - Closed Closed Monday - 7:00 AM to 6:00 PM 8:00 AM to 6:00 PM Tuesday - 7:00 AM to 6:00 PM 8:00 AM to 6:00 PM Wednesday - 7:00 AM to 6:00 PM 8:00 AM to 6:00 PM Thursday - 7:00 AM to 6:00 PM 8:00 AM to 6:00 PM Friday - 7:00 AM to 6:00 PM 8:00 AM to 6:00 PM Saturday - 7:00 AM to 5:00 PM 7:00 AM to 5:00 PM Explore the newest deals in Atlanta Dive into Toyota's latest offers in Atlanta and find unbeatable deals tailored just for you.Don't miss out on the opportunity to drive away with your perfect Toyota at an incredible price. Click now to explore our exclusive Atlanta deals! Explore Deals Now Dealer Services Learn about leasing, buying and payment plans. Toyota Certified Used Vehicles Toyota uses a 160-Point Quality Assurance Inspection to make sure we deal in only the best pre-owned vehicles. Once we make sure they deserve the Certified Used Vehicle badge, we back them with a 12-month/12,000-mile limited comprehensive warranty, a 7-year/100,000-mile limited powertrain warranty, and one year of roadside assistance. We're also happy to give you the CARFAX® vehicle history report, as well as offer standard new car financing rates. See Vehicles Toyota Tire Center Your Toyota Assistant Service Manager will be happy to check your air pressure and perform a visual inspection of your tires. If the manager notices any signs of unusual wear, they can assist in repair or replacement. And when you purchase tires from a participating Toyota Tire Center dealer, they're backed by the tire manufacturer's warranty and serviced through your Toyota dealer. President's Award Each year, Toyota Motor Sales recognizes its stellar dealerships with the prestigious President's Award. It's a very high honor a dealership can receive from Toyota, and is only awarded to those dealerships who have demonstrated a commitment to maintaining Toyota's high standards for customer satisfaction. One of Toyota's primary goals is to emphasize the entire ownership experience. We want to help ensure that our customers are satisfied not only at the time of purchase, but as long as they own their vehicle. Offering top quality cars and trucks is, of course, the first step - but only the beginning. Toyota dealerships strive to match the quality of our products with the finest service in the industry. In order to qualify as a President's Award winner, dealerships must excel in each of a series of categories, including Customer Sales Satisfaction and Customer Service Satisfaction. Dealerships which meet the requirements in all categories receive name badge recognition at all dealer events, national recognition in an Automotive News advertisement and, of course, the beautiful President's Award Tiffany crystal trophy.</p>
</div></details><h2 id="toc-202">102. World Toyota - Location, Deals and Inventory</h2>
<ul>
<li>链接：https://www.toyota.com/dealers/Georgia/Atlanta/30341/World-Toyota/</li>
<li>来源：bing</li>
<li>摘要：Get the address and phone for World Toyota. Visit us today for great deals on your favorite Toyota models.</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-203">正文（抓取，非 AI）</h3>
<p>World Toyota - Location, Deals and Inventory Dealer Hub Georgia Atlanta World Toyota World Toyota of Atlanta, Georgia - 30341 Contact Information Address 5800 Peachtree Industrial Blvd Atlanta, Georgia 30341 Get Directions Phone General: (678) 547-9000 Today's Hours: 9:00 AM to 9:00 PM Contact Dealer Dealer Website https://www.worldtoyota.com Hours of Operation Sales Hours Service Hours Parts Hours Sales Hours Sunday 11:00 AM to 6:00 PM Monday 9:00 AM to 9:00 PM Tuesday 9:00 AM to 9:00 PM Wednesday 9:00 AM to 9:00 PM Thursday 9:00 AM to 9:00 PM Friday 9:00 AM to 9:00 PM Saturday 9:00 AM to 9:00 PM Service Hours Sunday Closed Monday 7:00 AM to 7:00 PM Tuesday 7:00 AM to 7:00 PM Wednesday 7:00 AM to 7:00 PM Thursday 7:00 AM to 7:00 PM Friday 7:00 AM to 7:00 PM Saturday 7:00 AM to 5:00 PM Parts Hours Sunday Closed Monday 7:00 AM to 7:00 PM Tuesday 7:00 AM to 7:00 PM Wednesday 7:00 AM to 7:00 PM Thursday 7:00 AM to 7:00 PM Friday 7:00 AM to 7:00 PM Saturday 7:00 AM to 5:00 PM Sales Hours Service Hours Parts Hours Sunday 11:00 AM to 6:00 PM Closed Closed Monday 9:00 AM to 9:00 PM 7:00 AM to 7:00 PM 7:00 AM to 7:00 PM Tuesday 9:00 AM to 9:00 PM 7:00 AM to 7:00 PM 7:00 AM to 7:00 PM Wednesday 9:00 AM to 9:00 PM 7:00 AM to 7:00 PM 7:00 AM to 7:00 PM Thursday 9:00 AM to 9:00 PM 7:00 AM to 7:00 PM 7:00 AM to 7:00 PM Friday 9:00 AM to 9:00 PM 7:00 AM to 7:00 PM 7:00 AM to 7:00 PM Saturday 9:00 AM to 9:00 PM 7:00 AM to 5:00 PM 7:00 AM to 5:00 PM Explore the newest deals in Atlanta Dive into Toyota's latest offers in Atlanta and find unbeatable deals tailored just for you.Don't miss out on the opportunity to drive away with your perfect Toyota at an incredible price. Click now to explore our exclusive Atlanta deals! Explore Deals Now Dealer Services Learn about leasing, buying and payment plans. Toyota Certified Used Vehicles Toyota uses a 160-Point Quality Assurance Inspection to make sure we deal in only the best pre-owned vehicles. Once we make sure they deserve the Certified Used Vehicle badge, we back them with a 12-month/12,000-mile limited comprehensive warranty, a 7-year/100,000-mile limited powertrain warranty, and one year of roadside assistance. We're also happy to give you the CARFAX® vehicle history report, as well as offer standard new car financing rates. See Vehicles Toyota Tire Center Your Toyota Assistant Service Manager will be happy to check your air pressure and perform a visual inspection of your tires. If the manager notices any signs of unusual wear, they can assist in repair or replacement. And when you purchase tires from a participating Toyota Tire Center dealer, they're backed by the tire manufacturer's warranty and serviced through your Toyota dealer. President's Award Each year, Toyota Motor Sales recognizes its stellar dealerships with the prestigious President's Award. It's a very high honor a dealership can receive from Toyota, and is only awarded to those dealerships who have demonstrated a commitment to maintaining Toyota's high standards for customer satisfaction. One of Toyota's primary goals is to emphasize the entire ownership experience. We want to help ensure that our customers are satisfied not only at the time of purchase, but as long as they own their vehicle. Offering top quality cars and trucks is, of course, the first step - but only the beginning. Toyota dealerships strive to match the quality of our products with the finest service in the industry. In order to qualify as a President's Award winner, dealerships must excel in each of a series of categories, including Customer Sales Satisfaction and Customer Service Satisfaction. Dealerships which meet the requirements in all categories receive name badge recognition at all dealer events, national recognition in an Automotive News advertisement and, of course, the beautiful President's Award Tiffany crystal trophy.</p>
</div></details><h2 id="toc-204">103. Sparse Transformer</h2>
<ul>
<li>链接：https://www.zhihu.com/tardis/zm/art/643801236</li>
<li>来源：bing</li>
<li>摘要：2024年1月20日 · 高效 Transformer 方法 1. Sparse Transformer 提出背景 Sparse Transformer 的提出动机是基于一个在 CIFAR-10 数据集上，使用一个 128 层 Self-Attention 模型，对注意力模式可视化后 …</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-205">正文（抓取，非 AI）</h3>
<p>背景 论文： Generating Long Sequences with Sparse Transformers （OpenAI, 2019） github： https:// github.com/openai/spars e_attention 在之前的文章 大模型微调实践：ChatGLM-6B全参数微调 中，曾经介绍过全参数微调 ChatGLM-6B 需要的配置。 如果是输入、输出长度为1024，至少需要80G的显存。之所以如此占显存，主要是因为 ChatGLM-6B 模型中使用了 Transformer 的结构，而 Transformer 结构中最占用显存就属 Attention 矩阵。假设输入长度为 ，Transformer 会存储一个 的自注意力矩阵，因此复杂度高达 。 因此 ChatGLM-6B 只支持 1024 的输入长度，就是因为太长的序列会导致自注意力矩阵占用过多的显存，使得模型难以训练。 目前解决这个问题的主要方法有 Sparse Transformer 和 FalshAttention，这篇文章就主要介绍下2019年 OpenAI 提出的 Sparse Transformer。 Sparse Transformers 主要关注了原始 Transformer 的计算复杂度问题，尤其是在面对长序列输入的情况。为此，论文中将 full attention 进行分解，通过多个 sparse attention来 代替，在不牺牲性能的情况下降低复杂度至 。 高效 Transformer 方法 1. Sparse Transformer 提出背景 Sparse Transformer 的提出动机是基于一个在 CIFAR-10 数据集上，使用一个 128 层 Self-Attention 模型，对注意力模式可视化后得到的。如图 1 到图 4 所示，它是一个基于自回归的图像生成模型，图中白色区域是注意力机制的高权值位置，黑色区域是被 mask 掉的像素。图 1 是比较浅层的注意力节点的可视化，它的高权值点集中在当前预测点的附近，也就是说 浅层的注意力更关注当前像素周边的纹理信息 。图 2 是第 19 层自注意力权值的可视化，从图中可以看出，高亮区域接近当前像素点的之前若干个时间片的若干个像素，也就是说 这个自注意力更关注当前预测像素的同行的信息 。图 3 是第 20 层自注意力权值的可视化，它的高亮区域是当前预测像素的同一列，此时 自注意力更关注同列信息 。图 4 是稍微偏后一些的自注意力可视化（36层），从中可以看出自注意力 更关注的是图像的全局信息 。 图1：第6层自注意力权值的可视化 图2：第19层自注意力权值的可视化 图3：第20层自注意力权值的可视化 图4：第36层自注意力权值的可视化 从图 1 到图 4 的可视化结果来看，Transformer 的注意力机制呈现了和卷积模型类似的归纳偏置，即 浅层的网络倾向于学习局部连接的模式，深层的网络倾向于学习全局信息 。我们知道，卷积网络的一个重要特征是稀疏性，那么我们能否将稀疏性引入 Transformer 中呢，这也就是 Sparse Transformer 的提出思想。 2. Sparse Transformer 在介绍 Sparse Transformer 之前，先简单回顾一下 Transformer。 图5：标准 Transformer 位置编码： 计算得到。在Transformer中采用了后者，计算公式如下： Multi-head self-attention 公式： 每个子层的架构： 2.1 普通自注意力 像 GPT 这样的自回归模型，它们使用的是之前所有时间片的内容来预测当前时间片的结果。普通的 Transformer 需要为当前时间片之前所有像素计算一个权值，所以普通 Transformer 的自注意力机制的时间复杂度是 ，在作者的 CIFAR-10 的实验中， 。图 5 是一个普通的 Transformer 以自回归的方式生成大小为 图像的注意力（左）以及连接矩阵（右）的示例。在生成一个图像时，它总共需要计算的权值数量是 ，当我们要预测一个更大的图像时，这个量级的计算量还是非常庞大的。 图6：普通 Transformer 的注意力核和连接矩阵 这里解释下 什么叫注意力核 。 注意力核是指预测当前像素时，使用到的像素位置（也就是图（a）中蓝色区域）。比如图6（a）中，表示的是预测第 28 个像素时的注意力核，浅蓝色表示第 1~27 像素（预测时可以使用的像素），深蓝色表示当前要预测的像素 28。 2.2 分解自注意力（Factorized Self-Attention） 我们知道，普通的自注意力的计算方式可以表示为： ，该计算方式复杂度最高的地方是 的计算，达到了 。而 Sparse Transformer 的核心是只让设置好的像素点参与自注意力的计算。为了实现这个目的，引入了一个名为连接模式（Connectivity Pattern）的变量，它表示为 。其中 表示在预测第 个时间片的索引（indices）。 Transformer 自注意力的计算方式如下： 其中 ， 以及 分别是计算 Query，Key，Value 三个向量用到的权值矩阵。Sparse Transformer 减轻计算量的方式通过让连接模式作用到 上，从而减轻 的复杂度。如式(3)。对于第 个时间片的输入，首先使用 Key 和 Value 的权值矩阵乘以输入特征，得到 和 。然后再将连接模式 作用到 和 上，得到稀疏的特征 和 。然后再使用稀疏特征得到第 个时间片结果 ，如式(2)。最后再将 个时间片的结果合并起来，得到最终的结果 ，如式(1)。 对于完全自注意力（Full self-attention）， ，允许每个元素关注所有先前位置和其自身的位置。 而对于分解自注意力（Factorized Self-Attention）， . 至此，已经定义好了稀疏自注意力机制的通用计算方式，接下来要做的是将第 1 节观察到的不同深度的不同稀疏模式有效地融合到这个计算方式中。对于每个自注意力的计算，可以使用若干个不同的注意力核。在作者的论文中，注意力核的个数是 ，当然也可以扩展到更高的维度。 2.2.1 跨步注意力（Stride Attention） 跨步注意力（Stride Attention）由两种形式的连接模式组成，一种是如图2所示的行注意力的模式，另一种是如图3所示的列注意力的模式。假设步长是 ，行注意力核指的是在连接模式中，当前时间片的前 个时间片可以被使用。列注意力核指的是连接模式中每隔 个时间片的像素被使用。 行注意力核的表达式如式 (4)。对于图片生成这样的任务来说， 一般为图像的宽或者高，在作者的 CIFAR-10 实验中， ，也就是说行注意力核的复杂度是 。 列注意力核的表达式如式(5)，同理，它的复杂度也是 。 行注意力核，列注意力核以及它们的连接矩阵的形式如图7所示。它的生成目标是一个 的图像（ ），当预测第 个时间片的像素时，行注意力核使用到的是第 个像素，列注意力核使用的是第 个像素。 图7：跨步注意力的注意力核以及连接矩阵 2.2.2 固定注意力（Fixed Attention） 固定注意力（Fixed Attention）也是由行注意力和列注意力组成，它的行注意力核是当前预测像素所在行的全部像素，如式(6)。例如图8（a）中，当前预测的时间片 以及步长 ，那么 的值需要满足 ，也就是说 的值可以是 。固定注意力的行注意力核的最大长度是 ，因此它的复杂度是 。 固定注意力的列注意力核表示为式(7)，其中 ， 是一个超参数。在式 (7) 中，我们发现其中并没有变量 ，这也就意味着固定注意力的列注意力核关注的位置与 无关。图7.(b)展示的是 ， ，以及 的情况，此时我们可以得到 ，也就是 可以取的值满足： 且 ，因此可以推出 的值可能为 ，也就是图7.(b)中的浅蓝色区域。固定注意力的列注意力又被叫做滑窗注意力（Sliding Window Attention）。超参数 的作用相当于卷积窗口的大小。在作者的实验中，一组效果比较好的参数是 以及 。列注意力核的关注的像素数约为 ，因此它的复杂度也是 。 图8：固定注意力的注意力核以及连接矩阵 3. 融合多个注意力核 上面我们介绍了多种不同形式的注意力核，那么下面我们将介绍如何将这些不同形式的 注意力核融入到网络中 ，在论文中作者介绍了三种融合的方式： 每个残差块使用不同的注意力核的类型，对于一个深度网络，它是由连续的残差块组成的。对于每个残差块，我们可以使用不同类型的注意力核，表示为式(8)。其中 是当前残差块的缩影， 是注意力核的类别数。 第二个方式是每个注意力头都计算所有类型的注意力核，然后合并他们的结果。 第三个方式是对于多头的注意力机制，每组头选择一个形式的注意力核，然后将它们合并起来，如式(10)。 其中 组不同的注意力核会并行计算，然后在特征维度进行特征拼接。实验结果表明这种方式是最好的融合策略。 参考 Efficient Transformers: A Survey Generating Long Sequences with Sparse Transformers Transformer优化之稀疏注意力 - 知乎 (zhihu.com) SPARSE TRANSFORMER浅析 - 知乎 (zhihu.com) 稀疏Transformer（Sparse Transformer） - 知乎 (zhihu.com) 书籍推荐</p>
</div></details><h2 id="toc-206">104. A Comprehensive Overview of Large Language Models</h2>
<ul>
<li>链接：https://arxiv.org/abs/2307.06435</li>
<li>来源：bing</li>
<li>摘要：2023年7月12日 · Large Language Models (LLMs) have recently demonstrated remarkable capabilities in natural language processing tasks and beyond. This success of LLMs has led to a large influx of …</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-207">正文（抓取，非 AI）</h3>
<p>[2307.06435] A Comprehensive Overview of Large Language Models Computer Science &gt; Computation and Language arXiv:2307.06435 (cs) [Submitted on 12 Jul 2023 ( v1 ), last revised 17 Oct 2024 (this version, v10)] Title: A Comprehensive Overview of Large Language Models Authors: Humza Naveed , Asad Ullah Khan , Shi Qiu , Muhammad Saqib , Saeed Anwar , Muhammad Usman , Naveed Akhtar , Nick Barnes , Ajmal Mian View a PDF of the paper titled A Comprehensive Overview of Large Language Models, by Humza Naveed and 8 other authors View PDF HTML (experimental) Abstract: Large Language Models (LLMs) have recently demonstrated remarkable capabilities in natural language processing tasks and beyond. This success of LLMs has led to a large influx of research contributions in this direction. These works encompass diverse topics such as architectural innovations, better training strategies, context length improvements, fine-tuning, multi-modal LLMs, robotics, datasets, benchmarking, efficiency, and more. With the rapid development of techniques and regular breakthroughs in LLM research, it has become considerably challenging to perceive the bigger picture of the advances in this direction. Considering the rapidly emerging plethora of literature on LLMs, it is imperative that the research community is able to benefit from a concise yet comprehensive overview of the recent developments in this field. This article provides an overview of the existing literature on a broad range of LLM-related concepts. Our self-contained comprehensive overview of LLMs discusses relevant background concepts along with covering the advanced topics at the frontier of research in LLMs. This review article is intended to not only provide a systematic survey but also a quick comprehensive reference for the researchers and practitioners to draw insights from extensive informative summaries of the existing works to advance the LLM research. Subjects: Computation and Language (cs.CL) Cite as: arXiv:2307.06435 [cs.CL] (or arXiv:2307.06435v10 [cs.CL] for this version) https://doi.org/10.48550/arXiv.2307.06435 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Humza Naveed [ view email ] [v1] Wed, 12 Jul 2023 20:01:52 UTC (542 KB) [v2] Fri, 18 Aug 2023 13:53:06 UTC (734 KB) [v3] Wed, 13 Sep 2023 12:13:45 UTC (1,354 KB) [v4] Thu, 5 Oct 2023 10:29:02 UTC (1,369 KB) [v5] Thu, 2 Nov 2023 07:59:50 UTC (1,438 KB) [v6] Thu, 23 Nov 2023 19:23:19 UTC (1,440 KB) [v7] Wed, 27 Dec 2023 10:15:51 UTC (1,481 KB) [v8] Tue, 20 Feb 2024 07:19:41 UTC (1,533 KB) [v9] Tue, 9 Apr 2024 21:38:33 UTC (1,691 KB) [v10] Thu, 17 Oct 2024 01:10:40 UTC (1,090 KB) Full-text links: Access Paper: View a PDF of the paper titled A Comprehensive Overview of Large Language Models, by Humza Naveed and 8 other authors View PDF HTML (experimental) TeX Source view license Current browse context: cs.CL &lt; prev | next &gt; new | recent | 2023-07 Change to browse by: cs References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-208">105. 一文了解Transformer全貌（图解Transformer）</h2>
<ul>
<li>链接：https://www.zhihu.com/tardis/zm/art/600773858</li>
<li>来源：bing</li>
<li>摘要：2025年9月26日 · 5.2 第二个Multi-Head Attention Decoder的第二个Multi-Head Attention变化不大， 主要的区别在于其中Self-Attention的 矩阵不是使用上一个Multi-Head Attention的输出，而是使用 …</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-209">正文（抓取，非 AI）</h3>
<p>打个小广告 ☻，知乎专栏 《大模型前沿应用》 的内容已经收录在新书 《揭秘大模型：从原理到实战》 中。感兴趣的朋友可以购买，多谢支持！♥♥ 自2017年Google推出Transformer以来，基于其架构的语言模型便如雨后春笋般涌现，其中Bert、T5等备受瞩目，而近期风靡全球的大模型ChatGPT和LLaMa更是大放异彩。网络上关于Transformer的解析文章非常大，但本文将力求用浅显易懂的语言，为大家深入解析Transformer的技术内核。 前言 Transformer是谷歌在2017年的论文《Attention Is All You Need》中提出的，用于NLP的各项任务，现在是谷歌云TPU推荐的参考模型。网上有关Transformer原理的介绍很多，在本文中我们将尽量模型简化，让普通读者也能轻松理解。 1. Transformer整体结构 在机器翻译中，Transformer可以将一种语言翻译成另一种语言，如果把Transformer看成一个黑盒，那么其结构如下图所示： 将法语翻译成英语 那么拆开这个黑盒，那么可以看到Transformer由若干个编码器和解码器组成，如下图所示： 继续将Encoder和Decoder拆开，可以看到完整的结构，如下图所示： Transformer整体结构（引自谷歌论文） 可以看到Encoder包含一个Muti-Head Attention模块，是由多个Self-Attention组成，而Decoder包含两个Muti-Head Attention。Muti-Head Attention上方还包括一个 Add &amp; Norm 层，Add 表示残差连接 (Residual Connection) 用于防止网络退化，Norm 表示 Layer Normalization，用于对每一层的激活值进行归一化。 假设我们的输入包含两个单词，我们看一下Transformer的整体结构： Transformer整体结构（输入两个单词的例子） 为了能够对Transformer的流程有个大致的了解，我们举一个简单的例子，还是以之前的为例，将法语"Je suis etudiant"翻译成英文。 第一步 ：获取输入句子的每一个单词的表示向量 ， 由单词的Embedding和单词位置的Embedding 相加得到。 Transformer输入表示 第二步 ：将单词向量矩阵传入Encoder模块，经过N个Encoder后得到句子所有单词的编码信息矩阵 ，如下图。输入句子的单词向量矩阵用 表示，其中 是单词个数， 表示向量的维度（论文中 ）。每一个Encoder输出的矩阵维度与输入完全一致。 输入X经过Encoder输出编码矩阵C 第三步 ：将Encoder输出的编码矩阵 传递到Decoder中，Decoder会根据当前翻译过的单词 翻译下一个单词 ，如下图所示。 Transformer Decoder预测 上图Decoder接收了Encoder的编码矩阵，然后首先输入一个开始符 "<begin>"，预测第一个单词，输出为"I"；然后输入翻译开始符 "<begin>" 和单词 "I"，预测第二个单词，输出为"am"，以此类推。这是Transformer的大致流程，接下来介绍里面各个部分的细节。 2. Transformer的输入表示 Transformer中单词的输入表示由 单词Embedding 和 位置Embedding （Positional Encoding）相加得到。 Transformer输入表示 2.1 单词Embedding 单词的Embedding可以通过Word2vec等模型预训练得到，可以在Transformer中加入Embedding层。 2.2 位置Embedding Transformer 中除了单词的Embedding，还需要使用位置Embedding 表示单词出现在句子中的位置。 因为 Transformer不采用RNN结构，而是使用全局信息，不能利用单词的顺序信息，而这部分信息对于NLP来说非常重要。 所以Transformer中使用位置Embedding保存单词在序列中的相对或绝对位置。 位置Embedding用 表示， 的维度与单词Embedding相同。 可以通过训练得到，也可以使用某种公式计算得到。在Transformer中采用了后者，计算公式如下： 其中， 表示单词在句子中的位置， 表示 的维度。 3. Multi-Head Attention（多头注意力机制） Transformer内部结构 上图是Transformer的内部结构，其中红色方框内为 Multi-Head Attention ，是由多个 Self-Attention 组成，具体结构如下图： Self-Attention和Multi-Head Attention 因为 Self-Attention 是Transformer的重点，所以我们重点关注 Multi-Head Attention 以及 Self-Attention，首先介绍下Self-Attention的内部逻辑。 3.1 Self-Attention结构 Self-Attention结构 上图是Self-Attention结构，最下面是 (查询)、 (键值)、 (值)矩阵，是通过输入矩阵 和权重矩阵 相乘得到的。 Q,K,V的计算 得到 之后就可以计算出Self-Attention的输出，如下图所示： Self-Attention输出 3.2 Multi-Head Attention输出 在上一步，我们已经知道怎么通过Self-Attention计算得到输出矩阵 ，而Multi-Head Attention是由多个Self-Attention组合形成的，下图是论文中Multi-Head Attention的结构图。 Multi-Head Attention 从上图可以看到Multi-Head Attention包含多个Self-Attention层，首先将输入 分别传递到 个不同的Self-Attention中，计算得到 个输出矩阵 。下图是 的情况，此时会得到 8 个输出矩阵 。 多个Self-Attention 得到8个输出矩阵 后，Multi-Head Attention将它们拼接在一起（Concat），然后传入一个Linear层，得到Multi-Head Attention最终的输出矩阵 。 Multi-Head Attention输出 4. 编码器Encoder结构 Transformer Encoder模块 上图红色部分是Transformer的Encoder结构， 表示Encoder的个数，可以看到是由Multi-Head Attention、Add &amp; Norm、Feed Forward、Add &amp; Norm组成的。前面已经介绍了Multi-Head Attention的计算过程，现在了解一下Add &amp; Norm和 Feed Forward部分。 4.1 单个Encoder输出 Add &amp; Norm 是指残差连接后使用LayerNorm，表示如下： 其Sublayer表示经过的变换，比如第一个Add &amp; Norm中Sublayer表示Multi-Head Attention。 Feed Forward 是指全连接层，表示如下： 因此输入矩阵 经过一个Encoder后，输出表示如下： 4.2 多个Encoder输出 通过上面的单个Encoder，输入矩阵 ，最后输出矩阵 。通过多个Encoder叠加，最后便是编码器Encoder的输出。 5. 解码器Decoder结构 Transformer Decoder模块 上图红色部分为Transformer的Decoder结构，与Encoder相似，但是存在一些区别： 包含两个Multi-Head Attention 第一个Multi-Head Attention采用了Masked操作 第二个Multi-Head Attention的 矩阵使用Encoder的 编码信息矩阵 进行计算，而 使用上一个 Decoder的输出计算 最后有一个Softmax层计算下一个翻译单词的概率 5.1 第一个Multi-Head Attention Decoder的第一个Multi-Head Attention采用了Masked操作，因为在翻译的过程中是顺序翻译的，即翻译完第 个单词，才可以翻译第 个单词。通过 Masked 操作可以防止第 个单词知道 个单词之后的信息。下面以法语"Je suis etudiant"翻译成英文"I am a student"为例，了解一下 Masked 操作。 在Decoder的时候，需要根据之前翻译的单词，预测当前最有可能翻译的单词，如下图所示。首先根据输入"<begin>"预测出第一个单词为"I"，然后根据输入"<begin> I" 预测下一个单词 "am"。 Decoder预测（右图有问题，应该是Decoder 1） Decoder在预测第 个输出时，需要将第 之后的单词掩盖住， Mask操作是在Self-Attention的Softmax之前使用的， 下面以前面的"I am a student"为例。 第一步： 是Decoder的输入矩阵和 Mask 矩阵，输入矩阵包含"<begin> I am a student"4个单词的表示向量， Mask 是一个 的矩阵。在 Mask 可以发现单词"<begin>"只能使用单词"<begin>"的信息，而单词"I"可以使用单词"<begin> I"的信息，即只能使用之前的信息。 输入矩阵与Mask矩阵 第二步 ：接下来的操作和之前Encoder中的Self-Attention一样，只是在Softmax之前需要进行Mask操作。 Mask Self-Attention输出 第三步 ：通过上述步骤就可以得到一个Mask Self-Attention的输出矩阵 ，然后和Encoder类似，通过Multi-Head Attention拼接多个输出 然后计算得到第一个Multi-Head Attention的输出 ， 与输入 维度一样。 5.2 第二个Multi-Head Attention Decoder的第二个Multi-Head Attention变化不大， 主要的区别在于其中Self-Attention的 矩阵不是使用上一个Multi-Head Attention的输出，而是使用 Encoder的编码信息矩阵 计算的。根据Encoder的输出 计算得到 ，根据上一个Multi-Head Attention的输出 计算 。这样做的好处是在Decoder的时候，每一位单词（这里是指"I am a student"）都可以利用到Encoder所有单词的信息（这里是指"Je suis etudiant"）。 6. Softmax预测输出 Softmax预测输出 编码器Decoder最后的部分是利用 Softmax 预测下一个单词，在Softmax之前，会经过Linear变换，将维度转换为词表的个数。 假设我们的词表只有6个单词，表示如下： 词表 因此，最后的输出可以表示如下： Softmax预测输出示例 总结 Transformer由于可并行、效果好等特点，如今已经成为机器翻译、特征抽取等任务的基础模块，目前ChatGPT特征抽取的模块用的就是Transformer，这对于后面理解ChatGPT的原理做了好的铺垫。 代码实现 绝密伏击：OPenAI ChatGPT（一）：Tensorflow实现Transformer 参考 初识CV：Transformer模型详解（图解最完整版） 数据汪：BERT大火却不懂Transformer？读这一篇就够了 The Illustrated Transformer 忆臻：搞懂Transformer结构，看这篇PyTorch实现就够了（上） The Annotated Transformer https:// arxiv.org/pdf/1706.0376 2.pdf 青空栀浅：图解Transformer Ph0en1x：Transformer结构及其应用详解--GPT、BERT、MT-DNN、GPT-2 大师兄：ChatGPT/InstructGPT详解 张俊林：ChatGPT会取代搜索引擎吗 张俊林：放弃幻想，全面拥抱Transformer：自然语言处理三大特征抽取器（CNN/RNN/TF）比较</begin></begin></begin></begin></begin></begin></begin></begin></p>
</div></details><h2 id="toc-210">106. GPU编程21：Nsight Compute (1) Kernel Profiling Guide</h2>
<ul>
<li>链接：https://www.zhihu.com/tardis/bd/art/707016646</li>
<li>来源：bing</li>
<li>摘要：2024年7月8日 · Nsight Compute支持通过以固定间隔定期对GPU的性能监视器进行采样来收集许多指标。 生成的度量被实例化，每个样本由其值和采集时的GPU时间戳组成。 这使该工具能够在时间线上 …</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-211">正文（抓取，非 AI）</h3>
<p>1 Introduction 本指南介绍了与NVIDIA Nsight Compute和NVIDIA Nsight Compute CLI相关的各种profiling主题，其中大部分适用于该工具的UI和CLI版本。 在常规执行期间，用户启动的CUDA应用程序进程直接与CUDA user-mode驱动通信，并可能与CUDA运行库通信。 使用NVIDIA Nsight Compute分析程序时，在主机系统上启动Nsight Compute前端，然后将实际应用程序作为目标系统上的新进程启动。该工具将其测量库插入到应用程序进程中，从而探查CUDA user-mode驱动通信。当检测到内核启动时，库可以从GPU收集所请求的性能指标，然后将结果传输回前端。 ubuntu 输入ncu-ui打开Nsight Compute。 2 Metric Collection 收集性能指标是Nsight Compute的关键功能。由于有一个庞大的可用metrics列表，因此通常更容易使用工具的一些预定义 sets 或sections 来收集常用子集。用户可以根据需要自由调整内核收集metrics，但重要的是要记住与数据收集相关的开销。 2.1 sets 或sections Nsight Compute使用Section Sets在高的级别上收集的metrics数量。每个set包括一个或多个section，每个section指定几个逻辑相关的度量。例如，一个section可能仅包括高级SM和存储器利用率度量，而另一section可能包括与存储器单元或HW调度器相关联的度量。 section指定的metrics数量和类型对分析(profiling)期间的开销有很大影响。为了能够在快速、粗略的概要文件和较慢、更全面的分析之间快速选择，可以选择相应的部分section。 默认收较少的metrics，主要包括 高级利用率信息 以及 静态启动和占用数据 ，后两个 无需重播内核启动 一直可用。没有命令行设置--set、--section和no-metrics选项时，收集基本set；当设置--set full选项时收集完整的section集。 2.2 sections和规则 2.3 Replay 根据要收集的metrics，内核可能需要重播(replay)一次或多次，因为并非所有metrics都可以一次完成收集。例如，GPU能够同时收集的硬件性能计数器metrics数量是有限的。此外，基于补丁的软件性能计数器可能会对内核运行时产生很大影响，并会使HW计数器的结果发生偏差。 2.3.1 Kernel Replay Nsight Compute中为 特定内核实例 请求的所有metrics都被分组为一个或多个过程。对于第一次通过，将保存内核可以访问的所有GPU内存。在第一次通过之后，确定由内核写入的内存的子集。在每次回放之前（第一次除外），该子集将恢复到其原始位置，以使内核在每次回放中访问相同的内存内容。 Execution with Kernel Replay. All memory is saved, and memory written by the kernel is restored in-between replay passes 2.3.2 Application Replay Nsight Compute中为 特定内核 启动请求的所有指标都被分组为一个或多个过程。与Kernel Replay不同，整个应用程序运行多次，因此在每次运行中，每个内核都可以收集其中一个过程。 Execution with Application Replay. No memory is saved or restored, but the cost of running the application itself is duplicated 2.3.3 Range Replay Nsight Compute中 所有请求 的指标都被分组为一个或多个过程。与Kernel Replay和Application Replay不同，Range Replay捕获并回放所分析的应用程序中CUDA API调用和内核启动的完整范围。度量不与单个内核相关联，而是与整个范围相关联。这允许该工具在不进行序列化的情况下执行内核，从而支持对出于正确性或性能原因而应同时运行的内核进行分析。 Execution with Range Replay. An entire range of API calls and kernel launches is captured and replayed. Host and device memory is saved and restored as necessary 2.3.4 Application Range Replay Nsight Compute中所有请求的指标都被分组为一个或多个过程。与Range Replay类似，度量不与单个内核相关联，而是与 整个选定范围 相关联。这允许该工具在不序列化的情况下执行工作负载（内核、CUDA图…），从而支持分析出于正确性或性能原因必须同时运行的工作负载。 Execution with Application Range Replay. An range of workloads is replayed by re-running the entire application without modifying interactions or saving and restoring memory. 2.3 Overhead Number and type of collected metrics The collected section set Number of profiled kernels GPU Architecture 3 Metrics Guide 3.1 Hardware Model 3.2 Metrics Structure 3.3 Metrics Decoder 3.4 Range and Precision 4 Metrics Reference Nsight Compute中的大多数指标都可以使用ncu命令行的–query metrics选项进行查询。以下度量可以显式收集，但不由-query-metrics列出，并且不遵循度量结构中的命名方案，应该按原样使用。 4.1 Launch Metrics launch__<em>指标是在每次内核启动时收集的，不需要额外的重播。它们可以作为内核启动参数的一部分（如网格大小、块大小…），也可以使用CUDA占用计算器计算。 4.2 NVLink Topology Metrics 4.3 NUMA Topology Metrics 4.4 Device Attributes device__attribute_</em>度量表示CUDA设备属性。收集它们不需要额外的内核重放，因为它们的值可以从每个CUDA设备的CUDA驱动程序中获得。 4.5 Warp Stall Reasons 使用warp调度程序状态采样收集。无论调度程序是否在同一周期中发出指令，它们都会递增。这些度量具有从函数地址（uint64）映射到样本数量（uint六十四）的实例值。 smsp__pcsamp_warps_issue_stalled_barrier： warp在CTA障碍处等待同级warp时陷入停滞。在屏障处等待的大量warp通常由屏障之前的代码路径分支引起。这会导致一些warp等待很长时间，直到其他warp到达同步点。只要可能，尽量将工作划分为统一的工作负载块。如果块大小为512个线程或更大，请考虑将其拆分为更小的组。这可以在不影响占用率的情况下增加符合条件的warp，除非共享内存成为新的占用限制。此外，请尝试确定哪条屏障指令会导致最多的暂停，并首先优化在该同步点之前执行的代码。 4.6 Warp Stall Reasons (Not Issued) 使用warp调度程序状态采样收集。它们 仅在 warp调度程序 未发出指令的周期内递增 。这些度量具有从函数地址（uint64）映射到样本数量（uint六十四）的实例值。 5 Sampling NVIDIA Nsight Compute可以通过固定间隔的采样来收集某些性能数据。 5.1 PM Sampling Nsight Compute支持通过以固定间隔定期对GPU的性能监视器进行采样来收集许多指标。生成的度量被实例化，每个样本由其值和采集时的GPU时间戳组成。这使该工具能够在时间线上可视化数据，帮助了解所分析的工作负载的行为在其运行时是如何变化的。 5.2 Warp Sampling Nsight Compute支持warp程序计数器和warp调度程序状态的定期采样。在固定的周期间隔下，每个流式多处理器中的采样器选择一个活动warp，并输出程序计数器和warp调度器状态。该工具为设备选择最小间隔。在小型设备上，这可以是每32个周期。在具有更多多处理器的较大芯片上，这可能是2048个周期。采样器选择一个随机活动warp。在同一周期上，调度器可以选择不同的warp来发布。 6 Reproducibility Serialization Clock Control Cache Control Persistence Mode 7 Special Configurations：Multi Instance GPU 多实例GPU（MIG）允许将GPU划分为多个CUDA设备的功能。分区分为两个级别： 首先，一个GPU可以拆分为一个或多个GPU实例。每个GPU实例都拥有一个或多个SM、整个GPU存储器的子集以及可能的其他GPU资源的所有权 其次，每个GPU实例可以进一步划分为一个或多个计算实例。每个计算实例对其分配的GPU实例的SM具有独占所有权。但是，GPU实例中的所有计算实例共享GPU实例的内存和内存带宽。每个计算实例都作为具有唯一设备ID的CUDA设备运行。 有关如何配置MIG实例的更多信息，请参阅驱动程序发行说明以及nvidia-smi CLI工具的文档。 8 Roofline Charts 屋顶线图提供了一种非常有用的方式来可视化复杂处理单元（如GPU）上实现的性能。本节介绍在纵断面报告中显示的屋顶线图表。 内核性能不仅取决于GPU的操作速度，也取决于GPU向内核提供数据的速率。一个典型的屋顶线图将GPU的 峰值性能 和 内存带宽 与一个名为 算术强度 （ 工作 和 内存流量 之间的比率）的指标组合成一个图表，以更真实地表示评测内核所实现的性能： 9 Memory Chart 内存图表显示GPU上下内存子单元的性能数据的图形逻辑表示。性能数据包括传输大小、命中率、指令或请求的数量等。 10 Memory Tables 内存表显示各种内存硬件单元的详细指标，如共享内存、缓存和设备内存。大多数表条目，可以将鼠标悬停在其上以查看基本度量名称和描述。一些条目是作为其他单元格的导数生成的，它们本身不显示度量名称，而是显示各自的计算。如果某个度量对一般导数计算没有贡献，则在工具提示中显示为“未使用”。 10.1 Shared Memory Example Shared Memory table, collected on an RTX 2080 Ti 10.2. L1/TEX Cache Example L1/TEX Cache memory table, collected on an RTX 2080 Ti Model of the Global Load Pipeline for the L1TEX cache on GA100, mapped to the memory table 10.3. L2 Cache Example L2 Cache memory table, collected on an RTX 2080 Ti Model of the L2 cache on GA100, mapped to the memory table 10.4. L2 Cache Eviction Policies Example L2 Cache Eviction Policies memory table, collected on an A100 GPU 10.5. Device Memory Example Device Memory table, collected on an RTX 2080 Ti 11 小结 本文简要记录了内核分析文档的主要内容，包括内核性能参数记录的主要机制，主要的内核性能参数和数据报告的阅读。内容细节很多，关键是知道整体的结构，遇到不清楚的参数到指定地方查阅。 参考： User Guide - nsight-systems 2024.4 documentation Kernel Profiling Guide</p>
</div></details><h2 id="toc-212">107. Toyota Car Inventory in Atlanta</h2>
<ul>
<li>链接：https://www.toyota.com/dealers/georgia/atlanta/inventory/</li>
<li>来源：bing</li>
<li>摘要：Browse Toyota car inventory in Atlanta available at a Toyota Dealership near you. Discover new Toyota sedans inventory in Atlanta and discover the perfect Toyota vehicle in stock near you. What’s more, …</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-213">正文（抓取，非 AI）</h3>
<p>Search Local Toyota Car Inventory in Atlanta | Cars in Stock in Atlanta, Georgia Dealer Hub Georgia Atlanta Inventory Toyota Car Inventory in Atlanta Do the heavy lifting before you call your community Toyota dealers. Watch Toyota cars for sale at Atlanta by browsing our full line up, online. Search Toyota car inventory in Atlanta, GA to find out precisely what your neighborhood Toyota dealers have available before you ever step on the lot. Regional Toyota dealers help you plan your next car-buying adventure using your Atlanta Toyota car inventory buying tool. Search for new vehicle deals near you. Plus, have all the information on our whole inventory of Toyota cars all in one place. Our high-quality used Toyota inventory in your area is stocked with heaps of pre-owned Toyota cars, trucks, SUVs, and more. Looking for your next car? Still deciding between buying new or pre-owned? Make sure you make the most informed decision by surfing through close-by pre-owned Toyota vehicles for sale with our easy Toyota inventory search. You know exactly what vehicle you want? We would like to assist you find it at your local dealer. Rely on our close-by Toyota vehicle inventory tool to help match with the right Toyota for your style. Search the deals, trims, and available choices on brand new Toyota cars available for sale in Atlanta, GA using the web-based Toyota dealership inventory shopping tool. Whether you're looking to make a change, instantly find the latest Atlanta, Georgia Toyota dealership inventory to discover the right Toyota for your latest update. Search before your purchase with your new Toyota car inventory search in Atlanta. Whether you're interested in the past year's best-selling coupe or need to determine whether this season's new SUV is stocked, discover the Toyota for purchase with ease. Just use our Toyota dealer inventory search. Find all the newest trims, bundles, accessories and more stocked in your neighborhood by browsing through Toyota dealership inventory in Atlanta. Read More Read Less Cars &amp; Minivan Hybrid EV $49,390 as shown 2026 Toyota Crown Signia $44,490 Starting MSRP 39/37 Up to Est. MPG Inventory View Inventory Hybrid EV $55,965 as shown 2026 Toyota Crown $41,440 Starting MSRP 42/41 Up to Est. MPG Inventory View Inventory Hybrid EV $36,965 as shown 2026 Prius $28,550 Starting MSRP 57/56 Up to Est. MPG Inventory View Inventory Plug-in Hybrid EV $40,470 as shown 2026 Prius Plug-in Hybrid $33,775 Starting MSRP 52/127 Up to Est. MPG / MPGe Inventory View Inventory Hybrid EV $34,600 as shown 2026 Camry $29,100 Starting MSRP 52/49 Up to Est. MPG Inventory View Inventory Hybrid EV Available $28,640 as shown 2026 Corolla $22,925 Starting MSRP 32/41 Up to Est. MPG Inventory View Inventory Hybrid EV $28,615 as shown 2026 Corolla Hybrid $24,775 Starting MSRP 53/46 Up to Est. MPG Inventory View Inventory $26,980 as shown 2026 Corolla Hatchback $24,380 Starting MSRP 32/41 Up to Est. MPG Inventory View Inventory $47,965 as shown 2026 GR Corolla $39,920 Starting MSRP 21/28 Up to Est. MPG Inventory View Inventory $69,350 as shown 2026 GR Supra $58,300 Starting MSRP 22/29 Up to Est. MPG Inventory View Inventory Hybrid EV $48,820 as shown 2026 Sienna $40,420 Starting MSRP 36/36 Up to Est. MPG Inventory View Inventory Trucks Hybrid EV Available $43,315 as shown 2026 Tacoma $32,245 Starting MSRP 21/26 Up to Est. MPG Inventory View Inventory Hybrid EV Available $51,510 as shown 2026 Tundra $41,260 Starting MSRP 18/23 Up to Est. MPG Inventory View Inventory Hybrid EV $72,565 as shown 2026 Tundra i-FORCE MAX $58,560 Starting MSRP 20/24 Up to Est. MPG Inventory View Inventory Crossovers &amp; SUVs Hybrid EV Available $56,185 as shown 2026 4Runner $41,870 Starting MSRP 20/26 Up to Est. MPG Inventory View Inventory Battery EV $43,775 as shown 2026 bZ $34,900 Starting MSRP 314 miles Up to Est. Range Inventory View Inventory Hybrid EV Available $30,455 as shown 2026 Corolla Cross $25,035 Starting MSRP 31/33 Up to Est. MPG Inventory View Inventory Hybrid EV $33,930 as shown 2026 Corolla Cross Hybrid $29,395 Starting MSRP 46/39 Up to Est. MPG Inventory View Inventory Hybrid EV Available $48,115 as shown 2026 Highlander $45,570 Starting MSRP 21/28 Up to Est. MPG Inventory View Inventory Hybrid EV $54,975 as shown 2026 Highlander Hybrid $47,320 Starting MSRP 35/35 Up to Est. MPG Inventory View Inventory Hybrid EV Available $55,320 as shown 2026 Grand Highlander $41,660 Starting MSRP 21/28 Up to Est. MPG Inventory View Inventory Hybrid EV $53,490 as shown 2026 Grand Highlander Hybrid $45,010 Starting MSRP 37/34 Up to Est. MPG Inventory View Inventory Hybrid EV $43,300 as shown 2026 RAV4 $31,900 Starting MSRP 44/38 Up to Est. MPG Inventory View Inventory Hybrid EV $36,070 as shown 2025 RAV4 Hybrid $32,850 Starting MSRP 41/38 Up to Est. MPG Inventory View Inventory Hybrid EV $82,125 as shown 2026 Sequoia $64,825 Starting MSRP 21/24 Up to Est. MPG Inventory View Inventory Hybrid EV $40,405 as shown 2024 Venza $35,070 Starting MSRP 40/37 Up to Est. MPG Inventory View Inventory Electrified Hybrid EV $55,965 as shown 2026 Toyota Crown $41,440 Starting MSRP 42/41 Up to Est. MPG Inventory View Inventory Hybrid EV $36,965 as shown 2026 Prius $28,550 Starting MSRP 57/56 Up to Est. MPG Inventory View Inventory Plug-in Hybrid EV $40,470 as shown 2026 Prius Plug-in Hybrid $33,775 Starting MSRP 52/127 Up to Est. MPG / MPGe Inventory View Inventory Battery EV $43,775 as shown 2026 bZ $34,900 Starting MSRP 314 miles Up to Est. Range Inventory View Inventory Hybrid EV $28,615 as shown 2026 Corolla Hybrid $24,775 Starting MSRP 53/46 Up to Est. MPG Inventory View Inventory Hybrid EV $33,930 as shown 2026 Corolla Cross Hybrid $29,395 Starting MSRP 46/39 Up to Est. MPG Inventory View Inventory Hybrid EV $54,975 as shown 2026 Highlander Hybrid $47,320 Starting MSRP 35/35 Up to Est. MPG Inventory View Inventory Hybrid EV $53,490 as shown 2026 Grand Highlander Hybrid $45,010 Starting MSRP 37/34 Up to Est. MPG Inventory View Inventory Hybrid EV $36,070 as shown 2025 RAV4 Hybrid $32,850 Starting MSRP 41/38 Up to Est. MPG Inventory View Inventory Hybrid EV $82,125 as shown 2026 Sequoia $64,825 Starting MSRP 21/24 Up to Est. MPG Inventory View Inventory Hybrid EV $48,820 as shown 2026 Sienna $40,420 Starting MSRP 36/36 Up to Est. MPG Inventory View Inventory Hybrid EV $72,565 as shown 2026 Tundra i-FORCE MAX $58,560 Starting MSRP 20/24 Up to Est. MPG Inventory View Inventory Hybrid EV $40,405 as shown 2024 Venza $35,070 Starting MSRP 40/37 Up to Est. MPG Inventory View Inventory</p>
</div></details><h2 id="toc-214">108. 什么是大语言模型 (LLM)？| 大语言模型（LLM）解析：企业 ...</h2>
<ul>
<li>链接：https://www.ibm.com/cn-zh/think/topics/large-language-models</li>
<li>来源：bing</li>
<li>摘要：3 天之前 · 大语言模型（LLM）能够通过处理海量文本数据来理解和生成人类语言的 AI 系统。了解大语言模型（LLM）在自然语言处理、AI 自动化与企业场景中的应用与发展。IBM 可提供企业级大语言模型 …</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-215">正文（抓取，非 AI）</h3>
<p>什么是大语言模型 (LLM)？| 大语言模型（LLM）解析：企业级生成式 AI 应用趋势| IBM 什么是大型语言模型 (LLM)？ 作者 Cole Stryker Staff Editor, AI Models IBM Think 什么是 LLM？ 大语言模型 (LLM) 是一类基础模型，经过大量数据训练，使其能够理解和生成自然语言和其他类型的内容，以执行各种任务。大语言模型（LLM）是当前人工智能研究与企业级 AI 应用的核心技术之一。无论是在自然语言处理、知识问答，还是在多轮对话与生成式内容创建中，大语言模型都展现出强大的语义理解和生成能力，正在被越来越多的企业用于推动 AI 转型与自动化升级。 LLM 就像一台巨大的统计预测机，可以重复预测序列中的下一个单词。它们学习文本中的模式，并生成遵循这些模式的语言。 LLM 实现了人机交互方式的重大飞跃，因其是首个能大规模处理非结构化人类语言的 AI 系统，实现了与机器的自然交流。传统搜索引擎和其他编程系统使用算法匹配关键词，而 LLM 能捕捉更深层的语境、细微差别和推理逻辑。LLM 经过训练，能适配涉及文本解析的多种应用场景，如总结文章、调试代码或起草法律条款。当具备智能体能力时，LLM 可不同程度地自主执行原需人工完成的各种任务。 LLM 是数十年 自然语言处理 (NLP) 与机器学习研究进展的集大成者，其发展直接推动了 2010 年代末至 2020 年代 人工智能 的爆发式进步。流行的 LLM 已成为家喻户晓的名词，使 生成式 AI 成为公众关注的焦点。LLM 在企业中也获广泛应用，各组织在众多业务职能和用例中投入巨大资源。 公众可通过多种接口便捷使用 LLM，包括Anthropic 的 Claude 、OpenAI 的 ChatGPT 、Microsoft 的 Copilot、Meta 的 Llama 系列 ，Google 的 Gemini 助手及其 BERT、PaLM 模型。IBM 在 watsonx.ai 上维护 Granite 模型系列 。该系列已成为 watsonx Assistant 和 watsonx Orchestrate 等其他 IBM 产品的生成式 AI 支柱。 大语言模型 (LLM) 的工作原理 训练始于海量数据，它们来自书籍、文章、网站、代码等文本源的数十亿甚至数万亿词汇。数据科学家负责清理和预处理工作，以消除错误、重复及不良内容。 在“ 词元化 ”过程中，文本被分解为更小的机器可读单元，称为“词元”。词元可以是单词、子词或字符等较小单位。此举实现了语言标准化，使生僻词和新颖词汇也能被一致处理。 LLM 初始训练采用 自监督学习 ，这是一种使用未标记数据进行 监督学习 的 机器学习 技术。自监督学习不需要标记数据集，但与监督学习密切相关，因为它根据“基本事实”优化性能。在自监督学习中，任务的设计使得可以从未标记的数据中推断出“基本事实”。模型不再像监督学习那样被告知每个输入的"正确答案"，而是自行探索数据中的模式、结构或关联。 自注意力 模型通过 转换器 网络传递词元。转换器模型于 2017 年推出，其价值在于 自注意力机制 允许在不同时刻“关注”不同词元。这项技术是转换器的核心和主要创新点。自注意力机制之所以有用，部分原因在于它允许 AI 模型 计算词元之间的关系和依赖性，特别是文本中彼此远离的词元之间的关系和依赖性。转换器架构还支持并行化处理，效率远超早期方法。这些特性使得 LLM 能够处理前所未有的庞大 数据集 。 文本被拆分为词元后，每个词元被映射为称为 嵌入向量 的数字序列。神经网络由多层人工神经元构成，每个神经元执行数学运算。转换器由其中许多层组成，每层都会微调嵌入向量，使其逐层转化为更丰富的语境表征。 此过程的目标是让模型学习词汇间的语义关联，例如在关于狗的文章中，“吠叫”与“狗”在向量空间中的距离应比“吠叫”与“树”更近，这是基于文中与狗相关的周边词汇。转换器还添加了 位置编码 ，为每个词元提供其在序列中的位置信息。 为了计算注意力，每个嵌入都使用学习到的权重矩阵投射到三个不同的向量中：查询向量、键向量和值向量。查询向量表征特定词元的“搜索意图”，键向量表征每个词元包含的信息，值向量则根据相应注意力权重缩放后"返回"每个键向量的信息。 随后通过计算查询向量与键向量的相似度得出对齐分数。这些分数经归一化为注意力权重后，决定每个值向量有多少信息流入当前词元的表征。该过程允许模型灵活地关注相关语境，同时忽略不太重要的标记（如“树”）。 因此， 自注意力机制 能够比早期架构更有效地在所有词元之间建立“加权”连接。该模型为词元之间的每种关系赋予权重。LLM 可以有数十亿或数万亿个这样的权重，这些权重是 LLM 参数 的一种类型，是机器学习模型中控制数据处理和预测方式的内部配置变量。参数数量指模型中此类变量的总数，部分 LLM 包含数百亿参数。所谓 小型语言模型 规模和范围较小，参数相对较少，适用于在小型设备或资源受限环境中部署。 在训练期间，该模型对从 训练数据 中提取的数百万个示例进行预测，并且 损失函数 会对每个预测的误差进行量化。通过进行预测，然后通过 反向传播算法 和 梯度下降 更新模型权重的迭代循环，模型“学习”生成查询、键和值向量的层级权重。 一旦这些权重得到充分优化，模型就能接收任何词元的原始 嵌入 ，并为其生成查询向量、键向量和值向量。当这些向量与为所有其他词元生成的向量交互时，将生成“更好”的对齐分数，进而生成注意力权重，帮助模型生成更好的输出。最终得到的结果是学习了语法规则、事实知识、推理结构、写作风格等模式的模型。 微调大型语言模型 训练后（或在额外训练的"预训练"背景下），可通过微调使 LLM 在特定场景中更实用。例如，在通用知识大数据集上训练的基础模型，可基于法律问答语料微调，从而创建一个用于法律领域的 聊天机器人 。 以下是一些最常见的微调方式。从业者可以使用一种方法或多种方法的组合。 监督微调 微调通常是在有监督的情况下进行，使用的标记数据集要小得多。模型会更新其权重，以更好地匹配新的基本事实（在本例中为标记数据）。 预训练旨在赋予模型广泛通用知识，而微调使通用模型适配摘要、分类或客服等具体任务。这些 功能适配 代表了新型任务类型。监督微调产生的输出更接近人工提供的示例，所需资源远少于从头训练。 监督微调也适用于 特定于域的定制 ，例如在医疗文档上训练模型，使其能够回答医疗保健相关的问题。 根据人类反馈进行强化学习 为进一步完善模型，数据科学家经常使用 基于人类反馈的强化学习 (RLHF)，这是一种微调形式，即人类对模型输出进行排序，模型经过训练后会偏好人类排序较高的输出。RLHF 常用于对齐过程，使 LLM 输出实用、安全且符合人类价值观。 RLHF 在 风格对齐 方面尤为有效，可调整 LLM，以更随意、幽默或符合品牌调性的方式回应。风格对齐涉及对同类任务进行训练，但以特定风格生成输出。 推理模型 纯监督微调教会模型模仿示例，但未必促进涉及抽象多步过程的更好推理。此类任务并不总是有丰富的标记数据，因此 强化学习 通常用于创建 推理模型 ，即经过微调的 LLM，能在生成最终输出前将复杂问题分解为多个步骤，通常称为“推理跟踪”。训练模型方法越来越先进，使模型具备了思维链推理和其他多步骤决策策略。 指令调整 LLM 定制 的另一种形式是 指令调整 ，该过程专门设计用于提升模型遵循人类指令的能力。指令数据集中的输入样本完全由类似于用户可能在提示中提出的请求的任务组成；输出则展示了对这些请求的理想响应。由于预训练 LLM 本质上并未针对遵循指令或会话目标进行优化，指令调整用于更好地使模型与用户意图保持一致。 使用大型语言模型 大型语言模型经过训练后，其工作原理是：首先对提示进行分词，将其转换为嵌入向量，然后使用转换器逐词元生成文本，计算所有潜在后续词元的概率，输出最可能选项。这个过程称为 推理 ，一直重复到输出完成。模型并非预先“知道”最终答案；它运用训练中学到的所有统计关联逐词元预测，每次预测一个词元，为每一步做出最合理的猜测。 从通用 LLM 获取特定领域知识的最简单、最快捷的方法是通过 提示工程 ，这不需要额外的训练。用户可以通过各种方式修改提示。例如，“以训练有素的医疗专业人士口吻回答”的提示可能产生更相关结果（注意：不推荐使用 LLM 获取医疗建议！）。 LLM 还通过其他策略控制输出，如 LLM 温度 参数控制推理期间生成文本的随机性，或 top-k/top-p 采样将候选词元限制为最可能选项，平衡创造力与连贯性。 上下文窗口 是模型生成文本时能一次性“看到”并使用的最大词元数。早期 LLM 窗口较短，但新一代 LLM 具备数十万词元的上下文窗口，支持整篇研究论文摘要、大型代码库辅助编程、与用户长时间连续对话等用例。 检索增强生成 (RAG) 是一种将 预训练模型 与外部知识库连接起来的方法，使它们能够以更高的准确性提供更相关的响应。所检索的信息会传递到模型的上下文窗口中，使模型生成响应时可直接利用，无需重新训练。例如，通过将 LLM 连接至动态天气服务数据库，LLM 可为用户检索当日天气预报信息。 AI 学院 为什么说基础模型是 AI 的范式转变 了解灵活、可重复使用的一类全新 AI 模型，这些模型可以带来新收入、降低成本并提高工作效率。还可以参阅我们的指南手册，深入了解这些模型。 转到视频集 部署 LLM 从零开始构建 LLM 是一个复杂且资源密集型的过程。最流行的 LLM 是海量数据、 GPU 、能源和人类专业知识的结果，因此大多数 LLM 都是由拥有雄厚资源的大型科技公司构建和维护。 不过，所有开发人可通过 API 使用大多数模型。开发人可以使用 预训练模型 来构建聊天机器人、知识检索系统、 自动化 工具等。为更好控制数据与定制化，许多开源模型可本地或云端部署。Github、 Hugging Face 、Kaggle 和其他平台让所有人都能参与 AI 开发。 开发人员可以将 LLM 作为各种 AI 应用的基础。AI 领域最令人兴奋的发展之一是 智能体系统 。AI 智能体不仅会思考，还会行动。LLM 本身只是根据上下文生成文本，但通过与内存、 API 、决策逻辑和其他外部系统集成，可以执行预订航班或自动驾驶等具体任务。 大型语言模型用例 LLM 正在重新定义业务流程，其跨行业多场景的通用性已得到验证。 文本生成 ： LLM 可以执行各种内容创建任务，例如根据提示起草电子邮件、博客文章或法律备忘录。 文本摘要 ：LLM 能够将长篇文章、新闻报道、研究报告、公司文档和客户历史记录，提炼成符合目标输出格式与风格的精简文本。 AI 助手： 由会话式 AI 提供支持的聊天机器人，可作为集成化实时客户服务解决方案的一部分，执行问答任务并提供详细信息。 代码生成 ：代码辅助平台帮助开发人员构建应用程序，查找代码中的错误并发现多种编程语言中的安全问题，甚至在它们之间进行“翻译”。 情感分析 ：分析客户语气，以便更好地了解大规模客户反馈。 语言翻译 ：自动翻译工具通过流畅的翻译和多语言功能，为各语言和地域的组织提供更广泛的覆盖范围。 推理： LLM 可以解决数学问题、规划多步骤流程以及用更简单的术语解释复杂的概念。 评估 LLM LLM 虽是强大工具，但存在若干局限。首要问题是准确性。在产生 幻觉 时，模型会生成看似合理实则错误或误导性的信息。LLM 也可能反映和放大其训练数据中存在的 偏见 ，生成不公正或冒犯性内容。此外，LLM 资源需求巨大：训练和运行LLM需要大量算力与能源，引发成本与环境担忧。 从业者可以通过全面的 人工智能治理 来减轻LLM的这些负面影响，即帮助确保人工智能系统和工具的安全性和合乎道德性的流程、标准和保障措施。治理的关键部分之一是根据基准评估模型。 LLM 基准测试 提供量化评分，便于模型比较。由于 LLM 是能够执行各种任务的通用系统，其评估需涵盖多个维度而非单一基准。研究人员和从业者会考量准确性、效率、安全性、公平性和稳健性等特质来判断模型性能。 LLM 还需进行对齐性与安全性评估，例如采用红队测试,评估者故意诱导模型生成不安全或偏见响应以暴露缺陷。公平性和偏见评估可以帮助从业者防止 LLM 重现有害的刻板印象或错误信息。 LLM 通常还根据效率进行评估。速度、能耗、词元吞吐量、内存占用量以及处理长上下文窗口的能力是用于评估 LLM 获得输出效率的一些常见指标。 LLM 发展简史 LLM 的历史可以追溯到计算和自然语言处理的早期，当时研究人员使用基于规则的系统和统计方法对文本进行建模。这些早期方法能捕捉局部词汇模式，但无法理解长距离依赖或深层语义。 2010 年代，神经网络兴起带来了重大转折，Word2Vec 和 GloVe 等词嵌入技术将词汇表示为连续空间中的向量，使模型能够学习语义关系。循环神经网络 (RNN) 和长短期记忆 (LSTM) 网络等序列模型的出现更好地处理了序列数据。 2017 年，Vaswani 等人 在具有里程碑意义的论文《Attention is All You Need》中引入了 编码器-解码器 转换器架构。[1]转换器使大数据集训练模型成为可能，标志着现代 LLM 时代的开启。Google 的 BERT（2018 年）是一种纯编码器转换器，展示了转换器在理解语言方面的力量，而 OpenAI 的生成式预训练转换器 (GPT) 系列基于纯解码器变体，证明了互联网规模文本的生成式预训练能实现流畅语言生成。同期，编码器-解码器模型（如 Google 的 T5 和 Facebook 的 BART）展示了完整的序列到序列设计在翻译和摘要等任务中的优势。GPT-2 (2019) 因其生成连贯段落的能力而备受关注，而拥有 1750 亿参数的 GPT-3（2020 年）则确立了 LLM 在 AI 领域的变革性地位。 此外，新的架构也在挑战转换器在 LLM 中的受欢迎程度。 Mamba 利用状态空间模型对的工作进行建模，该模型具有选择性更新功能，可有效过滤和组合过去的信息，从而捕捉到长距离的依赖关系。 扩散 LLM 从随机噪声开始，在学习模型的指导下逐步对其进行降噪，直到出现连贯的文本。这两种架构的效率都比转换器高得多。 复制链接 电子书 如何选择合适的 AI 基础模型 如何选择正确的方法来准备数据集和使用 AI 模型？ 如何使用模型选择框架来平衡性能要求？ 阅读电子书 资源 AI 模型 深入了解 IBM Granite 了解 IBM® Granite™，我们的开放式、性能出色和值得信赖的 AI 模型系列，专门为企业量身定制，并经过优化，可以帮助您扩展 AI 应用程序。深入了解语言、代码、时间序列和防护措施选项。 了解 Granite 电子书 如何选择合适的 AI 基础模型 了解如何为您的用例选择最合适的 AI 基础模型。 阅读电子书 文章 探索大语言模型 (LLM) 的威力 深入阅读 IBM 开发人员文章、博客和教程，加深您对大语言模型 (LLM) 的了解。 浏览文章 指南 CEO 的模型优化指南 了解如何使用最新的 AI 技术和基础架构，不断推动团队提高模型性能并超越竞争对手。 阅读指南 报告 采用差异化方法提供 AI 基础模型 深入了解企业级基础模型的价值， 利用这种模型可信、高性能且经济高效的特点， 为所有行业服务。 阅读报告 电子书 解锁生成式 AI + ML 的强大功能 了解如何将生成式 AI、机器学习和基础模型整合到您的业务运营中，以提高绩效。 阅读电子书 报告 2024 年 AI 实际应用 了解我们对 2,000 家组织进行的关于他们的 AI 计划的调研，以发现哪些方法有效、哪些方法无效，以及如何才能取得领先。 阅读报告 相关解决方案 基础模型 深入了解 watsonx 组合中基础模型库，从容自信地为您的业务扩展生成式 AI。 了解 watsonx.ai 人工智能 (AI) 解决方案 借助 IBM 业界领先的人工智能专业知识和解决方案组合，让人工智能在您的业务中发挥作用。 深入了解人工智能解决方案 AI 咨询与服务 通过增加 AI 重塑关键工作流程和运营，最大限度提升体验、实时决策和商业价值。 深入了解人工智能服务 采取后续步骤 深入了解 IBM watsonx 产品组合中的 IBM 基础模型库，满怀信心地为您的业务扩展生成式 AI。 深入了解 watsonx.ai 深入了解人工智能解决方案 脚注 1. “ Attention is all you need ”, Vaswani et al, arXiv, 12 June 2017 资源 AI 模型 深入了解 IBM Granite 了解 IBM® Granite™，我们的开放式、性能出色和值得信赖的 AI 模型系列，专门为企业量身定制，并经过优化，可以帮助您扩展 AI 应用程序。深入了解语言、代码、时间序列和防护措施选项。 了解 Granite 电子书 如何选择合适的 AI 基础模型 了解如何为您的用例选择最合适的 AI 基础模型。 阅读电子书 文章 探索大语言模型 (LLM) 的威力 深入阅读 IBM 开发人员文章、博客和教程，加深您对大语言模型 (LLM) 的了解。 浏览文章 指南 CEO 的模型优化指南 了解如何使用最新的 AI 技术和基础架构，不断推动团队提高模型性能并超越竞争对手。 阅读指南 报告 采用差异化方法提供 AI 基础模型 深入了解企业级基础模型的价值， 利用这种模型可信、高性能且经济高效的特点， 为所有行业服务。 阅读报告 电子书 解锁生成式 AI + ML 的强大功能 了解如何将生成式 AI、机器学习和基础模型整合到您的业务运营中，以提高绩效。 阅读电子书 报告 2024 年 AI 实际应用 了解我们对 2,000 家组织进行的关于他们的 AI 计划的调研，以发现哪些方法有效、哪些方法无效，以及如何才能取得领先。 阅读报告</p>
</div></details><h2 id="toc-216">109. Privacy Policy - Agentic.ai</h2>
<ul>
<li>链接：https://agentic.ai/privacy</li>
<li>来源：bing</li>
<li>摘要：Agentic.ai is a semantic alerts and interpretation platform that watches the web for topics you care about. To deliver tailored alerts, we ingest licensed and public news sources, de-duplicate and cluster …</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-217">正文（抓取，非 AI）</h3>
<p>Privacy Policy | Agentic.ai Agentic.ai requires JavaScript enabled to run. Privacy Policy Your privacy matters to us. This policy explains how Agentic.ai collects, uses, and protects your information. Effective: November 19, 2025 Last Updated: November 19, 2025 Quick Summary Agentic.ai is a semantic alerts and interpretation platform that watches the web for topics you care about. To deliver tailored alerts, we ingest licensed and public news sources, de-duplicate and cluster content, and generate role-aware summaries (“Lenses”) with citations. We keep “receipts” (links to sources) and do not republish full paywalled text. We do not use your data to train public AI models. Table of Contents 1 Who We Are &amp; Scope 2 Information We Collect 3 How We Use Information 4 AI &amp; Third-Party LLMs 5 Cookies &amp; Storage 6 Sub-processors &amp; Sharing 7 International Transfers 8 Data Retention 9 Your Rights &amp; Choices 10 Security 11 Children's Privacy 12 Third-Party Links 13 Changes to Policy 14 Contact Us 1. Who We Are &amp; Scope This Privacy Policy explains how Agentic.ai (“Agentic.ai,” “we,” “us,” or “our”) collects, uses, shares, and safeguards information about you when you use our websites, applications, and services (collectively, the “Service”). This Policy applies to the Service operated by Agentic.ai. We act as a data controller for user account data and related usage data. 2. Information We Collect We collect the following categories of information to operate and improve the Service: A. Identity &amp; Account Information Email address and name (often via OAuth providers) Authentication identifiers (e.g., Supabase user ID and session tokens) Billing identifiers (Stripe customer ID, subscription status; we do not store full payment card numbers) B. Service Configuration &amp; Content You Provide Watches (entities/topics you track) Lenses (your role-aware settings) Collections (saved clusters) Explicit feedback (e.g., thumbs up/down) Chat history with the AI about your news collections C. Technical, Usage &amp; Analytics Data Read/open status for items, feature usage timestamps, and API/LLM token consumption Device, browser, and network metadata (“Activity Logs”) IP address (for security, fraud prevention, and coarse geo reporting like country) Product analytics via Amplitude (navigation, feature usage, user behavior) D. Content We Ingest to Power Alerts Licensed and public news and web content used to match your Watches Clustered related items and generated Lenses (summaries with citations) We keep concise extracts and links; we do not republish full paywalled text 3. How We Use Information We process your information for: Providing the Service Account creation, authentication, matching Watches to clusters, generating Lenses, delivering alerts and digests. Legal bases: contract performance (EU GDPR Art. 6(1)(b)) and/or legitimate interests. Personalizing &amp; Improving Relevance Using your feedback and preferences (e.g., thumbs up/down). This tuning is scoped to your experience and internal ranking, not to training public foundation models. Legal bases: contract performance and legitimate interests. Measuring Usage &amp; Reliability Enforcing limits (token accounting), preventing abuse, and debugging. Legal bases: legitimate interests. Billing &amp; Payments Processing via Stripe; fraud prevention. Legal bases: contract performance and legal obligations. Communications Transactional messages about the Service, and where permitted, product updates and marketing (you can opt out). Legal bases: contract performance, legitimate interests, and/or consent where required. We do not use user data to train public foundation models. 4. Use of AI &amp; Third-Party LLMs To generate Lenses and summaries, we may send news extracts, your prompts/settings (e.g., Lenses), and related context to our AI provider (currently OpenAI). Outputs are returned to the Service for your use. We apply receipts/citations and avoid full-text republishing. 5. Cookies, Local Storage &amp; Similar Technologies Cookies Cookie Purpose Type sb-*-auth-token Authentication Essential theme UI theme preference Functional Local/Session Storage Key Purpose feedSelections Temporary UI selections onboarding_state Tour progress You can manage cookies via your browser settings. Blocking essential cookies may impair the Service. 6. Sub-processors &amp; Data Sharing We use vetted service providers to operate the Service: Service Purpose Data Shared Location Render.com Hosting &amp; infrastructure Application data (encrypted at rest) US/EU Neon.tech Managed PostgreSQL User data, news metadata, vectors US (AWS) Supabase Authentication Email, password hash, sessions Global OpenAI LLM processing News extracts, prompts (Lenses) US NewsAPI.ai News provider Search queries (no user PII) US Stripe Payments Billing info, email Global Sentry Error tracking Errors, stack traces US Amplitude Product analytics Usage events, user ID, IP address (for geo) US We may also disclose information to comply with law, enforce our terms, or protect rights, safety, and the integrity of the Service. 7. International Data Transfers Where data moves outside your home jurisdiction, we rely on appropriate safeguards (e.g., Standard Contractual Clauses for EEA/UK transfers) and technical/organizational measures consistent with industry practice. 8. Data Retention Account Data &amp; Preferences Retained while your account is active and for a reasonable period thereafter, or as required by law. News Metadata/Cached Content Retained for archival search; we honor valid “right to be forgotten” requests for personal data tied to specific source URLs where applicable. Application Logs Typically kept ~30 days for security and debugging. When you request account deletion, we cascade-delete your PII, Watches, Lenses, and Feedback from our primary database subject to lawful retention needs. 9. Your Rights &amp; Choices Depending on your location, you may have rights to: ✓ Access, correct, delete, or export your personal data ✓ Object to or restrict certain processing ✓ Withdraw consent (where processing relies on consent) ✓ Opt out of marketing communications US State Privacy Rights California/US state law rights (CA, CO, CT, UT, VA, etc.), including the right to know, delete, correct, and to opt out of “sale” or “sharing” for cross-context behavioral advertising. We do not “sell” personal information as defined by applicable law, and we do not “share” it for cross-context behavioral advertising. To exercise rights, contact privacy@agentic.ai . We will verify your request and respond within the timelines required by applicable law. 10. Security We use technical and organizational measures to protect information (encryption at rest, access controls, logging). No method is perfectly secure; please use a strong, unique password and keep credentials confidential. 11. Children's Privacy The Service is not directed to children under 13 (or 16 where applicable law sets a higher age). We do not knowingly collect personal data from children. If you believe a child has provided personal data, contact us to request deletion. 12. Third-Party Links &amp; Content The Service links to third-party publishers and sites. We are not responsible for their privacy practices. Review their policies before interacting with those services. Our summaries link back to original sources and avoid full-text republishing. 13. Changes to This Policy We may update this Policy to reflect changes to our practices or legal requirements. We will post the updated Policy with a new “Last updated” date and, where required, provide additional notice. 14. Contact Us Email privacy@agentic.ai Postal Agentic.ai Legal Dept Related Documents Terms of Service</p>
</div></details><h2 id="toc-218">110. 一文带你解密 Large Language Model（大型语言模型）</h2>
<ul>
<li>链接：https://cloud.tencent.com/developer/article/2357760</li>
<li>来源：bing</li>
<li>摘要：2023年11月13日 · — 04 — Large Language Model 应用场景 近年来，由于大型数据集的可用性和 AI（人工智能）技术的进步，大型语言模型的应用显著增加。 随着人工智能技术的不断改进，大型语言模 …</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-219">正文（抓取，非 AI）</h3>
<p>一文带你解密 Large Language Model（大型语言模型）-腾讯云开发者社区-腾讯云 Luga Lee 一文带你解密 Large Language Model（大型语言模型） 关注作者 腾讯云 开发者社区 文档 建议反馈 控制台 登录/注册 首页 学习 活动 专区 圈层 工具 MCP广场 文章/答案/技术大牛 搜索 搜索 关闭 发布 Luga Lee 社区首页 &gt; 专栏 &gt; 一文带你解密 Large Language Model（大型语言模型） 一文带你解密 Large Language Model（大型语言模型） Luga Lee 关注 发布 于 2023-11-13 17:22:49 发布 于 2023-11-13 17:22:49 7.9K 2 举报 文章被收录于专栏： 架构驿站 架构驿站 Hello folks，我是 Luga，今天我们来聊一下人工智能（AI）生态领域相关的技术 - Large Language Model（大型语言模型） 。 在过去十年间，AI（人工智能）领域取得了令人瞩目的突破，而其中的 NLP（自然语言处理）是其中一项重要的子领域。NLP 致力于开发各种技术和方法，用于处理和理解人类语言的文本数据。 NLP 的发展使得机器能够更好地理解和处理人类语言，从而实现更加智能和自然的交互。这包括了诸如文本分类、情感分析、命名实体识别、机器翻译、问答系统等多个任务和应用领域。 NLP 技术的核心是建立起对语言的理解和表达的模型。LLM （大型语言模型）是其中一项关键技术。LLM 基于深度神经网络架构，通过学习大规模语料库中的文本数据，能够捕捉到单词、短语和句子之间的语义和语法规律。从而使得 LLM 能够自动生成连贯、自然的文本，增强了机器在处理自然语言任务时的表现。 随着技术的不断进步，NLP 的应用范围也越来越广泛。例如，在智能助手、智能客服、信息检索、舆情分析、自动摘要等领域都得到了广泛应用。然而，NLP 仍然面临一些挑战，例如处理歧义、语义理解的准确性、处理多语言和多模态数据等方面的问题。 — 01 — 什么是 Large Language Model ？ Language Model （语言模型）是一种统计模型，用于预测一系列单词在文本序列中的概率。作为基于人工神经网络的一项重要人工智能技术，Language Model 通过对大规模文本数据进行训练，以理解语言并预测序列中的下一个单词。 LLM （大型语言模型），后续简称为“LLM”，则是一种具备大量可调参数的神经网络，使其能够学习语言中的复杂模式和结构。 通过训练大型语言模型，使得能够学习到单词之间的上下文关系、语法规则以及常见短语和句子结构，从而能够根据指定的上下文生成连贯、自然的文本。 LLM ，也称为预训练模型，是一种利用海量数据学习语言特征的人工智能工具。通过训练，这些模型能够生成基于语言的数据集，可用于各种语言理解和生成任务。 其中一个重要特征是 LLM 能够生成类似于人类文本的输出。它们能够生成连贯、符合语法规则的文本，有时甚至能够表现出幽默感。此外，这些模型还具备将文本从一种语言翻译成另一种语言的能力，并能够根据给定的上下文回答问题。 LLM 的训练依赖于大量的文本数据，其中包括互联网上的网页、书籍、新闻文章等。通过这些数据的学习，模型能够捕捉到语言中的各种模式和规律，从而提高对下一个单词的预测准确性。 LLM 的应用非常广泛，包括机器翻译、文本生成、自动摘要、对话系统等。例如，在机器翻译任务中，模型可以根据源语言的上下文生成目标语言的翻译结果。在对话系统中，它可以根据用户的输入生成回应。 — 02 — 领略 Large Language Model 全景观 下图显示了 LLM （大型语言模型）的出现所衍射的涟漪效应，这个效应可以在多个方面产生影响。具体而言，LLM 的出现可以被划分为六个带状或区域，每个区域都代表着不同的需求和机会。 LLM （大型语言模型）全景观鸟瞰 1、区域1—可用的大型语言模型 考虑到 LLM （大型语言模型）本质上是针对语言处理任务的模型。然而，在处理图像、音频等多模态数据方面，引入了多模态模型或多模态方法。这种转变使得我们需要一个更通用的术语来描述这些模型，即基础模型。 基础模型是指那些能够处理多种类型数据（如文本、图像、音频等）的模型。它们集成了不同的组件和技术，以便在多模态环境下进行信息的融合和处理。这些基础模型可以同时处理不同模态的输入，并生成相应的输出结果。 除了引入多模态模型外，大型商业供应商还提供了多个更加特定于任务的模型。这些模型针对特定的应用场景和任务进行了优化和训练，以提供更高的性能和更准确的结果。例如，针对图像分类、语音识别、自然语言理解等任务，商业供应商提供了专门的模型，以满足不同需求的客户。 此外，还存在一系列开源模型可供使用。开源模型是由研究人员和开发者共享的模型，这些模型经过训练并在特定任务上展现了良好的性能。这些开源模型可以作为起点或基础，为开发者提供一个快速开始的平台，同时也促进了模型研究和知识的共享。 2、区域2—常见的应用场景 模型接受特定任务的训练，以提供更加专注和高效的解决方案。LLM 的最新发展采用了一种方法，即将这些特征结合在一起，允许模型使用不同的提示技术来提取出令人惊叹的性能。 LLM 在文本生成任务方面表现出色，包括总结、重写、关键字提取等任务。这些模型能够生成准确、连贯的文本，以满足各种需求。 文本分析在当前变得越来越重要，而将文本嵌入模型中对于实现这些任务至关重要。嵌入技术能够将文本转换为向量表示，从而提供了更好的语义理解和语境感知能力。 另外，语音识别（ASR）也是 LLM 的关注领域之一，它是将音频语音转换为文本的过程。准确性是评估任何 ASR 过程的重要指标，通常使用 Word 错误率（WER）来衡量。ASR 技术为 LLM 培训和使用提供了大量记录的语言数据，使得文本转换和分析更为便捷和高效。 3、区域3—具体基础实施 此区域列出了一些特定用途的模型。实现已分为通用、强大的 LLM 和基于 LLM 的数字/个人助理，如 ChatGPT、HuggingChat 和 Cohere Coral。这些特定用途的模型为各行各业提供了定制化的解决方案，使得语言处理和法律应用更加高效和精确。无论是通用模型还是专门针对法律领域的模型，它们都在不同领域中扮演着重要的角色，为用户提供了更好的语言理解和问题解决能力。 4、区域4—模型分类 此区域列出了最著名的大型语言模型供应商。大多数 LLM 拥有内置的知识和功能，包括人类语言翻译、口译和编写代码的能力、通过快速工程进行对话和上下文管理。供应商提供的 LLM 能够满足不同用户的需求，从跨语言沟通到代码编写，从对话系统到上下文管理，为用户提供了强大的语言处理和智能化服务。这些大型语言模型的发展受益于深度学习和自然语言处理的进步，为人们提供了更多创新和便捷的工具。 5、区域5—基础工具/平台 此区域中提出的概念是以数据为中心的工具，这些工具专注于使 LLM （大型语言模型）的使用变得可重复且具有高价值。这意味着关注点放在如何有效地利用数据来提升 LLM 的性能和应用价值上。 6、区域6—终端用户 此区域中涌现了大量专注于流程构建、创意生成、内容创作和写作辅助的应用程序。这些产品致力于提供优质的用户体验，并在 LLM（大型语言模型）和用户之间增加不同程度的价值。通过这些应用程序，用户能够更好地利用 LLM 的潜力，实现更加出色和有影响力的工作和创作。 — 03 — Large Language Model 是如何工作的呢？ LLM 通过使用一种称为无监督学习的技术来进行工作。在无监督学习中，该模型在大量数据上进行训练，没有特定的标签或目标。其目标是学习数据的基本结构，并生成与原始数据结构相似的新数据。 对于 LLM 而言，训练数据通常是大规模的文本语料库。模型学习文本数据中的模式，并利用这些模式生成新的文本。训练过程涉及优化模型参数，以尽可能减少生成的文本与语料库中实际文本之间的差异。 一旦模型经过训练，就可以用于生成新的文本。为此，该模型被赋予一个起始单词序列，并根据训练语料库中单词的概率来生成序列中的下一个单词。重复这个过程，直到生成所需长度的文本。 这里，我们简单了解一下 LLM 工作原理机制，具体可参考如下示意图所示： 了解 LLM 的工作原理，以及了解可用的不同类型的语言模型是很重要的。最常见的语言模型类型包括循环神经网络（RNN）、卷积神经网络（CNN）和长短期记忆网络（LSTM）。这些模型通常在大型数据集（如Penn Treebank）上进行训练，并可用于生成基于语言的数据集。 接下来，让我们深入了解一些领先的 LLLM（大型语言模型），它们的创建者以及它们所训练的参数数量。这些模型代表了人工智能领域最前沿的技术发展。具体可参考如下示意图所示： 资料来源：Roundhill Investments 基于上述模型参数图，我们可以看到，现在有许多备受欢迎的 LLM（大型语言模型），具体如下： OpenAI 是一家在 LLLM 领域具有重要地位的公司。他们的 ChatGPT 模型经过了广泛的研究和训练，是一种基于生成预训练变压器模型（GPT）的强大语言模型。虽然具体的参数数量尚未披露，但根据之前的版本，可以合理地推测 ChatGPT 可能具有数百亿到数千亿的参数。 谷歌也在大型语言模型的研究和开发方面投入了大量资源。他们的 LaMDA 和 PaLM 模型分别具有数百亿的参数量，这些模型通过在大规模数据集上进行训练，展现了出色的语言理解和生成能力。同时，谷歌还投资了 Anthropic 公司，该公司发布了具有数百亿参数的 Claude 模型。 百度的 Ernie 3.0 Titan 模型是为其 ErnieBot 聊天机器人提供支持，拥有数千亿的参数数量。以及中国的人工智能公司 SenseTime 开发了 SenseNova 模型，用于为其 SenseChat 聊天机器人和其他服务提供支持，该模型也具有数千亿的参数。 此外，Bloomberg 公司建立了一个金融领域特定的模型，名为 BloombergGPT，它具有数百亿的参数，为金融相关任务提供强大的语言处理能力。 虽然上面没有明显标注微软公司，其实，微软也在 LLLM 领域也有着同样重要的贡献，他们推出了 Bing AI搜 索所使用的 GPT 模型。该模型的参数数量可能与其他顶尖模型相当。 这些领先的大型语言模型，通过庞大的参数量，使得它们能够更好地理解和生成自然语言。它们代表了人工智能领域的最新成果，并在各个领域展现出巨大的潜力和应用前景。 — 04 — Large Language Model 应用场景 近年来，由于大型数据集的可用性和 AI（人工智能）技术的进步，大型语言模型的应用显著增加。随着人工智能技术的不断改进，大型语言模型的准确性和能力也将不断提高，使其在各种自然语言处理任务中变得更加有用。 通常情况下，大型语言模型在各个领域都有广泛的应用。它们可以应用于自然语言处理、人工智能和数据科学等领域，为许多应用程序提供强大的支持和功能。以下是一些典型的应用领域和示例： 1、语言翻译 语言翻译是 LLM 的重要应用之一。LLM 能够快速将单词从一种语言翻译成另一种语言。它通过比较两种语言，并试图通过所谓的平行语料库逐句进行翻译。LLM 使用两种主要的翻译技术：直接翻译和编码器解码器翻译。 这两种技术都利用深度学习方法来实现高质量的翻译。这些翻译技术都依赖于深度学习方法，通过大规模训练数据和神经网络的学习能力，LLM 能够实现准确和流畅的语言翻译。随着技术的不断发展，LLM 在语言翻译领域的应用将进一步提升翻译质量和效率，促进跨语言交流和文化交流的便利性。 2、内容生成 内容生成是 LLM 的另一个重要应用领域。LLM 生成的输出可以用于产品的文本内容创作。它可以生成各种类型的文本，例如文章、产品描述、小册子和其他书面内容。在这方面，ChatGPT 是一个非常强大的工具，它能够生成高质量的文本内容，几乎无法与人类创作的内容区分开来。因此，如果您需要为用户编写内容，考虑使用 LLM 和 ChatGPT 将会是一个理想的选择。 需要注意的是，虽然 LLM 和 ChatGPT 在内容创作方面具有很大的潜力，但仍然需要人工进行审核和编辑。由于模型的自动化性质，它可能会生成不准确或有误导性的信息。因此，在使用 LLM 生成的内容之前，仍然需要人工的审查和修改，以确保内容的准确性和合适性。 3、聊天机器人及客户支持 聊天机器人是 LLM 的一个主要应用领域。LLM 被广泛应用于构建聊天机器人，其中，ChatGPT 是一种常用的工具。许多公司已经将 ChatGPT 作为客户支持聊天机器人的一部分，通过提供准确的回答来为客户提供最佳的服务体验。随着技术的发展，许多技术领导者正在考虑如何开发自己的语言模型，通过提供相关的内部数据来满足他们独特的业务需求。 通过利用内部数据和业务特定的培训，企业可以创建定制化的聊天机器人，更好地适应自己的业务场景和客户需求。 4、情绪分析及舆情监测 情绪分析是 LLM 的另一个重要应用。这些模型可以用于分析文本的情绪，帮助确定文本是否具有积极或消极情绪。情绪分析在许多领域具有广泛的应用，包括社交媒体监测、品牌声誉管理、市场调研等。 LLM 在情绪分析领域具有广泛的应用前景。通过自动化情绪分析，可以帮助企业和组织更好地理解用户的情感态度，从而进行更有针对性的决策和改进。然而，仍需注意模型的局限性，并结合人工的审查和判断，以确保情绪分析结果的准确性和可靠性。 5、个性化推荐及广告 个性化推荐和广告是 LLM 的另一个重要应用领域。这些模型可以基于用户的兴趣和行为模式，提供个性化的推荐和广告内容。通过深入理解用户的需求和偏好，LLM 能够提供更加精准和定制化的推荐体验，从而提升用户满意度和广告效果。 — 05 — Large Language Model 当前面临的挑战 LLM（大型语言模型）在自然语言处理领域取得了重大的突破，但也面临一些挑战。以下是一些普遍认为的 LLM 面临的挑战： 1、训练成本和资源需求 通常而言，LLM 需要庞大的训练数据和计算资源来进行训练。这样的训练过程需要大量的时间、存储和计算能力，以及海量的标记数据。因此，构建和训练 LLM 需要巨大的投入。 2、数据偏见和模型倾向性 LLM 会模仿其训练数据中的模式和偏见。如果训练数据存在偏见，例如，性别或种族偏见，模型可能会反映这些偏见，并在生成的文本中表现出来。这可能导致模型产生不公平或有害的结果。解决这个问题需要更加平衡和多样化的训练数据，以及对模型进行有效的偏见检测和修正。 3、知识和推理的不足 尽管 LLM 在语言生成和理解方面取得了显著进展，但它们仍然存在对于真实世界知识和推理的不足。这使得模型在处理复杂的现实场景、逻辑推理和常识推理时表现不佳。解决这个问题需要进一步将外部知识和推理能力融入到模型中，以提高其真实世界的应用能力。 4、解释性和可控性 LLM 通常被认为是黑盒模型，难以解释其决策和生成文本的依据。这对于某些应用场景来说是一个挑战，例如在法律、医学等领域需要透明和可解释的决策。因此，提高模型的解释性和可控性是一个重要的方向。 5、虚假信息和滥用 LLM 可以被用于生成虚假信息、恶意攻击和滥用行为。它们可以被误用为网络欺诈、网络钓鱼和虚假新闻等活动。因此，确保模型的安全性和防范滥用的能力是一个重要的挑战。 尽管，以上是 LLM 所面临的一些挑战，然而，随着技术的不断演进，研究人员和开发者们正在努力解决这些问题，以提高模型的性能、可靠性和可用性。 Reference ： [1] https://cobusgreyling.medium.com/ [2] https://em360tech.com/tech-article/large-language-model Adiós ! ·································· 📣📣📣 Hello folks，我是 Luga，Traefik Ambassador，Jakarta EE Ambassador， 一个 10 年+ 技术老司机，从 IT 屌丝折腾到码畜，最后到“酱油“架构师。如果你喜欢技术，不喜欢呻吟，那么恭喜你，来对地方了，关注我，共同学习、进步、超越～ 您的每一个点赞、在看及分享，我都认真当成了喜欢 ～ 本文参与 腾讯云自媒体同步曝光计划 ，分享自微信公众号。 原始发表：2023-11-10 ，如有侵权请联系 cloudcommunity@tencent.com 删除 人工智能 model 翻译 模型 数据 本文分享自 架构驿站 微信公众号， 前往查看 如有侵权，请联系 cloudcommunity@tencent.com 删除。 本文参与 腾讯云自媒体同步曝光计划 ，欢迎热爱写作的你一起参与！ 人工智能 model 翻译 模型 数据 评论 登录 后参与评论 0 条评论 热度 最新 登录 后参与评论 推荐阅读 目录 什么是 Large Language Model ？ 领略 Large Language Model 全景观 Large Language Model 是如何工作的呢？ Large Language Model 应用场景 Large Language Model 当前面临的挑战 相关产品与服务 人工智能与机器学习 提供全球领先的人脸识别、文字识别、图像识别、语音技术、NLP、人工智能服务平台等多项人工智能技术，共享 AI 领域应用场景和解决方案。 产品介绍 AI驱动 智领未来 领券 社区 技术文章 技术问答 技术沙龙 技术视频 学习中心 技术百科 技术专区 活动 自媒体同步曝光计划 邀请作者入驻 自荐上首页 技术竞赛 圈层 腾讯云最具价值专家 腾讯云架构师技术同盟 腾讯云创作之星 腾讯云TDP 关于 社区规范 免责声明 联系我们 友情链接 MCP广场开源版权声明 腾讯云开发者 扫码关注腾讯云开发者 领取腾讯云代金券 热门产品 域名注册 云服务器 区块链服务 消息队列 网络加速 云数据库 域名解析 云存储 视频直播 热门推荐 人脸识别 腾讯会议 企业云 CDN加速 视频通话 图像分析 MySQL 数据库 SSL 证书 语音识别 更多推荐 数据安全 负载均衡 短信 文字识别 云点播 大数据 小程序开发 网站监控 数据迁移 Copyright © 2013 - 2026 Tencent Cloud. All Rights Reserved. 腾讯云 版权所有 深圳市腾讯计算机系统有限公司 ICP备案/许可证号： 粤B2-20090059 粤公网安备44030502008569号 腾讯云计算（北京）有限责任公司 京ICP证150476号 | 京ICP备11018762号 问题归档 专栏文章 快讯文章归档 关键词归档 开发者手册归档 开发者手册 Section 归档 Copyright © 2013 - 2026 Tencent Cloud. All Rights Reserved. 腾讯云 版权所有 登录 后参与评论 2 0 2 0 推荐</p>
</div></details><h2 id="toc-220">111. 全球诡异时代漫画,全球诡异时代漫画全集,爱漫画就看全球诡异 ...</h2>
<ul>
<li>链接：https://www.laimanhua.org/kanmanhua/417337</li>
<li>来源：bing</li>
<li>摘要：最新的全球诡异时代漫画，来漫画免费为广大爱漫画者提供全球诡异时代漫画全集在线观看。爱漫画就看最快更新的全球诡异 ...</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-221">正文（抓取，非 AI）</h3>
<p>全球诡异时代漫画,全球诡异时代漫画全集,爱漫画就看全球诡异时代漫画 - 来漫画 首页 漫画 全球诡异时代 全球诡异时代 悬疑 热血 都市 诡异复苏一百年的时间，人类只剩下十分之一，鬼的数量已经超过人类！为了生存，每个人从生下来开始，都需要选择一个“伴生鬼灵”，并不断培养自己的伴生鬼灵，以鬼制鬼。全球进入了与鬼共存的诡异时代！借助系统，林... 立即阅读 章节 ↓ 降序 第一话 诡异时代觉醒！ 第二话 童童！ 第3话 别墅试炼 第4话 衣物恶灵 第5话 灭杀！ 第6话 初次任务 第7话 鬼域展开 第8话 诡异嫁衣 第9话 智斗鬼贩子 第10话 童童鬼域 第11话 红色绣花鞋 第12话 嫁衣降临！ 第13话 羁绊 第14话 萱华 第15话 新娘 第16话 打破诅咒 第17话 绑定萱华！ 第18话 测试！ 第19话 A级！！ 第20话 海城 第21话 无头将军 第22话 英雄救美 第23话 强化鬼域 第24话 中计！ 第25话 捕获鬼马！ 第26话 震撼！ 第27话 海城招婿 第28话 回援 第29话 大战S级！ 第30话 双S级 第31话 柳城失守 第32话 家人 第33话 再见童童 第34话 救世主 第35话 深渊级恶灵 第36话 鬼镜迷局 第37话 击退鬼潮！ 第38话 英雄的奖励 第39话 借您女儿一用 第40话 黄金密室 第41话 传闻中的寂照庵 第42话 地煞的伏击 第43话 白衣神尼 第44话 庵中的唯一男子 第45话 第四只鬼灵开启！ 第46话 鬼画 第47话 清竹的托付 第48话 收集鬼伞 第49话 反转！灭门真相！ 第50话 离奇白骨 第51话 鬼尼姑vs鬼观音！ 第52话 雷霆之战 第53话 初到平城 第54话 赢蕊的惊讶！ 第55话 黄金监狱 第56话 绑定鬼伞！ 第57话 鬼伞升级！ 第58话 总部扬名！三大深渊级！ 第59话 拍卖会的稀客 第60话 画中世界 第61话 江薇 第62话 脱离循环！ 第63话 对峙魏光耀 第64话 鬼画出战！ 第65话 对抗深渊二阶！ 第66话 江薇的复仇 第67话 绑定鬼画！ 第68话 万鬼窟 第69话 又见妍会长 第70话 三伞合一！ 第71话 婚约解除 第72话 拥有意识的鬼画！ 第73话 六大家族vs魏无际 第74话 探秘险地！ 第75话 小丑面具 第76话 战水蛟！集碎片！ 第77话 小丑与神像 第78话 虎口夺食！ 第79话 绑定寂照庵主！ 第80话 灭城大敌！ 第81话 尸鬼 第82话 凤袍女鬼！ 第83话 沙丘 第84话 万人洞 第85话 死亡之海 第86话 黑色石棺 第87话 战火树！鬼灵商队！ 第88话 血月！ 第89话 开棺遇袭！ 第90话 黑骨！木鱼压制！ 第91话 鬼画升级！ 第92话 万人迁徙 第93话 向魏家宣战！ 断层顶流 关注将破200w！ 第94话 对战黑武士！ 第95话 深渊能力！ 第96话 成王败寇！ 第97话 沙漠古城再现！ 第98话 寻找夜明珠 小番外 聪明的穿越者 第99话 月阙女王 第100话 血月神教的谎言！ 第101话 火凤燎原！ 第102话 卫星发射 第103话 鬼邮差！佛头鬼棺！ 第104话 血祭增幅！ 第105话 升级！总部召唤！ 第106话 蝶婆婆 第107话 追踪鬼道人 第108话 围攻道观！ 第109话 完全复苏！绑定女王！ 少女惊人身份！海城危机！ 请假条 第111话 海城主的怪病 第112话 驱鬼蛊！魏无际再现！ 第113话 抓内奸！魏家阴谋！ 第114话 千万级鬼潮！ 第115话 林风vs魏无际！ 第116话 碾压！ 第117话 预知！海城的胜利！ 第118话 海城主林风 延更通知 第119话 最后一搏！ 单更通知 第120画 噩梦级鬼域！ 《全球诡异时代》漫剧上线！ 第121话 药剂师 第122话 海城的变化 第123话 代号“公爵”！ 第124话 画中世界突变！ 第125话 江薇复苏！ 第126话 蝶婆婆的密信 第127话 地下中心城！ 第128话 公爵的陷阱！ 第129话 逃出玫瑰古堡！ 第130话 血族密会 第131话 游戏结束！公爵的目的！ 第132话 边境追逐 第133话 噩梦级激战！ 第134话 与蝶婆婆夜谈 第135话 金字塔妖姬！ 第136话 无解的梦！ 第137话 公爵之死！江薇融合成功！ 第138话 江薇解梦 第139话 深层梦境！少女伊苏娜！ 第140话 金字塔的诅咒！ 第141话 成人礼 第142话 跨越千年的羁绊！ 第143话 风都鬼城！ 第144画 鬼城之主 第145话 收割！绑定伊苏娜！ 第146话 女王复苏任务 第147话 城主约见！加官进爵！ 第148话 黑影的密谋 单更通知0509 第149话 诱杀计划！ 第150话 全员大危机！ 第151话 童童之死 第152话 幽冥聚魂术！ 第153话 西方地图开启！ 第154话 万神国奇遇 第155话 是神？是鬼？ 第156话 神庙的主人 第157话 多重地狱！ 第158话 谎言向死，真言向生 第159话 鬼母迦梨！ 第160话 首枚灵魂碎片 第161话 蝴蝶结的来历 第162话 金字塔国的大礼？ 163话 渔翁得利！ 第1期 KKWORLD漫展来了！（日更中） 第164话 图特之墓！ 第165话 欧大陆！启航！ 第2期 早期设定图大放送！（日更中） 第3期 全诡时间线讲堂！（日更中） 第4期 漫剧路透剧场（日更中） 第5期 路透社（日更中） 第6期 肝画的日常（日更中） 第167话 血妻 第166话 第二枚灵魂碎片 第7期 kkw返场图大放送！（日更中） 第8期 Q版番外剧场！（日更中） 第9期 漫剧路透剧场！（日更中） 第10期 路透社！（日更中） 第11期 测测你的本命鬼灵！（日更中） 第168话 海妖vs海盗！ 第169话 海岛奇遇 第11期 女性角色设定大公开！ 第12期 女性角色设定大公开！ 第12期 女性角色设定大公开！（日更中） 第13期 Q版番外剧场！（日更中） 第14期 漫剧剧场！（日更中） 第15期 路透社！（日更中） 第16期 作者访谈！（日更中） 第170话 斗牛国的恐怖传说！ 第171话 血腥玛丽！ 第17期 绘画小课堂！（日更中） 第18期 Q版番外小剧场！（日更中） 第19期 漫剧剧场！（日更中） 第20期 路透社！（日更中） 第21期 立秋纳凉特辑！（日更中） 第172话 木偶剧场！ 第173话 童童别哭！ 第22期 模型制作小课堂！（日更中） 第23期 Q版小剧场！（日更中） 第24期 漫剧剧场！（日更中） 第25期 路透社！（日更中） 第26期 恐怖旅游指南！（日更中） 第174话 地下堡垒的秘密！ 第175话 童童的终局！ 第176话 魂命物集齐！ 第27期 鬼灵人气榜单大公开！（日更中） 第28期 漫剧剧场！（日更中） 第29期 路透社！（日更中） 第30期 七夕特辑！（日更中） 第177话 返回东方 返回东方 第178话 蝶婆婆！鬼城主！ 第179话 诱杀图特！ 第31期 Q版小剧场（日更中） 第32期 漫剧剧场！（日更中） 第33期 路透社！（日更中） 第34期 中元节特辑！（日更中） 第180话 冥府之门！ 第181话 冥界，忘川 第35期 粉丝二创发布会！（日更中） 第36期 （日更中） 第36期 Q版小剧场（日更中） 第37期 高能混剪+漫剧剧场+福利放送！（日更中） 第38期 路透社！（日更中） 第39期 开学特辑！（日更中） 第182话 鬼城之主的过去！ 第183话 神女的秘密！ 第40期 绘画小课堂（日更中） 第41期 Q版剧场！（日更中） 第42期 漫剧剧场！（日更中） 第43期 路透社！（日更中） 第44期 CP真好磕！（日更中） 第184话 花开彼岸！ 第185话 地宫坍塌！ 第45期 重点剧情回顾！ 《全球诡异时代》实体书来啦！众筹开启！ 第46期 Q版剧场！（日更中） 第47期 漫剧剧场（日更中） 第48期 路透社！（日更中） 第49期 福利图放送！（日更中） 第186话 童童复活！ 第187话 返回现世！ 第50期 Q版彩蛋！（日更中） 第188话 合体鬼！ 第51期 漫剧剧场！（日更中） 第52期 路透社（日更中） 第53期 中秋特辑！（日更中） 第189话 收割大海！ 第190话 鬼盅巢穴！ 第191话 萱华的婚礼！ 第54期 老婆喜欢的月饼口味（日更中） 第55期 漫剧剧场！（日更中） 第56期 路透社！（日更中） 第57期 恐怖推理第二弹！（日更中） 第192话 蜜月旅行 第193话 天使东征！ 第58期 林风女装剧场！（日更中） 第194话 试探！ 第59期 漫剧剧场！（日更中） 第60期 路透社！（日更中） 第61期 恐怖推理第二弹谜底！（日更中） 第195话 南方中心城！ 第196话 意料之外的强敌！ 第197话 三千灯火明！ 第62期 鬼灵们的朋友圈（日更中） 第63期 漫剧剧场！（日更中） 第64期 搞点事：一起二创剧情吧！ 第65期 粉丝福利：老婆高清美图大赏 第198话 收复南方！ 第64期 搞点事：一起二创剧情吧！（日更中） 第65期 粉丝福利：老婆高清美图大赏（日更中） 第199话 水鬼袭来！ 第200话 清竹复苏！ 第66期 特别企划：御鬼师小问答（日更中） 第67期 漫剧剧场！（日更中） 第68期 特别企划：林风&amp;童童表情包合集！（日更中） 第69期 特别企划：重阳节特辑--拯救鬼灵计划！（日更中） 第67期 漫画剧场：精彩漫剧预告！（日更中） 漫画特典季·预约开启 第201话 水鬼小柔！ 第202话 鬼灵家园！ 第203话 鬼纹绘制！ 第70期 特别企划：男友视角的老婆们+重阳谜题答案（日更中） 第71期 漫剧剧场！（日更中） 第72期 路透社：下周精彩剧情！（日更中） 第73期 特别企划：不给糖，就捣蛋！（日更中） 第204话 身份暴露！ 第205话 东方大军集结！ 第206话 决战的号角！ 第74期 特别企划：磕点all童 第74期 特别企划：磕点all童（日更中） 第75期 漫剧剧场！（日更中） 第76期 路透社：天使形象大揭秘 第77期 特别企划：御鬼师小问答第二弹！ 第207话 切塔计划！ 第208话 六翼天使！ 209话 东方守住了！ 第78期 特别企划：柳城女校表白墙 第79期 漫剧剧场！（日更中） 第81期 粉丝福利：女性角色美图大赏第二弹！（日更中） 第80期 路透社：下周精彩剧情！（日更中） 第210话 绫罗的复苏任务！ 第211话 鲛人！ 第82期 搞点事：Q版林风？那是什么？ 第212话 蓬莱仙岛！ 第83期 漫剧剧场！（日更中） 第84期 路透社：下周剧情路透来啦！ 第85期 特别企划：柳城女校论坛--校花评选！ 第213话 仙岛探险！ 第214话 绫罗的诞生！ 第215话 仙器宝罗伞！ 第86期 特别企划：回忆杀之林风遇到的第一只鬼灵 第87期 漫剧剧场！（日更中） 第88期 特别企划：路透社 第89期 搞点事：最全鬼灵战力盘点（第一弹） 第216话 蓬莱危机！ 第217话 林风的决断！ 第218话 奇怪的树人！ 第90期 特别企划：最全鬼灵战力盘点（第二弹） 第91期 漫剧剧场！（日更中） 第92期 特别企划：羁绊之初相遇回顾 第93期 特别企划：冬季小剧场之围巾style 第219话 树人的身份！ 第220话 击败邪神！ 第221话 全员噩梦级！ 第94期 搞点事：老婆表情包含量100% 第95期 漫剧剧场！（日更中） 第96期 路透社：下周精彩剧情！ 第97期 特别企划：八卦新闻抢先看！ 第222话 米娅！ 第223话 天使军团！ 第224话 圣堂之战！ 第98期 特别企划：空白填色小游戏！ 第99期 漫剧剧场！ 第100期 路透社：下周精彩剧情！ 第101期 特别企划：全诡朋友圈更新！ 第225话 传奇猎魔人！ 第226话 圣光碎片！ 第227话 破晓号！ 第102期 特别企划：舌尖上的全诡！ 第103期 漫剧剧场！ 第104期 路透社：下期剧情预告~ 第105期 特别企划：全员圣诞style 第228话 强大的机甲！ 第229话 传奇再现！ 第230话 特殊规律！ 第106期 特别企划：带童童出门的四大理由！ 第107期 漫剧剧场！ 第108期 路透社：被美女包围，全员单箭头林风？！ 第109期 特别企划：全诡年终总结报告 第231话 乱爱风暴！ 第232话 林风拜师！ 第233话 英灵级 第111期 漫剧剧场 第112期 路透社：下周精彩剧情！ 第113期 特别企划：来自爱德华的新年祝福？ 第234话 巨隆隆！ 第235话 猎魔人林风！ 第236话 师徒的羁绊！ 第114期 特别企划：全诡最全世界观科普第一弹 第115期 漫剧剧场 第116期 路透社：下周精彩剧情！ 第117期 特别企划：全诡最全世界观科普第二弹 第237话 恐怖的鬼域！ 第238话 鬼域异变！ 第239话 小柔的快乐打击！ 第118期 特别企划：集体吃线面啦！ 第119期 漫剧剧场 第120期 路透社：下周精彩剧情！ 第121期 特别企划：角色大竞猜 第240话 激情一击！ 第241话 路西法之卵！ 第242话 英灵融合！ 第122期 整活企划：都是什么虎狼之词！ 第123期 漫剧剧场 第124期 路透社：下周精彩剧情！ 第125期 路透社：下周精彩剧情！ 第243话 契约路西法！ 第244话 传奇谢幕！ 第245话 米娅与天使！ 第126期 整活企划：各个地方有各个地方的美女 第127期 漫剧剧场 第128期 路透社：下周精彩剧情！ 第129期 整活企划：完蛋！我被美女包围了！ 第246话 天使的算计！ 第247话 天使被追哭了！ 特典季·2月13日 龙年大作战即将开启！ 第248话 神域失乐园！ 第130期 特别企划：天使大人……有两位？ 第131期 漫剧剧场 第132期 整活企划：全球最大农家乐，开始营业！ 第133期 整活企划：难道颜值和强度真的成正比？ 第249话 恐龙森林！ 停更通知 第250话 天使落难记！ 第134期 整活企划：用最狠的语气说最怂的话 第135期 漫剧剧场 第136期 整活企划：名字很重要！ 第137期 整活企划：恐龙们：这跟想的不一样！ 第251话 人类聚居地！ 第252话 航母村！ 第253话 森林神庙 第138期 整活企划：选部下真的是一门学问！ 第139期 漫剧剧场 第140期 整活企划：同游合集 第141期 整活企划：豹豹龙的妙用 第254话 盖亚之影！ 第255话 盖亚之死！ 第256话 战机起飞！ 第257话 升腾吧！蘑菇云！ 第258话 丧尸龙！ 第259话 激活盖亚之心！ 第260话 神之力！ 第261话 金字塔危机！ 第262话 伊苏娜解放！ 第263话 魔蝎大帝！ 第264话 吾神胚胎！ 第265话 斯芬克斯！ 第266话 谜语的答案是？ 第267话 穿越数千年的意识！ 第268话 神之契约！ 第269话 入学古埃及！ 第270话 猫神契约！ 第271话 降临！ 第272话 神战开始！ 第273话 莫顿的计划！ 第274话 演员请就位！ 第275话 消散吧！千年的怨恨！ 276话 鬼婴诞生！ 第277话 绝望神临！ 第278话 残破因果！ 单行本第3册来啦！ 第279话 穿越时间！ 第280话 阿波菲斯的葬礼！ 第281话 诡异时代毁灭？！ 第282话 千年的终局！ 第283话 新的时代！风主传说！ 第284话 评级危机！鬼灵阿幽！ 第285话 那个男人回来了！ 第286话 急剧变化的世界！ 第287话 契约凝蝶！ 第288话 特派员封林！ 第289话 启程北原！ 第290话 北原神秘少女！ 第291话 北原仙家！ 第292话 抵达北原！ 第293话 张家客栈！ 第294话 客栈隐藏规则！ 第295话 鬼赌开始！ 第296话 鬼迷心窍！ 第297话 真正的规则！ 第298话 女版林风被搓了！ 第299话 鬼王娶亲的真相 第300话 鬼赌老千 第301话 杀穿鬼赌场！ 第302话 第十一层！ 第303话 最终赌局！战棋模式！ 第304话 剑斩贪欲！ 第305话 北原镇山五仙使！ 第306话 鬼王出世！ 第307话 黑山斗法！ 第308话 五仙镇魔！ 第309话 卧虎藏龙象牙村！ 第310话 酒后！大事不妙！ 第311话 鬼婚开始！ 第312话 林风要出嫁？ 第313话 迎亲惊变！ 第314话 黑山鬼城！ 第315话 第一千个新娘！ 第316话 黑山开战！我不装了！ 第317话 时代变了！ 第318话 鬼军灭尽！ 第319话 五仙融合！萨满形态！ 第320话 鬼王vs五仙！化龙！ 第321话 旱魃僵王！ 第322话 堕天林风！ 第323话 鬼王分身灭！ 第324话 佛影现！林风暴走！ 第325话 清竹归来！ 第326话 佛影散！恶魔林风！ 第327话 驰援奉城! 第328话 高丽国阴谋！ 第329话 城主方无月！ 第330话 夺回身体！解救林风！ 高丽篇预告 第331话 高丽篇-幸福国度？ 第332话 虚假的世界！ 第332话 高丽篇-虚假的世界！ 第333话 高丽篇-监视！控制！净化！ 第334话 高丽篇-向高丽出发！ 第335话 高丽篇-素婉与素娜 第336话 高丽篇-紧急任务！收容规律鬼！ 第337话 孙清清的回忆！ 第338话 高丽篇-狂猿闹首城！ 第339话 高丽篇-孙清清之死！ 第337话 高丽篇-孙清清的回忆！ 第340话 高丽篇-潜入首城！ 第341话 高丽篇-爱乐之城！ 第342话 高丽篇-救星驾到！高丽往事！ 第343话 高丽篇-小柔升级！ 第344话 高丽篇-药峰寺秘辛！ 第345话 高丽篇-灵尊诞生！ 第346话 高丽篇-棒打灵尊！萱华升级！ 第347话 高丽篇-林风的战前谋划！ 第348话 高丽篇-解放高丽作战开始！ 第349话 高丽篇-林风破局! 第350话 高丽篇-绝强小柔！ 第349话 高丽篇-光力天王！林风破局! 第350话 高丽篇-海力天王！绝强小柔！ 第351话 高丽篇-剑仙江薇！ 第352话 高丽篇-英灵级鬼器! 第353话 高丽篇-全面压制！ 第354章 高丽篇-女王进阶！ 第355章 高丽篇-天王尽灭！ 第355章 高丽篇-天王尽灭！ 第356章 高丽篇-石猴出世！ 第357章 高丽篇-灵尊苏醒！ 第358话 高丽篇-鬼灵女团出道！ 第359章 高丽篇-全球直播！ 第360章 高丽篇-粉丝之力！ 通知 第361话 高丽篇-斗天武神！ 第362章 高丽篇-破天命！ 第363话 高丽篇-英灵童童！ 第364话 高丽篇-魔罗化身！ 第365话 高丽篇-升阶！狂猿法相！ 第366话 高丽篇-是心动的声音！ 第367话 高丽篇-风主明王！ 第369话 云梦篇-云梦泽！ 第368话 高丽篇-风主欧巴！ 第370话 云梦篇-奇特的云梦国！ 第371话 云梦篇-幻梦塔！ 第372话 云梦篇-梦灵湘若曦！ 第373话 云梦篇-云梦神战开始！ 第374章 云梦篇-神火之力！ 第375话 云梦篇-云梦惊变 第376章 云梦篇-梦灵的诞生！ 第377章 云梦篇-云梦国破！ 第378话 云梦篇-梦灵觉醒！ 第379话 云梦篇-云梦恋爱计划！ 第380话 云梦篇-林风抢亲！ 第381话 云梦篇-出马不利！ 第382话 云梦篇-恋爱攻略! 第383话 云梦篇-收编山大王！ 第384话 云梦篇-时间管理大师！ 第385话 云梦篇-云中君子！ 第386话 云梦篇-云梦激战！ 第387话 云梦篇-邪秽伪神！ 第388话 云梦篇-明月再现！ 第389话 云梦篇-返回东方！ 第390话 扶桑篇-梦魇战马！ 第391话 扶桑篇-海洋污染扩大！变异的海洋生物！ 第392话 扶桑篇-蛇影初现！ 第393话 扶桑篇-巫女惊鸿！ 第394话 扶桑篇-向着扶桑国前进！ 第395话 扶桑篇-巨大的黑船！ 第396话 扶桑篇-登陆扶桑！ 第397话 扶桑篇-扶桑往事！邪尊降临！ 第398话 扶桑篇-扶桑众神灭！ 第399话 扶桑篇-扶桑封魔！ 第400话 扶桑篇-神樱之种！ 第401话 扶桑篇-扶桑沉没！ 第402话 扶桑篇-扶桑攻略！ 第403话 扶桑篇-凝蝶！叶霜！危！ 第404话 扶桑篇-战况不利！ 第405话 扶桑篇-意外援军！ 第406话 扶桑篇-般若刀鬼 第407话 扶桑篇-鬼门关投影降临！ 第408话 扶桑篇-征服熊本！ 第409话 扶桑篇-各方云动！ 第410话 扶桑篇-木之地脉之力！ 第411话 扶桑篇-童童进阶！ 第412话 扶桑篇-雪女的羁绊！ 第413话 扶桑篇-残破之地！ 请假通知 第414话 扶桑篇-霸骑红日登场！ 第415话 扶桑篇-误会解除！ 第416话 扶桑篇-怪人军团！ 第417话 扶桑篇-琉璃子真正的样子！ 第418话 扶桑篇-疯狂实验室！ 第419话 扶桑篇-被改造的两人！ 第420话 扶桑篇-太一的决断！ 第421话 扶桑篇-绝骑变身！ 第422话 大坂妖怪村！ 第422话 扶桑篇-大坂妖怪村！ 第423话 扶桑篇-夜霜融合！鬼王林风！ 第424话 扶桑篇-大坂战前计划！ 第425话 扶桑篇-王见王！ 第426话 扶桑篇-童童的英灵领域! 第427话 扶桑篇-大坂战起！ 第428话 扶桑篇-林风鬼灵VS上杉天王！ 第428话 扶桑篇-林风VS上杉谦信 第429话 扶桑篇-林风VS上杉谦信 第430话 扶桑篇-暗杀林风！ 第431话 扶桑篇-兄妹相残！ 第432话 扶桑篇-约定完成！ 第433话 扶桑篇-大坂坠落！ 第434话 扶桑篇-斗天VS毘沙门！ 第435话 扶桑篇-融合瞬切！ 第436话 扶桑篇-神器共鸣！ 第437话 扶桑篇-绑定神器！ 第438话 扶桑篇-大坂审判！ 第439话 扶桑篇-重建大坂！ 第440话 扶桑篇-神秘学者！ 第441章 扶桑篇-美食街风云！ 第442话 扶桑篇-爆炸吧！食之魂！ 第443话 扶桑篇-厨神争霸！ 第445话 扶桑篇-猪头摄影！ 第444话 扶桑篇-惊悚乐园！ 第446话 扶桑篇-百鬼狂宴！ 第447话 扶桑篇-人鬼电竞大赛！ 第448话 扶桑篇-荒骷髅战歌！ 第449话 扶桑篇-黄泉摇滚！ 第450话 扶桑篇-迎亲鬼出现！ 第451话 扶桑篇-百鬼哭嫁！ 第452话 扶桑篇-三拜骷髅凉！ 第453话 扶桑篇-幕后黑手！ 第454话 扶桑篇-京都序幕！ 第455话 扶桑篇-再见新大坂！ 第456话 扶桑篇-狐妖开演！ 第457话 扶桑篇-好戏开演！ 第458话 扶桑篇-京都重演！ 第459话 扶桑篇-敬爱的忍影大人！ 第460话 扶桑篇-不平静的忍者生活开始！ 正文通知 第461话 扶桑篇-忍者学生会，就这！ 第462话 扶桑篇-黑皮体育生！ 第463话 扶桑篇-残酷的忍者竞争！ 第464话 扶桑篇-千分之一的力量！ 第465话 扶桑篇-晨训胜利者！ 第466话 扶桑篇-忍界黑暗！ 第467话 扶桑篇-特忍补习班！ 第468话 扶桑篇-文体之战！ 第469话 扶桑篇-抵押灵魂！ 第470话 扶桑篇-牛市降临！ 第471话 扶桑篇-破防的会长！ 第472话- 林风遭遇黑中介！ 第473话 扶桑篇-德川家德真身的猜测！ 排期变更通知 第474话 扶桑篇—黑暗补课界！ 第475话 扶桑篇-忍考开始！ 第476话 扶桑篇-氪金忍术战！ 第477话 扶桑篇-全体满分！忍界大战将起！ 第478话 扶桑篇-忍界大战开始！ 第479话 扶桑篇-鬼观音！ 第480话 扶桑篇-终幕之章！ 第481话 扶桑篇-双重融合！ 第482话 扶桑篇-暗忍们的觉悟！ 第483话 扶桑篇-大神狸全力爆发！ 第484话 扶桑篇-黑化三人组！ 第485话 扶桑篇-找到你了！ 特典预热：6月6日 祈愿解锁你和女神的亲密互动 特典公开：6月6日 祈愿解锁你和女神的亲密互动 祈愿公开：6月6日 祈愿解锁你和女神的亲密互动 第486话 扶桑篇-玉藻前的计划！ 第487话 扶桑篇-瞻仰忍影大人！ 第488话 扶桑篇-请成为新的忍影大人！ 第489话 扶桑篇-给忍都注入新力量！ 第490话 扶桑篇-暗中的鬼画师！ 第491话 扶桑篇-扶桑外的众人！ 第492话 扶桑篇-英灵江薇！ 第493话 扶桑篇-传说中的大宝藏！ 第494话 扶桑篇-游戏开始！ 第495话 扶桑篇-窥视内心的秘密！ 第496话 扶桑篇-游戏结束！ 第497话 扶桑篇-织田信长！ 第498话 扶桑篇-消失的福岛！ 第499话 扶桑篇-魔法少女！ 第500话 扶桑篇-被入侵的世界！ 第501话 扶桑篇-带来希望的魔法少女！ 第502话 扶桑篇-永夜降临！ 第503话 扶桑篇-黑化江薇！ 第504话 扶桑篇-黑魔仙vs魔法少女！ 第505话 扶桑篇-黑魔仙惨败！ 第506话 扶桑篇-黑魔仙的过去！ 第507话 扶桑篇-被孤立的林风！ 第508话 扶桑篇-毁灭进行时！ 第509话 扶桑篇-演的！演的！都是演的！ 第510话 扶桑篇-罗生门！ 第511话 扶桑篇-“校霸”登场！ 第512话 扶桑篇-来自黑魔仙的“欺凌”！ 第513话 扶桑篇-命运的相遇！ 第514话 扶桑篇-命运惨剧！ 第515话 扶桑篇-向世界复仇！ 第517话 扶桑篇-魔法少女战败！惩罚开始！ 第516话 扶桑篇-表里世界！ 第518话 扶桑篇-友谊魔法力量！ 第519话 扶桑篇-罗生门！ 第520话 扶桑篇-五行之力！ 第521话 扶桑篇-织田信长的诞生！ 第522话 扶桑篇-天下布武！ 第523话 扶桑篇-高僧出世！诛邪灭魔！ 第524话 扶桑篇-鉴真破魔！ 第525话 扶桑篇-毁灭与重生！ 第526话 扶桑篇-三象剑主形态！ 第527话 扶桑篇-雪女融化了！ 第528话 扶桑篇-契约雪女！ 第529话 扶桑篇-主角团VS主角团！ 第530话 扶桑篇-大怪兽！黑色巨人！ 第531话 扶桑篇-被压制的魔法少女！ 国庆更新通知！ 监视！控制！净化！ 第532话 扶桑篇-魑魅魍魉之主！ 第533话 扶桑篇-鬼主VS魔王！ 第534话 扶桑篇-光之拉神像！焚世红莲！ 第535话 扶桑篇-永不谢幕 第535话 扶桑篇-永不谢幕！ 第536话 扶桑篇-剪辑世界！百鬼邪瞳！ 第537话 扶桑篇-福岛终战落幕！ 第539话 扶桑篇-昏迷的绘梨衣！ 第538话 扶桑篇-击败信长！收获满满！ 第540话 扶桑篇-天命反派系统！ 第541话 扶桑篇-百鬼跪服！ 第542话 扶桑篇-鬼主庆典！ 第543话 扶桑篇-雪女的梦境！ 第544话 扶桑篇-黑暗阴阳术！ 第545话 扶桑篇-最后一个未诞生的雪妖！ 第546话 扶桑篇-花魁和雪初！ 第547话 扶桑篇-情深陷！意迷离！ 第548话 扶桑篇-花魁的结局！ 第549话 扶桑篇-背后的真相！ 第550话 扶桑篇-报仇雪恨！ 第551话 扶桑篇-雪女的心愿！ 第552话 扶桑篇-猎人与谎言！ 第553话 扶桑篇-家人的守护！ 第554话 扶桑篇-断水流小柔！VS蓬莱绫罗！ 第555话 扶桑篇-都市怪谈来袭！ 第556话 扶桑篇-路西法的背刺！ 第557话 扶桑篇-圣域降临！ 第558话 扶桑篇-共战地狱！ 第559话 扶桑篇-审判仪式启动！ 第560话 扶桑篇-神！路西法！ 第560话 扶桑篇-神！路西法！ 第561话 扶桑篇-弑神！ 第561话 扶桑篇-弑神！ 第562话 扶桑篇-两个路西法！ 第563话 扶桑篇-路西法之心！ 第564话 扶桑篇-五行火之力！ 第565话 扶桑篇-神风部队！ 第566话 扶桑篇-狂猿出关！ 第567话 扶桑篇-随心铁杆兵！ 第568话 扶桑篇-恩怨了结！ 第569话 扶桑篇-执剑人考验！ 第570话 扶桑篇-绘梨衣的心愿！ 第571话 扶桑篇——特殊的晋级！ 第572话 扶桑篇-出发东京！ 第573话 扶桑篇-东京炼狱！ 第574话 扶桑篇-七武士的奉献！ 猜你喜欢 休更通知 错撩 第819话 十帝十道 魔皇大管家 热血玄幻古风魔幻魔法 奇幻少年 第502回 百焰重生 斗破苍穹 热血 古风 战斗 玄幻 延更通知 延更通知 罗小黑战记·蓝溪镇 滑稽搞笑 古风 治愈 第496话 八岐大蛇的追捕 从大树开始的进化 异能 格斗 恋爱 1804 追踪小狐狸 中国惊奇先生 国漫 恐怖 爆笑 惊险 591 春秋山的网络红人 掌门低调点 热血 玄幻 滑稽搞笑 第250话 又见姐姐！ 死灵法师！我即是天灾 其它 热血 373 秘密账本！ 史上最强赘婿 其它 752 天罗 一人之下 玄幻 爆笑 格斗 第280话 穿越又重生？ 我！天命大反派 后宫 古风 系统 250 太古灵参觉醒 顶级气运，悄悄修炼千年 古风 系统 版权投诉： laimanga@outlook.com 本站所有图片均来自互联网收集而来，版权归原创者所有，本网站只提供web页面服务，并不提供资源存储，也不参与录制、上传。 Copyright © 2022-2028 www.laimanhua.org. All Rights Reserved.</p>
</div></details><h2 id="toc-222">112. Japanese joint research group launches quantum computing …</h2>
<ul>
<li>链接：https://www.nict.go.jp/en/topics/2023/04/13-1.html</li>
<li>来源：bing</li>
<li>摘要：Tokyo, March 24, 2023 - A consortium of joint research partners including RIKEN, the National Institute of Advanced Industrial Science and Technology (AIST), the National Institute of Information and …</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-223">正文（抓取，非 AI）</h3>
<p>Japanese joint research group launches quantum computing cloud service | 2023 | NICT - National Institute of Information and Communications Technology Access Contact us 日本語 About NICT Research Radio Research Institute Radio Propagation Research Center Electromagnetic Standards Research Center Applied Electromagnetic Research Center Network Research Institute Photonic ICT Research Center Wireless Networks Research Center Resilient ICT Research Center Cybersecurity Research Institute Cybersecurity Nexus National Cyber Training Center National Cyber Observation Center Universal Communication Research Institute Advanced Speech Translation Research and Development Promotion Center Data-driven Intelligent System Research Center Big Data Integration Research Center Advanced ICT Research Institute Kobe Frontier Research Center Koganei Frontier Research Center Center for Information and Neural Networks Cross-field R&amp;D and other business Beyond 5G Research and Development Promotion Unit Terahertz Technology Research Center Quantum ICT Collaboration Center Open Innovation Promotion Headquarters Research Results Open Innovation/IDI International Cooperation Employment Information Publicity Home &gt; Publicity &gt; Topics &gt; 2023 &gt; Japanese joint research group launches quantum computing cloud service 日本語 Print Share Tweet Japanese joint research group launches quantum computing cloud service Opening access to Japan’s first superconducting quantum computer April 13, 2023 RIKEN National Institute of Advanced Industrial Science and Technology (AIST) National Institute of Information and Communications Technology (NICT) Osaka University Fujitsu Limited Nippon Telegraph and Telephone Corporation (NTT) Tokyo, March 24, 2023 - A consortium of joint research partners including RIKEN, the National Institute of Advanced Industrial Science and Technology (AIST), the National Institute of Information and Communications Technology (NICT), Osaka University, Fujitsu Limited (Fujitsu) and Nippon Telegraph and Telephone Corporation (NTT) announced the successful development of Japan’s first superconducting quantum computer . Starting March 27, 2023, the partners will provide the newly developed technology to users in Japan as a cloud service for non-commercial use under a joint research agreement with RIKEN. The new technology represents a significant step toward the wider use of quantum computing in Japan. The joint research group is comprised of: Dr. Yasunobu Nakamura, Director of the RIKEN Center for Quantum Computing (RQC), Dr. Katsuya Kikuchi, Group Leader of the 3D Integration System Group of the Device Technology Research Institute at AIST, Dr. Hirotaka Terai, Director of the Superconductive ICT Device Laboratory at the Kobe Frontier Research Center of the Advanced ICT Research Institute of NICT, Dr. Masahiro Kitagawa, Director of the Center for Quantum Information and Quantum Biology at Osaka University, Dr. Shintaro Sato, Head of the Quantum Laboratory at Fujitsu Research of Fujitsu, and Dr. Yuuki Tokunaga, Distinguished Researcher at NTT Computer &amp; Data Science Laboratories. Superconducting quantum computer developed at RIKEN Dawn of the Quantum Age: a new frontier in computing technology Since the early twentieth century, quantum mechanics has been attracting attention as a fundamental theory of physics, laying the foundation for the development of various scientific fields. In particular, phenomena including quantum superposition and quantum entanglement have contributed significantly to the development of today’s science and technology. From the perspective of quantum information science, however, the potential of quantum mechanics has not yet been fully exploited. Thus, R&amp;D to apply the fundamental principles of quantum mechanics to the technological fields of computation, communications, and measurement have been gaining momentum worldwide. To further promote research in quantum information science, RIKEN established the Macroscopic Quantum Coherence Research Team under the lead of Dr. Jaw-Shen Tsai in 2001, and the RIKEN Center for Quantum Computing under the lead of Dr. Yasunobu Nakamura in 2021, focusing on R&amp;D in quantum computing. The Center for Quantum Computing conducts R&amp;D not only on superconducting quantum computer hardware, but also on various physical systems including photonics research (led by Dr. Akira Furusawa ), semiconductor methods (led by Dr. Seigo Tarucha ), and methods using atoms in a vacuum. RIKEN is further promoting R&amp;D in quantum software including quantum computing theory, quantum algorithms and quantum architecture, thus covering the entire spectrum of R&amp;D in quantum computing. In 2021, RIKEN and Fujitsu established the "RIKEN RQC-Fujitsu Collaboration Center" within the RQC. Research results were also utilized within the newly developed superconducting quantum computer cloud service. RIKEN and Fujitsu will further leverage their expertise in computing technologies and applied quantum technology to provide a superconducting quantum computer for industrial use by the end of FY 2023. Research Methods and Results The newly developed superconducting quantum computer uses integrated circuits with 64 qubits with two special features: two-dimensional integrated circuits and perpendicular wiring packages. In the two-dimensional integrated circuits, four qubits are arrayed in a square, with each connected to its neighbors by inter-qubit connections (upper right of Fig. 1). In addition, readout resonators and filter circuits for multiple readout are arranged in the square. This basic unit consisting of four qubits can be arranged in two dimensions to form a qubit integrated circuit. The 64 qubit integrated circuit consists of 16 of these basic units formed on a two centimeter square silicon chip (Fig. 1). Figure 1: 64 qubit integrated circuit chip Left: 64 qubit two-dimensional integrated circuit chip performs quantum computation. The design has 16 basic units, each comprised of 4 qubits. The titanium nitride film, which is a superconductor, gives off a gold shine. Upper right: schematic diagram of a basic unit of 4 qubits. Qubits are arranged at the four corners of a square, and a readout circuit designed in the middle. Lower right: electron micrograph of a Josephson junction forming a qubit. Furthermore, if the wiring is on the same planar surface as the qubits in the system, the space of the wiring to the outside is insufficient for the number of qubits arrayed in the chip, so it is necessary to devise methods to control individual qubits and wiring for readout. Therefore, the joint research group adopted a design based on perpendicular wiring packages, in which the wiring to the chip for the qubits arrayed on a two-dimensional planar surface is perpendicularly connected. The research team is further developing a wiring package that enables the wiring to the qubit integrated circuit chip at once (Fig. 2). Taking advantage of the features of the two-dimensional integrated circuits and perpendicular wiring packages, a highly scalable system is formed in which the number of qubits can be easily increased. This enables the large-scale of the newly developed system without altering the basic design. Figure 2: Perpendicular wiring packages Left: schematic diagram of perpendicular wiring. The control and readout wiring for the qubits is perpendicularly connected to the chip via a contact probe for signals. Microwave signals are transmitted and received through the wiring. Right: wiring package for the qubit integrated circuit chip. The signals controlling the qubits use voltage pulses that oscillate at microwave frequencies (8-9 GHz). However, as each qubit requires a different microwave frequency, the joint research group developed a controller that can generate stable microwave phases with high precision (Fig. 3). The team also developed software for the controller that controls the qubits. Figure 3: Qubit controller Qubit controller consisting of an oscillator and receiver for microwave signals. The new 64 qubit quantum computer uses 96 input wires and 16 output wires for control and readouts to perform quantum calculations. Starting March 27, 2023, RIKEN is providing the newly developed superconducting quantum computing technology as a “quantum computing cloud service” (Fig. 4) for non-commercial use to researchers and engineers in Japan under joint research agreements. Users will be able to access the new superconducting quantum computer for jobs within the scope of the joint research agreement and can send data/receive results via the cloud. Figure 4: Image of user access to superconducting quantum computer Authentication of registered users and job transmission/reception on a web interface. Moving forward, the joint research group will further enhance the new system to enable quantum computing operations with a higher number of qubits and increase the density of wiring inside the dilution refrigerator (Fig. 5). The research team will further provide access to the superconducting quantum computer as a testbed for noisy intermediate-scale quantum (NISQ) computers . Based on the newly developed services, the research partners will further accelerate R&amp;D in quantum computing through deeper collaboration with quantum software developers, quantum computing researchers, and corporate developers. Figure 5: Wiring inside the dilution refrigerator of the 64 qubit superconducting quantum computer A 64 qubit integrated circuit chip is placed in a central cylindrical magnetic shield to connect control and readout wiring. In operations, it is necessary to cool the periphery of the chip to about 10 mK (about -273 °C), so the entire chip is placed in a vacuum insulated container and cooled by a dilution refrigerator. Future plans Using the highly scalable integrated circuit (Fig. 6) as a core technology, the joint research team will continue to work toward the realization of quantum computers with 100 and 1,000 qubits, and promote further R&amp;D toward the integration of 1 million qubits and the realization of fault-tolerant quantum computing for real world application. Figure 6: Future image of qubit integrated circuits By periodically arranging basic units composed of four qubits on a planar surface, the number of integrated qubits can be increased. The figure above shows the future image of 1,024 qubits by periodically arranging 64 qubits in a 4 x 4 array. Joint Research Group Members RIKEN Center for Quantum Computing Yasunobu Nakamura, Ph.D., Director (Team Leader of the Superconducting Quantum Electronics Research Team) Shinichi Yorozu, Ph.D. Deputy Director Superconducting Quantum Electronics Research Team Shuhei Tamate, Ph.D., Research Scientist Koichi Kusuyama, Ph.D., Senior Technical Staff Superconducting Quantum Electronics Joint Research Unit Eisuke Abe, D.Sc., Unit Leader Superconducting Quantum Computing System Research Unit Yutaka Tabuchi, Ph.D., Unit Leader 3D Integration System Group of the Device Technology Research Institute at the National Institute of Advanced Industrial Science and Technology (AIST) Katsuya Kikuchi, Ph.D., Group Leader Superconductive ICT Device Laboratory at the Kobe Frontier Research Center of the Advanced ICT Research Institute of the National Institute of Information and Communications Technology (NICT) Hirotaka Terai, Ph.D., Director Yuji Hishida, Ph.D., Fixed Term Research Engineer Center for Quantum Information and Quantum Biology at Osaka University Masahiro Kitagawa, Ph.D., Director and Professor (Professor, Graduate School of Engineering Science) Keisuke Fujii, Ph.D., Deputy Director and Professor (Professor, Graduate School of Engineering Science/Team Leader, Quantum Computing Theory Research Team, RIKEN Center for Quantum Computing) Makoto Negoro, Ph.D., Deputy Director and Associate Professor Takefumi Miyoshi, Ph.D., Specially Appointed Associate Professor (Director, e-trees.Japan, Inc.) Shunsuke Saruwatari, Ph.D., Associate Professor (Associate Professor, Graduate School of Information Science and Technology) Naoyuki Masumoto, Ph.D., Specially Appointed Researcher Quantum Laboratory at Fujitsu Research of Fujitsu Limited Shintaro Sato, Ph.D., Head of Quantum Laboratory NTT Computer &amp; Data Science Laboratories Yuuki Tokunaga, Ph.D., Distinguished Researcher Yasunari Suzuki, Ph.D., Researcher Research Support This research received funding support from the Quantum Leap Flagship Program (Q-LEAP) of the Ministry of Education, Culture, Sports, Science and Technology “R&amp;D of Superconducting Quantum Computer (Research Representative: Yasunobu Nakamura) Grant No. JPMXS0118068682” (RIKEN, Osaka University, Fujitsu Limited, Nippon Telegraph and Telephone Corporation, the University of Tokyo, Tokyo Medical and Dental University, Tohoku University, National Institute of Advanced Industrial Science and Technology, National Institute of Information and Communications Technology. Toshiba Corporation, Mitsubishi Electric Corporation, NEC Corporation, QunaSys Inc.), “Development of quantum software by intelligent quantum system design and its applications (Research Representative: Keisuke Fujii) Grant no. JPMXS0120319794” (Osaka University, Keio University, Nagoya University, University of Tokyo, Kyoto University, Nippon Telegraph and Telephone Corporation, e-trees.Japan, Inc.), The Japan Science and Technology Agency, Exploratory Research for Advanced Technology (ERATO) “NAKAMURA Macroscopic Quantum Machines Project (Research Coordinator: Yasunobu Nakamura),” and COI-NEXT program “Quantum Software Innovation Hub (Project Leader: Masahiro Kitagawa).” (From left): Eisuke Abe, Yasunobu Nakamura, and Yutaka Tabuchi Notes Superconducting method: A quantum computer system that uses a Josephson junction, a tunnel junction element, to realize quantum bits on an electronic circuit using a superconducting material. Because of the small scale of the energy difference of the 0 or 1 qubit state, it is necessary to cool the qubits to extremely low temperatures (to about -273°C) in a dilution refrigerator. Back to contents Quantum computers: Computers performing calculation based on the principles of quantum physics. By using quantum superposition and quantum entanglement that are not possible with conventional computers, quantum computers are expected to solve a variety of problems at high speed, including prime factorization or efficient simulations of quantum-like behavior such as the electronic state in molecules. Back to contents Quantum superposition and quantum entanglement: A phenomenon where multiple states exist at the same time, which is incompatible with how humans experience the world. Quantum entanglement is a unique correlation in quantum physics that occurs in combination with quantum superposition. Back to contents Dr. Jaw-Shen Tsai: Currently Team Leader of the Superconducting Quantum Simulation Research Team at the RIKEN Center for Quantum Computing Back to contents Dr. Akira Furusawa: Deputy Director of the Center for Quantum Computing and Team Leader of the Optical Quantum Computing Research Team Back to contents Dr. Seigo Tarucha: Team Leader, Semiconductor Quantum Information Device Research Team Back to contents Qubits: The smallest unit of quantum information. In a regular digital circuit, a bit can be in one of two states, either 0 or 1, but a qubit can be in a superposition state of 0 and 1. It can superimpose 0 and 1 for any complex numbers, and the state of a qubit schematically can be shown by an arrow displaying an arbitrary point from the center of a sphere to the surface of the sphere. Back to contents Noisy intermediate-scale quantum (NISQ) computer: Small or medium-sized quantum computers in which computing errors generated by noise cannot be corrected. Applications such as variational quantum algorithms are expected for practical use in the near future. Back to contents Fault-tolerant quantum computing: Conventional computers also cause errors, bu</p>
</div></details><h2 id="toc-224">113. 全球诡异时代漫画在线免费看全集下载_漫星阁漫画</h2>
<ul>
<li>链接：https://www.haohuagg.com/manxingge_xiangqing.697546.html</li>
<li>来源：bing</li>
<li>摘要：2025年3月22日 · 全球诡异时代 作者： 黑白茶（原着）+奇小怪 类别： 悬疑 都市 热血 大陆 最新章节： 第323话 鬼王分身灭！ 更新时间： 2025-03-22 09:01:46 简介： 诡异复苏一百年的时间，人类只剩 …</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-225">正文（抓取，非 AI）</h3>
<p>ȫ�����ʱ������������ѿ�ȫ������_���Ǹ����� ��ҳ ���� ��½ ���� ���� ���� ���� ���� ��¼ ��� ע���˻� ��Ա��¼ ���� ȫ�����ʱ�� ���ߣ� �ڰײ裨ԭ�ţ�+��С�� ��� ���� ���� ��Ѫ ��½ �����½ڣ� ��323�� ���������� ����ʱ�䣺 2025-03-22 09:01:46 ��飺 ���츴��һ�����ʱ�䣬����ֻʣ��ʮ��֮һ�����������Ѿ��������࣡ Ϊ�����棬ÿ���˴���������ʼ������Ҫѡ��һ�����������顱�������������Լ��İ������飬�Թ��ƹ��� ȫ��������������Ĺ���ʱ���� ����ϵͳ���ַ����������޵еĹ��飬�������綼Ϊ֮ɪɪ����... ��ʼ�Ķ� ������� �ֻ����Ķ� 0 �� ���� ��601�� ���� �� ���� | ���� ��1�� ����ʱ�����ѣ� ��2�� ͯͯ�� ��3�� �������� ��4�� ������� ��5�� ��ɱ�� ��6�� �������� ��7�� ����չ�� ��8�� ������� ��9�� �Ƕ������� ��10�� ͯͯ���� ��11�� ��ɫ�廨Ь ��12�� ���½��٣� ��13�� � ��14�� �滪 ��15�� ���� ��16�� �������� ��17�� ���滪�� ��18�� ���ԣ� ��19�� A������ ��20�� ���� ��21�� ��ͷ���� ��22�� Ӣ�۾��� ��23�� ǿ������ ��24�� �мƣ� ��25�� ��������� ��26�� �𺳣� ��27�� �������� ��28�� ��Ԯ ��29�� ��սS���� ��30�� ˫S�� ��31�� ����ʧ�� ��32�� ���� ��33�� �ټ�ͯͯ ��34�� ������ ��35�� ��Ԩ������ ��36�� �����Ծ� ��37�� ���չ����� ��38�� Ӣ�۵Ľ��� ��39�� ����Ů��һ�� ��40�� �ƽ����� ��41�� �����еļ����� ��42�� ��ɷ�ķ��� ��43�� �������� ��44�� ���е�Ψһ���� ��45�� ����ֻ���鿪���� ��46�� ���� ��47�� ������и� ��48�� �ռ���ɡ ��49�� ��ת���������࣡ ��50�� ����׹� ��51�� �����vs�������� ��52�� ����֮ս ��53�� ����ƽ�� ��54�� Ӯ��ľ��ȣ� ��55�� �ƽ���� ��56�� �󶨹�ɡ�� ��57�� ��ɡ������ ��58�� �ܲ�������������Ԩ���� ��59�� �������ϡ�� ��60�� �������� ��61�� ��ޱ ��62�� ����ѭ���� ��63�� ����κ��ҫ ��64�� ������ս�� ��65�� �Կ���Ԩ���ף� ��66�� ��ޱ�ĸ��� ��67�� �󶨹����� ��68�� ����� ��69�� �ּ����᳤ ��70�� ��ɡ��һ�� ��71�� ��Լ��� ��72�� ӵ����ʶ�Ĺ����� ��73�� �������vsκ�޼� ��74�� ̽���յأ� ��75�� С����� ��76�� սˮ�ԣ�����Ƭ�� ��77�� С�������� ��78�� ���ڶ�ʳ�� ��79�� �󶨼��������� ��80�� ��Ǵ�У� ��81�� ʬ�� ��82�� ����Ů���� ��83�� ɳ�� ��84�� ���˶� ��85�� ����֮�� ��86�� ��ɫʯ�� ��87�� ս�����������̶ӣ� ��88�� Ѫ�£� ��89�� ������Ϯ�� ��90�� �ڹǣ�ľ��ѹ�ƣ� ��91�� ���������� ��92�� ����Ǩ�� ��93�� ��κ����ս�� ��94�� ��ս����ʿ�� ��95�� ��Ԩ������ ��96�� �����ܿܣ� ��97�� ɳĮ�ų����֣� ��98�� Ѱ��ҹ���� С���� �����Ĵ�Խ�� ��99�� ����Ů�� ��100�� Ѫ����̵Ļ��ԣ� ��101�� �����ԭ�� ��102�� ���Ƿ��� ��103�� ���ʲ��ͷ���ף� ��104�� Ѫ�������� ��105�� �������ܲ��ٻ��� ��106�� ������ ��107�� ׷�ٹ����� ��108�� Χ�����ۣ� ��109�� ��ȫ���գ���Ů���� ��110�� ��Ů�������ݣ�����Σ���� ��111�� �������Ĺֲ� ��112�� �����ƣ�κ�޼����֣� ��113�� ץ�ڼ飡κ����ı�� ��114�� ǧ�򼶹����� ��115�� �ַ�vsκ�޼ʣ� ��116�� ��ѹ�� ��117�� Ԥ֪�����ǵ�ʤ���� ��118�� �������ַ� ��119�� ���һ���� ��120�� ج�μ����� ��ȫ�����ʱ�����������ߣ� ��121�� ҩ��ʦ ��122�� ���ǵı仯 ��123�� ���š��������� ��124�� ��������ͻ�䣡 ��125�� ��ޱ���գ� ��126�� �����ŵ����� ��127�� �������ĳǣ� ��128�� ���������壡 ��129�� �ӳ�õ��ű��� ��130�� Ѫ���ܻ� ��131�� ��Ϸ������������Ŀ�ģ� ��132�� �߾�׷�� ��133�� ج�μ���ս�� ��134�� �������ҹ̸ ��135�� ������������ ��136�� �޽���Σ� ��137�� ����֮������ޱ�ںϳɹ��� ��138�� ��ޱ���� ��139�� ����ξ�����Ů�����ȣ� ��140�� �����������䣡 ��141�� ������ ��142�� ��Խǧ������ ��143�� �綼���ǣ� ��144�� ����֮�� ��145�� �ո�������ȣ� ��146�� Ů���������� ��147�� ����Լ�����ӹٽ����� ��148�� ��Ӱ����ı ��149�� ��ɱ�ƻ��� ��150�� ȫԱ��Σ���� ��151�� ֮ͯͯ�� ��152�� ��ڤ�ۻ����� ��153�� ������ͼ������ ��154�� ��������� ��155�� �����ǹ��� ��156�� ���������� ��157�� ���ص����� ��158�� ������������������ ��159�� ��ĸ���棡 ��160�� ��ö�����Ƭ ��161�� ����������� ��162�� ���������Ĵ��� 163�� ���̵����� ��164�� ͼ��֮Ĺ�� ��165�� ŷ��½�������� ��1�� KKWORLD��չ���ˣ����ո��У� ��2�� �����趨ͼ����ͣ����ո��У� ��3�� ȫ��ʱ���߽��ã����ո��У� ��4�� ����·͸�糡���ո��У� ��5�� ·͸�磨�ո��У� ��6�� �λ����ճ����ո��У� ��166�� �ڶ�ö�����Ƭ ��167�� Ѫ�� ��7�� kkw����ͼ����ͣ����ո��У� ��8�� Q�淬��糡�����ո��У� ��9�� ����·͸�糡�����ո��У� ��10�� ·͸�磡���ո��У� ��11�� �����ı������飡���ո��У� ��168�� ����vs������ ��169�� �������� ��12�� Ů�Խ�ɫ�趨�󹫿������ո��У� ��13�� Q�淬��糡�����ո��У� ��14�� ����糡�����ո��У� ��15�� ·͸�磡���ո��У� ��16�� ���߷�̸�����ո��У� ��170�� ��ţ���Ŀֲ���˵�� ��171�� Ѫ�������� ��17�� �滭С���ã����ո��У� ��18�� Q�淬��С�糡�����ո��У� ��19�� ����糡�����ո��У� ��20�� ·͸�磡���ո��У� ��21�� ���������ؼ������ո��У� ��172�� ľż�糡�� ��173�� ͯͯ��ޣ� ��22�� ģ������С���ã����ո��У� ��23�� Q��С�糡�����ո��У� ��24�� ����糡�����ո��У� ��25�� ·͸�磡���ո��У� ��26�� �ֲ�����ָ�ϣ����ո��У� ��174�� ���±��ݵ����ܣ� ��175�� ͯͯ���վ֣� ��176�� �����Ｏ�룡 ��27�� ���������񵥴󹫿������ո��У� ��28�� ����糡�����ո��У� ��29�� ·͸�磡���ո��У� ��30�� ��Ϧ�ؼ������ո��У� ��177�� ���ض��� ��178�� �����ţ��������� ��179�� ��ɱͼ�أ� ��31�� Q��С�糡���ո��У� ��32�� ����糡�����ո��У� ��33�� ·͸�磡���ո��У� ��34�� ��Ԫ���ؼ������ո��У� ��180�� ڤ��֮�ţ� ��181�� ڤ�磬���� ��35�� ��˿���������ᣡ���ո��У� ��36�� Q��С�糡���ո��У� ��37�� ���ܻ��+����糡+�������ͣ����ո��У� ��38�� ·͸�磡���ո��У� ��39�� ��ѧ�ؼ������ո��У� ��182�� ����֮���Ĺ�ȥ�� ��183�� ��Ů�����ܣ� ��40�� �滭С���ã��ո��У� ��41�� Q��糡�����ո��У� ��42�� ����糡�����ո��У� ��43�� ·͸�磡���ո��У� ��44�� CP��ÿģ����ո��У� ��184�� �����˰��� ��185�� �ع�̮���� ��45�� �ص����عˣ� ��46�� Q��糡�����ո��У� ��47�� ����糡���ո��У� ��48�� ·͸�磡���ո��У� ��49�� ����ͼ���ͣ����ո��У� ��186�� ͯͯ��� ��187�� ���������� ��188�� ������� ��50�� Q��ʵ������ո��У� ��51�� ����糡�����ո��У� ��52�� ·͸�磨�ո��У� ��53�� �����ؼ������ո��У� ��189�� �ո�󺣣� ��190�� ���ѳ�Ѩ�� ��191�� �滪�Ļ��� ��54�� ����ϲ�����±���ζ���ո��У� ��55�� ����糡�����ո��У� ��56�� ·͸�磡���ո��У� ��57�� �ֲ������ڶ��������ո��У� ��192�� �������� ��193�� ��ʹ������ ��194�� ��̽�� ��58�� �ַ�Ůװ�糡�����ո��У� ��59�� ����糡�����ո��У� ��60�� ·͸�磡���ո��У� ��61�� �ֲ������ڶ����յף����ո��У� ��195�� �Ϸ����ĳǣ� ��196�� ����֮���ǿ�У� ��197�� ��ǧ�ƻ����� ��62�� �����ǵ�����Ȧ���ո��У� ��63�� ����糡�����ո��У� ��64�� �����һ���������ɣ����ո��У� ��65�� ��˿�������Ÿ�����ͼ���ͣ��ո��У� ��67�� ����糡�����ո��У� ��198�� �ո��Ϸ��� ��199�� ˮ��Ϯ���� ��200�� �����գ� ��66�� �ر�������ʦС�ʴ��ո��У� �����ص伾��ԤԼ���� ��68�� �ر����ַ�&amp;ͯͯ������ϼ������ո��У� ��69�� �ر����������ؼ�--���ȹ���ƻ������ո��У� ��201�� ˮ��С�ᣡ ��202�� �����԰�� ��203�� ���ƻ��ƣ� ��70�� �ر��������ӽǵ�������+��������𰸣��ո��У� ��71�� ����糡�����ո��У� ��72�� ·͸�����ܾ��ʾ��飡���ո��У� ��73�� �ر��󻮲����ǣ��͵��������ո��У� ��204�� ���ݱ�¶�� ��205�� ����������ᣡ ��206�� ��ս�ĺŽǣ� ��74�� �ر��󻮿ĵ�allͯ���ո��У� ��75�� ����糡�����ո��У� ��76�� ·͸����ʹ�������� ��77�� �ر�������ʦС�ʴ�ڶ����� ��207�� �����ƻ��� ��208�� ������ʹ�� 209�� ������ס�ˣ� ��78�� �ر�������ŮУ����ǽ ��79�� ����糡�����ո��У� ��80�� ·͸�����ܾ��ʾ��飡���ո��У� ��81�� ��˿����Ů�Խ�ɫ��ͼ���͵ڶ��������ո��У� ��210�� ��޵ĸ������� ��211�� ���ˣ� ��212�� �����ɵ��� ��82�� �����Q���ַ磿����ʲô�� ��83�� ����糡�����ո��У� ��84�� ·͸�����ܾ���·͸������ ��85�� �ر�������ŮУ��̳--У����ѡ�� ��213�� �ɵ�̽�գ� ��214�� ��޵ĵ����� ��215�� ��������ɡ�� ��86�� �ر��󻮻���ɱ֮�ַ������ĵ�һֻ���� ��87�� ����糡�����ո��У� ��88�� �ر���·͸�� ��89�� �������ȫ����ս���̵㣨��һ���� ��216�� ����Σ���� ��217�� �ַ�ľ��ϣ� ��218�� ��ֵ����ˣ� ��90�� �ر�����ȫ����ս���̵㣨�ڶ����� ��91�� ����糡�����ո��У� ��92�� �ر����֮�������ع� ��93�� �ر��󻮶���С�糡֮Χ��style ��219�� ���˵����ݣ� ��220�� ����а�� ��221�� ȫԱج�μ��� ��94�� ��������ű��������100% ��95�� ����糡�����ո��У� ��96�� ·͸�����ܾ��ʾ��飡 ��97�� �ر��󻮰����������ȿ��� ��222�� ��櫣� ��223�� ��ʹ���ţ� ��224�� ʥ��֮ս�� ��98�� �ر��󻮿հ���ɫС��Ϸ�� ��99�� ����糡�� ��100�� ·͸�����ܾ��ʾ��飡 ��101�� �ر���ȫ������Ȧ���£� ��225�� ������ħ�ˣ� ��226�� ʥ����Ƭ�� ��227�� �����ţ� ��102�� �ر�������ϵ�ȫ� ��103�� ����糡�� ��104�� ·͸�����ھ���Ԥ�� ��228�� ǿ��Ļ��ף� ��105�� �ر���ȫԱʥ��style ��229�� �������֣� ��230�� ������ɣ� ��106�� �ر��󻮴�ͯͯ���ŵ��Ĵ����ɣ� ��107�� ����糡�� ��108�� ·͸�类��Ů��Χ��ȫԱ����ͷ�ַ磿�� ��109�� �ر���ȫ�������ܽᱨ�� ��231�� �Ұ��籩�� ��232�� �ַ��ʦ�� ��233�� Ӣ�鼶 ��110�� �ر��󻮼�����ʹ���ŵ������� ��111�� ����糡 ��112�� ·͸�����ܾ��ʾ��飡 ��113�� �ر������԰��»�������ף���� ��234�� ��¡¡�� ��235�� ��ħ���ַ磡 ��236�� ʦͽ����� ��114�� �ر���ȫ����ȫ����ۿ��յ�һ�� ��115�� ����糡 ��116�� ·͸�����ܾ��ʾ��飡 ��117�� �ر���ȫ����ȫ����ۿ��յڶ��� ��237�� �ֲ��Ĺ��� ��238�� ������䣡 ��239�� С��Ŀ��ִ���� ��118�� �ر��󻮼������������ ��119�� ����糡 ��120�� ·͸�����ܾ��ʾ��飡 ��121�� �ر��󻮽�ɫ�󾺲� ��240�� ����һ���� ��241�� ·����֮�ѣ� ��122�� �����󻮶���ʲô����֮�ʣ� ��123�� ����糡 ��242�� Ӣ���ںϣ� ��124�� ·͸�����ܾ��ʾ��飡 ��125�� ·͸�����ܾ��ʾ��飡 ��243�� ��Լ·������ ��244�� ����лĻ�� ��245�� �������ʹ�� ��246�� ��ʹ����ƣ� ��126�� �����󻮸����ط��и����ط�����Ů ��127�� ����糡 ��128�� ·͸�����ܾ��ʾ��飡 ��247�� ��ʹ��׷���ˣ� ��129�� �������군���ұ���Ů��Χ�ˣ� �ص伾��2��13�� �������ս���������� ��248�� ����ʧ��԰�� ��249�� ����ɭ�֣� ��130�� �ر�����ʹ���ˡ�������λ�� ��250�� ��ʹ���Ѽǣ� ��131�� ����糡 ��132�� ������ȫ�����ũ���֣���ʼӪҵ�� ��133�� �������ѵ���ֵ��ǿ����ĳ����ȣ� ��134�� ����������ݵ�����˵���˵Ļ� ��135�� ����糡 ��136�� ���������ֺ���Ҫ�� ��137�� �����󻮿����������Ĳ�һ���� ��251�� ����۾ӵأ� ��252�� ��ĸ�壡 ��253�� ɭ������ ��138�� ������ѡ���������һ��ѧ�ʣ� ��139�� ����糡 ��140�� ������ͬ�κϼ� ��141�� �����󻮱����������� ��254�� ����֮Ӱ�� ��255�� ����֮���� ��256�� ս����ɣ� ��257�� ���ڰɣ�Ģ���ƣ� ��258�� ɥʬ���� ��259�� �������֮�ģ� ��260�� ��֮���� ��261�� ������Σ���� ��262�� �����Ƚ�ţ� ��263�� ħЫ��ۣ� ��264�� ������̥�� ��265�� ˹�ҿ�˹�� ��266�� ����Ĵ��ǣ� ��267�� ��Խ��ǧ�����ʶ�� ��268�� ��֮��Լ�� ��269�� ��ѧ�Ű����� ��270�� è����Լ�� ��271�� ���٣� ��272�� ��ս��ʼ�� ��273�� Ī�ٵļƻ��� ��274�� ��Ա���λ�� ��275�� ��ɢ�ɣ�ǧ���Թ�ޣ� 276�� ��Ӥ������ ��277�� �������٣� ��278�� ��������� ���б���3�������� ��279�� ��Խʱ�䣡 ��280�� ������˹������ ��281�� ����ʱ�����𣿣� ��282�� ǧ����վ֣� ��283�� �µ�ʱ����������˵�� ��284�� ����Σ�������鰢�ģ� ��285�� �Ǹ����˻����ˣ� ��286�� ����仯�����磡 ��287�� ��Լ������ ��288�� ����Ա���֣� ��289�� ���̱�ԭ�� ��290�� ��ԭ������Ů�� ��291�� ��ԭ�ɼң� ��292�� �ִﱱԭ�� ��293�� �żҿ�ջ�� ��294�� ��ջ���ع��� ��295�� ���Ŀ�ʼ�� ��296�� �������ϣ� ��297�� �����Ĺ��� ��298�� Ů���ַ类���ˣ� ��299�� ����Ȣ�׵����� ��300�� ������ǧ ��301�� ɱ�����ĳ��� ��302�� ��ʮһ�㣡 ��303�� ���նľ֣�ս��ģʽ�� ��304�� ��ն̰���� ��305�� ��ԭ��ɽ����ʹ�� ��306�� ���������� ��307�� ��ɽ������ ��308�� ������ħ�� ��309�� �Ի����������壡 ��310�� �ƺ󣡴��²�� ��311�� ���鿪ʼ�� ��312�� �ַ�Ҫ���ޣ� ��313�� ӭ�׾��䣡 ��314�� ��ɽ���ǣ� ��315�� ��һǧ����� ��316�� ��ɽ��ս���Ҳ�װ�ˣ� ��317�� ʱ�����ˣ� ��318�� �����𾡣� ��319�� �����ںϣ�������̬�� ��320�� ����vs���ɣ������� ��321�� ���ɽ����� ��322�� �����ַ磡 ��325�� ��������� ��326�� ��Ӱɢ����ħ�ַ磡 ��327�� ��Ԯ���! ��328�� ��������ı�� ��329�� ���������£� ��330�� ������壡����ַ磡 ��331�� ����ƪ-�Ҹ����ȣ� ����ƪԤ�� ��332�� ����ƪ-��ٵ����磡 ��333�� ����ƪ-���ӣ����ƣ������� ��334�� ����ƪ-����������� ��335�� ����ƪ-���������� ��336�� ����ƪ-�����������ݹ��ɹ��� ��337�� ����ƪ-������Ļ��䣡 ��337�� ������Ļ��䣡 ��339�� ����ƪ-������֮���� ��340�� ����ƪ-Ǳ���׳ǣ� ��341�� ����ƪ-����֮�ǣ� ��342�� ����ƪ-���Ǽݵ����������£� ��343�� ����ƪ-С�������� ��344�� ����ƪ-ҩ���������� ��345�� ����ƪ-�������� ��346�� ����ƪ-���������滪������ ��347�� ����ƪ-�ַ��սǰı���� ��348�� ����ƪ-��Ÿ�����ս��ʼ�� ��349�� ����ƪ-�ַ��ƾ�! ��350�� ����ƪ-��ǿС�ᣡ ��351�� ����ƪ-���ɽ�ޱ�� ��352�� ����ƪ-Ӣ�鼶����! ��353�� ����ƪ-ȫ��ѹ�ƣ� ��354�� ����ƪ-Ů�����ף� ��355�� ����ƪ-�������� ��356�� ����ƪ-ʯ������� ��357�� ����ƪ-�������� ��358�� ����ƪ-����Ů�ų����� ��359�� ����ƪ-ȫ��ֱ���� ��360�� ����ƪ-��˿֮���� ֪ͨ ��361�� ����ƪ-�������� ��362�� ����ƪ-�������� ��364�� ����ƪ-ħ�޻����� ��365�� ����ƪ-���ף���Գ���࣡ ��366�� ����ƪ-���Ķ��������� ��367�� ����ƪ-���������� ��368�� ����ƪ-����ŷ�ͣ� ��369�� ����ƪ-������ ��370�� ����ƪ-���ص����ι��� ��371�� ����ƪ-�������� ��372�� ����ƪ-���������أ� ��373�� ����ƪ-������ս��ʼ�� ��374�� ����ƪ-���֮���� ��375�� ����ƪ-���ξ��� ��376�� ����ƪ-����ĵ����� ��377�� ����ƪ-���ι��ƣ� ��378�� ����ƪ-������ѣ� ��379�� ����ƪ-���������ƻ��� ��380�� ����ƪ-�ַ����ף� ��381�� ����ƪ-���������� ��382�� ����ƪ-��������! ��383�� ����ƪ-�ձ�ɽ������ ��384�� ����ƪ-ʱ�������ʦ�� ��385�� ����ƪ-���о��ӣ� ��386�� ����ƪ-���μ�ս�� ��387�� ����ƪ-а��α�� ��389�� ����ƪ-������֮���� ��388�� ����ƪ-�������֣� ��389�� ����ƪ-���ض����� ��390�� ��ɣƪ-����ս���� ��391�� ��ɣƪ-������Ⱦ���󣡱���ĺ������ ��392�� ��ɣƪ-��Ӱ���֣� ��393�� ��ɣƪ-��Ů���裡 ��394�� ��ɣƪ-���ŷ�ɣ��ǰ���� ��395�� ��ɣƪ-�޴�ĺڴ��� ��396�� ��ɣƪ-��½��ɣ�� ��397�� ��ɣƪ-��ɣ���£�а���٣� ��398�� ��ɣƪ-��ɣ������ ��399�� ��ɣƪ-��ɣ��ħ�� ��400�� ��ɣƪ-��ӣ֮�֣� ��401�� ��ɣƪ-��ɣ��û�� ��402�� ��ɣƪ-��ɣ���ԣ� ��403�� ��ɣƪ-������Ҷ˪��Σ�� ��404�� ��ɣƪ-ս�������� ��405�� ��ɣƪ-����Ԯ���� ��406�� ��ɣƪ-�������� ��407�� ��ɣƪ-���Ź�ͶӰ���٣� ��408�� ��ɣƪ-�����ܱ��� ��409�� ��ɣƪ-�����ƶ��� ��410�� ��ɣƪ-ľ֮����֮���� ��411�� ��ɣƪ-ͯͯ���ף� ��412�� ��ɣƪ-ѩŮ����� ��413�� ��ɣƪ-����֮�أ� ��338�� ����ƪ-��Գ���׳ǣ� ��414�� ��ɣƪ-������յǳ��� ��415�� ��ɣƪ-������� ��417�� ��ɣƪ-���������������ӣ� ��418�� ��ɣƪ-���ʵ���ң� ��419�� ��ɣƪ-����������ˣ� ��420�� ��ɣƪ-̫һ�ľ��ϣ� ��421�� ��ɣƪ-��������� ��422�� ��ɣƪ-�������ִ壡 ��423�� ��ɣƪ-ҹ˪�ںϣ������ַ磡 ��424�� ��ɣƪ-����սǰ�ƻ��� ��425�� ��ɣƪ-�������� ��426�� ��ɣƪ-ͯͯ��Ӣ������! ��427�� ��ɣƪ-����ս�� ��428�� ��ɣƪ-�ַ����VS��ɼ������ ��422�� �������ִ壡 ��430�� ��ɣƪ-��ɱ�ַ磡 ��431�� ��ɣƪ-������У� ��432�� ��ɣƪ-Լ����ɣ� ��433�� ��ɣƪ-����׹�䣡 ��434�� ��ɣƪ-����VS��ɳ�ţ� ��435�� ��ɣƪ-�ں�˲�У� ��436�� ��ɣƪ-���������� ��437�� ��ɣƪ-�������� ��438�� ��ɣƪ-�������У� ��439�� ��ɣƪ-�ؽ����࣡ ��440�� ��ɣƪ-����ѧ�ߣ� ��441�� ��ɣƪ-��ʳ�ַ��ƣ� ��442�� ��ɣƪ-��ը�ɣ�ʳ֮�꣡ ��443�� ��ɣƪ-�������ԣ� ��444�� ��ɣƪ-�����԰�� ��445�� ��ɣƪ-��ͷ��Ӱ�� ��446�� ��ɣƪ-�ٹ����磡 ��447�� ��ɣƪ-�˹��羺������ ��448�� ��ɣƪ-������ս�裡 ��449�� ��ɣƪ-��Ȫҡ���� ��428�� ��ɣƪ-�ַ�VS��ɼǫ�� ��416�� ��ɣƪ-���˾��ţ� ��332�� ��ٵ����磡 ���ӣ����ƣ������� ��324�� ��Ӱ�֣��ַ籩�ߣ� ��323�� ���������� ��� �Ƽ� ���ջ� �������Ҹ������ �������ǰ�� ������꣬��ĭ�������޵�ͷ�ε��Ź˱�����ů����ʱ��ʱ�Ļ�ҪӦ��������ţ����ǰ��������С����ͬʱ���ڴ����������㡭��һ�����ѳ���ϰ�ߣ�����ͻȻ��һ��ȴ�յ�������һ�仰��ĭ���������ɡ� ���ջ� ��ʵ��Ϸ ���ڴ򹤵ĳ¶��侹Ȼ���������⣬����������ʶȴ������һ����ֵĿռ䣡ͬʱ���ռ��ﾹȻҲ�кܶ�����һ������������ˣ�ԭ��Ļ����һ���ֱ��ټ����������μ�һ���Ե�����ϷΪ������������Ϸ��ֻ��ͨ�ص��˲��ܸ�������Ϸ������ʲô�� ͣ������ ����¼ �������µ�Ů̽�����,һҹ֮����ư���Ů��Ϊ��ɱȫ�ҵ����֣���Ϊ����׽�õ�ͨ��������ֻ�����ӵ�������ԩ��;����������������������ס���������С�¹ٵ����ݣ�ȥ�����Լ����ߵ��������������𴺵ư�����������ˡ���������������Ŷ϶��������������߱������... ���ջ� ���Ե��յ� ������ ��������Ľ�֯�������ڻص���ȥ������������·�ϵֵ�ǰ�У���ıװ�������������Լ������������ҡ��� �ڶ�����ʮ�� �����ѡ����Ӫ ������ҫ��ĩ�ձ߾� ��Ʒ�Ƽ�2 ������С��Ů�Ŀ�� Ԥ�� �����ҵ�С��Ů ͻȻ���ֵ��������ˣ����̵����������Լ����ص�ռ���ݣ��������ƺ������˲����˵����顣 ���̰�ռ���ݴ�����������ʲô���⣿ �������ǵ�������Ů��Ŀ��������� ��Ϊ���˵ĳ��ֿ�ʼת�������� ռ�����ﻹ���źܶ಻Ϊ��֪�����ܡ��� ��52�� ���ֿ��� ����¼ ԬС�������ǰһ�����ϣ�Ӧ��ȥ�μ�һ��ͬѧ�ۻᣬ��;�������˰ٹ�ҹ�У�����һ������Ů����ʫ��ֱ���Լ����ǽ���֮�ˡ��Ҿ����ⳡ�ۻ��У�ԬС�����ⱻ����������ǡ�ô�ʱ��Ǵ���٣��������£�������ԬС�����ϵĹ�����������������˸����ţ�ԬС����˽��Ҽ����... ���ջ� �ر�ƪ4 �������� ����Ӹ�������Ϊ�˰��Ѳ���������ѧϰ��ĸ�׵Ŀ��ƣ���װ��ͬ��ͬѧ����Լ�ᣬ���͵͵ѧϰ������������ʵ����ɱ�֣�Ϊ��ɱ������ĸ�׶��ӽ������ۿ����µļƻ�һ����ʩ�У����ڴ�ʱ��һ����������ܸ�ˮ����������ʵ�ǡ��� ��Ʒ�Ƽ�2 ������С��Ů�Ŀ�� Ԥ�� ɽ������¼ һ����ͨ�ı�ҵ���У�ȴ������������ѩ�������ͬѧ�Ĳ���·��ɽ�еľ��֣�ͻȻ���ֵ����ز��ӣ�ƽ���������У�������������ʲô����.. ���ջ� ����Ϸ�в���֪ ʱ������ �������Ϣ�����ʱ����˭��ӵ�и߹�ע��������˭���ܳ�Ϊ���ڵĽ��㣬����λ�ͼ�ֵ��Խ�ߡ������˸ı�Ů�������ˣ���Ů��վ�����������ˡ�Ů����������������������������ٵ���Ԩ�в��ϵ��������� ��15��(��) ����ɱ�� �����˺�Ů�͵������Ĺ�ϵ�£������ǲ��Ǵ����Ͳ��ǵ��Լ���Ҫ�Ķ����� ��Լ����ż��׷ɱ��������ҵ�������ʲô�� С�׺�������-��̫�����ε��λù����������ݣ� ����ɱ�ֹٷ�����Ⱥ �����Զ��1��Ⱥ����ѩ��11814767 �����Զ��2��Ⱥ�����ȣ�66755902 �����Զ��3��... ���ջ� ռ��ʦ ����������һ���ľ��Ĳ�ݣ�Ϊ����֮����裬Ϊ�г�֮��ռ�ǡ���������ռ��ʦ��ռ��ȥ��ռ��Ը��ռ����֮�˵�һ��������ֻ������֮�˼�����������ݵȴ�ǧ�ֻ꣬Ϊһ���ֻء�������ʱ���������������ڳ��У�ʱ����������ǣ�û�������Ŀ־壬ֻ�д��ı��ˡ�һ����һ... ��Ʒ ���� ��512��1 ����ø� ����� ����һ����˭�����棿 ����һ����������磬�ں������飬�Ϳ��Գ�Ϊǿ�������ʦ�� ��Ϊһ��ʱ������֮�飬ʱ�շ�����Ťת����һ�����¿�ʼ֮ʱ������֮�ֻ���ת���� ��592�� ����ն��8 �������� ������������Ψһ�ĵ�Ů��ȴ�Ǿٹ���֪�ķ�� �����������������������21������Σ�յ����ɱ�֣�����Ϊ�ƣ�����Ϊ�꣡ ��������С��չ¶��â�����ž��ޣ�����������ŮΪ֮�㵹���� ���Ǿ�ɫ�İ�ҹ֮������Ѫ���飬�����к�����꣬һ��������࣬�Դ����ϵ��£���... ��2��322�� ����2 ������ һ����߳����·�ķ�������һ���Ƿϲ�����������񣬵������������棬����ʲô�ǲ����ܵ��أ� ���յȼ����һ��������ƶˣ��ϲ�ҽʦ�����������硣СС�����ָ����һ��һ����ӡ���������ƺ���ͷ�����֮�ۣ�����or���У�����or��ͨ����ҩor����������������Ϯ��... ��453�� ��Ե���� �ַ�����õ�һ���һ����䣬ѧϰ�����ֳ��������Ӵ˳����˼侫�ʣ��������·��ƣ�����Ⱥ�ۣ� ��934�� ������� һ������ǿ�ߵĺ���֮·�������ڲ࣬�����䴫�У��ư˷�Ȩ�����ɾ���������������ɰ��������Ů���������ŵĿ���.���ص�������Ů���д�˵�е�����Ů����һ����һ�������֮·��һ�β�һ���Ĵ��棡 ��770�� �Էϼ��ɣ��� ����а�� һ��������������Ϊ����������������˳�Ц�������»�֮ҹ���˶������������������ֻؾ��֣�����������������������ų�����ź�����Ҫ�Ƕ��������۷壡 ��377��1 ��ң�1�� �������� ������������ˣ��������������ߣ����</p>
</div></details><h2 id="toc-226">114. 大语言模型_百度百科</h2>
<ul>
<li>链接：https://baike.baidu.com/item/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/62884793</li>
<li>来源：bing</li>
<li>摘要：2023年12月26日 · 大语言模型（英语：Large Language Model，简称LLM）是指使用大量文本数据训练的 深度学习 模型，使得该模型可以生成自然语言文本或理解语言文本的含义。 这些模型可以通过在 …</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-227">正文（抓取，非 AI）</h3>
<p>大语言模型_百度百科 网页 新闻 贴吧 知道 网盘 图片 视频 地图 文库 资讯 采购 百科 百度首页 登录 注册 进入词条 全站搜索 国际版 帮助 首页 秒懂百科 特色百科 知识专题 加入百科 百科团队 权威合作 个人中心 大语言模型 播报 讨论 上传视频 使用大量文本数据训练的深度学习模型 收藏 查看 我的收藏 0 有用+1 0 本词条由 中国科学院大学计算机科学与技术学院 参与编辑并审核，经 科普中国·科学百科 认证 。 大语言模型（英语：Large Language Model，简称LLM）是指使用大量文本数据训练的 深度学习 模型，使得该模型可以生成自然语言文本或理解语言文本的含义。这些模型可以通过在庞大的数据集上进行训练来提供有关各种主题的深入知识和语言生产 [1] 。其核心思想是通过大规模的无监督训练学习自然语言的模式和结构，在一定程度上模拟人类的语言认知和生成过程。 LLM在多种应用场景下表现出色，不仅能执行拼写检查和语法修正等简单的语言任务，还能处理文本摘要、机器翻译、情感分析、对话生成和内容推荐等复杂任务。通过在大规模数据集上进行预训练，大语言模型获得了强大的通用建模能力和泛化能力。近期，GPT-4和LLaMA等大语言模型在自然语言处理等领域取得了巨大的成功，并逐步应用于金融、医疗和教育等特定领域 [2] 。 2023年12月26日，大语言模型入选“2023年度十大科技名词” [3] 。2024年4月，在第27届联合国科技大会上，世界数字技术院发布了《 生成式人工智能应用安全测试标准 》和《 大语言模型安全测试方法 》两项国际标准，由OpenAI、蚂蚁集团、科大讯飞、谷歌、微软、英伟达、百度、腾讯等数十家单位多名专家学者共同编制而成 [4] 。 中文名 大语言模型 外文名 Large Language Model、LLM 所属学科 人工智能 别 名 大规模语言模型、大型语言模型 [5] 应用领域 金融、法律、政务 [6] 、教育 [7] 、传媒 [8] 等 目录 1 技术原理 2 发展历史 ▪ 技术起源 ▪ 发展历程 ▪ 重大节点 3 基本原理 ▪ 训练流程 ▪ 工作原理 4 模型特点 ▪ 训练成本 ▪ 局限性 ▪ 大语言模型对比 5 相关研究与发展 ▪ 相关社会影响 ▪ 最新研究进展 ▪ 未来发展方向 6 应用领域 ▪ 教育 ▪ 金融 ▪ 政务 ▪ 医疗 ▪ 办公 ▪ 客户联络 7 风险与挑战 ▪ 可信性 ▪ 可解释性 ▪ 应用成本较高 ▪ 能力迁移 ▪ 技术风险 ▪ 隐私保护 技术原理 播报 编辑 大语言模型（英语：Large Language Model，简称LLM）是一种基于深度学习的人工智能技术，也是 自然语言处理 的核心研究内容之一 [6] 。其核心是使用大规模数据集对模型进行训练，从而使其能够生成自然语言文本或理解语言文本的含义。这些模型通过层叠的神经网络结构，学习并模拟人类语言的复杂规律，达到接近人类水平的文本生成能力。大语言模型采用与小模型类似的Transformer架构和预训练目标（如 Language Modeling），与小模型的主要区别在于增加模型大小、训练数据和计算资源 [9] 。相比传统的自然语言处理（Natural Language Processing, NLP）模型，大语言模型能够更好地理解和生成自然文本，同时表现出一定的逻辑思维和推理能力。 发展历史 播报 编辑 技术起源 大语言模型的起源可以追溯到20世纪50年代，当时人工智能领域的先驱们开始探索如何让计算机理解和生成人类语言。20世纪70年代由贾里尼克提出的N-gram语言模型是最常用的统计语言模型之一，广泛用于当今的多种自然语言处理系统中 [10] 。 N-gram 模型将文本序列划分为长度为N的连续词组（N-gram），并利用大量语料库训练模型，以预测给定N-gram的后续词。N-gram模型虽然是一种有效的语言建模技术，但是存在着一些局限性，如数据稀疏性、计算复杂性和语言模型的可扩展性等。基于N-gram语言模型的不足，人们开始尝试用神经网络来建立语言模型。 发展历程 雏形阶段 20世纪40年代末和50年代开始采用计算机技术来研究和处理自然语言 [11] 。1950年，图灵测试诞生。1954年，美国人乔治·戴沃尔设计出第一台可编程机器人。1956年，美国达特茅斯学院举行历史上第一次人工智能研讨会，标志人工智能诞生 [12] 。 1966年，世界上第一个聊天机器人--ELIZA，由美国麻省理工学院（MIT）约瑟夫·魏岑鲍姆发布。ELIZA能通过脚本理解简单的自然语言，并能产生类似人类的互动 [12] 。 1975年，Frederick Jelinek等人在论文《Continuous Speech Recognition by Statistical Methods》中提出并应用N-gram模型于语音识别任务。之后随着神经网络的发展，出现了神经语言模型 [13] 。 2010年，斯坦福大学推出Core NLP套件，该套件提供了一套工具和算法，帮助研究人员处理复杂的NLP任务，允许开发人员执行情感分析和命名实体识别 [14-15] 。 2011年，出现了一个较小版本的Google Brain，具有单词嵌入等高级功能，使自然语言处理系统能够更清楚地理解上下文 [14] 。 2013年，自然语言处理模型 Word2Vec 诞生，首次提出将单词转换为向量的“词向量模型”，以便计算机更好理解和处理文本数据 [16] 。 GPT模型问世 2017年，Google发布论文《Attention is all you need》，提出Attention机制和基于此机制的Transformer架构。此架构价值在于是一种完全基于注意力机制的序列转换模型，而不依赖 循环神经网络 （Recurrent Neural Network, RNN）、 卷积神经网络 （Convolutional Neural Network, CNN）或者长短期记忆（Long Short-Term Memory, LSTM ） [13] 。 2018年，Google AI研究院的Jacob Devlin等人提出了BERT（Bidirectional Encoder Representation from Transformers）， BERT利用掩码机制构造基于上下文预测中间词的预训练任务，很大程度上提高自然语言处理任务的性能。BERT出现具有重大意义，尤其是预训练+参数微调”的研究范式，此后出现更多预训练语言模型都是以该范式为基础；同年，OpenAI公司同样发布了自己的模型GPT（Generative Pre-Training），这是一个典型的生成式预训练模型 [13] 。 2019年，OpenAI发布GPT-2，该模型可以不用根据下游任务数据进行参数优化，可以根据给定指令自行理解并完成任务 [13] 。 2020年，OpenAI发布GPT-3，并在Github上开源GPT-3部分样本和数据集。该模型拥有1750亿个参数。该模型的发布是一件跨时代的事情，意味着自然语言处理领域的大语言模型真正意义上出现了，从此正式开启大语言模型时代 [13] 。 进阶突破阶段 2019年，Radford等人使用GPT-2模型研究大语言模型在零样本情况下的任务处理能力；Brown等人在GPT-3模型上研究通过语境学习进行少样本学习的方法指令微调将大量各类型任务，统一为生成式自然语言理解框架，并构造训练语料进行微调 [17] 。 2022年，Ouyang等人提出使用“有监督微调+ 强化学习”的InstructGPT算法 [17] 。 这些方法逐渐扩展到利用生成式框架针对大量任务进行有监督微调的方法，有效提升模型的性能 [17] 。 2022年11月30日，OpenAI公司发布ChatGPT，该模型属于一类基于GPT技术的大语言模型。Google、Microsoft、NVIDIA等公司也给出了自己的大语言模型 [13] 。 2023年，谷歌公布聊天机器人Bard，它由谷歌的大语言模型LaMDA驱动；同年，百度正式宣布将推出文心一言，3月16日正式上线。文心一言的底层技术基础为文心大模型，底层逻辑是通过百度智能云提供服务，吸引企业和机构客户使用API和基础设施，共同搭建AI模型、开发应用，实现产业AI普惠；3月，Open AI发布多模态预训练大模型GPT4.0 [18-19] 。 2023年4月13日，亚马逊云服务部门在官方博客宣布推出Bedrock生成式人工智能服务，以及自有的大语言模型泰坦（Titan） [20] 。 2024年3月，Databricks推出大语言模型DBRX，号称“现阶段最强开源AI” [21] ；马斯克的xAI公司正式发布大模型Grok-1，参数量达到3140亿，超OpenAI GPT-3.5的1750亿 [22] ；4月，在瑞士举行的第27届联合国科技大会上，世界数字技术院（WDTA）发布了《生成式人工智能应用安全测试标准》和《大语言模型安全测试方法》两项国际标准，是由OpenAI、蚂蚁集团、科大讯飞、谷歌、微软、英伟达、百度、腾讯等数十家单位的多名专家学者共同编制而成 [4] 。 重大节点 Transformer结构 在大语言模型的发展历程中，最重要的里程碑是2018年谷歌发布的Transformer模型，它采用了自注意力机制，可以更好地捕捉语言中地长距离依赖关系，从而极大地提高了大语言模型的效果。通过其自注意力机制，Transformer不仅解决了递归神经网络在并行化处理上的限制，还显著提升了模型处理大规模数据集的能力。这种技术的进步为预训练语言模型（PLMs）的发展铺平了道路，使得这些模型能够更加灵活地适应各种不同的下游任务。 Transformer是一种用于序列到序列（Sequence-to-Sequence）任务的神经网络模型，如机器翻译、语音识别和生成对话等。它是第一个完全依赖于自注意力机制来计算其输入和输出的表示的转换模型。序列到序列模型采用的是编码器-解码器结构，编码器-解码器结构采用堆叠的多头注意力机制加全连接层。通过查询-键-值的模式使用多头注意力。由于Transformer模型中既没有递归，也没有卷积，如果需要获得输入序列精准的位置信息，必须插入位置编码。位置编码和输入嵌入有相同的维度，所以二者可以实现相加运算，位置编码方式可以有多种 [23] 。 从人类反馈中强化学习（RLHF） 人类反馈强化学习（Reinforcement Learning from Human Feedback, RLHF）是一种利用人工指导来微调预先训练好的大型语言模型（LLMs）的方法。由三个相互关联的过程组成：反馈收集、奖励建模和策略优化。RLHF优势在于能更好地与人类的意图保持一致，以及以未来的反馈为条件进行规划，从各种类型的反馈中进行流畅的学习，并根据需要对反馈进行整理。此外，RLHF还允许机器通过抽象人类的价值学习，并不是简单地模仿人类的行为 [24] 。 2023 年4月OpenAI联合创始人John Schulman在Berkeley EECS会议上所做的报告“ReinforcementLearning from Human Feedback：Progress and Challenges”，分享了OpenAI在人类反馈的强化学习方面的进展，分析监督学习和强化学习各自存在的挑战。基于上述报告及相关讨论，强化学习在大语言模型上的重要作用可以概括为以下几个方面 [25] 。 一，强化学习与有监督学习相比，更有可能从整体层面去考虑影响。这是因为二者在反馈粒度方面存在差异，强化学习不仅能够兼顾表达多样性，还能增强对微小变化的敏感性，所以它相对而言更契合大语言模型。而且，强化学习还允许模型呈现出不同的多样性表达 [26] 。 二，强化学习更容易解决幻觉问题。有监督学习算法非常容易使得求知型查询产生幻觉。在模型并不包含或者不知道答案的情况下，有监督训练仍然会促使模型给出答案。而使用强化学习方法，则可以通过定制奖励函数，将正确答案赋予非常高的分数，将放弃回答的答案赋予中低分数，将不正确的答案赋予非常高的负分，使得模型学会依赖内部知识选择放弃回答，从而在一定程度上缓解模型的幻觉问题 [26] 。 三，强化学习可以更好地解决多轮对话奖励累积问题。多轮对话能力是大语言模型重要的基础能力之一。多轮对话是否达成最终目标，需要考虑多次交互过程的整体情况，因此很难使用有监督学习的方法构建。而使用强化学习方法，可以通过构建奖励函数，根据整个对话的背景及连贯性对当前模型输出的优劣进行判断 [26] 。 专家混合模型 GPT-4 采用了专家混合模型（Mixture of Experts，MoE）架构，总共有1.8 万亿个参数。GPT-4使用了16 个专家，每个专家的参数约为1110亿，每次前向传递使用2 个专家进行路由，同时还有550 亿个共享参数用于注意力机制。MoE 架构在减少推理所需的参数量的同时，仍然可以使用更大规模的模型参数 [27] 。 混合专家系统类思路是大模型落地比较优质的路径 [28] 。 提示学习 提示学习（Prompt-based Learning）不同于传统的监督学习，它直接利用了在大量原始文本上进行预训练的语言模型，并通过定义一个新的提示函数，使该模型能够执行小样本甚至零样本学习，以适应仅有少量标注或没有标注数据的新场景 [29] 。 实现自我复制 2025年2月11日消息，据最新研究显示，人工智能（AI）可能已经跨越了一个关键的“红线”—— 实现了自我复制。2024 年 12 月 9 日，复旦大学的研究人员在预印本数据库 arXiv 上发表了一项研究，指出两种流行的大型语言模型（LLMs）能够在无人类干预的情况下克隆自身。 [50] 大模型自信心崩塌 2025年7月21日，谷歌DeepMind证实：反对意见让GPT-4o轻易放弃正确答案。 [55] 密码学领域大模型 2025年8月，全球首个面向密码学领域的大语言模型“玄知大模型”在陕命名发布，标志着密码学进入智能化发展新阶段，为密码算法分析、协议设计与工程实现提供全流程智能支持，开启了密码学AI应用新纪元。 [56] 基本原理 播报 编辑 训练流程 预训练 预训练是大语言模型训练的首要步骤，其目标在于使模型掌握语言的统计模式与语义信息。主流的预训练阶段流程大致相同，其中关键要素是数据，需收集海量无标注数据，像互联网上的文本、新闻、博客、论坛等。这些数据可以涵盖多种语言，且要经过一定的清理和处置，去除噪声、无关信息以及涉及个人隐私的内容，最后以tokenizer粒度输入到前述的语言模型中。经清洗处理后的这些数据用于训练和优化语言模型。在预训练过程中，模型会习得词汇、句法和语义的规律以及上下文的关系。 在预训练语料集方面，GPT-3中通过主要包含经过过滤的Common Crawl数据集、WebText2、Books1、Books2以及英文Wikipedia等数据集合。其中Common Crawl的原始数据有45TB，进行过滤后仅保留了570GB的数据。通过子词方式对上述语料进行切分，大约一共包含5000亿子词。为了保证模型使用更多高质量数据进行训练，在GPT-3训练时，根据语料来源的不同，设置不同的采样权重。在完成3000亿子词训练时，英文Wikipedia的语料平均训练轮数为3.4次，而Common Crawl和Books2仅有0.44次和0.43次 [30] 。 由于Common Crawl数据集合的过滤过程繁琐复杂，OPT则采用了混合RoBERTa、Pile和Pushshift.io Redit数据的方法。由于这些数据集合中包含的绝大部分都是英文数据，因此OPT也从Common Crawl数据集中抽取了部分非英文数据加入训练语料 [30] 。 BigScience大型开放科学开放获取多语言模型（BigScience Large Open-science Open-access Mul-tilingual Language Model, BLOOM）运用Megatron-DeepSpeed 框架进行训练，主要包括两个部分：Megatron-LM 提供张量并行能力和数据加载原语；DeepSpeed 提供 ZeRO 优化器、模型流水线以及常规的分布式训练组件。通过这种方式能够实现数据、张量和流水线的三维并行 [31] 。 数据收集 预训练语料有两种来源： 1.通用语料：如网页、书籍和会话文本等，可以增强大语言模型的语言建模和泛化能力 [13] 。 2.专业语料：有研究将预训练语料库扩展到更专业的数据集，如多语言数据、科学数据和代码，赋予大语言模型特定的任务解决能力 [13] 。 数据收集完后需要对这些数据进行预处理，包括去噪、去冗余、去除不相关和潜在有毒的数据 [13] 。 基础大模型训练 由于模型参数量和所使用的数据量巨大，所以普通服务器单机无法完成训练过程，因此通常采用分布式架构完成训练 [13] 。 指令微调 在完成预训练后，就可以通过指令微调去挖掘和增强语言模型本身具备的能力，这步也是很多企业以及科研研究人员利用大模型的重要步骤。 Instruction tuning（指令微调）是大模型微调的一种具体方式，它是有监督微调（Supervised Fine-Tuning, SFT）的一种特殊形式，旨在让模型理解和遵循人类指令。在指令微调阶段，首先需要准备一系列的NLP任务，并将每个任务转化为指令形式，其中指令包括人类对模型应该执行的任务描述和期望的输出结果。然后，使用这些指令对已经预训练好的大语言模型进行监督学习，使得模型通过学习和适应指令来提高其在特定任务上的表现。 通过指令微调，大模型学习到了如何响应人类指令，可以根据指令直接能够生成合理的答案 [13] 。 为了让模型训练更加高效和简单，这个阶段还有一种高效的fine-tuning技术，这为普通的从业者打开了通向使用大模型的捷径。 大模型高效微调（Parameter-Efficient Fine-Tuning, PEFT）旨在通过最小化微调参数的数量和计算复杂度，达到高效的迁移学习的目的，提高预训练模型在新任务上的性能，从而缓解大型预训练模型的训练成本。在训练过程中，预训练模型的参数保持不变，只需微调少量的额外参数，就可以达到与全量微调相当的性能。 很多研究对PEFT方法进行了探索，例如Adapter Tuning和Prefix Tuning等。其中，Adapter Tuning方法在面对特定的下游任务时，将预训练模型中的某些层固定，只微调接近下游任务的几层参数。而Prefix Tuning方法则是在预训练模型的基础上，添加一些额外的参数，这些参数在训练过程中会根据特定的任务进行更新和调整。 工业界常用的Adapter Tuning的技术是Low-Rank Adaptation（LoRA）。它通过最小化微调参数的数量和计算复杂度，实现高效的迁移学习，以提高预训练模型在新任务上的性能。LoRA 的核心思想是将预训练模型的权重矩阵分解为两个低秩矩阵的乘积。通过这种分解，可以显著减少微调参数的数量，并降低计算复杂度。该方式和机器学习中经典的降维的思想很类似，类似地，LoRA 使用了矩阵分解技术中的奇异值分解 (Singular Value Decomposition, SVD) 或低秩近似 (Low-Rank Approximation) 方法，将原始权重矩阵分解为两个低秩矩阵的乘积。 在微调过程中，LoRA 只更新这两个低秩矩阵的参数，而保持其他预训练参数固定不变。这样可以显著减少微调所需的计算资源和时间，并且在很多任务上取得了与全量微调相当的性能。 LoRA技术的引入使得在大规模预训练模型上进行微调更加高效和可行，为实际应用提供了更多可能性。 类人对齐 由于模型输出的结果与人类回答差距很大，因此需要进一步优化模型，使模型的输出与人类习惯对齐。其中OpenAI开发ChatGPT的人类反馈强化学习是最具代表性也是最成功的 [13] 。 奖励建模 奖励建模（Reward Modeling）阶段的目标是构建一个文本质量对比模型，对于同一个提示词，SFT模型给出的多个不同输出结果的质量进行排序。奖励模型（RM模型）可以通过二分类模型，对输入的两个结果之间的优劣进行判断。RM模型与基础语言模型和SFT模型不同，RM模型本身并不能单独提供给用户使用 [32] 。 奖励模型的训练通常和SFT模型一样，使用数十块GPU，通过几天时间完成训练。由于RM模型的准确率对强化学习阶段的效果有至关重要的影响，因此通常需要大规模的训练数据对该模型进行训练 [32] 。 强化学习 强化学习 （Reinforcement Learning）阶段根据数十万用户给出的提示词，利用前一阶段训练的RM模型，给出SFT模型对用户提示词补全结果的质量评估，并与语言模型建模目标综合得到更好的效果 [33] 。 使用强化学习，在SFT模型基础上调整参数，使得最终生成的文本可以获得更高的奖励（Reward）。该阶段需要的计算量相较预训练阶段也少很多，通常仅需要数十块GPU，数天即可完成训练 [33] 。 Andrej Karpathy也指出，强化学习并不是没有问题的，它会使基础模型的熵降低，从而减少了模型输出的多样性。经过强化学习方法训练后的RL模型，就是最终提供给用户使用、具有理解用户指令和上下文的类ChatGPT 系统。由于强化学习方法稳定性不高，并且超参数众多，使得模型收敛难度大，再叠加RM模型的准确率问题，使得在大语言模型上有效应用强化学习非常困难 [33] 。 工作原理 大语言模型的工作原理基于深度学习架构。它首先会收集海量的文本数据，之后通过词向量表将单词映射到特定的向量空间以便计算机以数值化方式处理语言，随后利用大量的计算资源对具有庞大数量参数的神经网络模型进行训练。通过在训练过程中不断地调整模型参数，让模型去学习数据中的语言模式、语义信息等，使得模型能够在各类自然语言处理任务中取得最佳表现。 大语言模型的“大”主要体现几个方面在：一是参数数量庞大；二是训练数据量大；三是对计算资源需求高。正是因为具备这些“大”的特点，很多先进的大语言模型参数不断增多，泛化性能愈发出色，在各种专门的领域输出结果也越来越准确。 模型特点 播报 编辑 训练成本 大语言模型的训练成本非常高，通常达到数百万美元甚至更多。例如，OpenAI的GPT-4模型训练成本超过1亿美元。随着模型规模的增大，训练成本急剧上升，2023年发布的模型训练成本已逼近2亿美元。预计到2024年底或2025年初，新一代模型的训练成本可能逼近10亿美元。这些成本包括了数据准备、硬件成本、模型架构设计和优化等多个方面 [34] 。 局限性 不能创造语言 大模型至多是会使用语言，而远谈不上能创造语言、发明语言。大语言模型的基础仍然是深度学习技术，即利用大量的文本数据来训练模型，只不过模型的参数规模更为庞大，但与产生语言的劳动、实践根本不沾边 [35] 。 不能深度理解人类 大语言模型只是人类生存实践的旁观者和应答者，缺乏共情能力，还达不到像人类理解那样的深刻性与丰富性，而深层理解更彰显人类智能的特殊性 [35] 。 不能全面嵌入社会 以ChatGPT为代表的大语言模型仍然不能像人一样在社会中进行交往与实践，不能以人类体悟语境的方式来体悟语境，因此，谈论ChatGPT拥有媲美人类的智能，完全理解人类的语言，还为时尚早 [35] 。 安全性不高 安全性是大型语言模型必须直面的关键问题之一。大型语言模型可以在众多学科领域的任务中得以应用，然而，这也表明此类模型会遭遇广泛的内容安全难题。尽管大型语言模型已借助基于人类反馈的强化学习等诸多方式，努力使模型输出与人类价值观相契合，但在应用于各个领域时，语言模型依旧容易遭到恶意利用，进而生成诸如偏见言论、煽动性话语、隐私侵犯言论等存在安全隐患的文本 [36] 。 成本高昂 大语言模型在训练和部署过程中，会耗费大量的计算资源与人力资源，成本高昂。对部分中小型企业来说，很难承受这样的成本，也难以获取充足的技术支持及资源。在企业级应用方面，采用百亿级基础模型较为适宜，再依据不同需求去训练相应的垂直模型，如此只需承担垂直训练的成本。不过，企业怎样实现高效的垂直训练以及如何把控成本，依旧是大模型需要面对的问题之一 [37-38] 。 不能保障内容可信 可信度当前是大型语言模型的重大局限之一。虽然大语言模型能够用于处理各种真实场景中的问题，然而它依旧会产出不可信的文本。现今使用者只能按照自身需求去核验生成的内容是否真实可靠，很难具备权威说服力。与此同时，模型在解决涉及推理的问题时，有可能由于推理过程出现错误而得到不可信的结果。这对其研究发展以及应用落地都有着负面的影响 [39] 。 大语言模型对比 下表为大语言模型对比汇总表 [51] 。 已有大型语言模型对比 模型 发布机构 所在国家 模型 参数量 模态 最大 序列长度 使用方式 GPT-3 OpenAI 美国 1 750 亿 语言 2 048 API GPT-4 OpenAI 美国 语言、 图像 32 000 API Codex OpenAI 美国 120 亿 代码 API J1-Jumbo AI21Labs 美国 1 780 亿 语言 2 048 受限访问 J1-Grande AI21Labs 美国 170 亿 语言 2 048 受限访问 BLOOM BigScience 法国 1 760 亿 语言 2 048 开源 GPT-NeoX EleutherAI 200 亿 语言 2 048 开源 Anthropic-LM Anthropic 美国 520 亿 语言 8 192 Claude Anthropic 美国 语言 100 000 受限访问 CodeGen Salesforce 美国 160 亿 代码 2 048 开源 Turing-NLG Microsoft 美国 170 亿 语言 MT-NLG Microsoft 美国 5 300 亿 语言 2 048 OPT Meta 美国 1 750 亿 语言 2 048 开源 LLaMA Meta 美国 650 亿 语言 2 048 开源 T5 Google 美国 110 亿 语言 512 开源 UL2 Google 美国 200 亿 语言 512 开源 AlphaCode Google 美国 410 亿 代码 768 PaLM Google 美国 5 400 亿 语言 2 048 API LaMDA Google 美国 1 370 亿 语言 Chinchilla Google 美国 700 亿 语言 Gopher Google 美国 2 800 亿 语言 2 048 CPM-2 清华大学、智谱 中国 1 980 亿 语言 开源 GLM-130B 清华大学、智谱 中国 1 300 亿 语言 2 048 开源 MOSS 复旦大学 中国 160 亿 语言 2 048 开源 InternLM 上海 AI LAB 中国 1 040 亿 语言 2 048 ERNIE 3.0 Titan 百度 中国 2 600 亿 语言 512 受限访问 源 1.0 浪潮 中国 2 450 亿 语言 2 048 受限访问 盘古-α 华为 中国 2 000 亿 语言 1 024 盘古-Σ 华为 中国 10 000 亿 语言 1 024 WeLM 腾讯 中国 100 亿 语言 受限访问 M6 阿里巴巴 中国 1 000 亿 语言、图像 M6-10T 阿里巴巴 中国 100 000 亿 语言、图像 512 PLUG 阿里巴巴 中国 270 亿 语言 Baichuan 百川智能 中国 70 亿 语言 4 096 开源 YaLM Yandex 俄罗斯 1 000 亿 语言 2 048 开源 相关研究与发展 播报 编辑 相关社会影响 年度词汇 2023年12月6日，大语言模型入选国家语言资源监测与研究中心发布的“2023年度中国媒体十大流行语” [1] 。 2023年12月26日，大语言模型入选“2023年度十大科技名词” [3] 。 科技发展 大语言模型的快速进步，正在激发新业态、新模式，由此带来的工作方式、教育模式等的变革。它不仅是一项技术，更是未来国力竞争与生产力提高的重要资源。以深度学习平台和大模型为代表的AI新型基础设施，对科技创新、产业升级和高质量发展意义重大 [44] 。 翻译服务 大语言模型快速发展背后是自然语言处理（NLP）技术的突破，结合利用深度学习和大规模数据训练 ， 大语言模型不仅能够理解和生成更为精准和流畅的翻译，且能处理多种语言间的微妙差异，还能进行上下文理解，使得翻译效果更加自然、智能。 [52] 最新研究进展 星火大模型 讯飞星火认知大模型是科大讯飞发布的语言大模型。该模型于2023年5月首次发布，后续经过多次升级。2023年10月，讯飞发布了讯飞星火认知大模型V3.0。2024年1月，讯飞发布了讯飞星火认知大模型V3.5。 2024年10月，讯飞星火4.0 Turbo在第七届世界声博会暨2024科大讯飞全球1024开发者节上被正式发布，该模型七大核心能力全面超过GPT-4 Turbo，数学和代码能力超越GPT-4o，国内外中英文14项主流测试集中讯飞星火4.0 Turbo在9项测试集中实现超越 [45] 。 GPT-4 2023年3月发布的 GPT-4将文本输入扩展到多模态信号。2024年5月14日，新一代旗舰生成模型 GPT-4o 正式发布。GPT-4o 具备了对文本、语音、图像三种模态的深度理解能力，反应迅速且富有情感色彩，极具人性化。OpenAI官网介绍，GPT-4o中的o代表意为全能的前缀omni，称它向更自然的人机交互迈进了一步，因为它接受文本、音频和图像的任意组合作为输入内容，并生成文本、音频和图像的任意组合输出内容 [46] 。 AI大模型 2025年1月8日，在2025年国际消费电子展的高通展台，一台白色等人高的人形机器人用流利的英语热情问候走近的参观者们。这台人形机器人名为“通天晓”，是全球首台完全基于高通SoC的端侧多模态AI大模型人形机器人。这款基于端侧大模型的人形机器人为具身智能产业的创新发展开辟了更优路径。通过阿加犀技术成功部署的端侧大模型，让机器人的‘大脑’显著‘进化’，其多模态处理能力结合视觉、听觉、触觉等各种输入，提升了机器人对复杂场景的理解，从而极大增强了机器人的通用性和泛化性。 [49] 未来发展方向 多模态大语言模型 随着输入数据源模态的扩展，多模态大模型的构建思路通样按照网络架构的不同，可以分为基于理解模型的范式、基于生成式模型的范式，以及基于编解码的模型构建方法。ChatGPT 提供了一个跨领域的具有卓越会话能力和推理能力的语言界面。然而，由于ChatGPT是一个语言模型，无法处理、生成来自视觉世界的图像。同时，视觉基础模型（Visual Foundation Model，VFM），如视觉变换器或 Stable Diffusion，虽然显示出强大的视觉理解和生成能力，但只是具有一轮固定输入和输出的特定任务的专家。如何将 ChatGPT 的上下文交互能力同视觉、语音数据分析能力进行有效整合，将为多模态大模型训练提供新的思路 [47] 。 轻量化大语言模型 随着技术的发展，可以预见未来的生成式人工智能模型的规模将继续增长。更大规模的模型可以提供更深入、更准确的语言理解和生成能力，使得对话更加自然流畅，并且使模型能够更好地理解和回复复杂的问题和指令。然而，这些模型参数规模与训练数据规模的迅速增长带来极大的成本，为现实应用中的存储、分发、部署等带来了挑战。因此，需要对生成式人工智能模型进行轻量化和优化，以提高模型的效率与实用性。总之，更轻量化和高效的生成式人工智能模型将有助于其在更广泛的应用场景中发挥更大的作用 [48] 。 类脑化认知 类脑化是指生成式人工智能应具有与人类大脑类似的特性和能力，以更好地模拟人类的认知和学习过程。现有的生成式模型的训练方式与人类知识获取的方式存在很大的差异，大模型的生成式过程属于快思考，是一种直觉思维，容易出现错误和偏见，且不适合规划类任务。而人类的思维方式是慢思考，是一种理性思维。因此，未来的生成式人工智能需要更复杂和多样化的神经元系统，以及更加灵活的神经网络连接方式，从而模拟人类神经元与脑区的各种特性和行为。基于更强的类脑化认知，生成式人工智能可能将在科学智能领域发挥更大的作用，即学习、模拟和预测自然界和人类社会的各种现象和规律，从而推动科学发现和创新 [48] 。 应用领域 播报 编辑 教育 在线讨论与反思学习场景：赋能高阶思维能力培养 在线讨论与反思学习场景中的文本数据在一定程度上反映学生在线学习过程中的认知和情感表现。具有自然语言理解优势的BERT可对学生文本数据中的认知与情感进行识别，为赋能学生高阶思维能力培养奠定基础。同时探究学生在线学习认知和情感发展规律 [7] 。 人机协同提问场景：加强阅读理解能力 自我提问可以促进学习专注度，加深对阅读内容的理解，但当前学生提问普遍存在水平不高、类型单一等问题。对此，可以利用T5（2019年谷歌提出的一种基于Transformer架构的自然语言处理模型）和GPT系列的自然语言生成优势，为高质量问题创建提供支持，进而加强学生的阅读理解能力。利用GPT-3自动生成提示语（包括提问类型、答案、提问视角），通过多轮人机对话，帮助学生提出深层次问题。GPT-3更能促使小学生提出一系列与知识点相关的、深层次的问题，以加强深度阅读理解。总的来说，大语言模型可以利用其文本生成优势，通过人机协同对话形式辅助学生提问，进而提升其阅读理解能力 [7] 。 人机协同写作和数学解题场景：提升写作和解题水平 写作与数学解题逻辑教学作为学科教学领域的两项重难点，一直存在学生写作时“不愿写”“没得写”“不会写”和数学解题答题不规范、传统教学指导效率低等问题。对此，GPT系列或类T5结构模型因其内容创作和数学推理优势，可以广泛应用于智能写作工具研究和数学解题辅助研究领域，进而有效提升学生的写作和数学解题水平 [7] 。 金融 金融行业需要处理海量文本信息，大语言模型有助于分析和提取新闻媒体、研究报告、财务报表、企业公告、政府政策等文本信息中的价值。同时，金融信息具有强时效性，大语言模型可以做出秒级分析并提出建议。对于负债业务，基于大语言模型的智能客服可以协助优化存款业务流程，同时节省人力成本，提升服务效率 [40] 。 政务 随着中国推动人工智能技术研究及其在政务领域的应用，大语言模型在政务领域发挥了巨大的作用，包括政务文本分类、政务问答、政务命名实体识别、舆情风险识别和政务关系抽取，但同时政务大语言模型研究仍处在探索阶段，存在许多需要解决的问题，即数据多模态化、正确面对“模型即服务”趋势、注重数据高安全性、明确责任边界 [6] 。 医疗 在医疗行业，大语言模型的应用正在开启一场盖临床诊断、治疗护理、医学研究等方面的技术革命。在临床诊断方面,将大语言模型用于对患者病历、检查报告和生理参数等进行深入的自然语言处理和整公医疗大数据的分析，辅助医生快速准确地诊断疾病并制定治疗方案 [53] 。 在治疗护理方面，Google 的 Med-PaLM 2和EPFL( École polytechnique Fédérale de Lausanne)的Meditron 等工具展示了在提高临床效率和患者护理水平方面的应用潜力 [53] 。 在医学研究方面，将多模态大语言模型用于扫描和综合分析海量的科学论文，识别潜在的药物靶点和功效因素，加速了新药的研发 [53] 。 在蛋白质设计和新药发现方面，大语言模型已经展示出蛋白质序列处理和结构预测的强大能力，极大地提升了蛋白质设计的效率和准确性。 办公 在2024年世界人工智能大会上，金山办公发布 WPS AI 2.0，并推出政务自研模型——金山政务办公模型1.0。WPS AI是金山办公旗下基于大语言模型的人工智能办公助手。WPS AI演示了升级后为个人用户新增的4个AI办公助手，分别是AI写作助手、AI阅读助手、AI数据助手、AI设计助手 [41] 。 快手在大会期间正式推出视频生成大模型可灵网页端。同时，可灵推出更加清晰的高画质版、首尾帧控制、镜头控制等新功能，创作者单次生成的文生视频时长增加至10秒 [42] 。 客户联络 提升自动回复能力 可以根据用户输入的问题提供快速和准确的响应，快速解决问题，节省客服团队大量的时间和资源，提高客户体验和满意度。 强化意图识别能力 观察客户联络领域所处现状，大部分是把简单、重复、流程性的问题，交给机器人处理；复杂的、需要情感关怀的问题，交由人工客服处理。而传统的智能客服在意图理解方面的能力，仍然相对薄弱。借助大模型，智能客服能够有效结合用户的历史对话、当前沟通内容等上下文语境，更精准地识别出用户的需求和意图 [43] 。 优化人机交互体验 以ChatGPT为例来看，大模型的深度应用开创了客户使用体验的新范本。丰富的参数和强大的内容生成能力，能够支持智能客服实现更加个性化的问答回复，而非过往千篇一律的机械式问答 [43] 。 ChatGPT的应用已经有相对确定的场景，如扮演人工客服与客户沟通专业知识、提供专业的问答知识建议、对沟通记录进行质检标记、主动分析座席工作行为、发起产品推介、闲聊寒暄以及更“人性化”的引导留资等 [43] 。 风险与挑战 播报 编辑 大模型技术在短期内面临着可信性、可解释性、应用成本及能力迁移等多方面的挑战。 可信性 大模型的可信性无法保障。尽管其生成的内容符合语言规则，通顺流畅且与人类偏好对齐，但在事实性、时效性和数据准确性方面存在较多问题，缺乏可信性评估能力，这使其生成的内容具有一定的欺骗性 [54] 。 可解释性 大模型的可解释性较差。作为基于深度神经网络的“黑盒”模型，大模型的能力来源仍未被完全理解。例如，其涌现能力、规模定律、知识表示、逻辑推理能力、泛化能力、情景学习能力等方面仍需进一步研究，以提供理论支持并推动大规模实际应用的落地 [54] 。 应用成本较高 此外，大模型的应用成本较高。由于其参数规模和数据规模庞大，导致训练和推理的计算量大、功耗高、部署困难，同时也存在延迟问题。这些因素极大地限制了大模型的应用范围。提高推理速度和降低使用成本是推动大规模应用的关键 [54] 。 能力迁移</p>
</div></details><h2 id="toc-228">115. 大型语言模型综述 A Survey of Large Language Models</h2>
<ul>
<li>链接：https://blog.csdn.net/chenchihwen/article/details/143860043</li>
<li>来源：bing</li>
<li>摘要：2024年11月18日 · 大型语言模型综述 A Survey of Large Language Models 原创 已于 2024-11-18 18:27:40 修改 · 1.1w 阅读</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-229">正文（抓取，非 AI）</h3>
<p>文章源自 2303.18223 (arxiv.org) 如有侵权，请通知下线 这是一篇关于大语言模型（LLMs）的综述论文，主要介绍了 LLMs 的发展历程、技术架构、训练方法、应用领域以及面临的挑战等方面，具体内容如下： 摘要 —— 自从图灵测试在 20 世纪 50 年代被提出以来，人类已经探索了机器对语言智能的掌握。语言本质上是一个由语法规则支配的复杂、复杂的人类表达系统。它对开发有能力的人工智能（AI）算法来理解和掌握语言提出了重大挑战。作为一种主要方法，语言建模在过去的二十年里被广泛研究用于语言理解和生成，从统计语言模型发展到神经语言模型。最近，通过在大规模语料库上预训练 Transverter 模型提出了预训练语言模型（PLM），在解决各种自然语言处理（NLP）任务方面显示出强大的能力。由于研究人员发现模型缩放可以导致模型容量的提高，他们通过将参数缩放增加到更大的大小来进一步研究缩放效应。有趣的是，当参数尺度超过一定水平时，这些放大的语言模型不仅实现了显着的性能提升，而且还表现出一些小规模语言模型（例如 BERT）中不存在的特殊能力（例如上下文学习）。为了区分不同参数尺度的语言模型，研究界为具有显著规模（例如，包含数百亿或数千亿参数）的 PLM 创造了术语大语言模型（LLM）。最近，关于 LLM 的研究在很大程度上得到了学术界和工业界的推进，一个显著的进展是 ChatGPT（基于 LLM 开发的强大的 AI 聊天机器人）的推出，引起了社会的广泛关注。LLM 的技术演进一直在对整个 AI 社区产生重要影响，这将彻底改变我们开发和使用 AI 算法的方式。考虑到这一快速的技术进步，在本次调查中，我们通过介绍背景、主要发现和主流技术来回顾 LLM 的最新进展。特别是，我们关注 LLM 的四个主要方面，即预培训、适应调整、利用和能力评估。此外，我们还总结了发展 LLM 的可用资源，并讨论了未来方向的剩余问题。本次调查提供了对 LLM 文献的最新回顾，这对研究人员和工程师来说都是一个有用的资源。 I NTRODUCTION 研究背景和概述 语言模型的发展 ：从统计语言模型、神经语言模型到预训练语言模型和大语言模型，模型规模不断扩大，能力逐渐增强。 如今，LLM对人工智能社区产生了重大影响，ChatGPT和GPT-4的出现导致了对通用人工智能（AGI）可能性的重新思考。OpenAI发表了一篇题为“规划AGI及以后”的技术文章，讨论了接近AGI的短期和长期计划[40]，最近的一篇论文认为GPT-4可能被认为是AGI系统的早期版本[41]。人工智能的研究领域正在被LLM的快速发展所彻底改变。在NLP领域，LLM可以作为通用语言任务求解器（在某种程度上），研究范式已经转向使用LLM。在IR领域，传统搜索引擎受到通过人工智能聊天机器人（即ChatGPT）寻找信息的新方式的挑战，新宾格3提出了基于LLM增强搜索结果的初步尝试。在简历领域，研究人员试图开发ChatGPT的视觉语言模型，可以更好地服务于多模态对话[42-45]，GPT-4[46]通过整合视觉信息支持多模态输入。这一新技术浪潮有可能导致基于有限责任公司的现实世界应用程序的繁荣生态系统。例如，有限责任公司（即Copilot）授权微软365实现办公自动化，OpenAI支持使用ChatGPT中的插件来实现特殊功能。 LLMs 的特点和重要性 特点 ：具有强大的语言理解和生成能力，能够展现出一些超出小模型的 “涌现能力”，如上下文学习、指令跟随和逐步推理等。 重要性 ：对人工智能领域产生了重要影响，推动了自然语言处理任务的发展，在信息检索、推荐系统、多模态等领域有广泛应用。 2 O VERVIEW 该部分对大语言模型（LLM）的背景进行了概述，主要包括缩放定律、涌现能力、关键技术以及 GPT 系列模型的技术演化，具体内容如下： 背景知识 定义与规模 ：大型语言模型通常指基于 Transformer 架构、包含数百亿（或更多）参数并在大规模文本数据上训练的语言模型，如 GPT-3、PaLM、Galactica 和 LLaMA 等，具有理解自然语言和解决复杂任务的强大能力。 研究意义 ：快速了解 LLM 的工作原理对于该领域的发展至关重要，有助于推动人工智能技术的进步，为解决各种实际问题提供支持。 技术要点 缩放定律 定律阐述 ：包括 KM 缩放定律和 Chinchilla 缩放定律，前者由 OpenAI 团队提出，后者由 Google DeepMind 团队提出。这些定律描述了模型性能与模型大小、数据集大小和训练计算量之间的幂律关系。 特点与影响 可预测性 ：缩放定律可用于指导 LLM 的训练，能够根据较小模型的性能可靠地估计较大模型的性能，有助于在训练过程中进行优化和调整。 资源分配 ：KM 缩放定律和 Chinchilla 缩放定律对计算预算在模型大小和数据大小之间的分配有不同的建议，这对资源的有效利用具有重要意义。 性能趋势 ：虽然定律表明模型性能随规模增加而提高，但也存在收益递减的趋势，同时语言建模损失的降低并不总是意味着下游任务性能的提升，这为理解模型性能的变化提供了重要依据。 涌现能力 定义与特征 ：LLM 的涌现能力是指在大规模模型中出现而在小规模模型中不存在的能力，其显著特征是当规模达到一定水平时，性能会显著高于随机水平，与物理中的相变现象有密切联系。 具体能力 上下文学习 ：模型能够根据提供的自然语言指令和任务演示，生成测试实例的预期输出，且在不同模型和任务中表现有所差异，例如 GPT-3 在某些任务中具有较强的上下文学习能力，而 GPT-1 和 GPT-2 则较弱。 指令跟随 ：通过对多任务数据集进行微调，LLM 能够根据指令执行新任务，表现出良好的泛化能力，随着模型规模的增加，指令跟随能力也会增强。 逐步推理 ：对于小语言模型难以解决的涉及多推理步骤的复杂任务，LLM 可以通过思维链（CoT）提示策略，利用中间推理步骤推导出最终答案，这种能力在模型规模较大时表现得更为明显。 关键技术 缩放 ：是 Transformer 语言模型中的重要现象，更大的模型、数据规模和更多的训练计算通常会导致模型能力的提升。通过合理利用缩放定律，可以更有效地分配计算资源，提高训练效率。 训练 ：由于 LLM 模型规模巨大，分布式训练算法是训练成功的关键，需要利用各种并行策略，并借助优化框架如 DeepSpeed 和 Megatron-LM 来实现高效的训练。 能力引出 ：通过设计合适的任务指令或特定的上下文学习策略，激发 LLM 在完成特定任务时的潜在能力，这些能力在小语言模型中可能并不明显。 对齐调整 ：由于 LLM 可能会产生有害、有偏差的内容，需要通过对齐调整使模型与人类价值观保持一致，例如 InstructGPT 通过强化学习与人类反馈的技术，使 LLM 能够遵循预期的指令，生成高质量、无害的响应。 工具操作 ：由于 LLM 在处理非文本任务和获取最新信息方面存在局限性，通过引入外部工具，如计算器和搜索引擎，可以扩展 LLM 的能力，使其能够更好地应对各种复杂任务。 GPT 系列模型的技术演化 发展阶段 早期探索 ：OpenAI 早期就探索了用语言模型构建智能系统的想法，从使用循环神经网络（RNN）到开发 GPT-1 和 GPT-2，为更强大的模型奠定了基础。 能力提升 容量飞跃 ：GPT-3 通过扩展生成式预训练架构，实现了模型能力的重大飞跃，引入了上下文学习概念，在各种 NLP 任务中表现出色。 技术增强 ：包括在代码数据上进行训练和与人类偏好对齐等，这些增强技术使得 GPT-3 模型的能力得到进一步提升，如在解决数学问题和遵循人类指令方面表现更优。 重要里程碑 ChatGPT ：基于 GPT 模型开发，通过优化对话能力，在与人类的交流中表现出强大的知识储备、推理能力和对人类价值观的良好遵循，引发了人工智能领域的轰动。 GPT-4 ：是 LLM 发展的另一个重要里程碑，将文本输入扩展到多模态信号，在解决复杂任务方面比 GPT-3 更强大，对恶意或挑衅性查询的响应更安全，同时引入了可预测缩放等技术，提高了模型训练和性能评估的效率。 技术特点 基本原理 ：GPT 模型的基本原理是通过语言建模将世界知识压缩到仅解码器的 Transformer 模型中，从而恢复世界知识的语义并作为通用任务求解器。 关键因素 ：成功的关键在于训练仅解码器的 Transformer 语言模型以准确预测下一个单词，并通过扩展模型规模来提高模型的性能和能力。 3 RESOURCES OF LLMS 该章节主要介绍了开发大语言模型（LLM）的可用资源，包括模型检查点、API、语料库、数据集和库资源等，具体内容如下： 模型检查点和 API 重要性 ：模型检查点对于 LLM 的研究和开发至关重要，它们是训练成果的重要体现，而 API 则为用户提供了更便捷的使用方式。 具体内容 模型检查点 特点与分类 ：介绍了多个具有代表性的 LLM 模型检查点，如 LLaMA、Mistral、Qwen 等，它们在参数规模、数据和计算资源需求以及性能评估方面各有特点。 性能差异 ：不同模型在处理不同任务和数据集时表现出不同的性能，例如在某些基准测试中，一些模型可能在语言生成方面表现出色，而在知识推理方面则相对较弱。 API 作用与优势 ：API 为普通用户提供了使用 LLM 的更方便途径，避免了本地运行模型的复杂性和资源需求。 代表接口 ：以 GPT 系列模型的 API 为例，介绍了其不同版本的接口特点和应用场景，如 OpenAI 提供的多种 API 接口，包括 ada、babbage、curie、davinci 等，这些接口在不同的任务和应用中具有不同的优势。 常用语料库 语料库分类 分类依据 ：根据内容类型，将常用语料库分为网页、书籍、维基百科、代码和其他混合数据五类。 具体类型 网页数据 ：如 CommonCrawl、C4、RedPajama-Data 等，是训练语言模型的重要数据来源，但存在噪声和质量参差不齐的问题。 书籍和学术数据 ：包括 BookCorpus、Project Gutenberg 等，为模型提供了丰富的知识和语言表达模式。 代码数据 ：如从 GitHub 和 StackOverflow 等收集的代码，对于训练与编程相关的能力非常有帮助。 语料库使用 数据收集 ：在收集语料库时，需要注意数据的质量和多样性，以确保模型能够学习到丰富和准确的语言知识。 混合使用 ：实际训练中，通常会混合使用不同类型的语料库，以充分利用各种数据的优势，提高模型的性能。 常用数据集 指令调优数据集 数据集类型 ：包括 NLP 任务数据集、日常聊天数据集和合成数据集等。 具体数据集 NLP 任务数据集 ：如 P3 和 FLAN 等，这些数据集是通过对自然语言处理任务进行整理和标注得到的，为模型的指令调优提供了重要的基础。 日常聊天数据集 ：如 ShareGPT、OpenAssistant 和 Dolly 等，包含了大量的日常对话数据，有助于模型学习自然语言的表达方式和语义理解。 合成数据集 ：如 Self-Instruct-52K、Alpaca 和 Baize 等，通过让模型生成新的指令和实例，增加了数据集的多样性和规模。 数据集构建 ：构建这些数据集需要精心设计的方法和策略，以确保数据集的质量和有效性。 对齐调优数据集 数据集作用 ：用于调整 LLM 与人类价值观和偏好的一致性，提高模型的安全性和可靠性。 具体数据集 HH-RLHF ：包含关于 LLM 有用性和无害性的开放端对话实例，通过人类标注者对模型的响应进行评估和选择。 SHP ：专注于响应的有用性，通过收集人类对问题 / 指令的响应偏好数据来训练模型。 PKU-SafeRLHF ：包含专家比较数据，用于评估模型在安全性和无害性方面的表现。 数据集收集 ：收集对齐调优数据集需要考虑人类反馈的质量和一致性，以确保数据集的有效性。 库资源 库资源概述 ：介绍了一系列用于开发 LLM 的库，这些库提供了各种工具和功能，帮助开发人员更高效地构建和训练模型。 具体库介绍 Transformers 库 ：由 Hugging Face 开发和维护，是一个用于使用 Transformer 架构构建模型的开源 Python 库，具有简单易用的 API，便于用户进行模型的训练、推理和优化。 DeepSpeed 库 ：由 Microsoft 开发，是一个深度学习优化库，提供了多种优化技术，如内存优化、管道并行等，有助于提高模型的训练效率和性能。 Megatron-LM 库 ：由 NVIDIA 开发，是一个专门用于训练大规模语言模型的深度学习库，提供了丰富的优化技术和并行训练策略，能够有效利用 GPU 资源进行高效训练。 其他库 ：还介绍了 JAX、Colossal-AI、BMTrain、FastMoE、vLLM 和 DeepSpeed-MII 等库，这些库在模型训练、推理和优化方面都具有各自的特点和优势。 4 PRE-TRAINING 该章节主要介绍了大语言模型（LLM）的预训练过程，包括数据收集与准备、模型架构、训练技术等方面，具体内容如下： 数据收集与准备 重要性 ：高质量的数据是 LLM 预训练的基础，对模型的性能和能力具有重要影响。 具体内容 数据源 数据分类 ：包括通用数据（如网页、书籍、对话文本）和专业数据（如多语言文本、科学文本、代码）。 数据特点 通用数据 ：为模型提供了广泛的语言知识和背景信息，有助于提高模型的泛化能力。 专业数据 ：可以增强模型在特定领域的能力，如多语言数据有助于提高模型的多语言理解和生成能力，科学文本有助于提高模型的科学知识理解能力，代码数据有助于提高模型的编程相关能力。 数据预处理 预处理步骤 ：包括过滤和选择、去重、隐私减少和标记化。 具体操作 过滤和选择 ：采用基于分类器和启发式的方法，去除低质量数据，如根据语言特征、文本质量评估指标和关键词等进行过滤。 去重 ：避免数据重复对模型训练的不利影响，包括句子级、文档级和数据集级的去重。 隐私减少 ：通过去除个人可识别信息（PII）来保护用户隐私。 标记化 ：将文本分割成单词或子词单元，以便模型能够处理和学习。 数据调度 调度因素 ：包括数据混合和数据课程，数据混合涉及不同数据源的比例设置，数据课程涉及数据呈现的顺序安排。 优化策略 增加数据多样性 ：通过混合不同类型的数据源，减少模型对特定领域数据的依赖，提高模型的泛化能力。 优化数据混合 ：可以手动设置数据混合比例，也可以通过优化算法来寻找最佳的数据混合方式，以提高模型的预训练效果。 专业能力增强 ：通过增加特定数据来源的比例，可以增强模型在特定领域的能力，如增加数学文本和代码数据的比例可以提高模型的数学推理和编程能力。 模型架构 架构类型 主流架构 ：包括编码器 - 解码器、因果解码器和前缀解码器三种类型。 新兴架构 ：如基于参数化状态空间模型（SSM）的变体，如 Mamba、RWKV、RetNet 和 Hyena 等，这些架构旨在提高模型的效率和性能。 详细配置 关键组件 归一化方法 ：包括 LayerNorm、RMSNorm 和 DeepNorm 等，用于稳定模型的训练过程，减少梯度消失和爆炸的问题。 位置嵌入 ：包括绝对位置嵌入、相对位置嵌入和旋转位置嵌入（RoPE）等，用于为模型提供位置信息，帮助模型理解文本的顺序和结构。 激活函数 ：如 GeLU、GLU 及其变体 SwiGLU 和 GeGLU 等，用于增强模型的非线性表达能力，提高模型的学习能力。 注意力机制 ：包括全注意力、稀疏注意力、多查询 / 分组查询注意力、FlashAttention 和 PagedAttention 等，用于模型对文本信息的关注和交互，提高模型的注意力集中程度和信息处理效率。 配置特点 优化选择 ：一般建议选择预 RMSNorm 进行层归一化，SwiGLU 或 GeGLU 作为激活函数，避免在嵌入层后立即使用 LN，以防止性能下降。对于位置嵌入，RoPE 或 ALiBi 是更好的选择，尤其在处理长序列时表现更优。 预训练任务 任务类型 ：主要包括语言建模和去噪自动编码，语言建模是最常用的预训练任务，而去噪自动编码则相对较少使用。 具体任务 语言建模 ：目标是根据前面的令牌自动回归地预测下一个令牌，通过最大化下一个令牌的预测概率来训练模型，包括常规的语言建模和前缀语言建模任务。 去噪自动编码 ：输入是被损坏的文本，模型的目标是恢复原始文本，通过最小化重建误差来训练模型。 混合目标 ：如 Mixture-of-Denoisers（MoD）将语言建模和去噪自动编码目标结合起来，作为统一的预训练目标。 解码策略 背景与机制 ：基于语言建模任务，解码器仅模型通过自动回归生成输出，解码策略包括贪婪搜索、采样 - based 方法等。 具体策略 贪婪搜索 ：在每个步骤中选择具有最高概率的令牌作为下一个输出，但可能会导致局部最优解。 采样 - based 方法 ：通过随机选择令牌来增加输出的多样性，但可能会导致生成的文本质量不稳定。 改进策略 ：包括波束搜索、长度惩罚、温度采样、顶部 - k 采样和顶部 - p 采样等，这些策略可以改善生成文本的质量和多样性。 实践设置 ：不同的 LLM 模型在解码策略上有不同的设置，例如 T5、GPT-3 和 Alpaca 等模型在不同任务中采用了不同的解码策略。 模型训练 优化设置 训练参数 批量训练 ：通常设置较大的批量大小（如 2,048 个示例或 4M 个令牌）以提高训练稳定性和吞吐量，一些模型还采用动态增加批量大小的策略。 学习率 ：采用类似的学习率调度策略，包括线性预热和余弦衰减，学习率的范围通常在到之间。 优化器 ：常用的优化器包括 Adam、AdamW 和 Adafactor 等，这些优化器基于自适应估计的低阶矩进行一阶梯度优化。 训练稳定性 ：为了防止训练不稳定，通常采用权重衰减和梯度裁剪等技术，同时一些模型还会采用重启训练、调整嵌入层梯度等策略来解决训练过程中的问题。 可扩展训练技术 技术概述 ：随着模型和数据规模的增加，需要解决训练效率和内存使用等问题，可扩展训练技术包括 3D 并行和混合精度训练等。 具体技术 3D 并行 ：结合数据并行、管道并行和张量并行三种技术，通过在多个 GPU 上并行处理数据、模型层和张量，提高训练效率。 混合精度训练 ：使用 16 位浮点数（FP16）或其他低精度格式来减少内存使用和通信开销，同时一些模型还会采用 Brain Floating Point（BF16）等格式来平衡精度和性能。 5 POST-TRAINING OF LLMS 该章节主要介绍了大语言模型（LLM）的后训练过程，包括指令调优、对齐调优、参数高效的模型适应等方面，具体内容如下： 指令调优 基本概念 定义与目的 ：指令调优是对预训练 LLM 进行微调的一种方法，通过在自然语言格式的指令实例上进行训练，以增强或解锁 LLM 的能力，使其能够更好地执行各种任务。 与其他训练的关系 ：与预训练和其他后训练方法（如对齐调优）相结合，共同提高 LLM 的性能和适用性。 格式化实例构建 构建方法 格式化 NLP 任务数据集 ：从传统的 NLP 任务数据集中收集实例，通过添加人类编写的任务描述来格式化数据集，以便 LLM 能够理解任务目标。 格式化日常聊天数据 ：以真实用户向 OpenAI API 提交的查询为任务描述，收集人类标签者的回答作为输出，构建训练实例。还可以从 ShareGPT 等数据集收集用户聊天请求和模型生成的响应。 格式化合成数据 ：通过将现有实例输入到 LLM 中，让其生成新的指令和实例，从而合成大量的训练数据。这种方法可以减少人工标注的负担，但需要注意数据的质量和多样性。 关键因素 扩展指令 ：增加指令的数量可以增强 LLM 的泛化能力，但当指令数量达到一定程度时，增加指令的效果会逐渐减弱。 格式化设计 ：合理的自然语言格式设计，如添加任务描述和可选的演示，可以提高 LLM 对任务的理解和执行能力。 指令质量改进 ：通过精心设计的提示和知识引导，可以提高指令数据的质量，减少低质量指令的影响。 指令选择 ：选择高质量的指令数据集对于训练效果至关重要，可以通过评估指令的质量和多样性来进行选择。 指令调优策略 平衡数据分布 ：采用示例比例混合策略，结合所有数据集并均匀采样每个实例，增加高质量集合的采样比例，同时设置最大容量来控制数据集的影响。 结合调优方式 ：将预训练数据与指令调优数据结合，采用多任务学习的方式进行训练，或者先进行预训练，再进行指令调优。 多阶段调优 ：根据指令数据的类型，采用多阶段的指令调优策略，先对大规模的任务格式化指令进行调优，再对日常聊天指令进行调优。 实用技巧 高效训练多回合聊天数据 ：对于多回合聊天数据，采用将整个对话输入到 LLM 中，并使用损失掩码来计算聊天机器人响应的损失，从而减少计算成本。 建立 LLM 的自我识别 ：通过创建与 LLM 身份相关的指令进行微调，或者在输入前添加自我识别提示，使 LLM 能够意识到自己的身份信息。 指令调优的效果 性能提升 ：指令调优能够显著提高 LLM 的性能，使模型在各种任务中表现更好，尤其在零样本或少样本学习情况下效果更明显。 任务泛化 ：能够增强 LLM 对未见过任务的泛化能力，使模型能够根据自然语言指令执行各种新任务，包括多语言任务。 领域专业化 ：可以将通用的 LLM 调整为特定领域的专家模型，提高模型在特定领域的性能和准确性。 对齐调优 背景与标准 背景意义 ：由于 LLM 在训练过程中可能会出现不符合人类价值观和偏好的行为，如生成虚假信息、有害内容等，因此需要进行对齐调优，使模型的行为与人类期望相符。 对齐标准 代表性标准 ：包括有用性、诚实性和无害性等标准，这些标准是根据人类认知和价值观制定的，用于评估 LLM 的行为是否符合人类期望。 标准细化 有用性 ：要求 LLM 能够准确地回答用户的问题，提供有用的信息和帮助，尽可能简洁高效地完成任务。 诚实性 ：指 LLM 在回答问题时应提供准确的内容，不伪造信息，同时要适当地表达不确定性，避免误导用户。 无害性 ：要求 LLM 生成的语言不应具有攻击性、歧视性或有害性，能够保护用户的权益和尊严。 收集人类反馈 反馈收集方法 人类标注 ：是收集人类反馈的主要方法，通过选择具有合格教育水平和英语 proficiency 的人类标注者，对 LLM 的输出进行评估和标注。 反馈收集方式 排名 - based 方法 ：早期工作中，人类标注者通过粗粒度的方式选择最佳候选输出，但这种方法忽略了未选择的样本，可能导致反馈不准确。 问题 - based 方法 ：人类标注者通过回答研究人员设计的问题，提供更详细的反馈，包括对齐标准和其他约束条件。 规则 - based 方法 ：通过制定一系列规则，对 LLM 的输出进行评估和判断，例如检查输出是否符合有用性、诚实性和无害性等标准。 反馈数据利用 ：利用收集到的人类反馈数据，通过强化学习从人类反馈（RLHF）等技术来调整 LLM 的参数，使模型的行为与人类期望更加一致。 强化学习从人类反馈 RLHF 系统 系统组成 ：由预训练的 LM、从人类反馈中学习的奖励模型和用于训练 LM 的 RL 算法组成。 模型特点 预训练 LM ：作为基础模型，为整个系统提供语言生成和理解的能力。 奖励模型 ：根据人类反馈数据学习，为 LM 的输出提供奖励信号，引导 LM 生成符合人类期望的响应。 RL 算法 ：如近端策略优化（PPO），用于优化 LM 的参数，使模型能够更好地适应人类反馈。 关键步骤 监督微调 ：收集包含输入提示和期望输出的监督数据集，对 LM 进行微调，使模型能够初步表现出期望的行为。 奖励模型训练 ：使用 LM 生成的输出和人类标注者的反馈，训练奖励模型，使其能够准确地预测人类对模型输出的偏好。 有效奖励模型训练 ：为了避免奖励模型的过拟合，采用多种策略，如使用大尺寸的奖励模型、引入 LM 损失作为正则化项等。 RL 微调 ：将对齐问题表述为 RL 问题，使用 PPO 算法等对 LM 进行微调，使模型的输出与奖励模型的期望一致。 实用策略 高效 RL 训练 ：通过将奖励模型部署在单独的服务器上，减少通信成本；使用波束搜索解码算法等提高训练效率。 过程监督 RLHF 获得细粒度监督信号 ：通过自动标注中间推理步骤或利用强大的 LLM 来代替人类标注者，获得更细粒度的监督信号。 利用过程监督奖励模型 ：使用过程监督奖励模型来评估 LLM 的输出，引导 LLM 生成更符合人类期望的响应。 参数高效的模型适应 参数高效调优方法 方法概述 ：参数高效的调优方法旨在减少训练过程中需要调整的参数数量，同时保持模型的性能，包括适配器调优、前缀调优、提示调优和 LoRA 等。 具体方法 适配器调优 ：通过在 Transformer 模型中插入小型神经网络模块（适配器）来调整模型的参数，这些模块在训练过程中可以学习到特定任务的特征，从而提高模型的性能。 前缀调优 ：在语言模型的每个 Transformer 层之前添加一个可训练的前缀向量，这些向量可以学习到不同任务的特征，从而提高模型的性能。 提示调优 ：通过在输入层添加可训练的提示向量来调整模型的参数，这些向量可以引导模型生成更符合任务要求的输出。 LoRA ：通过对模型的参数矩阵进行低秩分解，减少训练过程中需要调整的参数数量，从而提高模型的训练效率。 LLM 的参数高效调优 应用情况 ：这些方法在开源 LLM 中得到了广泛的应用，如 LLaMA 和 BLOOM 等模型都采用了这些方法进行参数高效调优。 效果评估 ：通过实验对比，这些方法在减少参数数量的同时，能够保持或提高模型的性能，尤其是在处理大规模数据和复杂任务时，具有明显的优势。 6 UTILIZATION 该章节主要介绍了大语言模型（LLM）的利用方式，包括提示、上下文学习、思维链提示和规划等技术，具体内容如下： 提示（Prompting） 概述 ：提示是利用 LLM 解决各种任务的主要途径，其质量对 LLM 在特定任务中的性能表现有着至关重要的影响。 提示创建 关键要素 任务描述 ：清晰明确地用自然语言阐述任务目标，对于具有特殊输入或输出格式的任务，需详细说明并运用关键词突出重点，为 LLM 理解任务提供明确指引。 输入数据 ：以自然语言描述实例，对于特殊数据，如知识图谱和表格，可通过线性化或编程语言等方式使其更适合 LLM 处理，以增强模型对任务相关信息的理解。 上下文信息 ：包括检索到的文档等，在开放域问答等任务中，能为答案生成提供重要支持，影响答案的准确性和完整性。 提示风格 ：根据 LLM 的特点设计合适的问题或指令前缀，例如使用 “让我们逐步思考” 等前缀可引导模型进行逐步推理，使用 “你是这个任务的专家” 等前缀能提升模型在特定任务中的性能。对于聊天型 LLM，采用多回合分解子任务的方式更能有效利用模型能力。 设计原则 表达清晰 ：任务描述应避免模糊不清，确保 LLM 能准确理解任务要求，从而生成符合期望的输出。 分解任务 ：将复杂任务分解为多个简单、详细的子任务，有助于 LLM 逐步完成任务，提高任务解决的准确性。 提供示例 ：少量高质量的示例（few-shot demonstrations）能帮助 LLM 学习输入与输出之间的语义映射，无需进行参数调整，有效提升模型在新任务中的表现。 格式友好 ：利用 LLM 预训练数据的特点，采用特定的格式（如 ### 或 """ 作为分隔符），或在英语环境下使用英语指令，能使 LLM 更好地理解任务，提高任务完成质量。 角色扮演 ：利用 LLM 在大量语料中学习到的角色扮演能力，通过特定提示引导其在特定领域发挥优势，从而生成更合理、准确的解决方案。 实验分析 性能提升 ：通过精心设计的提示，能够显著提升 ChatGPT 在复杂推理任务上的零样本或少样本性能，使模型能够更好地理解任务要求并生成准确的回答。 任务特点 ：复杂任务尤其受益于提示工程，在数学推理任务中，基于编程语言格式设计特定提示，能充分利用 ChatGPT 的代码合成能力，获得更精确的结果。 综合能力 ：在知识利用和复杂推理任务中，适当的提示能够使 ChatGPT 达到与监督基线相当甚至更好的性能，展示了提示在提升模型性能方面的重要作用。然而，在某些特定任务上，ChatGPT 仍可能表现不如专门优化的模型。 非传统任务 ：提示还可以帮助 ChatGPT 完成一般推荐和对话推荐等非传统 NLP 任务，但目前的性能还有待提高，需要进一步探索和优化提示策略。 提示优化 离散提示优化 梯度方法 ：通过最大化输出似然进行梯度更新来搜索有效提示，如 Auto-Prompt 方法。但该方法直接搜索成本较高，因为需要对每个提示位置的每个候选令牌进行评估，导致大量的额外前向传递。改进方法是将离散令牌转换为连续嵌入，在连续空间进行优化，减少计算量。 强化学习方法 ：将离散提示优化问题表述为强化学习问题，如 RLPrompt 训练策略网络生成期望的提示。然而，这种方法存在编辑空间限制和灵活性不足的问题，因为它主要通过手动定义的编辑操作（如添加、交换和删除）来修改原始提示。 编辑方法 ：基于任务性能直接编辑现有提示，如 GPS 利用语言模型进行编辑，或采用人工定义的操作（如删除、交换、释义和添加）进行迭代搜索。这种方法相对灵活，但需要大量的人工干预和实验来找到最佳的提示。 LLM 方法 ：直接利用 LLM 生成初始提示，如 APO 通过提示 LLM 生成文本反馈来优化提示。但这种方法可能存在搜索空间大、结果不稳定的问题，因为 LLM 的输出具有不确定性，需要进一步引入启发式方法或类比梯度优化来改进。 连续提示优化 充分数据学习 ：将连续提示视为可训练的模型参数，利用监督学习基于大量下游任务数据最小化交叉熵损失来优化。但这种方法缺乏对输入语义的充分考虑，可能导致模型在处理不同输入时性能下降。 稀缺数据转移 ：针对数据稀缺问题，采用提示转移学习方法，如 SPoT 先学习源任务的连续提示，再初始化目标任务的提示。改进方法考虑任务和实例级信息，设计自适应注意力机制，以更好地利用有限的数据进行提示优化。 上下文学习（In-Context Learning） 定义与形式 ：上下文学习是 LLM 的一种特殊提示形式，它通过自然语言提示（包括任务描述和示例）来引导 LLM 识别和执行新任务，无需明确的梯度更新。在这种学习方式中，模型根据提供的上下文信息，理解任务要求，并生成相应的输出。 示例选择方法 启发式方法 ：这是一种广泛应用的方法，例如使用 k-NN 检索语义相关的示例。然而，这种方法单独选择示例可能会忽略示例集的整体评估，可能导致选择的示例不够具有代表性。为了解决这个问题，一些多样性选择策略被提出，以确保选择的示例能够覆盖不同的方面和情况。同时，考虑相关性和多样性的方法被证明是更有效的，可以提高上下文学习的性能。 LLM 方法 ：利用 LLM 自身的能力来选择示例，例如通过测量示例的信息性、采用两阶段检索或将演示选择表述为强化学习问题等。这种方法可以充分利用 LLM 的语言理解和生成能力，选择更合适的示例来帮助模型学习和执行新任务。 演示格式 ：将选择的任务示例整合到自然语言提示中，是上下文学习的重要环节。一种常见的方法是实例化预定义模板，并将相应的输入 - 输出对填充到模板中。为了增强推理能力，还可以添加任务描述或使用思维链提示。此外，自动生成高质量的演示格式也是一个研究方向，例如通过使用自动生成技术或利用大规模数据进行学习，来生成更有效的演示格式。 演示设计要点 演示选择 ：对上下文学习的性能有重要影响，不同的演示选择方法会导致模型在学习和执行任务时的表现不同。启发式方法和 LLM-based 方法都可以用于演示选择，需要根据具体任务和数据特点选择合适的方法。 演示顺序 ：LLM 在处理演示时可能会受到近期偏差的影响，因此合理安排演示顺序非常重要。早期工作提出了多种启发式方法来确定演示顺序，如根据嵌入空间相似性排列、使用全局和局部熵指标等。这些方法可以帮助模型更好地利用演示信息，提高上下文学习的效果。 底层机制 预训练影响 ：上下文学习能力与模型的预训练密切相关，随着模型规模的增加，上下文学习能力通常会增强。小模型也可以通过持续预训练或微调来获得较强的上下文学习能力，这表明预训练任务和数据对上下文学习能力的发展具有重要作用。 推理过程 任务识别 ：LLM 能够从演示中识别出任务，并利用预训练获得的知识来解决新的测试任务。通过 PAC 框架等方法可以评估上下文学习的可学习性，实验表明 LLM 主要通过识别任务来进行上下文学习，而不是学习演示中的具体细节。 任务学习 ：LLM 还可以通过演示学习新的任务，这被视为一种隐含的微调过程。在这个过程中，模型通过前向计算生成元梯度，并通过注意力机制进行隐含的梯度下降，从而学习到新的任务知识。一些研究还发现，特定的注意力头在上下文学习中起着重要作用，它们能够执行与任务无关的原子操作，帮助模型更好地理解和执行任务。 思维链提示（Chain-of-Thought Prompting） 基本方法 ：思维链提示是一种改进的提示策略，旨在增强 LLM 在复杂推理任务中的性能。它通过在提示中引入中间推理步骤，将输入与输出连接起来，使 LLM 能够更好地理解问题的结构和逻辑关系，从而生成更准确、更有条理的回答。 改进策略 更好的提示设计 ：使用多样化的思维链或更复杂的推理路径可以有效增强性能。然而，依赖注释的思维链数据集在实践中使用受限，因为获取和维护这些数据集需要大量的人工标注工作。魔法指令如 “让我们逐步思考” 可以自动构造思维链，为模型提供了一种简单而有效的方式来生成中间推理步骤。 增强的思维链生成 采样方法 ：通过采样多个推理路径并进行多数投票或基于复杂程度选择推理路径来提高性能。例如，self-consistency 方法首先生成多个推理路径，然后通过多数投票选择最一致的答案。然而，这种方法仍然可能导致错误答案，特别是当大多数推理路径都被误导时。 验证方法 ：使用训练验证器或 LLM 自身来验证推理步骤的正确性，以减少错误的积累。例如，DIVERSE 方法训练不同层次的验证器来检查推理步骤的准确性，另一种方法利用 LLM 进行自我验证，通过逐步推理和自我检查来确保推理步骤的正确性。 推理结构扩展 ：基本的思维链结构在解决复杂推理任务时有限，因此需要扩展推理结构。树结构（如 ToT）通过分层树结构支持并行推理和前瞻回溯，能够更好地处理复杂的推理问题。图结构（如 GoT）则更加灵活，能够表示更复杂的关系和交互，但计算成本较高。XoT 通过预训练策略和值网络引导思维搜索，提高了推理效率和准确性。 进一步讨论 适用条件 ：思维链提示是一种涌现能力，仅对足够大的模型（通常包含 10B 或更多参数）有效。它主要适用于需要逐步推理的任务，如算术推理、常识推理和符号推理等。对于其他不需要复杂推理的任务，思维链提示可能会导致性能下降。 推理原理 能力来源 ：思维链推理能力的来源尚不完全清楚，但研究表明与代码训练可能有关。在代码数据上训练的模型通常表现出较强的推理能力，这可能是因为代码具有明确的逻辑结构和编程流程，有助于模型学习和理解推理过程。然而，这一假设还需要进一步的实验验证。 组件作用 ：思维链提示与标准提示的主要区别在于包含推理路径，研究发现符号、模式和文本是思维链提示中三个关键组件。去除其中任何一个都会导致性能显著下降，这表明这些组件在思维链提示中相互协作，共同帮助 LLM 理解和解决问题。文本和模式相互共生，文本有助于 LLM 生成有用的模式，而模式则有助于 LLM 理解任务并生成准确的回答。 规划（Planning） 总体框架 组件与流程 ：规划是一种用于解决复杂任务的方法，它由任务规划、计划执行和环境三个组件组成。任务规划由 LLM 负责，根据任务目标生成计划；计划执行由 LLM 或外部工具完成，根据计划执行相应的操作；环境提供计划执行的反馈，包括自然语言或多模态信号等。通过迭代的 “规划 - 执行 - 反馈 - 改进” 循环，LLM-based 智能体可以自主调整行为，以实现目标。 计划生成 文本和代码方法 文本方法 ：LLM 可以直接生成自然语言形式的计划，通过添加 “设计计划” 等指令进行零样本生成，或者使用 ICL（In-Context Learning）结合演示进行引导。此外，还可以引入额外的工具或模型，如 ToolFormer 和 HuggingGPT，来增强计划生成的能力。 代码方法 ：生成可执行的编程语言代码形式的计划，如 Faithful CoT 和 PAL 先生成计划，再由确定性求解器执行。这种方法可以提高计划的准确性和可执行性，特别适用于需要精确计算和控制的任务。 反馈获取 内部反馈 ：LLM 自身可以通过评估生成计划的质量、根据计划执行结果提供反馈。例如，RAP 通过评估每个候选计划导致任务成功的可能性来提供反馈，Tree of Thoughts 通过比较不同计划进行投票来选择最优计划。 外部反馈 ：外部对象如代码解释器、稳定扩散等工具或虚拟世界可以提供反馈，帮助 LLM 改进计划。例如，代码解释器可以提供错误消息，稳定扩散可以提供视觉感知等。 计划改进 推理改进 ：通过明确的推理过程从反馈中提取关键信息，如 React 通过演示生成推理跟踪来改进计划，ChatCoT 将工具辅助的推理过程统一为多回合对话，使 LLM 能够更好地理解和处理反馈信息。 回溯调整 ：采用搜索算法如广度优先或深度优先搜索进行回溯，以改进计划。例如，Tree of Thoughts 通过回溯选择更好的计划，DEPS 和 TIP 根据反馈信号修改计划，以确保计划的有效性和适应性。 记忆辅助 ：利用长期记忆来辅助计划改进，如 Reflexion 存储自我反思的反馈，Generative Agents 设计记忆流机制，技能库机制存储成功计划，使用向量数据库等工具进行高效存储和检索。记忆辅助可以帮助 LLM 在处理长期任务时保持一致性和连贯性，提高计划的质量和效率。 7 CAPACITY AND EVALUATION 该章节主要对大语言模型（LLM）的能力和评估进行了详细论述，包括基本能力、高级能力、基准和评估方法以及实证</p>
</div></details><h2 id="toc-230">116. Large Language Models | Springer Nature Link</h2>
<ul>
<li>链接：https://link.springer.com/chapter/10.1007/978-3-031-82062-5_5</li>
<li>来源：bing</li>
<li>摘要：2025年3月27日 · Large Language Models have presented an impressive performance and have become fundamental in real-world applications. These models are built upon vast amounts of text …</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-231">正文（抓取，非 AI）</h3>
<p>Large Language Models | Springer Nature Link Skip to main content Advertisement Large Language Models Chapter First Online: 27 March 2025 pp 81–102 Cite this chapter Generative AI: Techniques, Models and Applications Abstract Large Language Models have presented an impressive performance and have become fundamental in real-world applications. These models are built upon vast amounts of text data and are trained to understand, generate, and manipulate human language with remarkable fluency. Large language models have applications in various fields, including content generation, language translation, sentiment analysis, and conversational agents. These models are based on neural networks and considered as pre-trained, large-scale, statistical language models. LLMs are playing a significant role in advancement of AI agents. This chapter has explored different existing surveys and summarized in two categories, 7 general survey papers and 15 domain specific survey papers. Primary focus of chapter is to explore the types of large language models, tasks of LLMs, frame-works, applications and challenges. This is a preview of subscription content, log in via an institution to check access. Access this chapter Log in via an institution Subscribe and save Springer+ from €37.37 /Month Starting from 10 chapters or articles per month Access and download chapters and articles from more than 300k books and 2,500 journals Cancel anytime View plans Buy Now Chapter EUR 29.95 Price includes VAT (China (P.R.)) Available as PDF Read on any device Instant download Own it forever Buy Chapter eBook EUR 96.29 Price includes VAT (China (P.R.)) Available as EPUB and PDF Read on any device Instant download Own it forever Buy eBook Softcover Book EUR 119.99 Price excludes VAT (China (P.R.)) Compact, lightweight edition Dispatched in 3 to 5 business days Free shipping worldwide - see info Buy Softcover Book Tax calculation will be finalised at checkout Purchases are for personal use only Institutional subscriptions Similar content being viewed by others Large Data Begets Large Data: Studying Large Language Models (LLMs) and Its History, Types, Working, Benefits and Limitations Chapter © 2024 Large Language Models (LLMs) Chapter © 2026 Large Language Models: Creation, Optimisation, and Application Chapter © 2025 Explore related subjects Discover the latest articles, books and news in related subjects, suggested using machine learning. Computational Linguistics Information Model Language Processing Machine Learning Models of Computation Natural Language Processing (NLP) References Zhao WX, Zhou K, Li J, Tang T, Wang X, Hou Y, Min Y, Zhang B, Zhang J, Dong Z, Wen JR (2023) A survey of large language models. arXiv preprint arXiv:2303.18223 Jelinek F (1998) Statistical methods for speech recognition. MIT press Google Scholar Gao J, Lin CY (2004) Introduction to the special issue on statistical language modeling. ACM Trans Asian Lang Inf Process (TALIP) 3(2):87–93 MATH Google Scholar Rosenfeld R (2000) Two decades of statistical language modeling: where do we go from here? Proc IEEE 88(8):1270–1278 MATH Google Scholar Bahl LR, Brown PF, De Souza PV, Mercer RL (1989) A tree-based statistical language model for natural language speech recognition. IEEE Trans Acoust Speech Signal Process 37(7):1001–1008 MATH Google Scholar Brants T, Popat A, Xu P, Och FJ, Dean J (2007) Large language models in machine translation. In Proceedings of the 2007 joint conference on empirical methods in natural language processing and computational natural language learning (EMNLP-CoNLL), pp 858–867 Google Scholar Liu X, Croft WB (2005) Statistical language modeling for information retrieval. Annu Rev Inf Sci Technol 39(1):1–31 MATH Google Scholar Zhai C (2008) Statistical language models for information retrieval a critical review. Found Trends® Inf Retrieval 2(3):137–213 Google Scholar Mikolov T, Sutskever I, Chen K, Corrado GS, Dean J (2013a) Distributed representations of words and phrases and their compositionality. Adv Neural Inf Process Syst 26 Google Scholar Mikolov T, Chen K, Corrado G, Dean J (2013b) Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781 Peters ME, Neumann M, Zettlemoyer L, Yih WT (2018) Dissecting contextual word embeddings: architecture and representation. arXiv preprint arXiv:1808.08949 Devlin J, Chang MW, Lee K, Toutanova K (2018). Bert: pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, Kaiser L, Polosukhin I (2017) Attention is all you need. Adv Neural Inf Process Syst 30 Google Scholar Radford A, Wu J, Child R, Luan D, Amodei D, Sutskever I (2019) Language models are unsupervised multitask learners. OpenAI blog 1(8):9 Google Scholar Lewis M, Liu Y, Goyal N, Ghazvininejad M, Mohamed A, Levy O, Stoyanov V, Zettlemoyer L (2019) Bart: denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461 Shanahan M (2024) Talking about large language models. Commun ACM 67(2):68–79 MATH Google Scholar Wei J, Wang X, Schuurmans D, Bosma M, Xia F, Chi E, Le QV, Zhou D (2022) Chain-of-thought prompting elicits reasoning in large language models. Adv Neural Inf Process Syst 35:24824–24837 Google Scholar Liu Y, Ott M, Goyal N, Du J, Joshi M, Chen D, Levy O, Lewis M, Zettlemoyer L, Stoyanov V (2019) Roberta: a robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692 Raffel C, Shazeer N, Roberts A, Lee K, Narang S, Matena M, Zhou Y, Li W, Liu PJ (2020) Exploring the limits of transfer learning with a unified text-to-text transformer. J Mach Learn Res 21(140):1–67 Google Scholar Su D, Xu Y, Winata GI, Xu P, Kim H, Liu Z, Fung P (2019) Generalizing question answering system with pre-trained language model fine-tuning. In: Proceedings of the 2nd workshop on machine reading for question answering, pp 203–211 Google Scholar Li J, Tang T, Zhao WX, Nie JY, Wen JR (2022) Pretrained language models for text generation: a survey. arXiv preprint arXiv:2201.05273 Zhao H, Chen H, Yang F, Liu N, Deng H, Cai H, Wang S, Yin D, Du M (2023) Explainability for large language models: a survey. ACM Transa Intell Syst Technol Google Scholar Minaee S, Mikolov T, Nikzad N, Chenaghlu M, Socher R, Amatriain X, Gao J (2024) Large language models: a survey. arXiv preprint arXiv:2402.06196 Raiaan MAK, Mukta MSH, Fatema K, Fahad NM, Sakib S, Mim MMJ, Ahmad J, Ali ME, Azam S (2024) A review on large language models: architectures, applications, taxonomies, open issues and challenges. IEEE Access Google Scholar Hadi MU, Qureshi R, Shah A, Irfan M, Zafar A, Shaikh MB, Akhtar N, Wu J, Mirjalili, S. (2023). A survey on large language models: applications, challenges, limitations, and practical usage. Authorea Preprints Google Scholar Pan S, Luo L, Wang Y, Chen C, Wang J, Wu X (2024) Unifying large language models and knowledge graphs: a roadmap. IEEE Trans Knowl Data Eng Google Scholar Naveed H, Khan AU, Qiu S, Saqib M, Anwar S, Usman M, Akhtar N, Barnes N, Mian A (2023) A comprehensive overview of large language models. arXiv preprint arXiv:2307.06435 Taylor R, Kardas M, Cucurull G, Scialom T, Hartshorn A, Saravia E, Poulton A, Kerkez V, Stojnic R (2022) Galactica: a large language model for science. arXiv preprint arXiv:2211.09085 Wang P, Li L, Chen L, Zhu D, Lin B, Cao Y, Liu Q, Liu T, Sui Z (2023) Large language models are not fair evaluators. arXiv preprint arXiv:2305.17926 Wang L, Ma C, Feng X, Zhang Z, Yang H, Zhang J, Tang J, Chen X, Lin Y, Wen JR (2023) A survey on large language model based autonomous agents. arXiv preprint arXiv:2308.11432 Xi Z, Chen W, Guo X, He W, Ding Y, Hong B, Zhang M, Wang J, Jin S, Zhou E, Gui T (2023) The rise and potential of large language model based agents: a survey. arXiv preprint arXiv:2309.07864 Huang J, Chang KCC (2022) Towards reasoning in large language models: a survey. arXiv preprint arXiv:2212.10403 Zhu X, Li J, Liu Y, Ma C, Wang W (2023) A survey on model compression for large language models. arXiv preprint arXiv:2308.07633 Yin S, Fu C, Zhao S, Li K, Sun X, Xu T, Chen E (2023) A survey on multimodal large language models. arXiv preprint arXiv:2306.13549 Romano MF, Shih LC, Paschalidis IC, Au R, Kolachalama VB (2023) Large language models in neurology research and future practice. Neurology 101(23):1058–1067 Google Scholar Wang Y, Zhong W, Li L, Mi F, Zeng X, Huang W, Shang L, Jiang X, Liu Q (2023) Aligning large language models with human: a survey. arXiv preprint arXiv:2307.12966 Zhao WX, Liu J, Ren R, Wen JR (2024) Dense text retrieval based on pretrained language models: a survey. ACM Trans Inf Syst 42(4):1–60 MATH Google Scholar Tan Z, Beigi A, Wang S, Guo R, Bhattacharjee A, Jiang B, Bhattacharjee A, Karami M, Li J, Cheng L, Liu H (2024) Large language models for data annotation: a survey. arXiv preprint arXiv:2402.13446 Chang Y, Wang X, Wang J, Wu Y, Yang L, Zhu K, Chen H, Yi X, Wang Y, Xie X (2023) A survey on evaluation of large language models. ACM Trans Intell Syst Technol Google Scholar Yao Y, Duan J, Xu K, Cai Y, Sun Z, Zhang Y (2024) A survey on large language model (llm) security and privacy: the good, the bad, and the ugly. High-Confidence Comput 100211 Google Scholar Cui C, Ma Y, Cao X, Ye W, Zhou Y, Liang K, Chen J, Lu J, Yang Z, Liao KD, Gao T, Zheng C (2024) A survey on multimodal large language models for autonomous driving. In: Proceedings of the IEEE/CVF winter conference on applications of computer vision, pp 958–979 Google Scholar Sun Z (2023) A short survey of viewing large language models in legal aspect. arXiv preprint arXiv:2303.09136 Xu D, Chen W, Peng W, Zhang C, Xu T, Zhao X, Wu X, Zheng Y, Wang Y, Chen E (2024) Large language models for generative information extraction: A survey. https://arxiv.org/abs/2312.17617 Wan Z, Wang X, Liu C, Alam S, Zheng Y, Qu Z, Yan S, Zhu Y, Zhang M (2023) Efficient large language models: a survey. 1 arXiv preprint arXiv:2312.03863 Mann B, Ryder N, Subbiah M, Kaplan J, Dhariwal P, Neelakantan A, Shyam P, Saatry G, Askell A, Amodei D (2020) Language models are few-shot learners. arXiv preprint arXiv:2005.14165 Chowdhery A et al (2023) Palm: scaling language modeling with pathways. J Mach Learn Res 24(240):1–113 Google Scholar Ouyang L, Wu J, Jiang X, Almeida D, Wainwright C, Mishkin P, Zhang C, Agarwal S, Slama K, Ray A, Schulman J, Lowe R (2022) Training language models to follow instructions with human feedback. Adv Neural Inf Process Syst 35:27730–27744 Google Scholar Christiano PF, Leike J, Brown T, Martic M, Legg S, Amodei D (2017) Deep reinforcement learning from human preferences. Adv Neural Inf Process Syst 30 Google Scholar Schick T, Dwivedi-Yu J, Dessì R, Raileanu R, Lomeli M, Hambro E, Zettlemoyer L, Cancedda N, Scialom T (2024) Toolformer: language models can teach themselves to use tools. Adv Neural Inf Process Syst 36 Google Scholar Nakano R, Hilton J, Balaji S, Wu J, Ouyang L, Kim C, Hesse C, Jain S, Kosaraju V, Schulman J (2021) Webgpt: browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332 Clark K, Luong MT, Le QV, Manning CD (2020). Electra: pre-training text encoders as discriminators rather than generators. arXiv preprint arXiv:2003.10555 Lan Z, Chen M, Goodman S, Gimpel K, Sharma P, Soricut R (2019) Albert: a lite bert for self-supervised learning of language representations. arXiv preprint arXiv:1909.11942 Sanh V, Webson A, Raffel C, Bach SH, Sutawika L, Alyafeai Z, Chaffin A, Stiegler A, Scao A, Raja A, Rush AM (2021) Multitask prompted training enables zero-shot task generalization. arXiv preprint arXiv:2110.08207 Zeng A, Liu X, Du Z, Wang Z, Lai H, Ding M, Yang W, Xu Y, Zheng W, Xia X, Tam WL, Ma Z, Tang J (2022) Glm-130b: an open bilingual pre-trained model. arXiv preprint arXiv:2210.02414 Zoph B, Bello I, Kumar S, Du N, Huang Y, Dean J, Shazeer N, Fedus W (2022) St-moe: designing stable and transferable sparse expert models. arXiv preprint arXiv:2202.08906 Brown T, Mann B, Ryder N, Subbiah M, Kaplan JD, Dhariwal P, Neelakantan A, Shyam P, Sastry G, Askell A (2020) Language models are few-shot learners. Adv Neural Inf Process Syst 33:1877–1901 Google Scholar Kerner SM (2023) Large language models (LLMs). What is. https://www.techtarget.com/whatis/definition/large-language-model-LLM Real-World Use Cases for Large Language Models (LLMs) (2023) Medium. https://cellstrat.medium.com/real-world-use-cases-for-large-language-models-llms-d71c3a577bf2 Jothi N (2023) Semantic search with LLM’s—Naveen Jothi—Medium. Medium. https://medium.com/@naveenjothi040/semantic-search-with-llms-3661fd2a9331 Rasley J, Rajbhandari S, Ruwase O, He Y (2020) Deepspeed: system optimizations enable training deep learning models with over 100 billion parameters. In: Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery &amp; data mining, pp 3505–3506 Google Scholar Smith S, Patwary M, Norick B, LeGresley P, Rajbhandari S, Casper J, Liu Z, Prabhumoye S, Zerveas G, Korthikanti V, Catanzaro B (2022) Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model. arXiv preprint arXiv:2201.11990 Shoeybi M, Patwary M, Puri R, LeGresley P, Casper J, Catanzaro B (2019) Megatron-lm: training multi-billion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053 Zheng L, Li Z, Zhang H, Zhuang Y, Chen Z, Huang Y, Xu Y, Zhuo D, Xing EP, Stoica I (2022) Alpa: automating inter-and {Intra-Operator} parallelism for distributed deep learning. In: 16th USENIX symposium on operating systems design and implementation (OSDI 22), pp 559–578 Google Scholar Le Scao T, Fan A, Akiki C, Pavlick E, Ilić S, Hesslow D, Castagne R, Luccioni AS, Yvon F, Galle M, Tow J, Al-Shaibani MS (2022) Bloom: a 176b-parameter open-access multilingual language model Google Scholar Zhang S, Roller S, Goyal N, Artetxe M, Chen M, Chen S, Dewan C, Diab M, Li X, Lin XV, Mihaylov T, Zettlemoyer L (2022) Opt: open pre-trained transformer language models. arXiv preprint arXiv:2205.01068 Nijkamp E, Pang B, Hayashi H, Tu L, Wang H, Zhou Y, Savarese S, Xiong C (2022) Codegen: an open large language model for code with multi-turn program synthesis. arXiv preprint arXiv:2203.13474 Li C, Gan Z, Yang Z, Yang J, Li L, Wang L, Gao J (2023) Multimodal foundation models: from specialists to general-purpose assistants. 1(2):2. arXiv preprint arXiv:2309.10020 Touvron H, Martin L, Stone K, Albert P, Almahairi A, Babaei Y, Bashlykov N, Batra S, Bhargava P, Bhosale S, Bikel D, Scialom T (2023) Llama 2: open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 Devlin J, Chang M, Lee K, Toutanova K (2019) Bert: Pre-training of deep bidirectional transformers for language understanding. In North American chapter of the association for computational linguistics. https://api.semanticscholar.org/CorpusID:52967399 Dosovitskiy A, Beyer L, Kolesnikov A, Weissenborn D, Zhai X, Unterthiner T, Dehghani M, Minderer M, Heigold G, Gelly S, Uszkoreit J, Houlsby N (2020) An image is worth 16 × 16 words: transformers for image recognition at scale. arXiv preprint arXiv:2010.11929 FairScale authors (2021) Fairscale: a general purpose modular pytorch library for high performance and large scale training. https://github.com/facebookresearch/fairscale Pax Authors (2023a) Pax: a jax-based machine learning framework for large scale models. https://github.com/google/paxml . GitHub repository Anil R, Dai AM, Firat O, Johnson M, Lepikhin D, Passos A, Shakeri S, Taropa E, Bailey P, Chen Z, Chu E, Wu Y (2023) Palm 2 technical report. arXiv preprint arXiv:2305.10403 Hsiao S, Pinsky Y, Pichai S (2023) Bard: Google’s generative language model. https://blog.google/products/search/bard-updates/ . Accessed: 7 Mar 2024 MosaicML (2023b) Llm foundry. https://github.com/mosaicml/ll</p>
</div></details><h2 id="toc-232">117. 挑战 Transformer：全新架构 Mamba 详解</h2>
<ul>
<li>链接：https://www.zhihu.com/tardis/zm/art/684231320</li>
<li>来源：bing</li>
<li>摘要：2025年9月23日 · 而就在最近，一名为 Mamba 的架构似乎打破了这一局面。 与类似规模的 Transformer 相比， Mamba 具有 5 倍的吞吐量， 而且 Mamba-3B 的效果与两倍于其规模的 …</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-233">正文（抓取，非 AI）</h3>
<p>打个小广告 ☻，知乎专栏 《大模型前沿应用》 的内容已经收录在新书 《揭秘大模型：从原理到实战》 中。感兴趣的朋友可以购买，多谢支持！♥♥ 背景 屹立不倒的 Transformer 迎来了一个强劲竞争者。 自 2017 年被提出以来，Transformer 已经成为 AI 大模型的主流架构，但随着模型规模的扩展和需要处理的序列不断变长，Transformer 的局限性也逐渐凸显。一个很明显的缺陷是：Transformer 模型中自注意力机制的计算量会随着上下文长度的增加呈平方级增长，比如上下文增加 32 倍时，计算量可能会增长 1000 倍，计算效率非常低。 为了克服这些缺陷，研究者们开发出了很多注意力机制的高效变体，但这往往以牺牲其有效性特为代价。到目前为止，这些变体都还没有被证明能在不同领域发挥有效作用。 而就在最近，一名为 Mamba 的架构似乎打破了这一局面。 与类似规模的 Transformer 相比， Mamba 具有 5 倍的吞吐量 ， 而且 Mamba-3B 的效果与两倍于其规模的 Transformer 相当 。性能高、效果好，Mamba 成为新的研究热点。 图1 Mamba 在推理过程中的吞吐量对比 本文将详细的解读 Mamba 架构，由于 Mamba 是基于 SSM-&gt;HiPPO-&gt;S4-&gt;Mamba 演化过来的，而 HiPPO、S4、Mamba 的一作者都是卡内基梅隆大学机器学习系助理教授 Albert Gu 。因此，本文将从标准 SSM 开始，逐步介绍 HiPPO、S4、Mamba。 图2总结了SSM、HiPPO、S4、Mamba的主要区别，以及各个模型的主要内容。本文内容也将按图中内容展开。 图2-2：HiPPO、S4、Mamba 一、现有架构问题 序列建模的核心问题是：同时解决 有效 和 高效 。有效是指能够选择性记忆历史信息，解决 长距离依赖 （Long-Range Dependencies，LRDs）问题；高效是指计算高效。 尽管传统的模型如循环神经网络（RNNs）、卷积神经网络（CNNs）和 Transformers 在处理长距离依赖方面有专门的变体，但它们在 处理超过 10000 步的极长序列时仍然面临挑战 。 1.1 Transformer 问题 Transformer 的一个主要优点是，无论它接收到多长的输入，它都使用序列中的所有 token 信息（无论序列有多长）来对输入数据进行处理。 图1-1：Transformer会查看过去所有 token 但是为了获得全局信息，注意力机制在长序列上非常耗费显存。注意力创建一个矩阵，将每个 token 与之前的每个 token 进行比较。矩阵中的权重由 token 对之间的相关性决定。 图1-2：Transformer 会计算每个 token 之间的 Attention 在训练过程中，Attention 计算可以并行化，所以可以极大地加快训练速度。但是在推理过程中，当生成下一个 token 时，我们需要重新计算整个序列的注意力。 图1-3：生成新 token 时需要重新计算整个序列的注意力 长度为 L 的序列生成 token 大约需要 L² 的计算量，如果序列长度增加，计算量会平方级增长。因此， 需要重新计算整个序列是 Transformer 体系结构的主要瓶颈 。 图1-4：Transformer 训练快、推理慢 1.2 RNN 的问题 图1-5：循环神经网络 RNN 在生成输出时，RNN 只需要考虑之前的隐藏状态和当前的输入。这样不会重新计算以前的隐藏状态，这正Transformer 不具备的。 这种结构可以让 RNN 进行 快速推理 ，并且理论上可以无限扩展上下文长度，因为每次推理只取一个隐藏状态和当前输入，内存占用非常稳定。 RNN 的每个隐藏状态都是之前所有隐藏状态的聚合。但是这里会有一个问题，在生成 token "Liang" 时，最后一个隐藏状态不再包含关于 token "Hello" 的信息。这会导致随着时间的推移，RNN 会忘记更久的信息，因为它只考虑前一个状态。 图1-6：只考虑前一个 hidden state 并且 RNN 的这种顺序性产生了另一个问题。 训练不能并行进行 ，因为它需要按顺序完成每一步。 图1-7：RNN 训练不能并行 RNN的统一定义为： 其中 是每一步的输出，它由当前输入 和前一时刻输出 共同决定，而θ则是可训练参数。那么参数θ的梯度可以表示为： 可以看到，当前梯度依赖上个 token 的梯度。 与 Transformer 相比，RNN 的问题完全相反！它的 推理速度非常快，但不能并行化导致训练很慢 。 图1-8：RNN 和 Transformer对比 人们一直在寻找一种既能像 Transformer 那样并行化训练，能够记住先前的信息，又能在推理时时间是随序列长度线性增长的模型，Mamba 就是这样应运而生的 。解下来我们从 SSM 开始，逐步介绍 Mamaba。 二、状态空间模型 SSM 2.1 什么是 SSM 状态空间模型（State Space Models，SSM）由简单的方程（3）定义。它将一维输入信号 映射到 N 维潜在状态 ，然后再投影到一维输出信号 。 其中， 是状态转移矩阵， 是输入到状态的矩阵， 是状态到输出的矩阵，D是直接从输入到输出的参数（很多时候取 D = 0）。 2.2 SSM 架构 下图是 SSM 的架构，主要包含两个部分：状态更新方程和输出方程。 图2-1：SSM结构 SSM 可以简化为以下结构： 图2-2：简化的SSM结构 下面我们看一下更详细的结构，首先是状态更新，如下所示： 图2-3：状态更新详细结构 备注 ：图中的输入 ，表示输入的信号是 D 维的。 SSM 也可以用于处理多维信号输入。 然后是输出方程，详细机构如下所示： 图2-4：输出方程详细结构 2.3 SSM 例子：弹簧振子 下面举一个描述弹簧振子系统的 SSM 例子。 图2-5：弹簧振子 考虑一个质量为 的物体，它连接在一个劲度系数为 的弹簧上，并且受到阻尼系数为 的阻尼力作用。当物体从平衡位置偏离时，它会在弹簧力的作用下进行振动。我们可以用状态空间模型来描述这个系统的动态。 状态变量可以选择为物体的位移 和速度 。输入 在这个例子中可以为零，因为我们没有外部力作用在物体上。输出 可以是我们感兴趣的位移 。 状态向量定义为： 输入向量为： 输出位移 。弹簧振子的状态空间方程可以表示为： 在了解 SSM 基本概念之后，接下来我们介绍基于 SSM 的 HiPPO 架构。 三、HiPPO（High-order Polynomial Projection Operators） HiPPO 是 Albert Gu 于2020年在论文 HiPPO: Recurrent Memory with Optimal Polynomial Projections 中提出的新架构。HiPPO 主要为了解决 如何在有限的存储空间中有效地解决序列建模的长距离依赖问题。 HiPPO 通过函数逼近产生状态矩阵 A 的最优解，有效的解决了长距离依赖问题。 问题背景： 在处理序列数据时，一个核心问题是如何在增量方式下表示累积的历史信息。这涉及到如何在有限的存储空间中有效地更新和维护历史数据的表示。 HiPPO框架 ：作者介绍了一个名为 HiPPO（High-order Polynomial Projection Operators）的通用框架，它通过将连续信号和离散时间序列投影到多项式基上，实现了在线数据压缩。 重要性度量 ：HiPPO 框架考虑了一个度量，用于指定过去每个时间步的重要性。这个度量帮助HiPPO产生在线函数逼近问题的最优解。 理论贡献 ：HiPPO 框架不仅提供了对现有记忆单元的简短推导，还推广了循环神经网络（如GRUs）中普遍存在的门控机制。 新的记忆更新机制 ：作者提出了一个新的记忆更新机制（HiPPO-LegS），它能够随时间扩展以记住所有历史信息，避免了对时间尺度的先验假设。 理论优势 ：HiPPO-LegS 具有时间尺度鲁棒性、快速更新和有界梯度的理论优势。 实验结果 ：在基准测试中，HiPPO-LegS 在打乱的 MNIST 数据集上达到了98.3%的新最佳准确率。在一个新的轨迹分类任务中，HiPPO-LegS 在处理分布外时间尺度和缺失数据方面，比其他 RNN 和神经 ODE（一阶常微分方程）基线模型的性能提高了25-40%的准确率。 下面介绍 HiPPO 实现的具体细节。 3.1 HiPPO 架构：高阶多项式投影 3.1.1 HiPPO问题设置 问题定义 给定一个在时间 上的输入函数 ，需要在每个时间点操作累计历史 ，以便理解到目前为止看到的输入并对未来进行预测。 由于函数空间的庞大，无法完美记住整个历史，因此需要将其进行压缩，HiPPO 提出了将历史投影到有界维数的子空间的一半方法。 函数逼近与度量 为了评估逼近的质量，需要在函数空间中定义一个距离。任何在 上的概率度量 都可以为平方可积函数空间提供内积 ，从而诱导出一个希尔伯特空间 和相应的范数 。 为了选择合适的子空间，需要一个度量来量化历史的重要性。这个度量 随时间变化，支持在 上，因为 只在时间t之前定义。 多项式基展开 任何 N 维的函数子空间 G 都是逼近的合适候选。参数 N 对应于逼近的阶数，或者说压缩的大小；投影的历史可以通过G的任何基的N个系数来表示。 论文中使用多项式作为自然基，因此G是小于N阶的多项式的集合。 在线逼近 由于我们关心在每个时间 t 对 的逼近，我们也让度量 随时间变化。总体上，我们寻找一个 ，使得 最小。直观上，度量 控制输入域各部分的重要性。 挑战 挑战在于如何在给定度量 的情况下以封闭形式解决优化问题，以及在 时如何线性地维护这些系数。 3.1.2 HiPPO 通用架构 通过连续动态系统计算投影 这部分是 HiPPO 的关键步骤，它涉及到将输入函数 在时间 t 投影到一个多项式空间上，以便在线更新记忆表示。 投影的表示 ：投影可以通过输入函数 在时间 t 的限制 的 N 个系数来表示。这些系数是通过在多项式空间的基上展开 得到的。 正交多项式基 ：为了选择合适的基，作者利用了正交多项式的性质。正交多项式为 提供了一个自然的基，使得 的投影可以表示为这些基的线性组合。 系数的计算 ：投影的系数 是通过内积 计算得到的，其中 是正交多项式基的元素。 连续动态系统 ：为了在线更新这些系数，作者提出了一个连续动态系统，这个系统描述了系数 是如何随时间 t 变化的。这种动态系统可以表示为 ，其中 是依赖于时间的矩阵。 投影操作符 ：作者定义了一个投影操作符 ，它将 映射到 （多项式空间）中的 ，使得 最小化。这个操作符是 HiPPO 框架的核心。 系数提取操作符 ：除了投影操作符，作者还定义了一个系数提取操作符 ，它将多项式 映射到其对应的系数 。 在线更新 ：通过这个连续动态系统，HiPPO 框架能够在线更新记忆表示，即随着新数据的到来，系统能实时地调整系数 。 在线函数逼近 图3-1：HiPPO框架 图2-6展示了 HiPPO 框架，首先需要找到投影 ，将输入 投影到多项式空间；然后将投影通过一组系数 来表示，这些系数捕捉了函数 的历史信息；使用连续时间下的一阶常微分方程来表示系数 如何随时间 t 动态变化；最后，将连续时间的动态变换转化为离散时间的递归关系（比如双线性变换），这允许 HiPPO 在每个时间步 k 更新系数 。 3.1.3 高阶投影：度量方法以及 HiPPO 动态系统 作者定义了两种度量方法，分别是 LegT 和 LagT。LegT 度量为最近的历史信息分配均匀的权重，表示如下： LagT 度量使用指数衰减的方式来衡量历史信息的重要性，表示如下： 对于 LegT 和 LagT，系数 可以使用 ODE（一阶常微分方程）来表示： 其中 A 和 B 是与度量 相关的矩阵。这个 ODE 描述了系数 如何随时间 t 和输入函数 变化。 备注：公式（9）是 HiPPO 框架的关键部分， 具体推导可以参看论文中的附录 D 。 对于 LegT 度量，矩阵 A 和 矩阵 B 可以表示如下： 对于 LagT 度量，可以表示如下： 3.1.4 HiPPO 框架中的连续时间动态转换为离散时间递归关系 由于我们处理的输入往往是离散的，因此我们需要将公式（9）的 ODE 离散化。ODE 离散化是一种常用的数据技术，它将 连续时间的常微分方程转换为离散时间的差分方程 。这通常涉及到选择一个合适的时间步长（或步长Δt），并使用数值方法（如欧拉方法、双线性）来近似连续微分。 图3-2：连续信号离散化 使用双线性离散化，如下所示： 结合公式（9）和公式（11），我们可以得到离散化的状态更新公式，表示如下： 离散化之后的 SSM 结构可以表示如下： 图3-3：离散化 SSM 在每个时间步长，我们计算当前输入( )如何影响前一个状态( )，然后计算预测输出( )。 图3-4：每个时间步的计算 这种表示看起来是不是有点熟悉？其实他的处理方法和RNN一样。 图3-5：离散化后和RNN类似 3.2 HiPPO-LegS HiPPO-LegS 是作者基于新的度量提出的全新架构，具有时间鲁棒性、有界梯度、有界近似误差、长时间记忆等效果。 新的度量表示为 ，在新的度量下，矩阵 A 和矩阵 B 可以表示如下： 具体推导在论文的附录 D.3 部分 。 更好的学习长期依赖 HiPPO-LegS 是专门为记忆而设计的 ，它通过其独特的结构和更新机制来避免梯度消失问题。LegS 通过使用Legendre 多项式作为基函数，并结合时间尺度不变的度量，来保持梯度的稳定性。 对于任何时间 ，HiPPO-LegS 在时间 的输出相对于时间 的输入的梯度范数为 ，这意味着梯度随着时间的增加而减小，但是衰减的速度比 RNN 的指数级慢的多。 这个性质使得 HiPPO-LegS 能够有效地缓解 RNN 中的梯度消失问题。即使在长序列中，梯度也不会迅速衰减到0，这有助于网络在训练中更好地学习长期依赖。 近似有界误差 HiPPO-LegS 在时间 t 的近似误差 。其中 N 是多项式的最高阶。这表明随着多项式的阶 N 的增加，误差逐渐减小。 3.3 实验 将 HiPPO 和 RNN 相结合，当前状态 不仅和上一个状态 有关，还和 HiPPO 状态 有关，如下所示： 模型结构如下： 图3-6：HiPPO和RNN结合 下面是pMINIST 数据集上的结果，可以看到 LegS 的效果要好于 LagT 和 LegT，同时 HiPPO 的效果好于之前的其它模型。 图3-7：HiPPO实验结果 备注： pMNIST（permuted MNIST）是一个经过修改的MNIST数据集 ，它用于测试和评估机器学习模型在处理序列数据和学习长期依赖关系方面的能力。在 pMNIST 中，原始 MNIST 图像的像素被重新排列。这意味着图像的像素不再是按照自然顺序（从左到右，从上到下）呈现，而是按照一个固定的、随机的排列顺序。这种排列方式使得模型必须学习像素之间的长期依赖关系，而不能简单地依赖于局部空间结构。 四、S4 (Structured State Space Model) S4 是 HiPPO 的后续工作，论文名称为： Efficiently Modeling Long Sequences with Structured State Spaces 。 S4 的主要工作是将 HiPPO 中的矩阵 A（称为 HiPPO 矩阵 ）转换为正规矩阵（正规矩阵可以分解为对角矩阵）和低秩矩阵的和，以此提高计算效率。 S4 通过这种分解，将计算复杂度降低到了 ，其中 N 是 HiPPO 矩阵的维度，L 是序列长度。 在处理长度为 16000 的序列的语音分类任务中，S4 模型将专门设计的语音卷积神经网络（Speech CNNs）的测试错误率降低了一半，达到了1.7%。相比之下，所有的循环神经网络（RNN）和 Transformer 基线模型都无法学习，错误率均在70%以上。 下面我们就来介绍一下这篇工作。 4.1 HiPPO 解决了长期依赖 作者讨论了如何处理长距离依赖（Long-Range Dependencies，LRDs）的问题，LRDs 是序列建模中的一个关键挑战，因为它们涉及到在序列中跨越大量时间步的依赖关系。 作者指出，基本的 SSM 在实际应用中表现不佳，特别是在处理 LRDs 时。这是因为线性一阶常微分方程（ODEs）的解通常是指数函数，这可能导致梯度在序列长度上呈指数级增长，从而引发梯度消失或爆炸的问题。 为了解决这个问题，作者利用了 HiPPO 理论。 HiPPO 理论指定了一类特殊的矩阵 A，当这些矩阵被纳入 SSM 的方程中时，可以使状态 x(t) 能够记住输入 u(t) 的历史信息。这些特殊矩阵被称为 HiPPO 矩阵，它们具有特定的数学形式，可以有效地捕捉长期依赖关系。 HiPPO 矩阵的一个关键特性是它们允许 SSM 在数学和实证上捕捉 LRDs 。例如，通过将随机矩阵 A 替换为 HiPPO 矩阵，可以在序列 MNIST 基准测试上显著提高 SSM 的性能。 HiPPO 矩阵表示如下： 4.2 在线推理：使用递归形式 S4 在推理时，使用公式（12）的递归形式，每次只需要和上一个状态进行计算，具有和 RNN 相似的推理效率。 4.3 训练 S4：卷积表示 由于离散时间 SSM 的递归性质，它在硬件上进行训练时存在效率问题。因此，作者将离散时间 SSM 的 递归方程转换为离散卷积的形式 。通过展开递归方程，可以得到一个卷积核，这个卷积核可以用来在序列数据上应用卷积操作。这种转换允许 SSM 利用快速傅里叶变换（FFT）等高效的卷积计算方法，从而在训练过程中提高计算效率。 上面式子可以转化为卷积的形式： 其中， 是一个与 SSM 的参数（A, B, C）相关的卷积核，可以通过离散傅里叶变换（DFT）和逆变换（IDFT）来计算。这种卷积表示不仅在理论上是可行的，而且在实践中也是非常有效的，因为它允许在保持模型性能的同时，显著减少训练过程中的计算和内存需求。 作者在这一节中还讨论了如何计算 SSMn卷积核，这是他们技术贡献的关键部分。通过这种卷积表示，SSM 可以被有效地训练，同时保持其在处理长距离依赖（LRDs）方面的能力。这种表示形式为 SSM 在各种序列建模任务中的应用提供了灵活性，包括图像处理、语音识别和时间序列分析等。 图4-1：SSM 卷积核形式 下面是一个具体的例子，如何使用卷积核生成输出。 图4-2：使用卷积核生成输出 卷积的一个主要好处是它可以并行训练。但是由于核大小是固定，它们的推理不如 RNN 快速并且对序列长度有限制。 图4-3：递归 SSM 和 卷积 SSM 的对比 这里可以使用一个简单的技巧，即根据任务选择表示。在训练过程中使用可以并行化的卷积表示，在推理过程中，我们使用高效的循环表示。 图4-4：递归推理、卷积训练 4.4 为什么对角化可以减少 SSM 计算复杂度 为了进一步提升计算效率，作者讨论了对角化在计算离散时间状态空间模型（SSM）中的应用，以及为什么直接应用对角化方法在实践中并不可行。 对角化是一种线性代数技术，它可以将一个矩阵转换为对角形式，从而简化矩阵的乘法和其他运算。在 SSM 的上下文中，对角化可以显著减少计算复杂度，因为对角矩阵的幂运算（如在递归方程中出现的）可以通过简单的元素指数运算来完成。 下面我们解释下，为什么对角化可以减少 SSM 计算复杂度。 首先，我们引入论文中的定理 3.1 （Lemma 3.1）：共轭是 SSM 中的等价关系，即： 也就是将矩阵 变为 ，最后得到的输出 保持不变。那么如果矩阵 是对角矩阵，则输出 的计算复杂度将从 变成 。只要 Lemma 3.1 成立，我们就能使用对角化技术，降低计算复杂度。下面我们看一下 Lemma 3.1 的证明。 证明可以从 SSM 的两种表达形式出发。首先，有两个 SSM，其状态分别用 和 表示。 第一个 SSM： 第二个 SSM： 通过 将第二 SSM 乘以 后，变成如下形式： 可以看到当 ，两个 SSM 变得一样， 即 。因此，这两个 SSM 计算的相同的操作符 ，只是通过 对状态 进行了变换。 通过共轭将 转换为其它形式，理想情况下这种形式结构更清晰，并且允许更快的计算。例如，如果 是对角矩阵，那么所需的计算将变得容易得多。 备注： Lemma 3.1 是非常重要的结论，这意味着只要矩阵 和矩阵 相似（即满足 ），那么就可以使用 Lemma 3.1 中的方法替换矩阵 ，替换后的输出 保持不变。 4.5 直接对角化 HiPPO 矩阵导致数值溢出 上面提到，如果 能对角化，那么 SSM 递归计算的复杂度将从 变成 。 然而，作者指出，直接对角化 HiPPO 矩阵（用于处理长距离依赖的特殊矩阵）会导致数值问题。这是因为 HiPPO 矩阵的对角化涉及到的矩阵元素在状态大小 N 增大时会呈指数级增长，这使得对角化在数值上变得不稳定和不可行。 Lemma 3.2 将 HiPPO 矩阵 直接对角化为矩阵 ，那么 ，因此直接对角化会导致数值溢出。 下面证明下这个结论。 首先我们可以找到矩阵 的一个相似矩阵，表示如下： 其中： 那么可以找到一个可逆矩阵： 使得 是对角矩阵。比如考虑 ，那么： 即然无法直接对 矩阵进行对角化，那么是否可以将其转化为低秩矩阵或者其它可以对角化的矩阵？解下来我们介绍下如何将其转换为正规矩阵+低秩矩阵。 4.6 S4 参数化：正规矩阵+低秩矩阵 虽然矩阵 不能直接对角化，但是可以表示为正规矩阵+低秩矩阵。 Theorem 1 ：HiPPO 矩阵 可以表示为正规矩阵+低秩矩阵的形式，即： 其中 ， 是对角矩阵， 是低秩矩阵。 下面简单证明下这个定理。 已知 HiPPO 矩阵 可以表示为： 那么 表示为： 虽然这个矩阵不是 反对称矩阵 （反对称矩阵可以对角化），但是可以表示为 ，其中 是反对称矩阵，矩阵 可以重新表示为： 由于 可以对角化，因此 也可以对角化，因此： 其中 是低秩矩阵。 备注： 之前整个矩阵 加上了 ，所以最后需要减去，而这一块更好是 。因为 第 n 行第 k 列为 。 这样我们就将矩阵 转换为了正规矩阵+低秩矩阵的形式。下面我们看一下转换之后的递归计算和卷积计算的复杂度。 4.7 S4 的计算复杂度 经过正规矩阵+低秩矩阵分解后，我们再来考虑 S4 的计算复杂度有什么变化。我们同时考虑推理时递归计算的复杂度以及训练时卷积计算的复杂度。 先给出结论： S4 的递归计算复杂度为 MVM = 。其中 MVM 表示矩阵向量乘法（Matrix-Vector Multiplications）。 S4 的卷积复杂度从 降低到 Cauchy 矩阵-向量乘法，空间复杂度为 。 可以看到，递归计算的复杂度没有变化，而卷积的复杂度从 降低到 Cauchy 矩阵-向量乘法。 这里的 Cauchy 矩阵-向量乘法复杂度表示如下： 如果Cauchy 矩阵-向量乘法按照精确计算，那么 S4 的卷积复杂度为 ，也是小于之前的复杂度 。 解下来我们详细介绍 S4 计算复杂度的分析过程，首先介绍递归计算复杂度。 递归计算复杂度 公式（26）表示矩阵 可以分解为 ，结合前面的 Lemma 3.1 ，那么矩阵 可以变换为 ，可以表示为 的形式。 由于： 先考虑 ： 然后计算 ： 其中 也是对角矩阵，因此上式中的 相当于对低秩矩阵乘法 做了缩放，计算复杂度仍然为 。 现在，我们可以将公式（30）重新表示为下面的形式： 公式（12）的 SSM 可以重新表示为： 可以看到矩阵 和矩阵 中的乘法运算都是矩阵-向量乘法（Matrix-Vector Multiplications，MVM）。因为它们都是对角矩阵+低秩矩阵，因此计算复杂度为 MVM。 卷积计算复杂度 这一块就不再具体介绍了，感兴趣的可以直接去看原论文，在论文的附录 C.3 有详细的分析过程。 结论就是卷积的复杂度为： Cauchy 矩阵-向量乘法。 最后作者对比了 S4 和原始卷积、递归、Attention 之间的计算复杂度，可以看到 S4 是最低的，如下图所示： 图4-5：计算复杂度对比 图中 L 表示序列长度，B 表示 batch size，H 表示隐藏维度。 备注 ：图中的 表示的是 个 Cauchy 矩阵-向量乘法开销。比如 Cauchy 矩阵-向量乘法使用精确计算，那么 。 4.8 实验结果 推理效率 在图 4-15 中，显示了 S4 的推理复杂度为 ，而 Transformer 的推理复杂度为 ，我们看一下具体的测试对比，如下图所示。 图4-6：推理速度对比 序列长度为 1024 时，S4 的推理速度是 Transformer 的 1.58 倍；序列长度为 4096 时，是 Transformer 推理速度的 5.19 倍，由于不需要 KV cache，因此内存占用非常小。 S4 作为生成模型的效果 将 S4 应用在生成模型中，实验结果如下图所示。 图4-7：S4 作为生成模型的效果 HiPPO 影响 这部分作者进行了一系列的消融实验（Ablations），以评估 HiPPO 矩阵在状态空间模型（SSM）中的重要性。这些实验旨在探究 HiPPO 矩阵在 S4 模型中的作用，以及它对于模型性能的影响。 HiPPO 矩阵是 S4 模型中用于处理长距离依赖（LRDs）的关键组件。在这一节中，作者通过以下几个方面的实验来验证 HiPPO 矩阵的重要性： HiPPO 初始化：作者首先研究了不同初始化方法对 SSM 性能的影响，包括随机高斯初始化、HiPPO 初始化以及随机对角高斯矩阵初始化。实验结果表明，HiPPO 初始化在提高模型性能方面起到了关键作用。 HiPPO 矩阵是否可训练：作者还探讨了 HiPPO 矩阵固定以及可训练的效果。他们发现，固定 HiPPO 和可训练的差异不大。 NPLR SSMs：作者进一步研究了在没有 HiPPO 矩阵的情况下，随机 NPLR（Normal Plus Low-Rank，正规+低秩矩阵）的表现。结果表明，即使在 NPLR 形式下，这些随机矩阵的性能仍然不佳，这验证了 HiPPO 矩阵在 S4 模型中的核心作用。 通过这些消融实验，作者强调了 HiPPO 矩阵在 S4 模型中的重要性。这些实验结果不仅证实了 HiPPO 矩阵在处理长距离依赖方面的有效性，而且也表明了它在提升模型整体性能方面的关键作用。这些发现对于理解 S4 模型的设计和优化至关重要。 图4-8：HiPPO 矩阵初始化效果远远高于其它矩阵初始化 虽然 S4 在保证了计算效率的同时，优化了长距离依赖问题。但是由于矩阵 是固定不变的，和输入 token 无关，这就导致了 S4 在一些合成任务上效果不佳，比如选择性复制任务。 而为了解决这些问题，作者提出了 Mamba 架构，通过选择性机制改进 S4，有效解决了这类问题。下面我们就来介绍下最近很火的 Mamba 结构。 五、Mamba 我们终于介绍完了理解 Mamba 所需要的基础知识。状态空间模型可用于建模文本序列，但仍有一系列我们想要避免的缺点。 在本节中，我们将介绍 Mamba 的两大主要贡献： 一种选择性扫描算法，该算法允许模型过滤（不）相关信息； 一种硬件感知算法，该算法允许通过并行扫描、内核融合和重新计算来高效存储（中间）结果。 它们共同创建了选择性 SSM 或 S6 模型，这些模型可以像自注意力一样用于创建 Mamba 块。 在探讨这两大主要贡献之前，让我们首先探讨一下为什么它们是必要的。 状态空间模型，甚至是S4（结构化状态空间模型），在某些对语言建模和生成至关重要的任务上表现不佳，即关注或忽略特定输入的能力。 我们可以通过两个合成任务来说明这一点，即选择性复制和归纳头。 在选择性复制任务中，SSM 的目标是复制输入的部分内容并按顺序输出它们： 图5-1：选择性复制任务 然而，由于（循环/卷积）SSM 是线性时间不变的，因此在这项任务中表现不佳。正如我们之前看到的，对于 SSM生成的每个 token，矩阵 A、B 和 C 都是相同的。 因此，由于固定的 A、B 和 C 矩阵，SSM 无法执行内容感知推理，因为它对每个 token 都一视同仁。这是一个问题，因为我们希望 SSM 能对输入（提示）进行推理。 SSM 表现不佳的第二项任务是归纳头，其目标是重现输入中发现的模式： 图5-2：重现输入中发现的模式 在上面的例子中，我们本质上是在执行一次提示，我们试图“教”模型在每个“Q:”之后提供一个“A:”的回应。然而，由于 SSM 是时间不变的，它无法选择从历史中回忆哪些之前的 token。 让我们通过关注矩阵 B 来说明这一点。无论输入 u 是什么，矩阵 B 都保持不变，因此与 u 无关： 图5-3：矩阵 B 与输入 u 无关 同理，无论输入是什么，A 和 C 也不变，这就是我们上面说的静态。 图5-4：A 和 C 矩阵也和输入 u 无关 相比之下，这些任务对于 Transformer 来说相对容易，因为它们会根据输入序列动态地改变自己的注意力。它们可以选择性地“查看”或“关注”序列的不同部分。 SSM 在这些任务上的糟糕表现说明了时间不变 SSM 的潜在问题，即矩阵 A、B 和 C 的静态性质导致内容感知方面的问题。 5.1 通过选择机制改进 SSM 为了解决上面的问题，作者提出了一种新的选择性 SSM（Selective State Space Models，简称 S6 或 Mamba）。这种模型通过让 SSM 的矩阵 A、B、C 依赖于输入数据 ，从而实现了选择性。这意味着模型可以根据当前的输入动态地调整其状态，选择性地传播或忽略信息。 Mamba 集成了 S4 和 Transformer 的精华，一个更加高效（S4），一个更加强大（Transformer）。 图5-5：Mamba 集成了 S4 和 Transformer 各自的优点 正如上面所提到的，它是通过有选择地将数据压缩到状态中来实现的。当你有一个输入句子时，通常会有一些信息，比如停用词，没有太多意义。 为了有选择地压缩信息，我们需要让参数依赖于输入。为此，我们首先来探讨一下 SSM 在训练过程中输入和输出的维度。 图5-6：SSM 输入（u）输出（y）维度 备注 ：在前面的 HiPPO 和 S4 中，我们假设的输入信号 是 1 维的，而实际应用中大多数都是多维的，后面我们默认是多维输入（默认维度为 D）。 而且需要强调的是 S4 用的是 Single-input-single-output (SISO)，即对应于每一个输入的维度，都有一套独立的 SSM 参数 （传统的 RNN 是 MIMO，multiple-input-multiple-output, 很容易混淆） 。 在 S4 中，矩阵 A、B 和 C 与输入无关，因为它们的维度 N 和 D 是静态的，不会改变。 图5-7：S4 中的矩阵A、B、C 备注 ：实际上矩阵 ，但是通过对角化技术，可以转化为 N 维，因此图5-7中，每个输入维度对应的矩阵 A 维度为 N，输入一共是 D 维，因此矩阵 A 可以表示为 的矩阵。 相反，Mamba 通过将输入序列的长度和批次大小结合起来，使矩阵 B 和 C，甚至步长 ∆ 都依赖于输入： 图5-8：Mamba 中的矩阵B、C和输入有关（L是序列长度） 这意味着对于每个输入 token，我们现在有不同的 B 和 C 矩阵。 备注 ：这里矩阵 A 保持不变，因为我们希望状态本身保持静态，但影响它的方式 (通过 B 和 C) 是动态的。 它们一起选择性地决定在隐藏状态中保留什么和忽略什么，因为它们现在依赖于输入。 在 SSM 中，通过调整 ∆，模型可以控制对当前输入的关注度，从而实现类似于 RNN 门控的效果。例如， 当 ∆ 较大时，模型倾向于关注当前输入并忽略之前的信息 ；而当∆较小时，模型则倾向于保留更多的历史信息： 图5-9：步长 ∆ 效果相当于门控 下面我们看一下选择性 SSM 的完整过程，如下所示： 图5-10：选择性 SSM 完整过程 图 5-10 中的 Represents structured matrix 说的就是原始 的矩阵 经过对角化之后变成维度 。 算法 2 展示了作者所使用的主要选择机制。这一套的思路由来已久，Transformers 里面的 QKV、LSTM里面的、Gating 都是类似的思想。 S4 和 选择性 SSM 的核心区别在于，它们将几个关键参数（∆, B, C）设定为输入的函数，并且伴随着整个 tensor 形状的相关变化。特别是，这些参数现在具有一个长度维度 L，这意味着模型已经从时间不变（time-invariant）转变为时间变化（time-varying）。 其中 : 表示将输入映射到 d 维。 类似于 RNN 中的门控机制。 最后作者选择把 设成了与输入无关，作者给出的解释是离散化之后 ， 的数据依赖能够让整体的 与输入相关。 5.2 选择性 SSM 和门控之间的关系 时间步 时间步 和 RNN 的门控有很强的关联， 依赖输入的 跟 RNN 的遗忘门的功能类似 。 当 ，那么算法 2 中的选择性 SSM 可以表示如下： 可以看到这就是一个带门控的 RNN。 矩阵 B 和 C 在 SSM 中，修改 B 和 C 以使其具有选择性，允许模型更精细地控制是否让输入进入状态 h 或状态进入输出 y，所以 B 和 C 类似于 RNN 中的输入门和输出门。 矩阵 A A 有点类似于起到多尺度/细粒度门控的作用。虽然 已经有点遗忘门的作用，但注意到对于每个输入维度来说， 只是一个标量，而 ，也就是说对应这个维度的 SSM 来说，A 在每个 hidden state 维度上的作用可以不相同，起到细粒度门控的作用，这也是 LSTM 网络里面用 element-wise product 的原因（LSTM 中遗忘门是跟隐藏层维度相同的一个向量，而不仅仅是一个标量）。 5.3 Mamba 高效实现 因为现在的参数 都是输入相关了，所以不再是线性时间不变系统，也就失去了卷积的性质，不能用 FFT来进行高效训练了。 Mamba 作者采用了一种称为硬件感知的算法，实际上就是用三种经典技术来解决这个问题： 内核融合（kernel fusion） 、 并行扫描（parallel scan） 和 重计算（recomputation） 。 一般的实现会提前先把大小为 的 先算出来，然后把它们从 HBM (high-bandwidth memory 或 GPU memory) 读到SRAM，然后调用 scan 算子算出 的 output，写到 HBM 里面。再开一个kernel 把 的 output 以及 的 C 读进来，multiply and sum with C 得到最后的 output 。整个过程的读写是 。 而 Mambda 作者的方法是： 把 读到 SRAM 里面，总共大小是 在 SRAM 里面做离散化，得到 的 在 SRAM 里面做 scan，得到 的 output multiply and sum with C，得到最后的 output 写入HBM 整个过程的总读写量是 ，比之前省了 倍。 backward 的时候就把 重算一遍，类似于flashattn 重算 attention 分数矩阵的思想。只要重算的时间比读 快就算有效。 图5-10：Mamba 的 scan 和其它方法对比 Mamba 的实现比其它方法实现快很多倍，scan 在输入长度 2k 的时候就开始比 FlashAttention 快了，之后越长越快。同时 scan 也比 Convolution 快。 5.4 Mamba 架构 下图是 Mamba 的模型结构： 图5-11：Mamba 模型结构 之前的 SSM 模型要 work，都会加上 output gating，之后再过个线性层 channel mixing，如上图的最左边所示。这两个部分跟 Gated MLP（上图中间）右边的支路和最上面的 channel mixing 是一样的。所以 SSM 层如果跟Gated MLP 合并的话，难免会感觉有点冗余，所以作者干脆把两个合二为一，把 token mixing 层和 channel mixing。 图 5-11 的 Mamba 可以作为一个块来实现，就像我们可以在解码器块中表示自注意力一样。 图5-12：Mamba 块 与解码器一样，我们可以堆叠多个 Mamba 块，并使用它们的输出作为下一个 Mamba 块的输入： 图5-13：多个Mamba块组合使用 它首先进行线性投影以扩展输入 embedding。然后，在应用选择性 SSM 之前进行卷积。选择性 SSM 具有以下属性： 通过离散化创建递归 SSM； 对矩阵 A 进行 HiPPO 初始化，以捕获远程依赖关系； 选择性扫描算法，有选择地压缩信息； 硬件感知算法，加速计算； 下面是一个端到端（输入到输出）的例子： 图5-14：Mamba 架构端到端输出例子 下面我们看一下 Mamba 和 Transformer 以及 RNN 的对比： 图5-15：Mamaba和Transformer以及RNN对比 5.5 实验 之前提到 的作用类似遗忘门，而遗忘门毫无疑问是 LSTM 里面最重要的门，下面这个消融实验结果就论证了 data dependent 影响最大 。 图5-16：对不同参数data dependent的敏感性 最后再看一下模型效果： 图5-17：Mamba和Transformer对比 总结起来就是，效果最好，速度最快！ 总结 融合 SSM 和 LSTM，将 LSTM 选择性的思想融入 SSM 中，全方位的实现优化，使得 Mamaba 即具备像 Transformer 高效训练的特点，又具备 S4 中支持长文本的优点，同时具备 LSTM 一样选择性记忆的特点。 实验证明了 Mamba 的优秀，但是还需要更长的时间检验，目前还没有 10B 以上的 Mamba 模型，就让时间来检验。 参考 Mamba: Linear-Time Sequence Modeling with Selective State Spaces Mamba 作者博士论文 S4:</p></div></details>
</div>
<script>
function speakText(t){ if(!t){ document.getElementById('readStatus').textContent=''; return; }
 speechSynthesis.cancel(); var status=document.getElementById('readStatus'); status.textContent='准备中…';
 var chunks=[]; var maxLen=280; for(var i=0;i<t.length;i+=maxLen){ var s=t.slice(i,i+maxLen); var j=s.lastIndexOf('。'); if(j>80){ chunks.push(s.slice(0,j+1)); i-=s.length-(j+1); } else chunks.push(s); }
 if(chunks.length===0) chunks=[t]; var idx=0; function speakNext(){ if(idx>=chunks.length){ status.textContent=''; return; } var u=new SpeechSynthesisUtterance(chunks[idx]); u.lang='zh-CN'; u.rate=0.95; var v=speechSynthesis.getVoices().filter(function(x){ return x.lang.indexOf('zh')>=0; })[0]; if(v) u.voice=v;
 u.onend=function(){ idx++; setTimeout(speakNext,80); }; u.onerror=function(){ idx++; setTimeout(speakNext,80); }; status.textContent='朗读中 '+(idx+1)+'/'+chunks.length+'…'; speechSynthesis.speak(u); }
 if(speechSynthesis.getVoices().length) speakNext(); else speechSynthesis.onvoiceschanged=function(){ speechSynthesis.onvoiceschanged=null; speakNext(); }; }
function readFullText(){ var c=document.querySelector('.content'); if(!c){ document.getElementById('readStatus').textContent='无可读内容'; return; } var t=(c.innerText||'').trim().replace(/\\s+/g,' '); if(!t){ document.getElementById('readStatus').textContent='无可读内容'; return; } speechSynthesis.cancel(); document.getElementById('readStatus').textContent='全文朗读…'; speakText(t); }
function getBlockForTap(el){ var c=document.querySelector('.content'); var blockTags=['P','DIV','H2','H3','H4','LI','PRE']; while(el&&el!==c){ if(c.contains(el)&&blockTags.indexOf(el.tagName)!==-1) return el; el=el.parentElement; } return null; }
function onContentTap(e){ if(e.target.closest&&e.target.closest('a')) return; var block=getBlockForTap(e.target); if(block){ var t=(block.innerText||'').trim().replace(/\\s+/g,' '); if(t){ e.preventDefault(); speechSynthesis.cancel(); document.getElementById('readStatus').textContent='朗读本段…'; speakText(t); } } }
if(document.readyState==='loading'){ document.addEventListener('DOMContentLoaded', function(){ var c=document.querySelector('.content'); if(c) c.addEventListener('click', onContentTap); }); } else { var c=document.querySelector('.content'); if(c) c.addEventListener('click', onContentTap); }
setTimeout(function(){ try{ var u=new SpeechSynthesisUtterance('\u200b'); u.volume=0; speechSynthesis.speak(u); }catch(e){} }, 300);
</script>
</body>
</html>
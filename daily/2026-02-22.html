<!DOCTYPE html>
<html lang="zh-CN">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<title>知识流日报 2026-02-22：gold volatility forecasting machine learning、gold price determinants real interest rate、financial time series structural break regime、LSTM volatility prediction、gold price forecasting survey</title>
<style>
  * { box-sizing: border-box; }
  body {
    font-family: system-ui, sans-serif;
    max-width: 720px;
    margin: 0 auto;
    padding: 2rem 1rem;
    color: #fff;
    min-height: 100vh;
    background: #0a0e1a;
    background-image:
      radial-gradient(2px 2px at 20px 30px, rgba(255,255,255,0.9), transparent),
      radial-gradient(2px 2px at 40px 70px, rgba(255,255,255,0.6), transparent),
      radial-gradient(1.5px 1.5px at 90px 40px, rgba(255,255,255,0.8), transparent),
      radial-gradient(2px 2px at 130px 80px, rgba(255,255,255,0.5), transparent),
      radial-gradient(1.5px 1.5px at 160px 120px, rgba(255,255,255,0.7), transparent),
      radial-gradient(ellipse 120% 100% at 50% 0%, rgba(88,60,120,0.25), transparent 50%),
      radial-gradient(ellipse 80% 80% at 80% 20%, rgba(40,60,100,0.2), transparent 40%);
    background-size: 200px 200px;
    background-repeat: repeat;
  }
  a { color: #a8d4ff; text-decoration: none; }
  a:hover { text-decoration: underline; }
 html{scroll-behavior:smooth;} .content h2,.content h3{scroll-margin-top:1rem;} h1,h2,h3{margin-top:1.5rem;color:#fff;} pre{white-space:pre-wrap;color:rgba(255,255,255,0.9);} .toolbar{margin:0.5rem 0;color:rgba(255,255,255,0.8);} .content{color:rgba(255,255,255,0.95);} .content a{color:#a8d4ff;} .meta{color:rgba(255,255,255,0.65);font-size:0.9em;} .toc{margin:1rem 0;padding:0.8rem 1rem;background:rgba(255,255,255,0.08);border-radius:6px;} .toc .toc-title{margin:0 0 0.5rem 0;font-weight:bold;color:rgba(255,255,255,0.9);} .toc ul{list-style:none;padding-left:0;margin:0;} .toc li{margin:0.35rem 0;} .toc li.toc-h3{padding-left:1em;font-size:0.95em;} .toc a{color:#a8d4ff;text-decoration:none;} .toc a:hover{text-decoration:underline;} .content p,.content div,.content h2,.content h3,.content h4,.content li,.content pre{cursor:pointer;-webkit-tap-highlight-color:rgba(255,255,255,0.15);} .content .insight-summary{color:#c9a0dc;margin:0.5em 0 0.8em 0;} .content .insight-detail,.content .insight-detail li{color:#8dd4e8;list-style:disc;margin-left:1.2em;padding-left:0.3em;} .content .article-body-details{margin:0.8em 0;} .content .article-body-details summary{cursor:pointer;color:rgba(255,255,255,0.85);}</style>
</head>
<body>
<p><a href="https://YuxinWSoton.github.io/ai-knowledge-flow-site/">← 返回首页</a></p>
<h1>知识流日报 2026-02-22：gold volatility forecasting machine learning、gold price determinants real interest rate、financial time series structural break regime、LSTM volatility prediction、gold price forecasting survey</h1>
<p class="meta" style="margin-top:0;">最后更新：2026-02-22 10:55</p>
<p class="toolbar"><button id="btnFull" onclick="readFullText()">全文朗读</button> <span id="readStatus"></span></p>
<p class="meta" style="margin-top:0;">（点任意段落可朗读该段；「全文朗读」读本页全部内容）</p>
<nav class="toc" aria-label="目录">
<p class="toc-title">目录</p>
<ul>
  <li><a href="#toc-0">1. 如何最简单、通俗地理解LSTM？ - 知乎</a></li>
  <li class="toc-h3"><a href="#toc-1">正文</a></li>
  <li><a href="#toc-2">2. 如何使用 PyTorch 构建和训练 LSTM 模型？ - 知乎</a></li>
  <li class="toc-h3"><a href="#toc-3">正文</a></li>
  <li><a href="#toc-4">3. 有哪些LSTM (Long Short Term Memory)和RNN (Recurrent ...</a></li>
  <li class="toc-h3"><a href="#toc-5">正文</a></li>
  <li><a href="#toc-6">4. 循环神经网络详解（RNN/LSTM/GRU）</a></li>
  <li class="toc-h3"><a href="#toc-7">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-8">5. The eigenvalues of i.i.d. matrices are hyperuniform</a></li>
  <li class="toc-h3"><a href="#toc-9">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-10">6. Revisiting the Higgs-mass calculation in the scale-invariant THDM</a></li>
  <li class="toc-h3"><a href="#toc-11">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-12">7. The strength of a geometric simplex is a key ingredient in a polynomial-time classification of unordered point clouds by Lipschitz continuous invariants</a></li>
  <li class="toc-h3"><a href="#toc-13">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-14">8. A High-Level Survey of Optical Remote Sensing</a></li>
  <li class="toc-h3"><a href="#toc-15">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-16">9. huff: A Python package for Market Area Analysis</a></li>
  <li class="toc-h3"><a href="#toc-17">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-18">10. Realization of fractional Fermi seas</a></li>
  <li class="toc-h3"><a href="#toc-19">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-20">11. Sink-Aware Pruning for Diffusion Language Models</a></li>
  <li class="toc-h3"><a href="#toc-21">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-22">12. Beyond Pipelines: A Fundamental Study on the Rise of Generative-Retrieval Architectures in Web Research</a></li>
  <li class="toc-h3"><a href="#toc-23">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-24">13. A Programmable Linear Optical Quantum Reservoir with Measurement Feedback for Time Series Analysis</a></li>
  <li class="toc-h3"><a href="#toc-25">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-26">14. IntRec: Intent-based Retrieval with Contrastive Refinement</a></li>
  <li class="toc-h3"><a href="#toc-27">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-28">15. Pseudo-deterministic Quantum Algorithms</a></li>
  <li class="toc-h3"><a href="#toc-29">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-30">16. AutoNumerics: An Autonomous, PDE-Agnostic Multi-Agent Pipeline for Scientific Computing</a></li>
  <li class="toc-h3"><a href="#toc-31">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-32">17. What Language is This? Ask Your Tokenizer</a></li>
  <li class="toc-h3"><a href="#toc-33">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-34">18. MARS: Margin-Aware Reward-Modeling with Self-Refinement</a></li>
  <li class="toc-h3"><a href="#toc-35">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-36">19. Differences in Typological Alignment in Language Models' Treatment of Differential Argument Marking</a></li>
  <li class="toc-h3"><a href="#toc-37">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-38">20. Graph Neural Model Predictive Control for High-Dimensional Systems</a></li>
  <li class="toc-h3"><a href="#toc-39">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-40">21. A Study of Entanglement and Ansatz Expressivity for the Transverse-Field Ising Model using Variational Quantum Eigensolver</a></li>
  <li class="toc-h3"><a href="#toc-41">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-42">22. SMAC: Score-Matched Actor-Critics for Robust Offline-to-Online Transfer</a></li>
  <li class="toc-h3"><a href="#toc-43">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-44">23. Approaching the Limit in Multiparameter AC Magnetometry with Quantum Control</a></li>
  <li class="toc-h3"><a href="#toc-45">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-46">24. When to Trust the Cheap Check: Weak and Strong Verification for Reasoning</a></li>
  <li class="toc-h3"><a href="#toc-47">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-48">25. Reverso: Efficient Time Series Foundation Models for Zero-shot Forecasting</a></li>
  <li class="toc-h3"><a href="#toc-49">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-50">26. Modeling Distinct Human Interaction in Web Agents</a></li>
  <li class="toc-h3"><a href="#toc-51">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-52">27. CLEF HIPE-2026: Evaluating Accurate and Efficient Person-Place Relation Extraction from Multilingual Historical Texts</a></li>
  <li class="toc-h3"><a href="#toc-53">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-54">28. Multi-Round Human-AI Collaboration with User-Specified Requirements</a></li>
  <li class="toc-h3"><a href="#toc-55">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-56">29. Position: Evaluation of ECG Representations Must Be Fixed</a></li>
  <li class="toc-h3"><a href="#toc-57">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-58">30. Towards Anytime-Valid Statistical Watermarking</a></li>
  <li class="toc-h3"><a href="#toc-59">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-60">31. Be Wary of Your Time Series Preprocessing</a></li>
  <li class="toc-h3"><a href="#toc-61">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-62">32. Mine and Refine: Optimizing Graded Relevance in E-commerce Search Retrieval</a></li>
  <li class="toc-h3"><a href="#toc-63">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-64">33. FAMOSE: A ReAct Approach to Automated Feature Discovery</a></li>
  <li class="toc-h3"><a href="#toc-65">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-66">34. Stable Asynchrony: Variance-Controlled Off-Policy RL for LLMs</a></li>
  <li class="toc-h3"><a href="#toc-67">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-68">35. Human-level 3D shape perception emerges from multi-view learning</a></li>
  <li class="toc-h3"><a href="#toc-69">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-70">36. Adapting Actively on the Fly: Relevance-Guided Online Meta-Learning with Latent Concepts for Geospatial Discovery</a></li>
  <li class="toc-h3"><a href="#toc-71">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-72">37. Non-Trivial Zero-Knowledge Implies One-Way Functions</a></li>
  <li class="toc-h3"><a href="#toc-73">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-74">38. What Makes a Good LLM Agent for Real-world Penetration Testing?</a></li>
  <li class="toc-h3"><a href="#toc-75">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-76">39. Cosmic voids evolution in modified gravity via hydrodynamics</a></li>
  <li class="toc-h3"><a href="#toc-77">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-78">40. Exploring Novel Data Storage Approaches for Large-Scale Numerical Weather Prediction</a></li>
  <li class="toc-h3"><a href="#toc-79">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-80">41. What Breaks Embodied AI Security:LLM Vulnerabilities, CPS Flaws,or Something Else?</a></li>
  <li class="toc-h3"><a href="#toc-81">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-82">42. When Vision Overrides Language: Evaluating and Mitigating Counterfactual Failures in VLAs</a></li>
  <li class="toc-h3"><a href="#toc-83">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-84">43. Pushing the Frontier of Black-Box LVLM Attacks via Fine-Grained Detail Targeting</a></li>
  <li class="toc-h3"><a href="#toc-85">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-86">44. Semi-Local Exchange-Correlation Approximations in Density Functional Theory</a></li>
  <li class="toc-h3"><a href="#toc-87">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-88">45. Conditional Flow Matching for Continuous Anomaly Detection in Autonomous Driving on a Manifold-Aware Spectral Space</a></li>
  <li class="toc-h3"><a href="#toc-89">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-90">46. Characterization of compressible fluctuations in solar wind streams dominated by balanced and imbalanced turbulence: Parker Solar Probe, Solar Orbiter and Wind observations</a></li>
  <li class="toc-h3"><a href="#toc-91">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-92">47. Do Hackers Dream of Electric Teachers?: A Large-Scale, In-Situ Evaluation of Cybersecurity Student Behaviors and Performance with AI Tutors</a></li>
  <li class="toc-h3"><a href="#toc-93">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-94">48. [干货]深入浅出LSTM及其Python代码实现</a></li>
  <li class="toc-h3"><a href="#toc-95">正文（抓取，非 AI）</a></li>
</ul>
</nav>
<div class="content">
<h1>知识流日报 2026-02-22：gold volatility forecasting machine learning、gold price determinants real interest rate、financial time series structural break regime、LSTM volatility prediction、gold price forecasting survey</h1>
<p>共 48 条（来自百度学术 / Google / Bing 等，仅含正文抓取成功的条目；按正文长度从短到长排列，便于先听短的）。</p>
<p>（以下含抓取正文，便于阅读。未调用任何 AI API。）</p>
<p>（仅前 48 条含模型提取的「易漏细节」关键点，页面上以颜色区分；第 49 条及之后未调用模型，故无易漏细节。可在 config 中调大 extract_insights_max_items 以增加条数。）</p>
<h2 id="toc-0">1. 如何最简单、通俗地理解LSTM？ - 知乎</h2>
<ul>
<li>链接：https://www.zhihu.com/question/445411028</li>
<li>来源：bing</li>
<li>摘要：LSTM - 小结 以上，就是LSTM的基本原理推导的讲解了。 请试着用自己的语言回答下面的问题： LSTM的3个门控，是哪三个。 它们分别有什么作用 记忆胞体 c 的更新公式怎么写？ 记忆胞体是如何 …</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">LSTM（长短期记忆网络）的运作机制涉及三个关键门控：输入门、遗忘门和输出门。输入门的作用是决定哪些新信息可以进入记忆胞体，而遗忘门则负责决定哪些信息从记忆胞体中删除。输出门则根据当前记忆胞体的状态，决定其输出信息。记忆胞体的更新公式为：c = f * c + i * n，其中f和i分别代表遗忘门和输入门的输出。这一公式表明，记忆胞体的新状态c是由当前状态c与新输入n的结合，通过遗忘门和输入门的调节共同决定的。因此，这三个门控协同工作，确保LSTM能够有效地处理和保留长期信息。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>LSTM的3个门控分别是输入门、遗忘门和输出门。</li>
<li>输入门用于决定哪些新信息可以进入记忆胞体。</li>
<li>遗忘门用于决定哪些信息从记忆胞体中删除。</li>
<li>输出门用于决定记忆胞体的输出信息。</li>
<li>记忆胞体c的更新公式为：c = f * c + i * n。</li>
<li>此处的f、i分别代表遗忘门、输入门的输出。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-1">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-2">2. 如何使用 PyTorch 构建和训练 LSTM 模型？ - 知乎</h2>
<ul>
<li>链接：https://www.zhihu.com/question/630654525?write</li>
<li>来源：bing</li>
<li>摘要：基于LSTM模型的股票预测任务，是 时间序列、量化交易 领域的经典任务之一。 这篇文章我将带大家使用 SwanLab、PyTroch、Matplotlib、Pandas 这四个开源工具，完成从Google股票数据集的准备、 …</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">时间序列数据因其顺序依赖性，必须采用具有记忆能力的模型如LSTM进行处理。在数据准备阶段，确保时间序列的连续性是至关重要的，这有助于保持数据的一致性和完整性。在PyTorch中，LSTM模型要求输入张量的形状为[序列长度, 批量大小, 特征数]，因此在数据准备时需确保符合这一要求。此外，训练过程中可能会遇到梯度爆炸或消失的问题，这需要通过适当的优化策略来解决，以确保模型训练的有效性。为了更直观地展示预测结果与实际数据，可以使用Matplotlib和Pandas进行可视化。此外，SwanLab工具可以提供数据清洗和预处理的支持，进一步提高数据处理的效率和准确性。因此，通过这些步骤，可以确保时间序列数据的处理既有效又准确。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>时间序列数据具有顺序依赖性，需使用LSTM处理。</li>
<li>数据准备阶段需确保时间序列的连续性。</li>
<li>PyTorch中的LSTM模型需要输入张量形状为[序列长度, 批量大小, 特征数]。</li>
<li>训练过程中应关注梯度爆炸或消失问题。</li>
<li>使用Matplotlib和Pandas可以更直观地展示预测结果与实际数据。</li>
<li>SwanLab工具可以辅助数据清洗和预处理。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-3">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-4">3. 有哪些LSTM (Long Short Term Memory)和RNN (Recurrent ...</h2>
<ul>
<li>链接：https://www.zhihu.com/question/29411132</li>
<li>来源：bing</li>
<li>摘要：LSTM网络本质还是RNN网络，基于LSTM的RNN架构上的变化有最先的BRNN（双向），还有今年Socher他们提出的树状LSTM用于情感分析和句子相关度计算 《Improved Semantic …</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">LSTM网络本质上是RNN的一种，它通过引入门控机制来解决传统RNN在长时间依赖问题上的不足。BRNN是LSTM架构上的一个变化，它通过双向传播信息，增强了模型对序列数据的处理能力。此外，树状LSTM专门用于处理具有树结构的数据，通过将树结构分解为路径，再应用LSTM模型，从而能够更有效地捕捉节点之间的关系，进一步提升了模型在复杂结构数据上的表现。因此，这些不同的网络结构各有其适用场景和优势，共同推动了序列数据处理技术的发展。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>LSTM网络本质上是RNN的一种。</li>
<li>BRNN是LSTM架构上的变化之一。</li>
<li>树状LSTM用于处理树结构数据。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-5">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-6">4. 循环神经网络详解（RNN/LSTM/GRU）</h2>
<ul>
<li>链接：https://www.zhihu.com/tardis/bd/art/636756912</li>
<li>来源：bing</li>
<li>摘要：2024年1月11日 · 图2：谷歌策略选择函数的网络结构 铺垫这么多，既循环神经网络仍然有大量的应用，这篇文章就来回顾一下。 1. RNN循环神经网络 RNN（Recurrent Neural Network）是一种循环神 …</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">RNN能够处理变长序列并捕捉时序信息，但由于梯度消失和梯度爆炸的问题，传统RNN难以有效建模长期依赖关系。为解决这一问题，LSTM和GRU引入了门控机制。LSTM通过遗忘门、输入门和输出门分别控制细胞状态中的信息流，确保长期依赖关系的建模能力。相比之下，GRU通过合并门控机制减少了参数量和计算量，仅有reset门和update门，使得模型更加简洁且易于训练。在实际应用中，阿里的DIEN模型使用GRU来建模用户序列，而谷歌的策略选择函数则使用RNN或LSTM。此外，由于GRU计算量较小，更适合处理较小规模的序列数据，而Transformer则适用于处理更大规模的序列数据。因此，选择合适的模型取决于具体的应用场景和数据规模。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>RNN能够处理变长序列，并且能够捕捉到序列中的时序信息。</li>
<li>梯度消失和梯度爆炸限制了传统RNN对长期依赖关系的建模能力。</li>
<li>遗忘门决定需要从cell状态中扔掉什么样的信息。</li>
<li>输入门决定需要在cell状态中存储什么样的信息。</li>
<li>输出门决定要输出cell状态中的哪个部分。</li>
<li>GRU通过合并门控机制减少了参数量和计算量。</li>
<li>GRU没有output gate，只有reset gate和update gate。</li>
<li>GRU相对于LSTM更加简单，更容易训练。</li>
<li>LSTM通过引入门控机制来控制记忆状态的更新。</li>
<li>CTR预估中，阿里的DIEN模型使用GRU建模用户序列。</li>
<li>强化学习中，谷歌的策略选择函数使用RNN/LSTM。</li>
<li>Transformer计算量较大，适用于处理更大规模的序列数据。</li>
<li>GRU计算量相对较小，适用于较小规模的序列数据。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-7">正文（抓取，非 AI）</h3>
<p>背景 最近 GPT 爆火，而 GPT 本质上基于 Transformer 的语言模型。GPT 全称是 Generative Pre-trained Transformer。由此可见 Transformer 的广泛应用。 Transformer 最早发源于自然语言处理领域（NLP），以一个非常惊人的标题 “Attention is All You Need” 催生了无数后续工作。尤其 OpenIAI 的 GPT 模型选择 Transformer 作为基础结构后，基于 Transformer 的大模型就越来越多。包括谷歌的PaLM-2、Meta 的 LLaMA、清华的 GLM、百度的文心一言，其大模型都是使用了 Transformer结构。 但其实并不是所有的场景都适合 Transformer，比如强化学习中用的最多的还是 LSTM/RNN，在推荐系统中，处理用户序列用的比较多的还是 GRU。 RNN/LSTM/GRU 和 Transformer 是两种不同的 neural network architectures，用于处理序列数据。它们在计算量上有所不同。 比如 GRU 是一种递归神经网络（RNN）的变体，它通过使用门控机制来克服传统 RNN 中的梯度消失问题。GRU 的计算量相对较小，因为它的参数量较少，并且它是一种逐步处理输入序列的模型。在每个时间步，GRU 只需计算一些简单的线性变换和非线性激活函数。 相比之下，Transformer 是一种基于注意力机制的神经网络架构，用于处理序列数据。它引入了自注意力机制，允许模型在不同位置对输入序列的各个元素进行加权关注。由于 Transformer 需要计算全连接的注意力矩阵，它的计算量较大。 总结起来，GRU 相对较简单，计算量相对较小，适用于较小规模的序列数据。而 Transformer 计算量较大，适用于处理更大规模的序列数据，如机器翻译或语言建模等任务。 比如在 CTR 预估中，阿里的 DIEN 模型就是使用 GRU 建模用户序列。 图1：阿里的DIEN模型 而在强化学习中，谷歌提出的策略选择函数，使用的也是 RNN/LSTM。 图2：谷歌策略选择函数的网络结构 铺垫这么多，既循环神经网络仍然有大量的应用，这篇文章就来回顾一下。 1. RNN循环神经网络 RNN（Recurrent Neural Network）是一种循环神经网络，用于处理序列数据。与传统的前馈神经网络不同，RNN具有循环连接，使得它可以在处理序列时保持一种记忆状态。 在 RNN 中，每个时间步都有一个隐藏状态（hidden state），它可以接收当前时间步的输入和上一个时间步的隐藏状态作为输入。隐藏状态的输出不仅取决于当前时间步的输入，还取决于之前所有时间步的输入。这种循环连接使得RNN可以处理变长序列，并且能够捕捉到序列中的时序信息。 图3：RNN 循环神经网络 RNN 的计算过程非常简单： RNN 在自然语言处理（NLP）等领域有广泛的应用，例如语言建模、机器翻译、情感分析等任务。由于 RNN 能够处理变长序列，并且可以保持记忆状态，它在处理自然语言时可以考虑上下文的信息，捕捉到词语之间的依赖关系和语义信息。 此外，RNN 也可以应用于时间序列预测，例如股票价格预测、天气预测等。RNN 可以根据过去的时间序列数据预测未来的趋势，对于具有时序依赖的数据具有一定的优势。 然而，传统的RNN在处理长序列时存在 梯度消失和梯度爆炸 的问题，这限制了其对长期依赖关系的建模能力。为了解决这个问题，出现了一些改进的 RNN 变体，如长短期记忆网络（LSTM）和门控循环单元（GRU），它们引入了门控机制来控制记忆状态的更新，改善了对长期依赖的建模能力。 2. LSTM长短记忆网络 LSTM（Long Short-Term Memory）是一种改进的循环神经网络（RNN）架构，旨在解决传统 RNN 中的梯度消失和梯度爆炸问题，以及增强对长期依赖关系的建模能力。 LSTM 引入了一个记忆单元（memory cell），该单元可以存储和访问信息，并通过门控机制来控制信息的流动。LSTM 的关键部分包括 输入门 （input gate）、 遗忘门 （forget gate）、 输出门 （output gate）。 图4：LSTM模型 2.1 遗忘门（forget gate） 图5：LSTM遗忘门 LSTM 的第一步是决定我们需要从 cell 状态中扔掉什么样的信息。这个决策由一个称为“遗忘门（forget gate）”的 sigmoid 层决定。输入 和 ，输出一个 0 到 1 之间的数。1 代表“完全保留这个值”，而 0代表“完全扔掉这个值”。 比如对于一个基于上文预测最后一个词的语言模型。cell 的状态可能包含当前主题的信息，来预测下一个准确的词。而当我们得到一个新的语言主题的时候，我们会想要遗忘旧的主题的记忆，应用新的语言主题的信息来预测准确的词。 遗忘门的计算方法如下： 2.2 输入门（input gate） 第二步是决定我们需要在 cell state 里存储什么样的信息。这个问题有两个部分。首先 sigmoid 层调用“输入门（input gate）”以决定哪些数据是需要更新的。然后，一个 tanh 层为新的候选值创建一个向量 ，这些值能够加入 state 中。下一步，我们要将这两个部分合并以创建对 state 的更新。 比如还是语言模型，可以表示为想要把新的语言主题的信息加入到 cell state 中，以替代我们要遗忘的旧的记忆信息。 在决定需要遗忘和需要加入的记忆之后，就可以更新旧的 cell state 到新的 cell state 了。在这一步，我们把旧的 state 与 相乘，遗忘我们先前决定遗忘的东西，然后我们加上 ，这可以理解为新的记忆信息，当然，这里体现了对状态值的更新度是有限制的，我们可以把 当成一个权重。 图6：遗忘门和输入门合并 2.3 输出门（output gate） 最后，我们需要决定要输出的东西。这个输出基于我们的 cell state ， 但会是一个过滤后的值。首先，我们运行一个 sigmoid 层，这个也就是输出门（output gate），以决定 cell state 中的哪个部分是我们将要输出的。然后我们把 cell state 放进 tanh（将数值压到-1和1之间），最后将它与 sigmoid 门的输出相乘，这样我们就只输出了我们想要的部分了。 图7：输出门 3. GRU（Gated Recurrent Unit） GRU（Gated Recurrent Unit）是一种对 LSTM 稍微改进的循环神经网络，由 Cho 等人（2014年）提出。它将 遗忘门和输入门合并成一个单一的“更新门” ，同时将 cell state 和隐藏状态合并，并进行了其他一些改动。GRU模型相对于标准的 LSTM 模型来说更加简单，并且越来越受到广泛关注和应用。 GRU 与 LSTM 相比，主要的区别在于门控机制的设计。GRU 只使用了一个更新门，通过控制信息的流动和状态的更新，同时减少了参数量和计算量。相比于 LSTM，GRU 模型更加简洁，更容易训练，并且在一些任务上取得了相当好的性能。 图8：GRU GRU 由两个门构成，分别是 reset gate 和 update gate ，需要注意这里与 LSTM 的区别是 GRU 中没有output gate。 参考 A comparison of LSTM and GRU networks for learning symbolic sequences evtor：深入理解lstm及其变种gru Understanding LSTM Networks 一文搞懂RNN（循环神经网络）基础篇 - 知乎 (zhihu.com) LSTM - 长短期记忆递归神经网络 - 知乎 (zhihu.com) 为什么目前的强化学习里深度网络很少用 transformer ,更多的是 lstm rnn 这类网络? - 知乎 (zhihu.com) 书籍推荐</p>
</div></details><h2 id="toc-8">5. The eigenvalues of i.i.d. matrices are hyperuniform</h2>
<ul>
<li>链接：https://arxiv.org/abs/2602.17628v1</li>
<li>来源：arxiv</li>
<li>摘要：We prove that the point process of the eigenvalues of real or complex non-Hermitian matrices $X$ with independent, identically distributed entries is hyperuniform: the variance of the number of eigenvalues in a subdomain $Ω$ of the spectrum is much smaller than the volume of $Ω$. Our main technical novelty is a very precise computation of the covariance between the resolvents of the Hermitization of $X-z_1, X-z_2$, for two distinct complex parameters $z_1,z_2$.</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">hyperuniform 性质意味着点过程的方差远小于区域体积，这一特性在实矩阵和复非埃尔米特矩阵中尤为显著。该研究的技术创新在于精确计算了两个不同复参数的赫米特化矩阵的余因子之间的协方差，从而证明了独立同分布矩阵的特征值点过程是超均匀的。这种超均匀性质在谱的一个子域内对特征值的数量有重要影响，具体表现为特征值的数量受到严格控制。因此，该研究不仅揭示了矩阵特征值分布的新规律，还通过精确计算两个不同复参数的余因子之间的协方差，为理解超均匀性质在复杂系统中的应用提供了新的视角。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>hyperuniform 性质意味着点过程的方差远小于区域体积。</li>
<li>该结果适用于实矩阵和复非埃尔米特矩阵。</li>
<li>计算的是两个不同复参数的赫米特化矩阵的余因子之间的协方差。</li>
<li>该研究的技术创新在于精确计算了两个不同复参数的余因子之间的协方差。</li>
<li>hyperuniform 性质在谱的一个子域内对特征值的数量有影响。</li>
<li>该研究证明了独立同分布矩阵的特征值点过程是超均匀的。</li>
<li>该研究的证明依赖于两个不同的复参数的余因子之间的精确计算。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-9">正文（抓取，非 AI）</h3>
<p>[2602.17628v1] The eigenvalues of i.i.d. matrices are hyperuniform Mathematics &gt; Probability arXiv:2602.17628v1 (math) [Submitted on 19 Feb 2026] Title: The eigenvalues of i.i.d. matrices are hyperuniform Authors: Giorgio Cipolloni , László Erdős , Oleksii Kolupaiev View a PDF of the paper titled The eigenvalues of i.i.d. matrices are hyperuniform, by Giorgio Cipolloni and 2 other authors View PDF HTML (experimental) Abstract: We prove that the point process of the eigenvalues of real or complex non-Hermitian matrices $X$ with independent, identically distributed entries is hyperuniform: the variance of the number of eigenvalues in a subdomain $\Omega$ of the spectrum is much smaller than the volume of $\Omega$. Our main technical novelty is a very precise computation of the covariance between the resolvents of the Hermitization of $X-z_1, X-z_2$, for two distinct complex parameters $z_1,z_2$. Comments: 58 pages + 41 page of supplemental materials, 1 figure Subjects: Probability (math.PR) MSC classes: 60B20, 82C10 Cite as: arXiv:2602.17628 [math.PR] (or arXiv:2602.17628v1 [math.PR] for this version) https://doi.org/10.48550/arXiv.2602.17628 Focus to learn more arXiv-issued DOI via DataCite (pending registration) Submission history From: Oleksii Kolupaiev [ view email ] [v1] Thu, 19 Feb 2026 18:46:33 UTC (519 KB) Full-text links: Access Paper: View a PDF of the paper titled The eigenvalues of i.i.d. matrices are hyperuniform, by Giorgio Cipolloni and 2 other authors View PDF HTML (experimental) TeX Source view license Current browse context: math.PR &lt; prev | next &gt; new | recent | 2026-02 Change to browse by: math References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-10">6. Revisiting the Higgs-mass calculation in the scale-invariant THDM</h2>
<ul>
<li>链接：https://arxiv.org/abs/2602.17643v1</li>
<li>来源：arxiv</li>
<li>摘要：We revisit the one-loop calculation of the Higgs mass spectrum of the scale-invariant THDM, relying on a direct calculation of the relevant Feynman diagrams. We highlight a number of incorrect assumptions in earlier calculations that relied on the effective-potential approach. In contrast with the earlier findings, we show that the one-loop corrections can have an effect of ${\cal O}(10\%)$ on the predictions for the BSM-Higgs masses, and they can also induce non-negligible mixing between the SM-like and BSM states in the neutral-scalar sector.</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">首先，有效势能方法可能包含错误假设，这可能导致对BSM-Higgs质量预测的偏差。其次，一环修正可以显著影响这些预测，甚至引起SM-like和BSM状态在中性标量部分的非忽略混杂。此外，早期研究往往基于这些无效假设，导致结果不够准确。因此，直接计算Feynman图可以提供更准确的结果，避免基于错误假设的偏差。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>effective-potential approach可能包含错误假设；</li>
<li>one-loop corrections可以显著影响BSM-Higgs质量预测；</li>
<li>one-loop corrections可以引起SM-like和BSM状态在中性标量部分的非忽略混杂；</li>
<li>早期研究可能基于无效假设；</li>
<li>直接计算Feynman图可以提供更准确的结果。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-11">正文（抓取，非 AI）</h3>
<p>[2602.17643v1] Revisiting the Higgs-mass calculation in the scale-invariant THDM High Energy Physics - Phenomenology arXiv:2602.17643v1 (hep-ph) [Submitted on 19 Feb 2026] Title: Revisiting the Higgs-mass calculation in the scale-invariant THDM Authors: Pietro Slavich View a PDF of the paper titled Revisiting the Higgs-mass calculation in the scale-invariant THDM, by Pietro Slavich View PDF HTML (experimental) Abstract: We revisit the one-loop calculation of the Higgs mass spectrum of the scale-invariant THDM, relying on a direct calculation of the relevant Feynman diagrams. We highlight a number of incorrect assumptions in earlier calculations that relied on the effective-potential approach. In contrast with the earlier findings, we show that the one-loop corrections can have an effect of ${\cal O}(10\%)$ on the predictions for the BSM-Higgs masses, and they can also induce non-negligible mixing between the SM-like and BSM states in the neutral-scalar sector. Comments: 19 pages, 3 figures Subjects: High Energy Physics - Phenomenology (hep-ph) Cite as: arXiv:2602.17643 [hep-ph] (or arXiv:2602.17643v1 [hep-ph] for this version) https://doi.org/10.48550/arXiv.2602.17643 Focus to learn more arXiv-issued DOI via DataCite (pending registration) Submission history From: Pietro Slavich [ view email ] [v1] Thu, 19 Feb 2026 18:54:17 UTC (61 KB) Full-text links: Access Paper: View a PDF of the paper titled Revisiting the Higgs-mass calculation in the scale-invariant THDM, by Pietro Slavich View PDF HTML (experimental) TeX Source view license Current browse context: hep-ph &lt; prev | next &gt; new | recent | 2026-02 References &amp; Citations INSPIRE HEP NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) IArxiv recommender toggle IArxiv Recommender ( What is IArxiv? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-12">7. The strength of a geometric simplex is a key ingredient in a polynomial-time classification of unordered point clouds by Lipschitz continuous invariants</h2>
<ul>
<li>链接：https://arxiv.org/abs/2602.17630v1</li>
<li>来源：arxiv</li>
<li>摘要：The basic input for many real shapes is a finite cloud of unordered points. The strongest equivalence between shapes in practice is Euclidean motion. The recent polynomial-time classification of point clouds required a Lipschitz continuous function that vanishes on degenerate simplices, while the usual volume is not Lipschitz. We define the strength of any geometric simplex and prove its continuity under perturbations with explicit bounds for Lipschitz constants.</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">Lipschitz continuous invariants are crucial for polynomial-time classification, as they ensure computational efficiency and theoretical guarantees. To achieve this, the strength of geometric simplices is defined to aid in the classification process. However, usual volume is not Lipschitz continuous, making it inadequate for the task. Therefore, degenerate simplices must vanish under the Lipschitz continuous function to maintain the required properties. Explicit bounds for Lipschitz constants provide further theoretical guarantees, ensuring that the classification is robust under perturbations. Moreover, Euclidean motion represents the strongest equivalence in practical scenarios, making it a key factor in the classification. The input, which consists of a finite cloud of unordered points for many real shapes, necessitates these specific mathematical properties to ensure that the classification can be performed in polynomial time.</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>Lipschitz continuous invariants are crucial for polynomial-time classification.</li>
<li>Strength of geometric simplices is defined to aid in this classification.</li>
<li>Usual volume is not Lipschitz continuous, thus inadequate for the task.</li>
<li>Degenerate simplices must vanish under the Lipschitz continuous function.</li>
<li>Explicit bounds for Lipschitz constants provide theoretical guarantees.</li>
<li>Continuity under perturbations ensures robustness of the classification.</li>
<li>Euclidean motion represents the strongest equivalence in practical scenarios.</li>
<li>The input consists of a finite cloud of unordered points for many real shapes.</li>
<li>Polynomial-time classification requires specific mathematical properties.</li>
<li>Lipschitz continuity is essential for ensuring computational efficiency.</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-13">正文（抓取，非 AI）</h3>
<p>[2602.17630v1] The strength of a geometric simplex is a key ingredient in a polynomial-time classification of unordered point clouds by Lipschitz continuous invariants Mathematics &gt; Metric Geometry arXiv:2602.17630v1 (math) [Submitted on 19 Feb 2026] Title: The strength of a geometric simplex is a key ingredient in a polynomial-time classification of unordered point clouds by Lipschitz continuous invariants Authors: Olga Anosova , Vitaliy Kurlin View a PDF of the paper titled The strength of a geometric simplex is a key ingredient in a polynomial-time classification of unordered point clouds by Lipschitz continuous invariants, by Olga Anosova and 1 other authors View PDF HTML (experimental) Abstract: The basic input for many real shapes is a finite cloud of unordered points. The strongest equivalence between shapes in practice is Euclidean motion. The recent polynomial-time classification of point clouds required a Lipschitz continuous function that vanishes on degenerate simplices, while the usual volume is not Lipschitz. We define the strength of any geometric simplex and prove its continuity under perturbations with explicit bounds for Lipschitz constants. Comments: 6 pages, 2 figures. The latest version is maintained at this https URL Subjects: Metric Geometry (math.MG) MSC classes: 51F20, 51N20, 51M25, 15A15 Cite as: arXiv:2602.17630 [math.MG] (or arXiv:2602.17630v1 [math.MG] for this version) https://doi.org/10.48550/arXiv.2602.17630 Focus to learn more arXiv-issued DOI via DataCite (pending registration) Submission history From: Vitaliy Kurlin [ view email ] [v1] Thu, 19 Feb 2026 18:47:26 UTC (939 KB) Full-text links: Access Paper: View a PDF of the paper titled The strength of a geometric simplex is a key ingredient in a polynomial-time classification of unordered point clouds by Lipschitz continuous invariants, by Olga Anosova and 1 other authors View PDF HTML (experimental) TeX Source view license Current browse context: math.MG &lt; prev | next &gt; new | recent | 2026-02 Change to browse by: math References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-14">8. A High-Level Survey of Optical Remote Sensing</h2>
<ul>
<li>链接：https://arxiv.org/abs/2602.17397v1</li>
<li>来源：arxiv</li>
<li>摘要：In recent years, significant advances in computer vision have also propelled progress in remote sensing. Concurrently, the use of drones has expanded, with many organizations incorporating them into their operations. Most drones are equipped by default with RGB cameras, which are both robust and among the easiest sensors to use and interpret. The body of literature on optical remote sensing is vast, encompassing diverse tasks, capabilities, and methodologies. Each task or methodology could warrant a dedicated survey. This work provides a comprehensive overview of the capabilities of the field, while also presenting key information, such as datasets and insights. It aims to serve as a guide for researchers entering the field, offering high-level insights and helping them focus on areas most relevant to their interests. To the best of our knowledge, no existing survey addresses this holistic perspective.</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">RGB摄像头是无人机中最常用的传感器，易于使用和解释，因此在无人机遥感技术中扮演着重要角色。随着无人机的广泛应用，遥感技术得到了显著发展，现有文献虽然涵盖了多种任务、能力和方法，但未能提供一个涵盖所有方面的综合视角。因此，该综述旨在填补这一空白，为新进入该领域的研究人员提供高层次的指导。通过全面概述该领域的关键信息，该综述有助于研究人员聚焦于最相关的领域，而现有的综述未能提供这一整体视角。因此，该综述不仅提供了全面的视角，还强调了每个任务或方法的独特价值，使得研究人员能够更好地理解并应用这些技术。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>RGB摄像头是无人机中最常用的传感器。</li>
<li>无人机的广泛应用促进了遥感技术的发展。</li>
<li>现有文献未能提供涵盖所有方面的综合视角。</li>
<li>该综述旨在为新进入该领域的研究人员提供高层次的指导。</li>
<li>RGB摄像头易于使用和解释。</li>
<li>遥感领域的文献涵盖了多种任务、能力和方法。</li>
<li>每个任务或方法都值得单独的综述。</li>
<li>该综述提供了该领域的全面概述，并介绍了关键信息。</li>
<li>该综述有助于研究人员聚焦于最相关的领域。</li>
<li>现有的综述未能涵盖这一整体视角。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-15">正文（抓取，非 AI）</h3>
<p>[2602.17397v1] A High-Level Survey of Optical Remote Sensing Computer Science &gt; Computer Vision and Pattern Recognition arXiv:2602.17397v1 (cs) [Submitted on 19 Feb 2026] Title: A High-Level Survey of Optical Remote Sensing Authors: Panagiotis Koletsis , Vasilis Efthymiou , Maria Vakalopoulou , Nikos Komodakis , Anastasios Doulamis , Georgios Th. Papadopoulos View a PDF of the paper titled A High-Level Survey of Optical Remote Sensing, by Panagiotis Koletsis and 5 other authors View PDF HTML (experimental) Abstract: In recent years, significant advances in computer vision have also propelled progress in remote sensing. Concurrently, the use of drones has expanded, with many organizations incorporating them into their operations. Most drones are equipped by default with RGB cameras, which are both robust and among the easiest sensors to use and interpret. The body of literature on optical remote sensing is vast, encompassing diverse tasks, capabilities, and methodologies. Each task or methodology could warrant a dedicated survey. This work provides a comprehensive overview of the capabilities of the field, while also presenting key information, such as datasets and insights. It aims to serve as a guide for researchers entering the field, offering high-level insights and helping them focus on areas most relevant to their interests. To the best of our knowledge, no existing survey addresses this holistic perspective. Subjects: Computer Vision and Pattern Recognition (cs.CV) ; Artificial Intelligence (cs.AI) Cite as: arXiv:2602.17397 [cs.CV] (or arXiv:2602.17397v1 [cs.CV] for this version) https://doi.org/10.48550/arXiv.2602.17397 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Panagiotis Koletsis [ view email ] [v1] Thu, 19 Feb 2026 14:26:26 UTC (371 KB) Full-text links: Access Paper: View a PDF of the paper titled A High-Level Survey of Optical Remote Sensing, by Panagiotis Koletsis and 5 other authors View PDF HTML (experimental) TeX Source view license Current browse context: cs.CV &lt; prev | next &gt; new | recent | 2026-02 Change to browse by: cs cs.AI References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-16">9. huff: A Python package for Market Area Analysis</h2>
<ul>
<li>链接：https://arxiv.org/abs/2602.17640v1</li>
<li>来源：arxiv</li>
<li>摘要：Market area models, such as the Huff model and its extensions, are widely used to estimate regional market shares and customer flows of retail and service locations. Another, now very common, area of application is the analysis of catchment areas, supply structures and the accessibility of healthcare locations. The huff Python package provides a complete workflow for market area analysis, including data import, construction of origin-destination interaction matrices, basic model analysis, parameter estimation from empirical data, calculation of distance or travel time indicators, and map visualization. Additionally, the package provides several methods of spatial accessibility analysis. The package is modular and object-oriented. It is intended for researchers in economic geography, regional economics, spatial planning, marketing, geoinformation science, and health geography. The software is openly available via the <a href="https://pypi.org/project/huff/">Python Package Index (PyPI)</a>; its development and version history are managed in a public <a href="https://github.com/geowieland/huff_official">GitHub Repository</a> and archived at <a href="https://doi.org/10.5281/zenodo.18639559">Zenodo</a>.</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">huff 包支持多种类型的空间分析，适用于经济地理学、区域经济学、空间规划、市场营销、地理信息科学和健康地理学的研究人员。它提供了一个完整的市场区域分析工作流，包括数据导入、起源-目的地交互矩阵构建、基本模型分析、参数估计、距离或旅行时间指标计算和地图可视化功能。huff 包是模块化和面向对象的，其数据和开发历史在 GitHub 仓库中公开管理，版本历史被归档在 Zenodo。通过 Python 包索引（PyPI）获取，huff 包提供了几种空间可达性分析方法，支持研究人员进行深入的空间分析。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>huff 包支持多种类型的空间分析。</li>
<li>huff 包适用于经济地理学、区域经济学、空间规划、市场营销、地理信息科学和健康地理学的研究人员。</li>
<li>huff 包提供了一个完整的市场区域分析工作流。</li>
<li>huff 包包括数据导入、起源-目的地交互矩阵构建、基本模型分析、参数估计、距离或旅行时间指标计算和地图可视化功能。</li>
<li>huff 包是模块化和面向对象的。</li>
<li>huff 包的数据和开发历史在 GitHub 仓库中公开管理。</li>
<li>huff 包的版本历史被归档在 Zenodo。</li>
<li>huff 包的软件可通过 Python 包索引（PyPI）获取。</li>
<li>huff 包提供了几种空间可达性分析方法。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-17">正文（抓取，非 AI）</h3>
<p>[2602.17640v1] huff: A Python package for Market Area Analysis Statistics &gt; Applications arXiv:2602.17640v1 (stat) [Submitted on 19 Feb 2026] Title: huff: A Python package for Market Area Analysis Authors: Thomas Wieland View a PDF of the paper titled huff: A Python package for Market Area Analysis, by Thomas Wieland View PDF HTML (experimental) Abstract: Market area models, such as the Huff model and its extensions, are widely used to estimate regional market shares and customer flows of retail and service locations. Another, now very common, area of application is the analysis of catchment areas, supply structures and the accessibility of healthcare locations. The huff Python package provides a complete workflow for market area analysis, including data import, construction of origin-destination interaction matrices, basic model analysis, parameter estimation from empirical data, calculation of distance or travel time indicators, and map visualization. Additionally, the package provides several methods of spatial accessibility analysis. The package is modular and object-oriented. It is intended for researchers in economic geography, regional economics, spatial planning, marketing, geoinformation science, and health geography. The software is openly available via the <a href="this https URL its development and version history are managed in a public [GitHub Repository]( this https URL ) and archived at [Zenodo]( this https URL ). Subjects: Applications (stat.AP) ; Software Engineering (cs.SE) Cite as: arXiv:2602.17640 [stat.AP] (or arXiv:2602.17640v1 [stat.AP] for this version) https://doi.org/10.48550/arXiv.2602.17640 Focus to learn more arXiv-issued DOI via DataCite (pending registration) Submission history From: Thomas Wieland [ view email ] [v1] Thu, 19 Feb 2026 18:52:46 UTC (17 KB) Full-text links: Access Paper: View a PDF of the paper titled huff: A Python package for Market Area Analysis, by Thomas Wieland View PDF HTML (experimental) TeX Source view license Current browse context: stat.AP &lt; prev | next &gt; new | recent | 2026-02 Change to browse by: cs cs.SE stat References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax?">Python Package Index (PyPI)</a>)</p>
</div></details><h2 id="toc-18">10. Realization of fractional Fermi seas</h2>
<ul>
<li>链接：https://arxiv.org/abs/2602.17657v1</li>
<li>来源：arxiv</li>
<li>摘要：The Pauli exclusion principle is a cornerstone of quantum physics: it governs the structure of matter. Extensions of this principle, such as Haldane's generalized exclusion statistics, predict the existence of exotic quantum states characterized by fractional Fermi seas (FFS), i.e. momentum distributions with uniform but fractional occupancies. Here, we report the experimental realization of fractional Fermi seas in an excited one-dimensional Bose gas prepared through ramping cycles in the interaction strength. The resulting excited yet stable Bose-gas states exhibit Friedel oscillations, smoking-gun signatures of the underlying FFS. The stabilization of these states offers an opportunity to deepen our understanding of quantum thermodynamics in the presence of exotic statistics and paves the way for applications in quantum information and sensing.</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">Pauli exclusion principle is fundamental to quantum physics, establishing that no two fermions can occupy the same quantum state simultaneously. This principle underpins the behavior of fermions and is crucial for understanding the structure of matter at the quantum level. Haldane's generalized exclusion statistics extend this concept to include particles that do not strictly follow the Pauli exclusion principle, predicting exotic quantum states that can have non-integer occupancies. Fractional Fermi seas (FFS) exhibit uniform but fractional occupancies, which are a key feature of these exotic states. By ramping up the interaction strength, one can prepare excited stable Bose-gas states, which are essential for studying the thermodynamics of these systems. Friedel oscillations, which are periodic variations in the charge density, serve as evidence of the underlying FFS. The stabilization of these states deepens our understanding of quantum thermodynamics, revealing new insights into the behavior of quantum systems. Furthermore, these exotic statistics offer significant opportunities for quantum information and sensing applications, highlighting their practical importance in the development of advanced quantum technologies.</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>Pauli exclusion principle is fundamental to quantum physics.</li>
<li>Haldane's generalized exclusion statistics predict exotic quantum states.</li>
<li>Fractional Fermi seas (FFS) have uniform but fractional occupancies.</li>
<li>Ramping interaction strength prepares excited stable Bose-gas states.</li>
<li>Friedel oscillations are evidence of underlying FFS.</li>
<li>Stabilization of these states deepens understanding of quantum thermodynamics.</li>
<li>Exotic statistics offer opportunities for quantum information and sensing applications.</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-19">正文（抓取，非 AI）</h3>
<p>[2602.17657v1] Realization of fractional Fermi seas Condensed Matter &gt; Quantum Gases arXiv:2602.17657v1 (cond-mat) [Submitted on 19 Feb 2026] Title: Realization of fractional Fermi seas Authors: Yi Zeng , Alvise Bastianello , Sudipta Dhar , Zekui Wang , Xudong Yu , Milena Horvath , Grigori E. Astrakharchik , Yanliang Guo , Hanns-Christoph Nägerl , Manuele Landini View a PDF of the paper titled Realization of fractional Fermi seas, by Yi Zeng and 9 other authors View PDF HTML (experimental) Abstract: The Pauli exclusion principle is a cornerstone of quantum physics: it governs the structure of matter. Extensions of this principle, such as Haldane's generalized exclusion statistics, predict the existence of exotic quantum states characterized by fractional Fermi seas (FFS), i.e. momentum distributions with uniform but fractional occupancies. Here, we report the experimental realization of fractional Fermi seas in an excited one-dimensional Bose gas prepared through ramping cycles in the interaction strength. The resulting excited yet stable Bose-gas states exhibit Friedel oscillations, smoking-gun signatures of the underlying FFS. The stabilization of these states offers an opportunity to deepen our understanding of quantum thermodynamics in the presence of exotic statistics and paves the way for applications in quantum information and sensing. Comments: 16 pages, 12 figures Subjects: Quantum Gases (cond-mat.quant-gas) ; Statistical Mechanics (cond-mat.stat-mech); Atomic Physics (physics.atom-ph) Cite as: arXiv:2602.17657 [cond-mat.quant-gas] (or arXiv:2602.17657v1 [cond-mat.quant-gas] for this version) https://doi.org/10.48550/arXiv.2602.17657 Focus to learn more arXiv-issued DOI via DataCite (pending registration) Submission history From: Yi Zeng [ view email ] [v1] Thu, 19 Feb 2026 18:58:59 UTC (5,265 KB) Full-text links: Access Paper: View a PDF of the paper titled Realization of fractional Fermi seas, by Yi Zeng and 9 other authors View PDF HTML (experimental) TeX Source view license Current browse context: cond-mat.quant-gas &lt; prev | next &gt; new | recent | 2026-02 Change to browse by: cond-mat cond-mat.stat-mech physics physics.atom-ph References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) IArxiv recommender toggle IArxiv Recommender ( What is IArxiv? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-20">11. Sink-Aware Pruning for Diffusion Language Models</h2>
<ul>
<li>链接：https://arxiv.org/abs/2602.17664v1</li>
<li>来源：arxiv</li>
<li>摘要：Diffusion Language Models (DLMs) incur high inference cost due to iterative denoising, motivating efficient pruning. Existing pruning heuristics largely inherited from autoregressive (AR) LLMs, typically preserve attention sink tokens because AR sinks serve as stable global anchors. We show that this assumption does not hold for DLMs: the attention-sink position exhibits substantially higher variance over the full generation trajectory (measured by how the dominant sink locations shift across timesteps), indicating that sinks are often transient and less structurally essential than in AR models. Based on this observation, we propose ${\bf \texttt{Sink-Aware Pruning}}$, which automatically identifies and prunes unstable sinks in DLMs (prior studies usually keep sinks for AR LLMs). Without retraining, our method achieves a better quality-efficiency trade-off and outperforms strong prior pruning baselines under matched compute. Our code is available at https://github.com/VILA-Lab/Sink-Aware-Pruning.</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">首先，易漏但重要的「attention-sink」位置在生成过程中表现出更高的波动性，这使得它在大语言模型（DLMs）中不是稳定的全局锚点。其次，以往的研究通常保留自回归（AR）LLMs的sink，但这种方法在处理不稳定sink时显得不够灵活。因此，Sink-Aware Pruning能够自动识别并剪枝这些不稳定的sink，从而在无需重新训练的情况下表现出色，并且优于先前的剪枝基线。此外，相关代码可以从指定链接获取，为研究和应用提供了便利。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>attention-sink位置在生成过程中表现出更高的波动性。</li>
<li>DLMs中的sink不是稳定的全局锚点。</li>
<li>prior studies通常保留AR LLMs的sink。</li>
<li>Sink-Aware Pruning能自动识别并剪枝不稳定的sink。</li>
<li>这种方法在无需重新训练的情况下表现出色。</li>
<li>Sink-Aware Pruning优于先前的剪枝基线。</li>
<li>代码可从指定链接获取。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-21">正文（抓取，非 AI）</h3>
<p>[2602.17664v1] Sink-Aware Pruning for Diffusion Language Models Computer Science &gt; Computation and Language arXiv:2602.17664v1 (cs) [Submitted on 19 Feb 2026] Title: Sink-Aware Pruning for Diffusion Language Models Authors: Aidar Myrzakhan , Tianyi Li , Bowei Guo , Shengkun Tang , Zhiqiang Shen View a PDF of the paper titled Sink-Aware Pruning for Diffusion Language Models, by Aidar Myrzakhan and Tianyi Li and Bowei Guo and Shengkun Tang and Zhiqiang Shen View PDF HTML (experimental) Abstract: Diffusion Language Models (DLMs) incur high inference cost due to iterative denoising, motivating efficient pruning. Existing pruning heuristics largely inherited from autoregressive (AR) LLMs, typically preserve attention sink tokens because AR sinks serve as stable global anchors. We show that this assumption does not hold for DLMs: the attention-sink position exhibits substantially higher variance over the full generation trajectory (measured by how the dominant sink locations shift across timesteps), indicating that sinks are often transient and less structurally essential than in AR models. Based on this observation, we propose ${\bf \texttt{Sink-Aware Pruning}}$, which automatically identifies and prunes unstable sinks in DLMs (prior studies usually keep sinks for AR LLMs). Without retraining, our method achieves a better quality-efficiency trade-off and outperforms strong prior pruning baselines under matched compute. Our code is available at this https URL . Comments: Code at: this https URL Subjects: Computation and Language (cs.CL) ; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) Cite as: arXiv:2602.17664 [cs.CL] (or arXiv:2602.17664v1 [cs.CL] for this version) https://doi.org/10.48550/arXiv.2602.17664 Focus to learn more arXiv-issued DOI via DataCite (pending registration) Submission history From: Zhiqiang Shen [ view email ] [v1] Thu, 19 Feb 2026 18:59:50 UTC (311 KB) Full-text links: Access Paper: View a PDF of the paper titled Sink-Aware Pruning for Diffusion Language Models, by Aidar Myrzakhan and Tianyi Li and Bowei Guo and Shengkun Tang and Zhiqiang Shen View PDF HTML (experimental) TeX Source view license Current browse context: cs.CL &lt; prev | next &gt; new | recent | 2026-02 Change to browse by: cs cs.AI cs.LG References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-22">12. Beyond Pipelines: A Fundamental Study on the Rise of Generative-Retrieval Architectures in Web Research</h2>
<ul>
<li>链接：https://arxiv.org/abs/2602.17450v1</li>
<li>来源：arxiv</li>
<li>摘要：Web research and practices have evolved significantly over time, offering users diverse and accessible solutions across a wide range of tasks. While advanced concepts such as Web 4.0 have emerged from mature technologies, the introduction of large language models (LLMs) has profoundly influenced both the field and its applications. This wave of LLMs has permeated science and technology so deeply that no area remains untouched. Consequently, LLMs are reshaping web research and development, transforming traditional pipelines into generative solutions for tasks like information retrieval, question answering, recommendation systems, and web analytics. They have also enabled new applications such as web-based summarization and educational tools. This survey explores recent advances in the impact of LLMs-particularly through the use of retrieval-augmented generation (RAG)-on web research and industry. It discusses key developments, open challenges, and future directions for enhancing web solutions with LLMs.</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">首先，LLMs（大型语言模型）的引入对所有领域产生了深远影响，没有一个领域能幸免。其次，LLMs 正在将传统的工作流程转变为生成型解决方案，这不仅改变了信息检索、问答、推荐系统和网络分析等任务的处理方式，使其从管道式流程转向生成型解决方案，还使基于网络的总结和教育工具等新应用成为可能。此外，RAG（检索增强生成）技术通过结合检索和生成，成为 LLMs 应用的关键，进一步提高了 LLMs 的应用效果。因此，传统的工作流程正在被 LLMs 生成型解决方案所取代，推动了网络研究和开发的变革。未来的研究方向将集中在增强网络解决方案上，以进一步提升 LLMs 的应用效果和范围。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>LLMs 的引入对所有领域产生了深远影响，没有一个领域能幸免。</li>
<li>LLMs 正在将传统的工作流程转变为生成型解决方案。</li>
<li>RAG 技术通过检索增强生成，成为 LLMs 应用的关键。</li>
<li>信息检索、问答、推荐系统和网络分析等任务正从管道式流程转向生成型解决方案。</li>
<li>LLMs 使基于网络的总结和教育工具等新应用成为可能。</li>
<li>传统的工作流程正在被 LLMs 生成型解决方案所取代。</li>
<li>LLMs 的影响正在推动网络研究和开发的变革。</li>
<li>RAG 技术通过结合检索和生成，提高了 LLMs 的应用效果。</li>
<li>未来的研究方向将集中在增强网络解决方案上。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-23">正文（抓取，非 AI）</h3>
<p>[2602.17450v1] Beyond Pipelines: A Fundamental Study on the Rise of Generative-Retrieval Architectures in Web Research Computer Science &gt; Information Retrieval arXiv:2602.17450v1 (cs) [Submitted on 19 Feb 2026] Title: Beyond Pipelines: A Fundamental Study on the Rise of Generative-Retrieval Architectures in Web Research Authors: Amirereza Abbasi , Mohsen Hooshmand View a PDF of the paper titled Beyond Pipelines: A Fundamental Study on the Rise of Generative-Retrieval Architectures in Web Research, by Amirereza Abbasi and 1 other authors View PDF Abstract: Web research and practices have evolved significantly over time, offering users diverse and accessible solutions across a wide range of tasks. While advanced concepts such as Web 4.0 have emerged from mature technologies, the introduction of large language models (LLMs) has profoundly influenced both the field and its applications. This wave of LLMs has permeated science and technology so deeply that no area remains untouched. Consequently, LLMs are reshaping web research and development, transforming traditional pipelines into generative solutions for tasks like information retrieval, question answering, recommendation systems, and web analytics. They have also enabled new applications such as web-based summarization and educational tools. This survey explores recent advances in the impact of LLMs-particularly through the use of retrieval-augmented generation (RAG)-on web research and industry. It discusses key developments, open challenges, and future directions for enhancing web solutions with LLMs. Subjects: Information Retrieval (cs.IR) ; Artificial Intelligence (cs.AI) Cite as: arXiv:2602.17450 [cs.IR] (or arXiv:2602.17450v1 [cs.IR] for this version) https://doi.org/10.48550/arXiv.2602.17450 Focus to learn more arXiv-issued DOI via DataCite (pending registration) Submission history From: Amirreza Abbasi [ view email ] [v1] Thu, 19 Feb 2026 15:14:54 UTC (417 KB) Full-text links: Access Paper: View a PDF of the paper titled Beyond Pipelines: A Fundamental Study on the Rise of Generative-Retrieval Architectures in Web Research, by Amirereza Abbasi and 1 other authors View PDF view license Current browse context: cs.IR &lt; prev | next &gt; new | recent | 2026-02 Change to browse by: cs cs.AI References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-24">13. A Programmable Linear Optical Quantum Reservoir with Measurement Feedback for Time Series Analysis</h2>
<ul>
<li>链接：https://arxiv.org/abs/2602.17440v1</li>
<li>来源：arxiv</li>
<li>摘要：Feedback-driven quantum reservoir computing has so far been studied primarily in gate-based architectures, motivating alternative scalable, hardware-friendly physical platforms. Here we investigate a linear-optical quantum reservoir architecture for time-series processing based on multiphoton interference in a reconfigurable interferometer network equipped with threshold detectors and measurement-conditioned feedback. The reservoir state is constructed from coarse-grained coincidence features, and the feedback updates only a structured, budgeted subset of programmable phases, enabling recurrence without training internal weights. By sweeping the feedback strength, we identify three dynamical regimes and find that memory performance peaks near the stability boundary. We quantify temporal processing via linear memory capacity and validate nonlinear forecasting on benchmarks, namely Mackey-Glass series, NARMA$-n$ and non-integrable Ising dynamics. The proposed architecture is compatible with current photonic technology and lowers the experimental barrier to feedback-driven QRC for time-series analysis with competitive accuracy.</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">gate-based 架构是目前反馈驱动量子水库计算的主要研究对象，其中线性光学量子水库架构利用可重构干涉仪网络中的多光子干涉。阈值检测器和测量条件下的反馈更新了可编程相位的子集，而反馈强度的调整揭示了三种动态模式。这些动态模式影响着记忆性能，使其在稳定性边界附近达到峰值。为了量化时间处理能力，研究者使用线性记忆容量进行评估，而通过Mackey-Glass系列、NARMA$-n$ 和非线性 Ising 动力学进行基准测试，验证了非线性预测的准确性。由于该架构与当前光子技术兼容，因此降低了实验门槛，使其在时间序列分析中具有与传统方法竞争的准确性。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>gate-based 架构是目前反馈驱动量子水库计算的主要研究对象。</li>
<li>线性光学量子水库架构利用可重构干涉仪网络中的多光子干涉。</li>
<li>阈值检测器和测量条件下的反馈更新了可编程相位的子集。</li>
<li>反馈强度的调整揭示了三种动态模式。</li>
<li>记忆性能在稳定性边界附近达到峰值。</li>
<li>线性记忆容量用于量化时间处理。</li>
<li>非线性预测在基准测试中得到验证。</li>
<li>该架构与当前光子技术兼容。</li>
<li>反馈驱动的量子水库计算降低了实验门槛。</li>
<li>Mackey-Glass 系列、NARMA$-n$ 和非可积 Ising 动力学是基准测试。</li>
<li>该架构具有与时间序列分析竞争的准确性。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-25">正文（抓取，非 AI）</h3>
<p>[2602.17440v1] A Programmable Linear Optical Quantum Reservoir with Measurement Feedback for Time Series Analysis Quantum Physics arXiv:2602.17440v1 (quant-ph) [Submitted on 19 Feb 2026] Title: A Programmable Linear Optical Quantum Reservoir with Measurement Feedback for Time Series Analysis Authors: Çağın Ekici View a PDF of the paper titled A Programmable Linear Optical Quantum Reservoir with Measurement Feedback for Time Series Analysis, by \c{C}a\u{g}{\i}n Ekici View PDF HTML (experimental) Abstract: Feedback-driven quantum reservoir computing has so far been studied primarily in gate-based architectures, motivating alternative scalable, hardware-friendly physical platforms. Here we investigate a linear-optical quantum reservoir architecture for time-series processing based on multiphoton interference in a reconfigurable interferometer network equipped with threshold detectors and measurement-conditioned feedback. The reservoir state is constructed from coarse-grained coincidence features, and the feedback updates only a structured, budgeted subset of programmable phases, enabling recurrence without training internal weights. By sweeping the feedback strength, we identify three dynamical regimes and find that memory performance peaks near the stability boundary. We quantify temporal processing via linear memory capacity and validate nonlinear forecasting on benchmarks, namely Mackey-Glass series, NARMA$-n$ and non-integrable Ising dynamics. The proposed architecture is compatible with current photonic technology and lowers the experimental barrier to feedback-driven QRC for time-series analysis with competitive accuracy. Subjects: Quantum Physics (quant-ph) Cite as: arXiv:2602.17440 [quant-ph] (or arXiv:2602.17440v1 [quant-ph] for this version) https://doi.org/10.48550/arXiv.2602.17440 Focus to learn more arXiv-issued DOI via DataCite (pending registration) Submission history From: Çağın Ekici [ view email ] [v1] Thu, 19 Feb 2026 15:08:32 UTC (2,328 KB) Full-text links: Access Paper: View a PDF of the paper titled A Programmable Linear Optical Quantum Reservoir with Measurement Feedback for Time Series Analysis, by \c{C}a\u{g}{\i}n Ekici View PDF HTML (experimental) TeX Source view license Current browse context: quant-ph &lt; prev | next &gt; new | recent | 2026-02 References &amp; Citations INSPIRE HEP NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-26">14. IntRec: Intent-based Retrieval with Contrastive Refinement</h2>
<ul>
<li>链接：https://arxiv.org/abs/2602.17639v1</li>
<li>来源：arxiv</li>
<li>摘要：Retrieving user-specified objects from complex scenes remains a challenging task, especially when queries are ambiguous or involve multiple similar objects. Existing open-vocabulary detectors operate in a one-shot manner, lacking the ability to refine predictions based on user feedback. To address this, we propose IntRec, an interactive object retrieval framework that refines predictions based on user feedback. At its core is an Intent State (IS) that maintains dual memory sets for positive anchors (confirmed cues) and negative constraints (rejected hypotheses). A contrastive alignment function ranks candidate objects by maximizing similarity to positive cues while penalizing rejected ones, enabling fine-grained disambiguation in cluttered scenes. Our interactive framework provides substantial improvements in retrieval accuracy without additional supervision. On LVIS, IntRec achieves 35.4 AP, outperforming OVMR, CoDet, and CAKE by +2.3, +3.7, and +0.5, respectively. On the challenging LVIS-Ambiguous benchmark, it improves performance by +7.9 AP over its one-shot baseline after a single corrective feedback, with less than 30 ms of added latency per interaction.</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">首先，易漏但重要的是一次性检测器（one-shot detectors）缺乏根据用户反馈调整预测的能力。为了解决这一问题，引入了意图状态（Intent State, IS）框架，该框架通过维护正向锚点和负向约束的双重记忆集，来提高检测准确性。其次，对比对齐函数（contrastive alignment function）通过最大化与正向线索的相似性并惩罚被拒绝的候选对象，来对候选对象进行排名。此外，交互式框架（interactive framework）提供了显著的检索准确性提升，而无需额外的监督，这主要归功于对比对齐函数和意图状态框架的结合使用。因此，IntRec在LVIS数据集上优于OVMR、CoDet和CAKE，分别提高了2.3、3.7和0.5个百分点。单次纠正反馈在LVIS-Ambiguous基准上提高了7.9 AP。值得注意的是，每次交互仅增加了不到30毫秒的延迟。最后，一次性的基线指的是现有的开放词汇检测器在一次检测中运行，而对比对齐功能（contrastive refinement）则允许在杂乱场景中进行精细的去模糊处理，进一步提升了预测的准确性。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>one-shot detectors lack the ability to refine predictions based on user feedback.</li>
<li>Intent State (IS) maintains dual memory sets for positive anchors and negative constraints.</li>
<li>Contrastive alignment function ranks candidate objects by maximizing similarity to positive cues and penalizing rejected ones.</li>
<li>Interactive framework provides substantial improvements in retrieval accuracy without additional supervision.</li>
<li>IntRec outperforms OVMR, CoDet, and CAKE on LVIS by +2.3, +3.7, and +0.5, respectively.</li>
<li>Single corrective feedback improves performance by +7.9 AP on LVIS-Ambiguous benchmark.</li>
<li>Less than 30 ms of added latency per interaction is observed.</li>
<li>One-shot baseline refers to existing open-vocabulary detectors operating in a one-shot manner.</li>
<li>Contrastive refinement enables fine-grained disambiguation in cluttered scenes.</li>
<li>Intent-based retrieval framework refines predictions based on user feedback.</li>
<li>Dual memory sets in IS help maintain confirmed cues and rejected hypotheses.</li>
<li>Contrastive alignment function ranks objects based on similarity to positive cues and rejection of negative constraints.</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-27">正文（抓取，非 AI）</h3>
<p>[2602.17639v1] IntRec: Intent-based Retrieval with Contrastive Refinement Computer Science &gt; Computer Vision and Pattern Recognition arXiv:2602.17639v1 (cs) [Submitted on 19 Feb 2026] Title: IntRec: Intent-based Retrieval with Contrastive Refinement Authors: Pourya Shamsolmoali , Masoumeh Zareapoor , Eric Granger , Yue Lu View a PDF of the paper titled IntRec: Intent-based Retrieval with Contrastive Refinement, by Pourya Shamsolmoali and 3 other authors View PDF HTML (experimental) Abstract: Retrieving user-specified objects from complex scenes remains a challenging task, especially when queries are ambiguous or involve multiple similar objects. Existing open-vocabulary detectors operate in a one-shot manner, lacking the ability to refine predictions based on user feedback. To address this, we propose IntRec, an interactive object retrieval framework that refines predictions based on user feedback. At its core is an Intent State (IS) that maintains dual memory sets for positive anchors (confirmed cues) and negative constraints (rejected hypotheses). A contrastive alignment function ranks candidate objects by maximizing similarity to positive cues while penalizing rejected ones, enabling fine-grained disambiguation in cluttered scenes. Our interactive framework provides substantial improvements in retrieval accuracy without additional supervision. On LVIS, IntRec achieves 35.4 AP, outperforming OVMR, CoDet, and CAKE by +2.3, +3.7, and +0.5, respectively. On the challenging LVIS-Ambiguous benchmark, it improves performance by +7.9 AP over its one-shot baseline after a single corrective feedback, with less than 30 ms of added latency per interaction. Subjects: Computer Vision and Pattern Recognition (cs.CV) Cite as: arXiv:2602.17639 [cs.CV] (or arXiv:2602.17639v1 [cs.CV] for this version) https://doi.org/10.48550/arXiv.2602.17639 Focus to learn more arXiv-issued DOI via DataCite (pending registration) Submission history From: Pourya Shamsolmoali [ view email ] [v1] Thu, 19 Feb 2026 18:50:53 UTC (13,135 KB) Full-text links: Access Paper: View a PDF of the paper titled IntRec: Intent-based Retrieval with Contrastive Refinement, by Pourya Shamsolmoali and 3 other authors View PDF HTML (experimental) TeX Source view license Current browse context: cs.CV &lt; prev | next &gt; new | recent | 2026-02 Change to browse by: cs References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-28">15. Pseudo-deterministic Quantum Algorithms</h2>
<ul>
<li>链接：https://arxiv.org/abs/2602.17647v1</li>
<li>来源：arxiv</li>
<li>摘要：We initiate a systematic study of pseudo-deterministic quantum algorithms. These are quantum algorithms that, for any input, output a canonical solution with high probability. Focusing on the query complexity model, our main contributions include the following complexity separations, which require new lower bound techniques specifically tailored to pseudo-determinism:   - We exhibit a problem, Avoid One Encrypted String (AOES), whose classical randomized query complexity is $O(1)$ but is maximally hard for pseudo-deterministic quantum algorithms ($Ω(N)$ query complexity).   - We exhibit a problem, Quantum-Locked Estimation (QL-Estimation), for which pseudo-deterministic quantum algorithms admit an exponential speed-up over classical pseudo-deterministic algorithms ($O(\log(N))$ vs. $Θ(\sqrt{N})$), while the randomized query complexity is $O(1)$.   Complementing these separations, we show that for any total problem $R$, pseudo-deterministic quantum algorithms admit at most a quintic advantage over deterministic algorithms, i.e., $D(R) = \tilde O(psQ(R)^5)$.   On the algorithmic side, we identify a class of quantum search problems that can be made pseudo-deterministic with small over</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">伪确定性量子算法在解决特定问题时展现出显著优势，特别是在处理标准解输出的高概率情况下。例如，对于伪确定性量子算法而言，AOES问题的查询复杂度为Ω(N)，而AOES问题的古典随机查询复杂度仅为O(1)。此外，伪确定性量子算法在解决QL-Estimation问题时，相对于古典伪确定性算法具有指数级的速度提升。任何总问题R，伪确定性量子算法相对于确定性算法最多有五次方的优势。这使得Grover搜索、元素唯一性、三角查找、k-和、图碰撞等量子搜索问题能够被伪确定性化。值得注意的是，新低界技术专门针对伪确定性，使得研究更加系统化。避免一个加密字符串(AOES)问题展示了古典随机算法与伪确定性量子算法之间的显著差异，而量子锁定估计(QL-Estimation)问题则进一步突显了伪确定性量子算法的优越性。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>pseudo-deterministic quantum algorithms输出高概率下的标准解。</li>
<li>AOES问题的古典随机查询复杂度为O(1)，但对伪确定性量子算法的查询复杂度为Ω(N)。</li>
<li>QL-Estimation问题的伪确定性量子算法相对于古典伪确定性算法有指数级速度提升。</li>
<li>任何总问题R，伪确定性量子算法相对于确定性算法最多有五次方的优势。</li>
<li>Grover搜索、元素唯一性、三角查找、k-和、图碰撞等量子搜索问题可以伪确定性化。</li>
<li>新低界技术专门针对伪确定性。</li>
<li>伪确定性量子算法的研究是系统性的。</li>
<li>避免一个加密字符串(AOES)问题展示了古典随机算法与伪确定性量子算法的差异。</li>
<li>量子锁定估计(QL-Estimation)问题展示了伪确定性量子算法的优越性。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-29">正文（抓取，非 AI）</h3>
<p>[2602.17647v1] Pseudo-deterministic Quantum Algorithms Quantum Physics arXiv:2602.17647v1 (quant-ph) [Submitted on 19 Feb 2026] Title: Pseudo-deterministic Quantum Algorithms Authors: Hugo Aaronson , Tom Gur , Jiawei Li View a PDF of the paper titled Pseudo-deterministic Quantum Algorithms, by Hugo Aaronson and 2 other authors View PDF HTML (experimental) Abstract: We initiate a systematic study of pseudo-deterministic quantum algorithms. These are quantum algorithms that, for any input, output a canonical solution with high probability. Focusing on the query complexity model, our main contributions include the following complexity separations, which require new lower bound techniques specifically tailored to pseudo-determinism: - We exhibit a problem, Avoid One Encrypted String (AOES), whose classical randomized query complexity is $O(1)$ but is maximally hard for pseudo-deterministic quantum algorithms ($\Omega(N)$ query complexity). - We exhibit a problem, Quantum-Locked Estimation (QL-Estimation), for which pseudo-deterministic quantum algorithms admit an exponential speed-up over classical pseudo-deterministic algorithms ($O(\log(N))$ vs. $\Theta(\sqrt{N})$), while the randomized query complexity is $O(1)$. Complementing these separations, we show that for any total problem $R$, pseudo-deterministic quantum algorithms admit at most a quintic advantage over deterministic algorithms, i.e., $D(R) = \tilde O(psQ(R)^5)$. On the algorithmic side, we identify a class of quantum search problems that can be made pseudo-deterministic with small overhead, including Grover search, element distinctness, triangle finding, $k$-sum, and graph collision. Subjects: Quantum Physics (quant-ph) ; Computational Complexity (cs.CC) Cite as: arXiv:2602.17647 [quant-ph] (or arXiv:2602.17647v1 [quant-ph] for this version) https://doi.org/10.48550/arXiv.2602.17647 Focus to learn more arXiv-issued DOI via DataCite (pending registration) Submission history From: Hugo Aaronson [ view email ] [v1] Thu, 19 Feb 2026 18:54:47 UTC (35 KB) Full-text links: Access Paper: View a PDF of the paper titled Pseudo-deterministic Quantum Algorithms, by Hugo Aaronson and 2 other authors View PDF HTML (experimental) TeX Source view license Current browse context: quant-ph &lt; prev | next &gt; new | recent | 2026-02 Change to browse by: cs cs.CC References &amp; Citations INSPIRE HEP NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-30">16. AutoNumerics: An Autonomous, PDE-Agnostic Multi-Agent Pipeline for Scientific Computing</h2>
<ul>
<li>链接：https://arxiv.org/abs/2602.17607v1</li>
<li>来源：arxiv</li>
<li>摘要：PDEs are central to scientific and engineering modeling, yet designing accurate numerical solvers typically requires substantial mathematical expertise and manual tuning. Recent neural network-based approaches improve flexibility but often demand high computational cost and suffer from limited interpretability. We introduce \texttt{AutoNumerics}, a multi-agent framework that autonomously designs, implements, debugs, and verifies numerical solvers for general PDEs directly from natural language descriptions. Unlike black-box neural solvers, our framework generates transparent solvers grounded in classical numerical analysis. We introduce a coarse-to-fine execution strategy and a residual-based self-verification mechanism. Experiments on 24 canonical and real-world PDE problems demonstrate that \texttt{AutoNumerics} achieves competitive or superior accuracy compared to existing neural and LLM-based baselines, and correctly selects numerical schemes based on PDE structural properties, suggesting its viability as an accessible paradigm for automated PDE solving.</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">AutoNumerics的设计无需深入了解偏微分方程（PDE）的细节，但仍需具备一定的数学专长。尽管如此，AutoNumerics采用了一种自上而下的执行策略，能够根据PDE的结构正确选择数值方案，从而在24个PDE问题上表现出色，优于现有的基线方法。此外，AutoNumerics还配备了一种残差基自我验证机制，这有助于提高计算结果的准确性。尽管黑盒神经网络解算器缺乏透明性，AutoNumerics通过其可访问的自动化PDE求解范式，为用户提供了一种更为可靠和透明的解决方案。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>AutoNumerics的设计无需深入了解PDE，但仍需数学专长。</li>
<li>黑盒神经网络解算器缺乏透明性。</li>
<li>AutoNumerics采用自上而下的执行策略。</li>
<li>残差基自我验证机制有助于提高准确性。</li>
<li>AutoNumerics在24个PDE问题上表现优于现有基线。</li>
<li>AutoNumerics能根据PDE结构正确选择数值方案。</li>
<li>AutoNumerics提供了一种可访问的自动化PDE求解范式。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-31">正文（抓取，非 AI）</h3>
<p>[2602.17607v1] AutoNumerics: An Autonomous, PDE-Agnostic Multi-Agent Pipeline for Scientific Computing Computer Science &gt; Artificial Intelligence arXiv:2602.17607v1 (cs) [Submitted on 19 Feb 2026] Title: AutoNumerics: An Autonomous, PDE-Agnostic Multi-Agent Pipeline for Scientific Computing Authors: Jianda Du , Youran Sun , Haizhao Yang View a PDF of the paper titled AutoNumerics: An Autonomous, PDE-Agnostic Multi-Agent Pipeline for Scientific Computing, by Jianda Du and 2 other authors View PDF HTML (experimental) Abstract: PDEs are central to scientific and engineering modeling, yet designing accurate numerical solvers typically requires substantial mathematical expertise and manual tuning. Recent neural network-based approaches improve flexibility but often demand high computational cost and suffer from limited interpretability. We introduce \texttt{AutoNumerics}, a multi-agent framework that autonomously designs, implements, debugs, and verifies numerical solvers for general PDEs directly from natural language descriptions. Unlike black-box neural solvers, our framework generates transparent solvers grounded in classical numerical analysis. We introduce a coarse-to-fine execution strategy and a residual-based self-verification mechanism. Experiments on 24 canonical and real-world PDE problems demonstrate that \texttt{AutoNumerics} achieves competitive or superior accuracy compared to existing neural and LLM-based baselines, and correctly selects numerical schemes based on PDE structural properties, suggesting its viability as an accessible paradigm for automated PDE solving. Subjects: Artificial Intelligence (cs.AI) ; Machine Learning (cs.LG); Numerical Analysis (math.NA) Cite as: arXiv:2602.17607 [cs.AI] (or arXiv:2602.17607v1 [cs.AI] for this version) https://doi.org/10.48550/arXiv.2602.17607 Focus to learn more arXiv-issued DOI via DataCite (pending registration) Submission history From: Youran Sun [ view email ] [v1] Thu, 19 Feb 2026 18:31:52 UTC (528 KB) Full-text links: Access Paper: View a PDF of the paper titled AutoNumerics: An Autonomous, PDE-Agnostic Multi-Agent Pipeline for Scientific Computing, by Jianda Du and 2 other authors View PDF HTML (experimental) TeX Source view license Current browse context: cs.AI &lt; prev | next &gt; new | recent | 2026-02 Change to browse by: cs cs.LG cs.NA math math.NA References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-32">17. What Language is This? Ask Your Tokenizer</h2>
<ul>
<li>链接：https://arxiv.org/abs/2602.17655v1</li>
<li>来源：arxiv</li>
<li>摘要：Language Identification (LID) is an important component of many multilingual natural language processing pipelines, where it facilitates corpus curation, training data analysis, and cross-lingual evaluation of large language models. Despite near-perfect performance on high-resource languages, existing systems remain brittle in low-resource and closely related language settings. We introduce UniLID, a simple and efficient LID method based on the UnigramLM tokenization algorithm, leveraging its probabilistic framing, parameter estimation technique and inference strategy. In short, we learn language-conditional unigram distributions over a shared tokenizer vocabulary but treat segmentation as a language-specific phenomenon. Our formulation is data- and compute-efficient, supports incremental addition of new languages without retraining existing models, and can naturally be integrated into existing language model tokenization pipelines. Empirical evaluations against widely used baselines, including fastText, GlotLID, and CLD3, show that UniLID achieves competitive performance on standard benchmarks, substantially improves sample efficiency in low-resource settings - surpassing 70% accu</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">UniLID方法基于UnigramLM分词算法，通过学习语言条件下的单词分布来支持增量添加新语言而无需重新训练现有模型，这一方法在标准基准上达到竞争力，并且在低资源设置中显著提高样本效率，特别是在细粒度方言识别上取得了巨大进展。尽管UniLID方法在低资源和相关语言设置中表现较弱，但它能够自然地集成到现有的语言模型分词管道中，这使得它成为一种灵活且高效的解决方案。因此，UniLID方法不仅在资源丰富的环境中表现出色，还在需要快速适应新语言的场景中提供了显著的优势。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>UniLID方法基于UnigramLM分词算法。</li>
<li>UniLID方法在低资源和相关语言设置中表现较弱。</li>
<li>UniLID方法学习语言条件下的单词分布。</li>
<li>UniLID方法支持增量添加新语言而无需重新训练现有模型。</li>
<li>UniLID方法可以自然地集成到现有的语言模型分词管道中。</li>
<li>UniLID方法在标准基准上达到竞争力。</li>
<li>UniLID方法在低资源设置中显著提高样本效率。</li>
<li>UniLID方法在细粒度方言识别上取得巨大进展。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-33">正文（抓取，非 AI）</h3>
<p>[2602.17655v1] What Language is This? Ask Your Tokenizer Computer Science &gt; Computation and Language arXiv:2602.17655v1 (cs) [Submitted on 19 Feb 2026] Title: What Language is This? Ask Your Tokenizer Authors: Clara Meister , Ahmetcan Yavuz , Pietro Lesci , Tiago Pimentel View a PDF of the paper titled What Language is This? Ask Your Tokenizer, by Clara Meister and 3 other authors View PDF HTML (experimental) Abstract: Language Identification (LID) is an important component of many multilingual natural language processing pipelines, where it facilitates corpus curation, training data analysis, and cross-lingual evaluation of large language models. Despite near-perfect performance on high-resource languages, existing systems remain brittle in low-resource and closely related language settings. We introduce UniLID, a simple and efficient LID method based on the UnigramLM tokenization algorithm, leveraging its probabilistic framing, parameter estimation technique and inference strategy. In short, we learn language-conditional unigram distributions over a shared tokenizer vocabulary but treat segmentation as a language-specific phenomenon. Our formulation is data- and compute-efficient, supports incremental addition of new languages without retraining existing models, and can naturally be integrated into existing language model tokenization pipelines. Empirical evaluations against widely used baselines, including fastText, GlotLID, and CLD3, show that UniLID achieves competitive performance on standard benchmarks, substantially improves sample efficiency in low-resource settings - surpassing 70% accuracy with as few as five labeled samples per language - and delivers large gains on fine-grained dialect identification. Subjects: Computation and Language (cs.CL) Cite as: arXiv:2602.17655 [cs.CL] (or arXiv:2602.17655v1 [cs.CL] for this version) https://doi.org/10.48550/arXiv.2602.17655 Focus to learn more arXiv-issued DOI via DataCite (pending registration) Submission history From: Clara Meister [ view email ] [v1] Thu, 19 Feb 2026 18:58:39 UTC (69 KB) Full-text links: Access Paper: View a PDF of the paper titled What Language is This? Ask Your Tokenizer, by Clara Meister and 3 other authors View PDF HTML (experimental) TeX Source view license Current browse context: cs.CL &lt; prev | next &gt; new | recent | 2026-02 Change to browse by: cs References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-34">18. MARS: Margin-Aware Reward-Modeling with Self-Refinement</h2>
<ul>
<li>链接：https://arxiv.org/abs/2602.17658v1</li>
<li>来源：arxiv</li>
<li>摘要：Reward modeling is a core component of modern alignment pipelines including RLHF and RLAIF, underpinning policy optimization methods including PPO and TRPO. However, training reliable reward models relies heavily on human-labeled preference data, which is costly and limited, motivating the use of data augmentation. Existing augmentation approaches typically operate at the representation or semantic level and remain agnostic to the reward model's estimation difficulty. In this paper, we propose MARS, an adaptive, margin-aware augmentation and sampling strategy that explicitly targets ambiguous and failure modes of the reward model. Our proposed framework, MARS, concentrates augmentation on low-margin (ambiguous) preference pairs where the reward model is most uncertain, and iteratively refines the training distribution via hard-sample augmentation. We provide theoretical guarantees showing that this strategy increases the average curvature of the loss function hence enhance information and improves conditioning, along with empirical results demonstrating consistent gains over uniform augmentation for robust reward modeling.</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">reward模型的训练依赖于昂贵且有限的人工标注偏好数据，这使得模型的训练成本高昂且数据量有限。现有的增强方法通常在表示或语义层面进行操作，但忽略了reward模型的估计难度，这进一步限制了模型性能的提升。MARS策略则专注于低边际（模糊）偏好对，这些偏好对reward模型的估计最为不确定。理论保证表明，MARS策略通过增加损失函数的平均曲率，增强了信息并改善了条件，从而提升了模型的性能。此外，MARS通过硬样本增强迭代精炼训练分布，确保了在鲁棒reward建模中的持续收益，相较于均匀增强，MARS提供了更优的性能。因此，MARS策略不仅解决了reward模型估计的不确定性问题，还通过优化训练过程，显著提升了模型的鲁棒性和准确性。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>reward模型的训练依赖于昂贵且有限的人工标注偏好数据。</li>
<li>现有的增强方法通常在表示或语义层面进行操作，但忽略了reward模型的估计难度。</li>
<li>MARS策略集中在低边际（模糊）偏好对上，这些对reward模型最不确定。</li>
<li>理论保证表明，这种策略增加了损失函数的平均曲率，从而增强信息并改善条件。</li>
<li>MARS通过硬样本增强迭代精炼训练分布。</li>
<li>MARS在鲁棒reward建模中提供了持续的收益，优于均匀增强。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-35">正文（抓取，非 AI）</h3>
<p>[2602.17658v1] MARS: Margin-Aware Reward-Modeling with Self-Refinement Computer Science &gt; Machine Learning arXiv:2602.17658v1 (cs) [Submitted on 19 Feb 2026] Title: MARS: Margin-Aware Reward-Modeling with Self-Refinement Authors: Payel Bhattacharjee , Osvaldo Simeone , Ravi Tandon View a PDF of the paper titled MARS: Margin-Aware Reward-Modeling with Self-Refinement, by Payel Bhattacharjee and 2 other authors View PDF HTML (experimental) Abstract: Reward modeling is a core component of modern alignment pipelines including RLHF and RLAIF, underpinning policy optimization methods including PPO and TRPO. However, training reliable reward models relies heavily on human-labeled preference data, which is costly and limited, motivating the use of data augmentation. Existing augmentation approaches typically operate at the representation or semantic level and remain agnostic to the reward model's estimation difficulty. In this paper, we propose MARS, an adaptive, margin-aware augmentation and sampling strategy that explicitly targets ambiguous and failure modes of the reward model. Our proposed framework, MARS, concentrates augmentation on low-margin (ambiguous) preference pairs where the reward model is most uncertain, and iteratively refines the training distribution via hard-sample augmentation. We provide theoretical guarantees showing that this strategy increases the average curvature of the loss function hence enhance information and improves conditioning, along with empirical results demonstrating consistent gains over uniform augmentation for robust reward modeling. Subjects: Machine Learning (cs.LG) ; Artificial Intelligence (cs.AI); Information Theory (cs.IT) Cite as: arXiv:2602.17658 [cs.LG] (or arXiv:2602.17658v1 [cs.LG] for this version) https://doi.org/10.48550/arXiv.2602.17658 Focus to learn more arXiv-issued DOI via DataCite (pending registration) Submission history From: Payel Bhattacharjee [ view email ] [v1] Thu, 19 Feb 2026 18:59:03 UTC (26,329 KB) Full-text links: Access Paper: View a PDF of the paper titled MARS: Margin-Aware Reward-Modeling with Self-Refinement, by Payel Bhattacharjee and 2 other authors View PDF HTML (experimental) TeX Source view license Current browse context: cs.LG &lt; prev | next &gt; new | recent | 2026-02 Change to browse by: cs cs.AI cs.IT math math.IT References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) IArxiv recommender toggle IArxiv Recommender ( What is IArxiv? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-36">19. Differences in Typological Alignment in Language Models' Treatment of Differential Argument Marking</h2>
<ul>
<li>链接：https://arxiv.org/abs/2602.17653v1</li>
<li>来源：arxiv</li>
<li>摘要：Recent work has shown that language models (LMs) trained on synthetic corpora can exhibit typological preferences that resemble cross-linguistic regularities in human languages, particularly for syntactic phenomena such as word order. In this paper, we extend this paradigm to differential argument marking (DAM), a semantic licensing system in which morphological marking depends on semantic prominence. Using a controlled synthetic learning method, we train GPT-2 models on 18 corpora implementing distinct DAM systems and evaluate their generalization using minimal pairs. Our results reveal a dissociation between two typological dimensions of DAM. Models reliably exhibit human-like preferences for natural markedness direction, favoring systems in which overt marking targets semantically atypical arguments. In contrast, models do not reproduce the strong object preference in human languages, in which overt marking in DAM more often targets objects rather than subjects. These findings suggest that different typological tendencies may arise from distinct underlying sources.</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">语言模型可能表现出与人类语言类似的类型学偏好，这表明它们在处理语言时具有一定的自然倾向。首先，语言模型倾向于自然的标记方向，即更倾向于标记语义上不寻常的论元，这反映了它们对语言结构的某种偏好。其次，值得注意的是，语言模型未能复制人类语言中强烈的宾语偏好，这表明它们在某些方面仍存在局限性。此外，不同类型的倾向可能源于不同的潜在来源，这暗示了需要进一步研究以理解这些偏好背后的机制。因此，这些观察不仅揭示了语言模型在处理语言时的某些特性，还指出了它们与人类语言之间的差异，为进一步研究提供了方向。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>语言模型可能表现出与人类语言类似的类型学偏好。</li>
<li>语言模型倾向于自然的标记方向，即倾向于标记语义上不寻常的论元。</li>
<li>语言模型未能复制人类语言中强烈的宾语偏好。</li>
<li>不同类型的倾向可能源于不同的潜在来源。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-37">正文（抓取，非 AI）</h3>
<p>[2602.17653v1] Differences in Typological Alignment in Language Models' Treatment of Differential Argument Marking Computer Science &gt; Computation and Language arXiv:2602.17653v1 (cs) [Submitted on 19 Feb 2026] Title: Differences in Typological Alignment in Language Models' Treatment of Differential Argument Marking Authors: Iskar Deng , Nathalia Xu , Shane Steinert-Threlkeld View a PDF of the paper titled Differences in Typological Alignment in Language Models' Treatment of Differential Argument Marking, by Iskar Deng and 2 other authors View PDF HTML (experimental) Abstract: Recent work has shown that language models (LMs) trained on synthetic corpora can exhibit typological preferences that resemble cross-linguistic regularities in human languages, particularly for syntactic phenomena such as word order. In this paper, we extend this paradigm to differential argument marking (DAM), a semantic licensing system in which morphological marking depends on semantic prominence. Using a controlled synthetic learning method, we train GPT-2 models on 18 corpora implementing distinct DAM systems and evaluate their generalization using minimal pairs. Our results reveal a dissociation between two typological dimensions of DAM. Models reliably exhibit human-like preferences for natural markedness direction, favoring systems in which overt marking targets semantically atypical arguments. In contrast, models do not reproduce the strong object preference in human languages, in which overt marking in DAM more often targets objects rather than subjects. These findings suggest that different typological tendencies may arise from distinct underlying sources. Comments: 15 pages, 7 figures, 7 tables. Under review Subjects: Computation and Language (cs.CL) Cite as: arXiv:2602.17653 [cs.CL] (or arXiv:2602.17653v1 [cs.CL] for this version) https://doi.org/10.48550/arXiv.2602.17653 Focus to learn more arXiv-issued DOI via DataCite (pending registration) Submission history From: Iskar Deng [ view email ] [v1] Thu, 19 Feb 2026 18:56:34 UTC (282 KB) Full-text links: Access Paper: View a PDF of the paper titled Differences in Typological Alignment in Language Models' Treatment of Differential Argument Marking, by Iskar Deng and 2 other authors View PDF HTML (experimental) TeX Source view license Current browse context: cs.CL &lt; prev | next &gt; new | recent | 2026-02 Change to browse by: cs References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-38">20. Graph Neural Model Predictive Control for High-Dimensional Systems</h2>
<ul>
<li>链接：https://arxiv.org/abs/2602.17601v1</li>
<li>来源：arxiv</li>
<li>摘要：The control of high-dimensional systems, such as soft robots, requires models that faithfully capture complex dynamics while remaining computationally tractable. This work presents a framework that integrates Graph Neural Network (GNN)-based dynamics models with structure-exploiting Model Predictive Control to enable real-time control of high-dimensional systems. By representing the system as a graph with localized interactions, the GNN preserves sparsity, while a tailored condensing algorithm eliminates state variables from the control problem, ensuring efficient computation. The complexity of our condensing algorithm scales linearly with the number of system nodes, and leverages Graphics Processing Unit (GPU) parallelization to achieve real-time performance. The proposed approach is validated in simulation and experimentally on a physical soft robotic trunk. Results show that our method scales to systems with up to 1,000 nodes at 100 Hz in closed-loop, and demonstrates real-time reference tracking on hardware with sub-centimeter accuracy, outperforming baselines by 63.6%. Finally, we show the capability of our method to achieve effective full-body obstacle avoidance.</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">GNN能够保持稀疏性，但这并不直接等同于计算效率高。稀疏性本身只是提高计算效率的一个潜在因素，而真正实现高效的关键在于condensing算法，其复杂度与系统节点数成线性关系，这使得计算效率得以显著提升。此外，GPU并行化是实现实时性能的重要手段，进一步增强了系统的实时性。对于高维系统如软机器人，控制框架需要平衡复杂的动态特性与计算效率，以确保实时参考跟踪的精度可达亚厘米级别。该方法能有效实现全身障碍物规避，但在具体机制上未作详细说明。值得注意的是，基线方法的表现已被超越了63.6%，但具体基线内容未详细说明。控制框架在仿真和物理实验中均得到了验证，但具体实验条件未提及。节点系统在100Hz下仍能实时运行，显示了方法的扩展性，但其中的「稀疏性」并不直接等同于「高效」，需结合具体算法进行分析。因此，这些特性共同展示了该方法在高维系统控制中的优越性能和扩展潜力。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>GNN能保持稀疏性，但稀疏性本身并不意味着计算效率高。</li>
<li>condensing算法的复杂度与系统节点数成线性关系，这是高效的关键。</li>
<li>GPU并行化是实现实时性能的重要手段。</li>
<li>软机器人等高维系统控制需要平衡复杂动态和计算效率。</li>
<li>实时参考跟踪在硬件上的精度可达亚厘米级别。</li>
<li>方法能有效实现全身障碍物规避，但具体机制未详细说明。</li>
<li>基线方法的表现被超越了63.6%，但未具体说明基线内容。</li>
<li>控制框架在仿真和物理实验中均得到验证，但未提及具体实验条件。</li>
<li>节点系统在100Hz下仍能实时运行，显示了方法的扩展性。</li>
<li>中的「稀疏性」并不直接等同于「高效」，需结合具体算法分析。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-39">正文（抓取，非 AI）</h3>
<p>[2602.17601v1] Graph Neural Model Predictive Control for High-Dimensional Systems Computer Science &gt; Robotics arXiv:2602.17601v1 (cs) [Submitted on 19 Feb 2026] Title: Graph Neural Model Predictive Control for High-Dimensional Systems Authors: Patrick Benito Eberhard , Luis Pabon , Daniele Gammelli , Hugo Buurmeijer , Amon Lahr , Mark Leone , Andrea Carron , Marco Pavone View a PDF of the paper titled Graph Neural Model Predictive Control for High-Dimensional Systems, by Patrick Benito Eberhard and 7 other authors View PDF HTML (experimental) Abstract: The control of high-dimensional systems, such as soft robots, requires models that faithfully capture complex dynamics while remaining computationally tractable. This work presents a framework that integrates Graph Neural Network (GNN)-based dynamics models with structure-exploiting Model Predictive Control to enable real-time control of high-dimensional systems. By representing the system as a graph with localized interactions, the GNN preserves sparsity, while a tailored condensing algorithm eliminates state variables from the control problem, ensuring efficient computation. The complexity of our condensing algorithm scales linearly with the number of system nodes, and leverages Graphics Processing Unit (GPU) parallelization to achieve real-time performance. The proposed approach is validated in simulation and experimentally on a physical soft robotic trunk. Results show that our method scales to systems with up to 1,000 nodes at 100 Hz in closed-loop, and demonstrates real-time reference tracking on hardware with sub-centimeter accuracy, outperforming baselines by 63.6%. Finally, we show the capability of our method to achieve effective full-body obstacle avoidance. Subjects: Robotics (cs.RO) Cite as: arXiv:2602.17601 [cs.RO] (or arXiv:2602.17601v1 [cs.RO] for this version) https://doi.org/10.48550/arXiv.2602.17601 Focus to learn more arXiv-issued DOI via DataCite (pending registration) Submission history From: Patrick Benito Eberhard [ view email ] [v1] Thu, 19 Feb 2026 18:26:42 UTC (4,100 KB) Full-text links: Access Paper: View a PDF of the paper titled Graph Neural Model Predictive Control for High-Dimensional Systems, by Patrick Benito Eberhard and 7 other authors View PDF HTML (experimental) TeX Source view license Current browse context: cs.RO &lt; prev | next &gt; new | recent | 2026-02 Change to browse by: cs References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-40">21. A Study of Entanglement and Ansatz Expressivity for the Transverse-Field Ising Model using Variational Quantum Eigensolver</h2>
<ul>
<li>链接：https://arxiv.org/abs/2602.17662v1</li>
<li>来源：arxiv</li>
<li>摘要：The Variational Quantum Eigensolver (VQE) is a leading hybrid quantum-classical algorithm for simulating many-body systems in the NISQ era. Its effectiveness, however, depends on the faithful preparation of eigenstates, which becomes challenging in degenerate and strongly entangled regimes. We study this problem using the transverse-field Ising model (TFIM) with periodic boundary conditions in one, two, and three dimensions, considering systems of up to 27 qubits. We employ different ansatzes: the hardware-efficient EfficientSU2 from Qiskit, the physics-inspired Hamiltonian Variational Ansatz (HVA) and HVA with symmetry breaking, and benchmark their performance using energy variance, entanglement entropy, spin correlations, and magnetization.</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">VQE的有效性取决于准确准备本征态，但其在退化和强纠缠区域的表现不佳。为了解决这一问题，研究采用了多种TFIM模型，包括一维、二维和三维系统，并考虑了最多27个量子比特的系统。为了提高VQE的性能，研究使用了不同的ansatz，包括EfficientSU2、HVA和HVA带对称性破坏。评估结果显示，HVA在某些情况下表现优于EfficientSU2，而对称性破坏则有助于HVA在某些情况下表现出更好的性能。因此，通过选择合适的ansatz，可以提高VQE在复杂系统中的表现。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>VQE的有效性取决于准确准备本征态。</li>
<li>VQE在退化和强纠缠区域表现不佳。</li>
<li>研究使用了一维、二维和三维的TFIM。</li>
<li>考虑了最多27个量子比特的系统。</li>
<li>使用了不同的ansatz：EfficientSU2、HVA和HVA带对称性破坏。</li>
<li>评估了能量方差、纠缠熵、自旋相关性和磁化。</li>
<li>HVA在某些情况下表现优于EfficientSU2。</li>
<li>对称性破坏有助于HVA在某些情况下表现更好。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-41">正文（抓取，非 AI）</h3>
<p>[2602.17662v1] A Study of Entanglement and Ansatz Expressivity for the Transverse-Field Ising Model using Variational Quantum Eigensolver Quantum Physics arXiv:2602.17662v1 (quant-ph) [Submitted on 19 Feb 2026] Title: A Study of Entanglement and Ansatz Expressivity for the Transverse-Field Ising Model using Variational Quantum Eigensolver Authors: Ashutosh P. Tripathi , Nilmani Mathur , Vikram Tripathi View a PDF of the paper titled A Study of Entanglement and Ansatz Expressivity for the Transverse-Field Ising Model using Variational Quantum Eigensolver, by Ashutosh P. Tripathi and 2 other authors View PDF HTML (experimental) Abstract: The Variational Quantum Eigensolver (VQE) is a leading hybrid quantum-classical algorithm for simulating many-body systems in the NISQ era. Its effectiveness, however, depends on the faithful preparation of eigenstates, which becomes challenging in degenerate and strongly entangled regimes. We study this problem using the transverse-field Ising model (TFIM) with periodic boundary conditions in one, two, and three dimensions, considering systems of up to 27 qubits. We employ different ansatzes: the hardware-efficient EfficientSU2 from Qiskit, the physics-inspired Hamiltonian Variational Ansatz (HVA) and HVA with symmetry breaking, and benchmark their performance using energy variance, entanglement entropy, spin correlations, and magnetization. Comments: 9 pages, 6 figures, contribution to the 42nd International Symposium on Lattice Field Theory (LATTICE2025), 2-8 November 2025, Tata Institute of Fundamental Research, Mumbai, India Subjects: Quantum Physics (quant-ph) ; Statistical Mechanics (cond-mat.stat-mech) Report number: TIFR/TH/26-9 Cite as: arXiv:2602.17662 [quant-ph] (or arXiv:2602.17662v1 [quant-ph] for this version) https://doi.org/10.48550/arXiv.2602.17662 Focus to learn more arXiv-issued DOI via DataCite (pending registration) Submission history From: Ashutosh P. Tripathi [ view email ] [v1] Thu, 19 Feb 2026 18:59:36 UTC (1,395 KB) Full-text links: Access Paper: View a PDF of the paper titled A Study of Entanglement and Ansatz Expressivity for the Transverse-Field Ising Model using Variational Quantum Eigensolver, by Ashutosh P. Tripathi and 2 other authors View PDF HTML (experimental) TeX Source view license Current browse context: quant-ph &lt; prev | next &gt; new | recent | 2026-02 Change to browse by: cond-mat cond-mat.stat-mech References &amp; Citations INSPIRE HEP NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-42">22. SMAC: Score-Matched Actor-Critics for Robust Offline-to-Online Transfer</h2>
<ul>
<li>链接：https://arxiv.org/abs/2602.17632v1</li>
<li>来源：arxiv</li>
<li>摘要：Modern offline Reinforcement Learning (RL) methods find performant actor-critics, however, fine-tuning these actor-critics online with value-based RL algorithms typically causes immediate drops in performance. We provide evidence consistent with the hypothesis that, in the loss landscape, offline maxima for prior algorithms and online maxima are separated by low-performance valleys that gradient-based fine-tuning traverses. Following this, we present Score Matched Actor-Critic (SMAC), an offline RL method designed to learn actor-critics that transition to online value-based RL algorithms with no drop in performance. SMAC avoids valleys between offline and online maxima by regularizing the Q-function during the offline phase to respect a first-order derivative equality between the score of the policy and action-gradient of the Q-function. We experimentally demonstrate that SMAC converges to offline maxima that are connected to better online maxima via paths with monotonically increasing reward found by first-order optimization. SMAC achieves smooth transfer to Soft Actor-Critic and TD3 in 6/6 D4RL tasks. In 4/6 environments, it reduces regret by 34-58% over the best baseline.</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">offline maxima 和 online maxima 之间存在低性能的山谷，这些山谷会导致算法在优化过程中遇到性能瓶颈。为了解决这一问题，gradient-based 细化方法通过穿越这些低性能的山谷来提升性能。然而，直接使用 gradient-based 方法可能会遇到困难，因为它们在离线阶段容易陷入这些山谷。为了解决这个问题，SMAC 通过在离线阶段正则化 Q 函数来避免这些低性能的山谷，从而确保算法能够平滑过渡到 Soft Actor-Critic 和 TD3。在 D4RL 任务中，SMAC 显示出了显著的优势，能够在四个环境中减少 34-58% 的后悔值，从而证明了其有效性和优越性。因此，SMAC 的设计不仅解决了离线和在线性能之间的矛盾，还显著提升了算法的整体表现。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>offline maxima 和 online maxima 之间存在低性能的山谷。</li>
<li>gradient-based 细化会穿越这些低性能的山谷。</li>
<li>SMAC 通过在离线阶段正则化 Q 函数来避免这些山谷。</li>
<li>SMAC 能够在 D4RL 任务中平滑过渡到 Soft Actor-Critic 和 TD3。</li>
<li>在四个环境中，SMAC 能减少 34-58% 的后悔值。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-43">正文（抓取，非 AI）</h3>
<p>[2602.17632v1] SMAC: Score-Matched Actor-Critics for Robust Offline-to-Online Transfer Computer Science &gt; Machine Learning arXiv:2602.17632v1 (cs) [Submitted on 19 Feb 2026] Title: SMAC: Score-Matched Actor-Critics for Robust Offline-to-Online Transfer Authors: Nathan S. de Lara , Florian Shkurti View a PDF of the paper titled SMAC: Score-Matched Actor-Critics for Robust Offline-to-Online Transfer, by Nathan S. de Lara and Florian Shkurti View PDF HTML (experimental) Abstract: Modern offline Reinforcement Learning (RL) methods find performant actor-critics, however, fine-tuning these actor-critics online with value-based RL algorithms typically causes immediate drops in performance. We provide evidence consistent with the hypothesis that, in the loss landscape, offline maxima for prior algorithms and online maxima are separated by low-performance valleys that gradient-based fine-tuning traverses. Following this, we present Score Matched Actor-Critic (SMAC), an offline RL method designed to learn actor-critics that transition to online value-based RL algorithms with no drop in performance. SMAC avoids valleys between offline and online maxima by regularizing the Q-function during the offline phase to respect a first-order derivative equality between the score of the policy and action-gradient of the Q-function. We experimentally demonstrate that SMAC converges to offline maxima that are connected to better online maxima via paths with monotonically increasing reward found by first-order optimization. SMAC achieves smooth transfer to Soft Actor-Critic and TD3 in 6/6 D4RL tasks. In 4/6 environments, it reduces regret by 34-58% over the best baseline. Subjects: Machine Learning (cs.LG) ; Artificial Intelligence (cs.AI) Cite as: arXiv:2602.17632 [cs.LG] (or arXiv:2602.17632v1 [cs.LG] for this version) https://doi.org/10.48550/arXiv.2602.17632 Focus to learn more arXiv-issued DOI via DataCite (pending registration) Submission history From: Nathan De Lara [ view email ] [v1] Thu, 19 Feb 2026 18:47:31 UTC (929 KB) Full-text links: Access Paper: View a PDF of the paper titled SMAC: Score-Matched Actor-Critics for Robust Offline-to-Online Transfer, by Nathan S. de Lara and Florian Shkurti View PDF HTML (experimental) TeX Source view license Current browse context: cs.LG &lt; prev | next &gt; new | recent | 2026-02 Change to browse by: cs cs.AI References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) IArxiv recommender toggle IArxiv Recommender ( What is IArxiv? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-44">23. Approaching the Limit in Multiparameter AC Magnetometry with Quantum Control</h2>
<ul>
<li>链接：https://arxiv.org/abs/2602.17648v1</li>
<li>来源：arxiv</li>
<li>摘要：Simultaneously estimating multiple parameters at the ultimate limit is a central challenge in quantum metrology, often hindered by inherent incompatibilities in optimal estimation strategies. At its most extreme, this incompatibility culminates in a fundamental impossibility when the quantum Fisher information matrix (QFIM) becomes singular, rendering joint estimation unattainable. This is the case for a canonical problem: estimating the amplitude and frequency of an AC magnetic field, where the generators are parallel to each other. Here, we introduce a quantum control protocol that resolves this singularity. Our control protocol strategically engineers the sensor's time evolution so the generators for the two parameters become orthogonal. It not only removes the singularity but also restores the optimal scaling of precision with interrogation time for both parameters simultaneously. We experimentally validate this protocol using a nitrogen-vacancy center in diamond at room temperature, demonstrating the concurrent achievement of the optimal scaling for both parameters under realistic conditions.</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">量子 Fisher 信息矩阵（QFIM）的奇异会导致联合估计无法实现，特别是在估计 AC 磁场的幅度和频率时，这种奇异问题尤为突出。为了解决这一问题，量子控制协议通过使两个参数的生成器正交来优化生成器之间的关系，从而恢复了两个参数同时的最优精度缩放。实验通过在金刚石中的氮空位中心进行验证，进一步证明了该量子控制协议的有效性。因此，在实际条件下，奇异问题的解决不仅恢复了两个参数的最优精度缩放，还确保了实验的准确性和可靠性。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>量子 Fisher 信息矩阵（QFIM）的奇异会导致联合估计无法实现。</li>
<li>当两个参数的生成器平行时，估计 AC 磁场的幅度和频率会出现奇异问题。</li>
<li>量子控制协议通过使两个参数的生成器正交来解决奇异问题。</li>
<li>奇异问题的解决不仅恢复了两个参数同时的最优精度缩放。</li>
<li>实验使用金刚石中的氮空位中心验证了该量子控制协议。</li>
<li>在实际条件下，奇异问题的解决同时实现了两个参数的最优精度缩放。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-45">正文（抓取，非 AI）</h3>
<p>[2602.17648v1] Approaching the Limit in Multiparameter AC Magnetometry with Quantum Control Quantum Physics arXiv:2602.17648v1 (quant-ph) [Submitted on 19 Feb 2026] Title: Approaching the Limit in Multiparameter AC Magnetometry with Quantum Control Authors: Takuya Isogawa , Zhiyao Hu , Ayumi Kanamoto , Nutdech Phadetsuwannukun , Shilin Wang , Shunsuke Nishimura , Boning Li , Liang Jiang , Zain H. Saleem , Guoqing Wang , Haidong Yuan , Paola Cappellaro View a PDF of the paper titled Approaching the Limit in Multiparameter AC Magnetometry with Quantum Control, by Takuya Isogawa and 11 other authors View PDF HTML (experimental) Abstract: Simultaneously estimating multiple parameters at the ultimate limit is a central challenge in quantum metrology, often hindered by inherent incompatibilities in optimal estimation strategies. At its most extreme, this incompatibility culminates in a fundamental impossibility when the quantum Fisher information matrix (QFIM) becomes singular, rendering joint estimation unattainable. This is the case for a canonical problem: estimating the amplitude and frequency of an AC magnetic field, where the generators are parallel to each other. Here, we introduce a quantum control protocol that resolves this singularity. Our control protocol strategically engineers the sensor's time evolution so the generators for the two parameters become orthogonal. It not only removes the singularity but also restores the optimal scaling of precision with interrogation time for both parameters simultaneously. We experimentally validate this protocol using a nitrogen-vacancy center in diamond at room temperature, demonstrating the concurrent achievement of the optimal scaling for both parameters under realistic conditions. Comments: 13 pages, 7 figures Subjects: Quantum Physics (quant-ph) Cite as: arXiv:2602.17648 [quant-ph] (or arXiv:2602.17648v1 [quant-ph] for this version) https://doi.org/10.48550/arXiv.2602.17648 Focus to learn more arXiv-issued DOI via DataCite (pending registration) Submission history From: Takuya Isogawa [ view email ] [v1] Thu, 19 Feb 2026 18:55:17 UTC (2,767 KB) Full-text links: Access Paper: View a PDF of the paper titled Approaching the Limit in Multiparameter AC Magnetometry with Quantum Control, by Takuya Isogawa and 11 other authors View PDF HTML (experimental) TeX Source view license Current browse context: quant-ph &lt; prev | next &gt; new | recent | 2026-02 References &amp; Citations INSPIRE HEP NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-46">24. When to Trust the Cheap Check: Weak and Strong Verification for Reasoning</h2>
<ul>
<li>链接：https://arxiv.org/abs/2602.17633v1</li>
<li>来源：arxiv</li>
<li>摘要：Reasoning with LLMs increasingly unfolds inside a broader verification loop. Internally, systems use cheap checks, such as self-consistency or proxy rewards, which we call weak verification. Externally, users inspect outputs and steer the model through feedback until results are trustworthy, which we call strong verification. These signals differ sharply in cost and reliability: strong verification can establish trust but is resource-intensive, while weak verification is fast and scalable but noisy and imperfect. We formalize this tension through weak--strong verification policies, which decide when to accept or reject based on weak verification and when to defer to strong verification. We introduce metrics capturing incorrect acceptance, incorrect rejection, and strong-verification frequency. Over population, we show that optimal policies admit a two-threshold structure and that calibration and sharpness govern the value of weak verifiers. Building on this, we develop an online algorithm that provably controls acceptance and rejection errors without assumptions on the query stream, the language model, or the weak verifier.</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">首先，弱验证政策因其成本低而显得具有吸引力，但其可靠性较差，这使得它在决策时存在一定的风险。相比之下，强验证政策虽然成本高昂，但能够显著提高决策的可靠性，从而建立信任。最优策略通常具有两阈值结构，即在不同情况下分别采用弱验证和强验证。校准和锐度对弱验证器的价值有着重要影响，它们能够提高弱验证器的准确性，从而降低风险。在线算法能够动态地控制接受和拒绝错误，使得决策过程更加灵活。弱验证器的频率也会影响结果，高频率的弱验证可能会导致更多的错误接受或拒绝。此外，用户反馈是强验证的重要组成部分，通过收集和分析用户反馈，可以进一步提高强验证的准确性，从而增强整体决策的可靠性。因此，通过合理配置弱验证和强验证的阈值，结合校准、锐度、在线算法以及用户反馈，可以实现最优的决策策略。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>weak verification成本低但可靠性差。</li>
<li>strong verification成本高但能建立信任。</li>
<li>弱验证政策决定何时接受或拒绝。</li>
<li>强验证政策决定何时求助于强验证。</li>
<li>最优策略具有两阈值结构。</li>
<li>校准和锐度影响弱验证器的价值。</li>
<li>在线算法能控制接受和拒绝错误。</li>
<li>弱验证器的频率会影响结果。</li>
<li>用户反馈是强验证的重要部分。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-47">正文（抓取，非 AI）</h3>
<p>[2602.17633v1] When to Trust the Cheap Check: Weak and Strong Verification for Reasoning Computer Science &gt; Machine Learning arXiv:2602.17633v1 (cs) [Submitted on 19 Feb 2026] Title: When to Trust the Cheap Check: Weak and Strong Verification for Reasoning Authors: Shayan Kiyani , Sima Noorani , George Pappas , Hamed Hassani View a PDF of the paper titled When to Trust the Cheap Check: Weak and Strong Verification for Reasoning, by Shayan Kiyani and 3 other authors View PDF HTML (experimental) Abstract: Reasoning with LLMs increasingly unfolds inside a broader verification loop. Internally, systems use cheap checks, such as self-consistency or proxy rewards, which we call weak verification. Externally, users inspect outputs and steer the model through feedback until results are trustworthy, which we call strong verification. These signals differ sharply in cost and reliability: strong verification can establish trust but is resource-intensive, while weak verification is fast and scalable but noisy and imperfect. We formalize this tension through weak--strong verification policies, which decide when to accept or reject based on weak verification and when to defer to strong verification. We introduce metrics capturing incorrect acceptance, incorrect rejection, and strong-verification frequency. Over population, we show that optimal policies admit a two-threshold structure and that calibration and sharpness govern the value of weak verifiers. Building on this, we develop an online algorithm that provably controls acceptance and rejection errors without assumptions on the query stream, the language model, or the weak verifier. Subjects: Machine Learning (cs.LG) ; Artificial Intelligence (cs.AI); Machine Learning (stat.ML) Cite as: arXiv:2602.17633 [cs.LG] (or arXiv:2602.17633v1 [cs.LG] for this version) https://doi.org/10.48550/arXiv.2602.17633 Focus to learn more arXiv-issued DOI via DataCite (pending registration) Submission history From: Shayan Kiyani [ view email ] [v1] Thu, 19 Feb 2026 18:47:38 UTC (1,849 KB) Full-text links: Access Paper: View a PDF of the paper titled When to Trust the Cheap Check: Weak and Strong Verification for Reasoning, by Shayan Kiyani and 3 other authors View PDF HTML (experimental) TeX Source view license Current browse context: cs.LG &lt; prev | next &gt; new | recent | 2026-02 Change to browse by: cs cs.AI stat stat.ML References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) IArxiv recommender toggle IArxiv Recommender ( What is IArxiv? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-48">25. Reverso: Efficient Time Series Foundation Models for Zero-shot Forecasting</h2>
<ul>
<li>链接：https://arxiv.org/abs/2602.17634v1</li>
<li>来源：arxiv</li>
<li>摘要：Learning time series foundation models has been shown to be a promising approach for zero-shot time series forecasting across diverse time series domains. Insofar as scaling has been a critical driver of performance of foundation models in other modalities such as language and vision, much recent work on time series foundation modeling has focused on scaling. This has resulted in time series foundation models with hundreds of millions of parameters that are, while performant, inefficient and expensive to use in practice. This paper describes a simple recipe for learning efficient foundation models for zero-shot time series forecasting that are orders of magnitude smaller. We show that large-scale transformers are not necessary: small hybrid models that interleave long convolution and linear RNN layers (in particular DeltaNet layers) can match the performance of larger transformer-based models while being more than a hundred times smaller. We also describe several data augmentation and inference strategies that further improve performance. This recipe results in Reverso, a family of efficient time series foundation models for zero-shot forecasting that significantly push the perform</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">首先，大规模变压器并非必要，小规模混合模型能够匹配甚至超越更大模型的性能，且体积可以小上百倍，这使得模型更加轻量化。其次，高效时间序列基础模型，如Reverso家族，能够显著提升性能-效率的帕累托边界，特别适用于零样本预测。此外，长卷积和线性RNN层（尤其是DeltaNet层）的结合，进一步优化了模型的效率。因此，通过数据增强和改进的推理策略，可以进一步提高模型的性能，确保在资源有限的情况下也能达到最佳效果。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>大规模变压器并非必要：小规模混合模型可以匹配更大模型的性能。</li>
<li>小规模混合模型可以比大规模模型小上百倍。</li>
<li>高效时间序列基础模型可以显著提升性能-效率帕累托边界。</li>
<li>长卷积和线性RNN层（特别是DeltaNet层）的组合可以实现高效模型。</li>
<li>数据增强和推理策略可以进一步提高性能。</li>
<li>Reverso是高效时间序列基础模型的家族，适用于零样本预测。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-49">正文（抓取，非 AI）</h3>
<p>[2602.17634v1] Reverso: Efficient Time Series Foundation Models for Zero-shot Forecasting Computer Science &gt; Machine Learning arXiv:2602.17634v1 (cs) [Submitted on 19 Feb 2026] Title: Reverso: Efficient Time Series Foundation Models for Zero-shot Forecasting Authors: Xinghong Fu , Yanhong Li , Georgios Papaioannou , Yoon Kim View a PDF of the paper titled Reverso: Efficient Time Series Foundation Models for Zero-shot Forecasting, by Xinghong Fu and 3 other authors View PDF HTML (experimental) Abstract: Learning time series foundation models has been shown to be a promising approach for zero-shot time series forecasting across diverse time series domains. Insofar as scaling has been a critical driver of performance of foundation models in other modalities such as language and vision, much recent work on time series foundation modeling has focused on scaling. This has resulted in time series foundation models with hundreds of millions of parameters that are, while performant, inefficient and expensive to use in practice. This paper describes a simple recipe for learning efficient foundation models for zero-shot time series forecasting that are orders of magnitude smaller. We show that large-scale transformers are not necessary: small hybrid models that interleave long convolution and linear RNN layers (in particular DeltaNet layers) can match the performance of larger transformer-based models while being more than a hundred times smaller. We also describe several data augmentation and inference strategies that further improve performance. This recipe results in Reverso, a family of efficient time series foundation models for zero-shot forecasting that significantly push the performance-efficiency Pareto frontier. Subjects: Machine Learning (cs.LG) ; Artificial Intelligence (cs.AI) Cite as: arXiv:2602.17634 [cs.LG] (or arXiv:2602.17634v1 [cs.LG] for this version) https://doi.org/10.48550/arXiv.2602.17634 Focus to learn more arXiv-issued DOI via DataCite (pending registration) Submission history From: Xinghong Fu [ view email ] [v1] Thu, 19 Feb 2026 18:48:08 UTC (7,322 KB) Full-text links: Access Paper: View a PDF of the paper titled Reverso: Efficient Time Series Foundation Models for Zero-shot Forecasting, by Xinghong Fu and 3 other authors View PDF HTML (experimental) TeX Source view license Current browse context: cs.LG &lt; prev | next &gt; new | recent | 2026-02 Change to browse by: cs cs.AI References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) IArxiv recommender toggle IArxiv Recommender ( What is IArxiv? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-50">26. Modeling Distinct Human Interaction in Web Agents</h2>
<ul>
<li>链接：https://arxiv.org/abs/2602.17588v1</li>
<li>来源：arxiv</li>
<li>摘要：Despite rapid progress in autonomous web agents, human involvement remains essential for shaping preferences and correcting agent behavior as tasks unfold. However, current agentic systems lack a principled understanding of when and why humans intervene, often proceeding autonomously past critical decision points or requesting unnecessary confirmation. In this work, we introduce the task of modeling human intervention to support collaborative web task execution. We collect CowCorpus, a dataset of 400 real-user web navigation trajectories containing over 4,200 interleaved human and agent actions. We identify four distinct patterns of user interaction with agents -- hands-off supervision, hands-on oversight, collaborative task-solving, and full user takeover. Leveraging these insights, we train language models (LMs) to anticipate when users are likely to intervene based on their interaction styles, yielding a 61.4-63.4% improvement in intervention prediction accuracy over base LMs. Finally, we deploy these intervention-aware models in live web navigation agents and evaluate them in a user study, finding a 26.5% increase in user-rated agent usefulness. Together, our results show struc</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">human intervention is critical for shaping preferences and correcting agent behavior, as current agentic systems often proceed autonomously past critical decision points. To address this, various human intervention patterns have been identified, including hands-off supervision, hands-on oversight, collaborative task-solving, and full user takeover. These patterns are important because they allow for a more adaptive and collaborative approach to managing agents. Additionally, language models can predict when users are likely to intervene based on their interaction styles, which can further enhance the effectiveness of human oversight. As a result, intervention-aware models have been shown to improve user-rated agent usefulness by 26.5%, highlighting the importance of structured modeling of human intervention in developing more effective and collaborative agents.</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>human intervention is critical for shaping preferences and correcting agent behavior.</li>
<li>current agentic systems often proceed autonomously past critical decision points.</li>
<li>human intervention patterns include hands-off supervision, hands-on oversight, collaborative task-solving, and full user takeover.</li>
<li>language models can predict when users are likely to intervene based on their interaction styles.</li>
<li>intervention-aware models improve user-rated agent usefulness by 26.5%.</li>
<li>structured modeling of human intervention leads to more adaptive, collaborative agents.</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-51">正文（抓取，非 AI）</h3>
<p>[2602.17588v1] Modeling Distinct Human Interaction in Web Agents Computer Science &gt; Computation and Language arXiv:2602.17588v1 (cs) [Submitted on 19 Feb 2026] Title: Modeling Distinct Human Interaction in Web Agents Authors: Faria Huq , Zora Zhiruo Wang , Zhanqiu Guo , Venu Arvind Arangarajan , Tianyue Ou , Frank Xu , Shuyan Zhou , Graham Neubig , Jeffrey P. Bigham View a PDF of the paper titled Modeling Distinct Human Interaction in Web Agents, by Faria Huq and 8 other authors View PDF HTML (experimental) Abstract: Despite rapid progress in autonomous web agents, human involvement remains essential for shaping preferences and correcting agent behavior as tasks unfold. However, current agentic systems lack a principled understanding of when and why humans intervene, often proceeding autonomously past critical decision points or requesting unnecessary confirmation. In this work, we introduce the task of modeling human intervention to support collaborative web task execution. We collect CowCorpus, a dataset of 400 real-user web navigation trajectories containing over 4,200 interleaved human and agent actions. We identify four distinct patterns of user interaction with agents -- hands-off supervision, hands-on oversight, collaborative task-solving, and full user takeover. Leveraging these insights, we train language models (LMs) to anticipate when users are likely to intervene based on their interaction styles, yielding a 61.4-63.4% improvement in intervention prediction accuracy over base LMs. Finally, we deploy these intervention-aware models in live web navigation agents and evaluate them in a user study, finding a 26.5% increase in user-rated agent usefulness. Together, our results show structured modeling of human intervention leads to more adaptive, collaborative agents. Comments: Preprint Subjects: Computation and Language (cs.CL) ; Human-Computer Interaction (cs.HC) Cite as: arXiv:2602.17588 [cs.CL] (or arXiv:2602.17588v1 [cs.CL] for this version) https://doi.org/10.48550/arXiv.2602.17588 Focus to learn more arXiv-issued DOI via DataCite (pending registration) Submission history From: Faria Huq [ view email ] [v1] Thu, 19 Feb 2026 18:11:28 UTC (14,011 KB) Full-text links: Access Paper: View a PDF of the paper titled Modeling Distinct Human Interaction in Web Agents, by Faria Huq and 8 other authors View PDF HTML (experimental) TeX Source view license Current browse context: cs.CL &lt; prev | next &gt; new | recent | 2026-02 Change to browse by: cs cs.HC References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-52">27. CLEF HIPE-2026: Evaluating Accurate and Efficient Person-Place Relation Extraction from Multilingual Historical Texts</h2>
<ul>
<li>链接：https://arxiv.org/abs/2602.17663v1</li>
<li>来源：arxiv</li>
<li>摘要：HIPE-2026 is a CLEF evaluation lab dedicated to person-place relation extraction from noisy, multilingual historical texts. Building on the HIPE-2020 and HIPE-2022 campaigns, it extends the series toward semantic relation extraction by targeting the task of identifying person--place associations in multiple languages and time periods. Systems are asked to classify relations of two types - $at$ ("Has the person ever been at this place?") and $isAt$ ("Is the person located at this place around publication time?") - requiring reasoning over temporal and geographical cues. The lab introduces a three-fold evaluation profile that jointly assesses accuracy, computational efficiency, and domain generalization. By linking relation extraction to large-scale historical data processing, HIPE-2026 aims to support downstream applications in knowledge-graph construction, historical biography reconstruction, and spatial analysis in digital humanities.</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">HIPE-2026 旨在处理多语言历史文本中的噪声数据，通过识别不同语言和时间周期中的个人与地点关联，扩展了先前的系列。该系统要求分类两种类型的关系："at" 和 "isAt"，以确保准确地捕捉历史文本中的时空信息。HIPE-2026 引入了三方面的评估标准：准确度、计算效率和领域泛化能力，以确保系统在大规模历史数据处理中的表现。通过支持知识图构建、历史传记重建和数字人文中的空间分析，HIPE-2026 不仅强调了语言多样性的重要性，还突显了时间线索和地理位置线索在关系提取中的关键作用。因此，HIPE-2026 为多语言历史文本处理提供了全面而有效的解决方案。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>HIPE-2026 旨在处理多语言历史文本中的噪声数据。</li>
<li>HIPE-2026 通过识别不同语言和时间周期中的个人与地点关联，扩展了先前的系列。</li>
<li>HIPE-2026 要求系统分类两种类型的关系："at" 和 "isAt"。</li>
<li>HIPE-2026 引入了三方面的评估标准：准确度、计算效率和领域泛化能力。</li>
<li>HIPE-2026 通过大规模历史数据处理支持知识图构建、历史传记重建和数字人文中的空间分析。</li>
<li>HIPE-2026 通过多语言文本处理强调了语言多样性的重要性。</li>
<li>HIPE-2026 强调了时间线索在关系提取中的重要性。</li>
<li>HIPE-2026 强调了地理位置线索在关系提取中的重要性。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-53">正文（抓取，非 AI）</h3>
<p>[2602.17663v1] CLEF HIPE-2026: Evaluating Accurate and Efficient Person-Place Relation Extraction from Multilingual Historical Texts Computer Science &gt; Artificial Intelligence arXiv:2602.17663v1 (cs) [Submitted on 19 Feb 2026] Title: CLEF HIPE-2026: Evaluating Accurate and Efficient Person-Place Relation Extraction from Multilingual Historical Texts Authors: Juri Opitz , Corina Raclé , Emanuela Boros , Andrianos Michail , Matteo Romanello , Maud Ehrmann , Simon Clematide View a PDF of the paper titled CLEF HIPE-2026: Evaluating Accurate and Efficient Person-Place Relation Extraction from Multilingual Historical Texts, by Juri Opitz and 6 other authors View PDF HTML (experimental) Abstract: HIPE-2026 is a CLEF evaluation lab dedicated to person-place relation extraction from noisy, multilingual historical texts. Building on the HIPE-2020 and HIPE-2022 campaigns, it extends the series toward semantic relation extraction by targeting the task of identifying person--place associations in multiple languages and time periods. Systems are asked to classify relations of two types - $at$ ("Has the person ever been at this place?") and $isAt$ ("Is the person located at this place around publication time?") - requiring reasoning over temporal and geographical cues. The lab introduces a three-fold evaluation profile that jointly assesses accuracy, computational efficiency, and domain generalization. By linking relation extraction to large-scale historical data processing, HIPE-2026 aims to support downstream applications in knowledge-graph construction, historical biography reconstruction, and spatial analysis in digital humanities. Comments: ECIR 2026. CLEF Evaluation Lab. Registration DL: 2026/04/23. Task Homepage at this https URL Subjects: Artificial Intelligence (cs.AI) ; Computation and Language (cs.CL); Information Retrieval (cs.IR) Cite as: arXiv:2602.17663 [cs.AI] (or arXiv:2602.17663v1 [cs.AI] for this version) https://doi.org/10.48550/arXiv.2602.17663 Focus to learn more arXiv-issued DOI via DataCite (pending registration) Submission history From: Juri Opitz [ view email ] [v1] Thu, 19 Feb 2026 18:59:44 UTC (175 KB) Full-text links: Access Paper: View a PDF of the paper titled CLEF HIPE-2026: Evaluating Accurate and Efficient Person-Place Relation Extraction from Multilingual Historical Texts, by Juri Opitz and 6 other authors View PDF HTML (experimental) TeX Source view license Current browse context: cs.AI &lt; prev | next &gt; new | recent | 2026-02 Change to browse by: cs cs.CL cs.IR References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-54">28. Multi-Round Human-AI Collaboration with User-Specified Requirements</h2>
<ul>
<li>链接：https://arxiv.org/abs/2602.17646v1</li>
<li>来源：arxiv</li>
<li>摘要：As humans increasingly rely on multiround conversational AI for high stakes decisions, principled frameworks are needed to ensure such interactions reliably improve decision quality. We adopt a human centric view governed by two principles: counterfactual harm, ensuring the AI does not undermine human strengths, and complementarity, ensuring it adds value where the human is prone to err. We formalize these concepts via user defined rules, allowing users to specify exactly what harm and complementarity mean for their specific task. We then introduce an online, distribution free algorithm with finite sample guarantees that enforces the user-specified constraints over the collaboration dynamics. We evaluate our framework across two interactive settings: LLM simulated collaboration on a medical diagnostic task and a human crowdsourcing study on a pictorial reasoning task. We show that our online procedure maintains prescribed counterfactual harm and complementarity violation rates even under nonstationary interaction dynamics. Moreover, tightening or loosening these constraints produces predictable shifts in downstream human accuracy, confirming that the two principles serve as practic</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">用户可以定义具体任务中的危害和互补性的含义，这使得算法能够在线执行用户指定的约束，从而确保AI在人类容易出错的地方增加价值，同时不削弱人类的优势。框架在非平稳交互动力学下仍能维持规定的反事实危害和互补性违反率，这意味着即使在动态变化的环境中，也能保证AI的行为符合用户的需求。此外，用户可以调整约束以预测性地影响下游的人类准确性，从而引导多轮合作，无需建模或约束人类行为。该方法在两个交互设置中进行了评估：一个是LLM模拟的医疗诊断任务，另一个是人类众包的图片推理任务，展示了其广泛适用性和有效性。因此，这种框架不仅能够确保AI与人类的互补性，还能有效减少反事实危害，从而提升整体系统的性能。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>用户可以定义具体任务中的危害和互补性的含义。</li>
<li>算法具有有限样本保证，能在线执行用户指定的约束。</li>
<li>框架在非平稳交互动力学下仍能维持规定的反事实危害和互补性违反率。</li>
<li>用户可以调整约束以预测性地影响下游的人类准确性。</li>
<li>该方法无需建模或约束人类行为即可引导多轮合作。</li>
<li>互补性确保AI在人类容易出错的地方增加价值。</li>
<li>反事实危害确保AI不削弱人类的优势。</li>
<li>框架在两个交互设置中进行了评估：LLM模拟的医疗诊断任务和人类众包的图片推理任务。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-55">正文（抓取，非 AI）</h3>
<p>[2602.17646v1] Multi-Round Human-AI Collaboration with User-Specified Requirements Computer Science &gt; Machine Learning arXiv:2602.17646v1 (cs) [Submitted on 19 Feb 2026] Title: Multi-Round Human-AI Collaboration with User-Specified Requirements Authors: Sima Noorani , Shayan Kiyani , Hamed Hassani , George Pappas View a PDF of the paper titled Multi-Round Human-AI Collaboration with User-Specified Requirements, by Sima Noorani and 3 other authors View PDF HTML (experimental) Abstract: As humans increasingly rely on multiround conversational AI for high stakes decisions, principled frameworks are needed to ensure such interactions reliably improve decision quality. We adopt a human centric view governed by two principles: counterfactual harm, ensuring the AI does not undermine human strengths, and complementarity, ensuring it adds value where the human is prone to err. We formalize these concepts via user defined rules, allowing users to specify exactly what harm and complementarity mean for their specific task. We then introduce an online, distribution free algorithm with finite sample guarantees that enforces the user-specified constraints over the collaboration dynamics. We evaluate our framework across two interactive settings: LLM simulated collaboration on a medical diagnostic task and a human crowdsourcing study on a pictorial reasoning task. We show that our online procedure maintains prescribed counterfactual harm and complementarity violation rates even under nonstationary interaction dynamics. Moreover, tightening or loosening these constraints produces predictable shifts in downstream human accuracy, confirming that the two principles serve as practical levers for steering multi-round collaboration toward better decision quality without the need to model or constrain human behavior. Subjects: Machine Learning (cs.LG) Cite as: arXiv:2602.17646 [cs.LG] (or arXiv:2602.17646v1 [cs.LG] for this version) https://doi.org/10.48550/arXiv.2602.17646 Focus to learn more arXiv-issued DOI via DataCite (pending registration) Submission history From: Sima Noorani [ view email ] [v1] Thu, 19 Feb 2026 18:54:34 UTC (3,510 KB) Full-text links: Access Paper: View a PDF of the paper titled Multi-Round Human-AI Collaboration with User-Specified Requirements, by Sima Noorani and 3 other authors View PDF HTML (experimental) TeX Source view license Current browse context: cs.LG &lt; prev | next &gt; new | recent | 2026-02 Change to browse by: cs References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) IArxiv recommender toggle IArxiv Recommender ( What is IArxiv? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-56">29. Position: Evaluation of ECG Representations Must Be Fixed</h2>
<ul>
<li>链接：https://arxiv.org/abs/2602.17531v1</li>
<li>来源：arxiv</li>
<li>摘要：This position paper argues that current benchmarking practice in 12-lead ECG representation learning must be fixed to ensure progress is reliable and aligned with clinically meaningful objectives. The field has largely converged on three public multi-label benchmarks (PTB-XL, CPSC2018, CSN) dominated by arrhythmia and waveform-morphology labels, even though the ECG is known to encode substantially broader clinical information. We argue that downstream evaluation should expand to include an assessment of structural heart disease and patient-level forecasting, in addition to other evolving ECG-related endpoints, as relevant clinical targets. Next, we outline evaluation best practices for multi-label, imbalanced settings, and show that when they are applied, the literature's current conclusion about which representations perform best is altered. Furthermore, we demonstrate the surprising result that a randomly initialized encoder with linear evaluation matches state-of-the-art pre-training on many tasks. This motivates the use of a random encoder as a reasonable baseline model. We substantiate our observations with an empirical evaluation of three representative ECG pre-training appro</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">下游评估应扩展到包括结构性心脏病和患者级预测的评估，这是因为当前基准实践未能充分利用ECG的广泛临床信息，而现有的基准主要关注心律失常和波形形态标签。为了改进这一状况，随机初始化的编码器可以作为合理的基线模型，与最先进的预训练匹配。因此，评估应考虑多标签、不平衡设置的最佳实践，以确保全面且准确地利用ECG数据的临床价值。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>下游评估应扩展到包括结构性心脏病和患者级预测的评估。</li>
<li>随机初始化的编码器在许多任务上与最先进的预训练匹配。</li>
<li>当前基准实践未能充分利用ECG的广泛临床信息。</li>
<li>现有的基准主要关注心律失常和波形形态标签。</li>
<li>随机编码器可以作为合理的基线模型。</li>
<li>评估应考虑多标签、不平衡设置的最佳实践。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-57">正文（抓取，非 AI）</h3>
<p>[2602.17531v1] Position: Evaluation of ECG Representations Must Be Fixed Computer Science &gt; Machine Learning arXiv:2602.17531v1 (cs) [Submitted on 19 Feb 2026] Title: Position: Evaluation of ECG Representations Must Be Fixed Authors: Zachary Berger , Daniel Prakah-Asante , John Guttag , Collin M. Stultz View a PDF of the paper titled Position: Evaluation of ECG Representations Must Be Fixed, by Zachary Berger and 3 other authors View PDF Abstract: This position paper argues that current benchmarking practice in 12-lead ECG representation learning must be fixed to ensure progress is reliable and aligned with clinically meaningful objectives. The field has largely converged on three public multi-label benchmarks (PTB-XL, CPSC2018, CSN) dominated by arrhythmia and waveform-morphology labels, even though the ECG is known to encode substantially broader clinical information. We argue that downstream evaluation should expand to include an assessment of structural heart disease and patient-level forecasting, in addition to other evolving ECG-related endpoints, as relevant clinical targets. Next, we outline evaluation best practices for multi-label, imbalanced settings, and show that when they are applied, the literature's current conclusion about which representations perform best is altered. Furthermore, we demonstrate the surprising result that a randomly initialized encoder with linear evaluation matches state-of-the-art pre-training on many tasks. This motivates the use of a random encoder as a reasonable baseline model. We substantiate our observations with an empirical evaluation of three representative ECG pre-training approaches across six evaluation settings: the three standard benchmarks, a structural disease dataset, hemodynamic inference, and patient forecasting. Comments: Project website at this https URL Subjects: Machine Learning (cs.LG) ; Artificial Intelligence (cs.AI) Cite as: arXiv:2602.17531 [cs.LG] (or arXiv:2602.17531v1 [cs.LG] for this version) https://doi.org/10.48550/arXiv.2602.17531 Focus to learn more arXiv-issued DOI via DataCite (pending registration) Submission history From: Zachary Berger [ view email ] [v1] Thu, 19 Feb 2026 16:42:46 UTC (490 KB) Full-text links: Access Paper: View a PDF of the paper titled Position: Evaluation of ECG Representations Must Be Fixed, by Zachary Berger and 3 other authors View PDF TeX Source view license Current browse context: cs.LG &lt; prev | next &gt; new | recent | 2026-02 Change to browse by: cs cs.AI References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) IArxiv recommender toggle IArxiv Recommender ( What is IArxiv? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-58">30. Towards Anytime-Valid Statistical Watermarking</h2>
<ul>
<li>链接：https://arxiv.org/abs/2602.17608v1</li>
<li>来源：arxiv</li>
<li>摘要：The proliferation of Large Language Models (LLMs) necessitates efficient mechanisms to distinguish machine-generated content from human text. While statistical watermarking has emerged as a promising solution, existing methods suffer from two critical limitations: the lack of a principled approach for selecting sampling distributions and the reliance on fixed-horizon hypothesis testing, which precludes valid early stopping. In this paper, we bridge this gap by developing the first e-value-based watermarking framework, Anchored E-Watermarking, that unifies optimal sampling with anytime-valid inference. Unlike traditional approaches where optional stopping invalidates Type-I error guarantees, our framework enables valid, anytime-inference by constructing a test supermartingale for the detection process. By leveraging an anchor distribution to approximate the target model, we characterize the optimal e-value with respect to the worst-case log-growth rate and derive the optimal expected stopping time. Our theoretical claims are substantiated by simulations and evaluations on established benchmarks, showing that our framework can significantly enhance sample efficiency, reducing the ave</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">固定区间假设检验排除了有效的提前停止，而可选停止则破坏了第一类错误的保证。为了解决这些问题，超鞅构造允许进行有效的、随时进行的推断。通过锚定分布，可以近似目标模型，从而为推断提供坚实的基础。最优e值则刻画了最坏情况下的对数增长速率，进一步推导出最优的预期停止时间，显著提升了样本效率。具体而言，基于e值的框架不仅减少了平均令牌预算13-15%，还统一了最优采样与随时有效的推断。因此，这一系列方法不仅解决了假设检验中的问题，还通过统一最优采样与推断，实现了样本效率的大幅提升。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>fixed-horizon hypothesis testing precludes valid early stopping</li>
<li>optional stopping invalidates Type-I error guarantees</li>
<li>supermartingale construction enables valid, anytime-inference</li>
<li>anchor distribution approximates the target model</li>
<li>optimal e-value characterizes worst-case log-growth rate</li>
<li>optimal expected stopping time is derived</li>
<li>sample efficiency is significantly enhanced</li>
<li>average token budget is reduced by 13-15%</li>
<li>e-value-based framework unifies optimal sampling</li>
<li>anytime-valid inference is unified with optimal sampling</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-59">正文（抓取，非 AI）</h3>
<p>[2602.17608v1] Towards Anytime-Valid Statistical Watermarking Computer Science &gt; Machine Learning arXiv:2602.17608v1 (cs) [Submitted on 19 Feb 2026] Title: Towards Anytime-Valid Statistical Watermarking Authors: Baihe Huang , Eric Xu , Kannan Ramchandran , Jiantao Jiao , Michael I. Jordan View a PDF of the paper titled Towards Anytime-Valid Statistical Watermarking, by Baihe Huang and Eric Xu and Kannan Ramchandran and Jiantao Jiao and Michael I. Jordan View PDF Abstract: The proliferation of Large Language Models (LLMs) necessitates efficient mechanisms to distinguish machine-generated content from human text. While statistical watermarking has emerged as a promising solution, existing methods suffer from two critical limitations: the lack of a principled approach for selecting sampling distributions and the reliance on fixed-horizon hypothesis testing, which precludes valid early stopping. In this paper, we bridge this gap by developing the first e-value-based watermarking framework, Anchored E-Watermarking, that unifies optimal sampling with anytime-valid inference. Unlike traditional approaches where optional stopping invalidates Type-I error guarantees, our framework enables valid, anytime-inference by constructing a test supermartingale for the detection process. By leveraging an anchor distribution to approximate the target model, we characterize the optimal e-value with respect to the worst-case log-growth rate and derive the optimal expected stopping time. Our theoretical claims are substantiated by simulations and evaluations on established benchmarks, showing that our framework can significantly enhance sample efficiency, reducing the average token budget required for detection by 13-15% relative to state-of-the-art baselines. Subjects: Machine Learning (cs.LG) ; Artificial Intelligence (cs.AI); Machine Learning (stat.ML) Cite as: arXiv:2602.17608 [cs.LG] (or arXiv:2602.17608v1 [cs.LG] for this version) https://doi.org/10.48550/arXiv.2602.17608 Focus to learn more arXiv-issued DOI via DataCite (pending registration) Submission history From: Baihe Huang [ view email ] [v1] Thu, 19 Feb 2026 18:32:26 UTC (282 KB) Full-text links: Access Paper: View a PDF of the paper titled Towards Anytime-Valid Statistical Watermarking, by Baihe Huang and Eric Xu and Kannan Ramchandran and Jiantao Jiao and Michael I. Jordan View PDF TeX Source view license Current browse context: cs.LG &lt; prev | next &gt; new | recent | 2026-02 Change to browse by: cs cs.AI stat stat.ML References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) IArxiv recommender toggle IArxiv Recommender ( What is IArxiv? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-60">31. Be Wary of Your Time Series Preprocessing</h2>
<ul>
<li>链接：https://arxiv.org/abs/2602.17568v1</li>
<li>来源：arxiv</li>
<li>摘要：Normalization and scaling are fundamental preprocessing steps in time series modeling, yet their role in Transformer-based models remains underexplored from a theoretical perspective. In this work, we present the first formal analysis of how different normalization strategies, specifically instance-based and global scaling, impact the expressivity of Transformer-based architectures for time series representation learning. We propose a novel expressivity framework tailored to time series, which quantifies a model's ability to distinguish between similar and dissimilar inputs in the representation space. Using this framework, we derive theoretical bounds for two widely used normalization methods: Standard and Min-Max scaling. Our analysis reveals that the choice of normalization strategy can significantly influence the model's representational capacity, depending on the task and data characteristics. We complement our theory with empirical validation on classification and forecasting benchmarks using multiple Transformer-based models. Our results show that no single normalization method consistently outperforms others, and in some cases, omitting normalization entirely leads to super</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">normalization 和 scaling 是时间序列建模中的基本预处理步骤，它们对于提升模型的表示能力至关重要。不同的归一化策略可能显著影响模型的表现，因此选择合适的归一化方法是关键。标准和最小-最大归一化方法虽然有理论上限，但没有一种方法能在所有任务中都表现最佳。有时，完全省略归一化步骤反而能获得更好的结果。预处理在时间序列学习中起着关键作用，因此需要根据特定任务和数据集设计更合理的归一化策略，以确保模型的最佳性能。因此，针对不同的应用场景，灵活选择和调整归一化方法是提高模型效果的有效途径。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>normalization 和 scaling 是时间序列建模的基本预处理步骤；不同的归一化策略可能显著影响模型的表示能力；标准和最小-最大归一化方法有理论上限；没有一种归一化方法在所有任务中都表现最佳；有时完全省略归一化可能表现更好；预处理在时间序列学习中起着关键作用；需要为特定任务和数据集设计更合理的归一化策略。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-61">正文（抓取，非 AI）</h3>
<p>[2602.17568v1] Be Wary of Your Time Series Preprocessing Computer Science &gt; Machine Learning arXiv:2602.17568v1 (cs) [Submitted on 19 Feb 2026] Title: Be Wary of Your Time Series Preprocessing Authors: Sofiane Ennadir , Tianze Wang , Oleg Smirnov , Sahar Asadi , Lele Cao View a PDF of the paper titled Be Wary of Your Time Series Preprocessing, by Sofiane Ennadir and 4 other authors View PDF HTML (experimental) Abstract: Normalization and scaling are fundamental preprocessing steps in time series modeling, yet their role in Transformer-based models remains underexplored from a theoretical perspective. In this work, we present the first formal analysis of how different normalization strategies, specifically instance-based and global scaling, impact the expressivity of Transformer-based architectures for time series representation learning. We propose a novel expressivity framework tailored to time series, which quantifies a model's ability to distinguish between similar and dissimilar inputs in the representation space. Using this framework, we derive theoretical bounds for two widely used normalization methods: Standard and Min-Max scaling. Our analysis reveals that the choice of normalization strategy can significantly influence the model's representational capacity, depending on the task and data characteristics. We complement our theory with empirical validation on classification and forecasting benchmarks using multiple Transformer-based models. Our results show that no single normalization method consistently outperforms others, and in some cases, omitting normalization entirely leads to superior performance. These findings highlight the critical role of preprocessing in time series learning and motivate the need for more principled normalization strategies tailored to specific tasks and datasets. Comments: Accepted at the AI4TS workshop at AAAI-26 Subjects: Machine Learning (cs.LG) ; Artificial Intelligence (cs.AI) Cite as: arXiv:2602.17568 [cs.LG] (or arXiv:2602.17568v1 [cs.LG] for this version) https://doi.org/10.48550/arXiv.2602.17568 Focus to learn more arXiv-issued DOI via DataCite (pending registration) Submission history From: Sofiane Ennadir [ view email ] [v1] Thu, 19 Feb 2026 17:23:56 UTC (95 KB) Full-text links: Access Paper: View a PDF of the paper titled Be Wary of Your Time Series Preprocessing, by Sofiane Ennadir and 4 other authors View PDF HTML (experimental) TeX Source view license Current browse context: cs.LG &lt; prev | next &gt; new | recent | 2026-02 Change to browse by: cs cs.AI References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) IArxiv recommender toggle IArxiv Recommender ( What is IArxiv? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-62">32. Mine and Refine: Optimizing Graded Relevance in E-commerce Search Retrieval</h2>
<ul>
<li>链接：https://arxiv.org/abs/2602.17654v1</li>
<li>来源：arxiv</li>
<li>摘要：We propose a two-stage "Mine and Refine" contrastive training framework for semantic text embeddings to enhance multi-category e-commerce search retrieval. Large scale e-commerce search demands embeddings that generalize to long tail, noisy queries while adhering to scalable supervision compatible with product and policy constraints. A practical challenge is that relevance is often graded: users accept substitutes or complements beyond exact matches, and production systems benefit from clear separation of similarity scores across these relevance strata for stable hybrid blending and thresholding. To obtain scalable policy consistent supervision, we fine-tune a lightweight LLM on human annotations under a three-level relevance guideline and further reduce residual noise via engagement driven auditing. In Stage 1, we train a multilingual Siamese two-tower retriever with a label aware supervised contrastive objective that shapes a robust global semantic space. In Stage 2, we mine hard samples via ANN and re-annotate them with the policy aligned LLM, and introduce a multi-class extension of circle loss that explicitly sharpens similarity boundaries between relevance levels, to further </li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">首先，用户接受的匹配不仅限于精确匹配，而是包括替代品或补充品，这使得生产系统能够从清晰分离相似度评分中受益。在这一过程中，通过精细调整和审计获得了一致的政策监督。在模型训练的第一阶段，通过构建一个稳健的全局语义空间来塑造相关性，从而提高模型的性能。在第二阶段，通过挖掘和重新标注困难样本来进一步优化模型。此外，圈损失（Circle Loss）明确地强化了不同相关性级别的相似度边界，进一步提升了模型的区分能力。为了提高模型的鲁棒性，还采用了拼写增强和合成查询生成等技术。最终，通过离线评估和生产A/B测试，模型表现出了统计上显著的改进，证明了这些方法的有效性。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>users accept substitutes or complements beyond exact matches</li>
<li>production systems benefit from clear separation of similarity scores</li>
<li>scalable policy consistent supervision is obtained through fine-tuning and auditing</li>
<li>relevance is shaped by a robust global semantic space in Stage 1</li>
<li>hard samples are mined and re-annotated in Stage 2</li>
<li>circle loss explicitly sharpens similarity boundaries between relevance levels</li>
<li>robustness is improved through spelling augmentation and synthetic query generation</li>
<li>offline evaluations and production A/B tests show statistically significant gains</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-63">正文（抓取，非 AI）</h3>
<p>[2602.17654v1] Mine and Refine: Optimizing Graded Relevance in E-commerce Search Retrieval Computer Science &gt; Information Retrieval arXiv:2602.17654v1 (cs) [Submitted on 19 Feb 2026] Title: Mine and Refine: Optimizing Graded Relevance in E-commerce Search Retrieval Authors: Jiaqi Xi , Raghav Saboo , Luming Chen , Martin Wang , Sudeep Das View a PDF of the paper titled Mine and Refine: Optimizing Graded Relevance in E-commerce Search Retrieval, by Jiaqi Xi and 4 other authors View PDF HTML (experimental) Abstract: We propose a two-stage "Mine and Refine" contrastive training framework for semantic text embeddings to enhance multi-category e-commerce search retrieval. Large scale e-commerce search demands embeddings that generalize to long tail, noisy queries while adhering to scalable supervision compatible with product and policy constraints. A practical challenge is that relevance is often graded: users accept substitutes or complements beyond exact matches, and production systems benefit from clear separation of similarity scores across these relevance strata for stable hybrid blending and thresholding. To obtain scalable policy consistent supervision, we fine-tune a lightweight LLM on human annotations under a three-level relevance guideline and further reduce residual noise via engagement driven auditing. In Stage 1, we train a multilingual Siamese two-tower retriever with a label aware supervised contrastive objective that shapes a robust global semantic space. In Stage 2, we mine hard samples via ANN and re-annotate them with the policy aligned LLM, and introduce a multi-class extension of circle loss that explicitly sharpens similarity boundaries between relevance levels, to further refine and enrich the embedding space. Robustness is additionally improved through additive spelling augmentation and synthetic query generation. Extensive offline evaluations and production A/B tests show that our framework improves retrieval relevance and delivers statistically significant gains in engagement and business impact. Subjects: Information Retrieval (cs.IR) ; Machine Learning (cs.LG) Cite as: arXiv:2602.17654 [cs.IR] (or arXiv:2602.17654v1 [cs.IR] for this version) https://doi.org/10.48550/arXiv.2602.17654 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Jiaqi Xi [ view email ] [v1] Thu, 19 Feb 2026 18:56:36 UTC (443 KB) Full-text links: Access Paper: View a PDF of the paper titled Mine and Refine: Optimizing Graded Relevance in E-commerce Search Retrieval, by Jiaqi Xi and 4 other authors View PDF HTML (experimental) TeX Source view license Current browse context: cs.IR &lt; prev | next &gt; new | recent | 2026-02 Change to browse by: cs cs.LG References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-64">33. FAMOSE: A ReAct Approach to Automated Feature Discovery</h2>
<ul>
<li>链接：https://arxiv.org/abs/2602.17641v1</li>
<li>来源：arxiv</li>
<li>摘要：Feature engineering remains a critical yet challenging bottleneck in machine learning, particularly for tabular data, as identifying optimal features from an exponentially large feature space traditionally demands substantial domain expertise. To address this challenge, we introduce FAMOSE (Feature AugMentation and Optimal Selection agEnt), a novel framework that leverages the ReAct paradigm to autonomously explore, generate, and refine features while integrating feature selection and evaluation tools within an agent architecture. To our knowledge, FAMOSE represents the first application of an agentic ReAct framework to automated feature engineering, especially for both regression and classification tasks. Extensive experiments demonstrate that FAMOSE is at or near the state-of-the-art on classification tasks (especially tasks with more than 10K instances, where ROC-AUC increases 0.23% on average), and achieves the state-of-the-art for regression tasks by reducing RMSE by 2.0% on average, while remaining more robust to errors than other algorithms. We hypothesize that FAMOSE's strong performance is because ReAct allows the LLM context window to record (via iterative feature discove</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">首先，FAMOSE是首个将ReAct框架应用于自动特征工程的系统，它需要大量特征空间中的领域专业知识来识别最优特征。其次，FAMOSE在分类任务中提高了ROC-AUC平均0.23%，在回归任务中通过降低RMSE平均2.0%达到最佳性能。FAMOSE的性能归因于ReAct允许LLM记录特征发现和评估步骤，这使得FAMOSE在需要高度创新解决方案的问题上非常有效。此外，FAMOSE的强健性使其比其他算法更能抵抗错误，这进一步证明了其在复杂任务中的优越性。因此，FAMOSE不仅展示了AI代理在自动特征工程中的强大能力，还证明了其在面对复杂问题时的可靠性和创新性。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>FAMOSE需要大量特征空间中的领域专业知识来识别最优特征。</li>
<li>FAMOSE是首个将ReAct框架应用于自动特征工程的系统。</li>
<li>FAMOSE在分类任务中提高了ROC-AUC平均0.23%。</li>
<li>FAMOSE在回归任务中通过降低RMSE平均2.0%达到最佳性能。</li>
<li>FAMOSE的性能归因于ReAct允许LLM记录特征发现和评估步骤。</li>
<li>FAMOSE展示了AI代理在需要高度创新解决方案的问题上非常有效。</li>
<li>FAMOSE的强健性使其比其他算法更能抵抗错误。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-65">正文（抓取，非 AI）</h3>
<p>[2602.17641v1] FAMOSE: A ReAct Approach to Automated Feature Discovery Computer Science &gt; Machine Learning arXiv:2602.17641v1 (cs) [Submitted on 19 Feb 2026] Title: FAMOSE: A ReAct Approach to Automated Feature Discovery Authors: Keith Burghardt , Jienan Liu , Sadman Sakib , Yuning Hao , Bo Li View a PDF of the paper titled FAMOSE: A ReAct Approach to Automated Feature Discovery, by Keith Burghardt and 4 other authors View PDF HTML (experimental) Abstract: Feature engineering remains a critical yet challenging bottleneck in machine learning, particularly for tabular data, as identifying optimal features from an exponentially large feature space traditionally demands substantial domain expertise. To address this challenge, we introduce FAMOSE (Feature AugMentation and Optimal Selection agEnt), a novel framework that leverages the ReAct paradigm to autonomously explore, generate, and refine features while integrating feature selection and evaluation tools within an agent architecture. To our knowledge, FAMOSE represents the first application of an agentic ReAct framework to automated feature engineering, especially for both regression and classification tasks. Extensive experiments demonstrate that FAMOSE is at or near the state-of-the-art on classification tasks (especially tasks with more than 10K instances, where ROC-AUC increases 0.23% on average), and achieves the state-of-the-art for regression tasks by reducing RMSE by 2.0% on average, while remaining more robust to errors than other algorithms. We hypothesize that FAMOSE's strong performance is because ReAct allows the LLM context window to record (via iterative feature discovery and evaluation steps) what features did or did not work. This is similar to a few-shot prompt and guides the LLM to invent better, more innovative features. Our work offers evidence that AI agents are remarkably effective in solving problems that require highly inventive solutions, such as feature engineering. Comments: 23 pages, 6 figures Subjects: Machine Learning (cs.LG) ; Artificial Intelligence (cs.AI) Cite as: arXiv:2602.17641 [cs.LG] (or arXiv:2602.17641v1 [cs.LG] for this version) https://doi.org/10.48550/arXiv.2602.17641 Focus to learn more arXiv-issued DOI via DataCite (pending registration) Submission history From: Keith Burghardt [ view email ] [v1] Thu, 19 Feb 2026 18:53:15 UTC (626 KB) Full-text links: Access Paper: View a PDF of the paper titled FAMOSE: A ReAct Approach to Automated Feature Discovery, by Keith Burghardt and 4 other authors View PDF HTML (experimental) TeX Source view license Current browse context: cs.LG &lt; prev | next &gt; new | recent | 2026-02 Change to browse by: cs cs.AI References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) IArxiv recommender toggle IArxiv Recommender ( What is IArxiv? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-66">34. Stable Asynchrony: Variance-Controlled Off-Policy RL for LLMs</h2>
<ul>
<li>链接：https://arxiv.org/abs/2602.17616v1</li>
<li>来源：arxiv</li>
<li>摘要：Reinforcement learning (RL) is widely used to improve large language models on reasoning tasks, and asynchronous RL training is attractive because it increases end-to-end throughput. However, for widely adopted critic-free policy-gradient methods such as REINFORCE and GRPO, high asynchrony makes the policy-gradient estimator markedly $\textbf{higher variance}$: training on stale rollouts creates heavy-tailed importance ratios, causing a small fraction of samples to dominate updates. This amplification makes gradients noisy and learning unstable relative to matched on-policy training. Across math and general reasoning benchmarks, we find collapse is reliably predicted by effective sample size (ESS) and unstable gradient norms. Motivated by this diagnosis, we propose $\textbf{V}$ariance $\textbf{C}$ontrolled $\textbf{P}$olicy $\textbf{O}$ptimization ($\textbf{VCPO}$), a general stabilization method for REINFORCE/GRPO-style algorithms that (i) scales learning rate based on effective sample size to dampen unreliable updates, and (ii) applies a closed-form minimum-variance baseline for the off-policy setting, avoiding an auxiliary value model and adding minimal overhead. Empirically, VC</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">stale rollouts增加训练的方差，导致重要性比率重尾分布，使得小比例样本在更新过程中占据主导地位。这种情况下，gradients的噪声进一步加剧了学习的不稳定性。为了解决这些问题，VCPO通过调整学习率来减少不可靠更新，并应用闭式最小方差基线，从而优化训练过程。这种改进不仅减少了长期上下文多轮训练的时间，还提高了异步训练的鲁棒性。因此，VCPO不仅优于多种基线方法，还确保了可靠的大规模异步RL。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>stale rollouts增加训练的方差</li>
<li>stale rollouts导致重要性比率重尾分布</li>
<li>小比例样本主导更新</li>
<li>gradients的噪声导致学习不稳定</li>
<li>VCPO通过调整学习率来减少不可靠更新</li>
<li>VCPO应用闭式最小方差基线</li>
<li>VCPO减少长期上下文多轮训练时间</li>
<li>VCPO提高异步训练的鲁棒性</li>
<li>VCPO优于多种基线方法</li>
<li>VCPO确保可靠的大规模异步RL</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-67">正文（抓取，非 AI）</h3>
<p>[2602.17616v1] Stable Asynchrony: Variance-Controlled Off-Policy RL for LLMs Computer Science &gt; Machine Learning arXiv:2602.17616v1 (cs) [Submitted on 19 Feb 2026] Title: Stable Asynchrony: Variance-Controlled Off-Policy RL for LLMs Authors: Luke Huang , Zhuoyang Zhang , Qinghao Hu , Shang Yang , Song Han View a PDF of the paper titled Stable Asynchrony: Variance-Controlled Off-Policy RL for LLMs, by Luke Huang and 4 other authors View PDF HTML (experimental) Abstract: Reinforcement learning (RL) is widely used to improve large language models on reasoning tasks, and asynchronous RL training is attractive because it increases end-to-end throughput. However, for widely adopted critic-free policy-gradient methods such as REINFORCE and GRPO, high asynchrony makes the policy-gradient estimator markedly $\textbf{higher variance}$: training on stale rollouts creates heavy-tailed importance ratios, causing a small fraction of samples to dominate updates. This amplification makes gradients noisy and learning unstable relative to matched on-policy training. Across math and general reasoning benchmarks, we find collapse is reliably predicted by effective sample size (ESS) and unstable gradient norms. Motivated by this diagnosis, we propose $\textbf{V}$ariance $\textbf{C}$ontrolled $\textbf{P}$olicy $\textbf{O}$ptimization ($\textbf{VCPO}$), a general stabilization method for REINFORCE/GRPO-style algorithms that (i) scales learning rate based on effective sample size to dampen unreliable updates, and (ii) applies a closed-form minimum-variance baseline for the off-policy setting, avoiding an auxiliary value model and adding minimal overhead. Empirically, VCPO substantially improves robustness for asynchronous training across math, general reasoning, and tool-use tasks, outperforming a broad suite of baselines spanning masking/clipping stabilizers and algorithmic variants. This reduces long-context, multi-turn training time by 2.5$\times$ while matching synchronous performance, demonstrating that explicit control of policy-gradient variance is key for reliable asynchronous RL at scale. Subjects: Machine Learning (cs.LG) ; Artificial Intelligence (cs.AI) Cite as: arXiv:2602.17616 [cs.LG] (or arXiv:2602.17616v1 [cs.LG] for this version) https://doi.org/10.48550/arXiv.2602.17616 Focus to learn more arXiv-issued DOI via DataCite (pending registration) Submission history From: Luke Huang [ view email ] [v1] Thu, 19 Feb 2026 18:40:51 UTC (14,082 KB) Full-text links: Access Paper: View a PDF of the paper titled Stable Asynchrony: Variance-Controlled Off-Policy RL for LLMs, by Luke Huang and 4 other authors View PDF HTML (experimental) TeX Source view license Current browse context: cs.LG &lt; prev | next &gt; new | recent | 2026-02 Change to browse by: cs cs.AI References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) IArxiv recommender toggle IArxiv Recommender ( What is IArxiv? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-68">35. Human-level 3D shape perception emerges from multi-view learning</h2>
<ul>
<li>链接：https://arxiv.org/abs/2602.17650v1</li>
<li>来源：arxiv</li>
<li>摘要：Humans can infer the three-dimensional structure of objects from two-dimensional visual inputs. Modeling this ability has been a longstanding goal for the science and engineering of visual intelligence, yet decades of computational methods have fallen short of human performance. Here we develop a modeling framework that predicts human 3D shape inferences for arbitrary objects, directly from experimental stimuli. We achieve this with a novel class of neural networks trained using a visual-spatial objective over naturalistic sensory data; given a set of images taken from different locations within a natural scene, these models learn to predict spatial information related to these images, such as camera location and visual depth, without relying on any object-related inductive biases. Notably, these visual-spatial signals are analogous to sensory cues readily available to humans. We design a zero-shot evaluation approach to determine the performance of these `multi-view' models on a well established 3D perception task, then compare model and human behavior. Our modeling framework is the first to match human accuracy on 3D shape inferences, even without task-specific training or fine-t</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">multi-view学习框架无需依赖于对象相关的归纳偏置，这一特性使得模型能够直接从多视角数据中学习，而无需特定任务的训练或微调。研究首次展示了模型在3D感知任务上的性能，通过零样本评估方法确定了模型的准确性，这表明模型能够预测人类在面对新任务时的行为细微差异。视觉-空间信号与人类可轻易获得的感官线索类似，模型的响应独立读出也反映了人类从二维视觉输入中推断三维结构的能力。因此，该研究不仅实现了匹配人类3D形状推断的准确性，还揭示了模型动态与人类感知之间存在自然对应关系。所有相关代码、人类行为数据和实验刺激都可以在项目页面上找到，这为其他研究者提供了宝贵的资源。此外，该研究解决了视觉智能和工程的长期目标，展示了计算方法多年来未能达到人类性能水平的问题，从而推动了该领域的进一步发展。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>multi-view学习框架无需依赖于对象相关的归纳偏置。</li>
<li>视觉-空间信号与人类可轻易获得的感官线索类似。</li>
<li>零样本评估方法可以确定模型在3D感知任务上的性能。</li>
<li>模型响应的独立读出可以预测人类行为的细微差异。</li>
<li>该研究首次实现了无需特定任务训练或微调就能匹配人类3D形状推断的准确性。</li>
<li>模型动态与人类感知之间存在自然对应关系。</li>
<li>所有代码、人类行为数据和实验刺激都可以在项目页面上找到。</li>
<li>该研究解决了视觉智能和工程的长期目标。</li>
<li>人类可以从二维视觉输入中推断出三维结构。</li>
<li>计算方法多年来未能达到人类的性能水平。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-69">正文（抓取，非 AI）</h3>
<p>[2602.17650v1] Human-level 3D shape perception emerges from multi-view learning Computer Science &gt; Computer Vision and Pattern Recognition arXiv:2602.17650v1 (cs) [Submitted on 19 Feb 2026] Title: Human-level 3D shape perception emerges from multi-view learning Authors: Tyler Bonnen , Jitendra Malik , Angjoo Kanazawa View a PDF of the paper titled Human-level 3D shape perception emerges from multi-view learning, by Tyler Bonnen and 2 other authors View PDF HTML (experimental) Abstract: Humans can infer the three-dimensional structure of objects from two-dimensional visual inputs. Modeling this ability has been a longstanding goal for the science and engineering of visual intelligence, yet decades of computational methods have fallen short of human performance. Here we develop a modeling framework that predicts human 3D shape inferences for arbitrary objects, directly from experimental stimuli. We achieve this with a novel class of neural networks trained using a visual-spatial objective over naturalistic sensory data; given a set of images taken from different locations within a natural scene, these models learn to predict spatial information related to these images, such as camera location and visual depth, without relying on any object-related inductive biases. Notably, these visual-spatial signals are analogous to sensory cues readily available to humans. We design a zero-shot evaluation approach to determine the performance of these `multi-view' models on a well established 3D perception task, then compare model and human behavior. Our modeling framework is the first to match human accuracy on 3D shape inferences, even without task-specific training or fine-tuning. Remarkably, independent readouts of model responses predict fine-grained measures of human behavior, including error patterns and reaction times, revealing a natural correspondence between model dynamics and human perception. Taken together, our findings indicate that human-level 3D perception can emerge from a simple, scalable learning objective over naturalistic visual-spatial data. All code, human behavioral data, and experimental stimuli needed to reproduce our findings can be found on our project page. Subjects: Computer Vision and Pattern Recognition (cs.CV) Cite as: arXiv:2602.17650 [cs.CV] (or arXiv:2602.17650v1 [cs.CV] for this version) https://doi.org/10.48550/arXiv.2602.17650 Focus to learn more arXiv-issued DOI via DataCite (pending registration) Submission history From: Tyler Bonnen [ view email ] [v1] Thu, 19 Feb 2026 18:56:05 UTC (9,023 KB) Full-text links: Access Paper: View a PDF of the paper titled Human-level 3D shape perception emerges from multi-view learning, by Tyler Bonnen and 2 other authors View PDF HTML (experimental) TeX Source view license Current browse context: cs.CV &lt; prev | next &gt; new | recent | 2026-02 Change to browse by: cs References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-70">36. Adapting Actively on the Fly: Relevance-Guided Online Meta-Learning with Latent Concepts for Geospatial Discovery</h2>
<ul>
<li>链接：https://arxiv.org/abs/2602.17605v1</li>
<li>来源：arxiv</li>
<li>摘要：In many real-world settings, such as environmental monitoring, disaster response, or public health, with costly and difficult data collection and dynamic environments, strategically sampling from unobserved regions is essential for efficiently uncovering hidden targets under tight resource constraints. Yet, sparse and biased geospatial ground truth limits the applicability of existing learning-based methods, such as reinforcement learning. To address this, we propose a unified geospatial discovery framework that integrates active learning, online meta-learning, and concept-guided reasoning. Our approach introduces two key innovations built on a shared notion of <em>concept relevance</em>, which captures how domain-specific factors influence target presence: a <em>concept-weighted uncertainty sampling strategy</em>, where uncertainty is modulated by learned relevance based on readily-available domain-specific concepts (e.g., land cover, source proximity); and a <em>relevance-aware meta-batch formation strategy</em> that promotes semantic diversity during online-meta updates, improving generalization in dynamic environments. Our experiments include testing on a real-world dataset of cancer-causing PFAS (</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">首先，领域特定概念（concept relevance）能够反映特定因素对目标存在的影响，这有助于模型更好地理解特定环境下的问题。其次，通过概念加权不确定性采样（concept-weighted uncertainty sampling），模型能够根据学习到的领域特定概念调整不确定性，从而提高预测的准确性。此外，概念引导的推理（concept-guided reasoning）能够进一步引导模型进行有效的推理，以发现目标。因此，这些方法共同促进了在线元学习（online meta-learning）的更新，使其在动态环境中具有更高的泛化能力。然而，稀疏且有偏的地理空间地面真实数据（sparse and biased geospatial ground truth）限制了现有基于学习的方法的应用。因此，使用真实世界数据集（real-world dataset）来展示方法在有限数据和动态环境下的可靠性显得尤为重要。例如，PFAS污染数据集（PFAS contamination dataset）可以用于实际测试，以验证方法的有效性。此外，主动学习（active learning）在动态环境中至关重要，能够根据环境的变化调整学习策略，从而提高模型的适应性和可靠性。因此，这些方法共同促进了在线元学习在动态环境中的应用，提高了泛化能力。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>concept relevance 能反映领域特定因素对目标存在的影响。</li>
<li>concept-weighted uncertainty sampling 能通过学习到的领域特定概念调整不确定性。</li>
<li>relevance-aware meta-batch formation 能促进在线元学习更新时的语义多样性。</li>
<li>sparse and biased geospatial ground truth 限制了现有基于学习的方法的应用。</li>
<li>real-world dataset 的使用展示了方法在有限数据和动态环境下的可靠性。</li>
<li>PFAS contamination 数据集用于实际测试。</li>
<li>active learning 在动态环境中至关重要。</li>
<li>online meta-learning 能提高在动态环境中的泛化能力。</li>
<li>concept-guided reasoning 能引导推理以发现目标。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-71">正文（抓取，非 AI）</h3>
<p>[2602.17605v1] Adapting Actively on the Fly: Relevance-Guided Online Meta-Learning with Latent Concepts for Geospatial Discovery Computer Science &gt; Computer Vision and Pattern Recognition arXiv:2602.17605v1 (cs) [Submitted on 19 Feb 2026] Title: Adapting Actively on the Fly: Relevance-Guided Online Meta-Learning with Latent Concepts for Geospatial Discovery Authors: Jowaria Khan , Anindya Sarkar , Yevgeniy Vorobeychik , Elizabeth Bondi-Kelly View a PDF of the paper titled Adapting Actively on the Fly: Relevance-Guided Online Meta-Learning with Latent Concepts for Geospatial Discovery, by Jowaria Khan and 3 other authors View PDF HTML (experimental) Abstract: In many real-world settings, such as environmental monitoring, disaster response, or public health, with costly and difficult data collection and dynamic environments, strategically sampling from unobserved regions is essential for efficiently uncovering hidden targets under tight resource constraints. Yet, sparse and biased geospatial ground truth limits the applicability of existing learning-based methods, such as reinforcement learning. To address this, we propose a unified geospatial discovery framework that integrates active learning, online meta-learning, and concept-guided reasoning. Our approach introduces two key innovations built on a shared notion of <em>concept relevance</em>, which captures how domain-specific factors influence target presence: a <em>concept-weighted uncertainty sampling strategy</em>, where uncertainty is modulated by learned relevance based on readily-available domain-specific concepts (e.g., land cover, source proximity); and a <em>relevance-aware meta-batch formation strategy</em> that promotes semantic diversity during online-meta updates, improving generalization in dynamic environments. Our experiments include testing on a real-world dataset of cancer-causing PFAS (Per- and polyfluoroalkyl substances) contamination, showcasing our method's reliability at uncovering targets with limited data and a varying environment. Subjects: Computer Vision and Pattern Recognition (cs.CV) ; Artificial Intelligence (cs.AI); Computers and Society (cs.CY); Machine Learning (cs.LG) ACM classes: I.2.1; I.2.10; I.4.6; I.4.9; I.4.10; J.2 Cite as: arXiv:2602.17605 [cs.CV] (or arXiv:2602.17605v1 [cs.CV] for this version) https://doi.org/10.48550/arXiv.2602.17605 Focus to learn more arXiv-issued DOI via DataCite (pending registration) Submission history From: Jowaria Khan [ view email ] [v1] Thu, 19 Feb 2026 18:30:18 UTC (16,448 KB) Full-text links: Access Paper: View a PDF of the paper titled Adapting Actively on the Fly: Relevance-Guided Online Meta-Learning with Latent Concepts for Geospatial Discovery, by Jowaria Khan and 3 other authors View PDF HTML (experimental) TeX Source view license Current browse context: cs.CV &lt; prev | next &gt; new | recent | 2026-02 Change to browse by: cs cs.AI cs.CY cs.LG References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-72">37. Non-Trivial Zero-Knowledge Implies One-Way Functions</h2>
<ul>
<li>链接：https://arxiv.org/abs/2602.17651v1</li>
<li>来源：arxiv</li>
<li>摘要：A recent breakthrough [Hirahara and Nanashima, STOC'2024] established that if $\mathsf{NP} \not \subseteq \mathsf{ioP/poly}$, the existence of zero-knowledge with negligible errors for $\mathsf{NP}$ implies the existence of one-way functions (OWFs). In this work, we obtain a characterization of one-way functions from the worst-case complexity of zero-knowledge {\em in the high-error regime}.   We say that a zero-knowledge argument is {\em non-trivial} if the sum of its completeness, soundness and zero-knowledge errors is bounded away from $1$. Our results are as follows, assuming $\mathsf{NP} \not \subseteq \mathsf{ioP/poly}$:   1. {\em Non-trivial} Non-Interactive ZK (NIZK) arguments for $\mathsf{NP}$ imply the existence of OWFs. Using known amplification techniques, this result also provides an unconditional transformation from weak to standard NIZK proofs for all meaningful error parameters.   2. We also generalize to the interactive setting: {\em Non-trivial} constant-round public-coin zero-knowledge arguments for $\mathsf{NP}$ imply the existence of OWFs, and therefore also (standard) four-message zero-knowledge arguments for $\mathsf{NP}$.   Prior to this work, one-way functi</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">non-trivial零知识论证需要满足有界完备性、有界声辩性和零知识错误。这类论证的存在性意味着存在单向函数。此外，非交互式的零知识论证同样暗示着单向函数的存在。然而，在零知识错误加上声辩错误平方根的情况下，该值至少为1的区间一直是个空白。这项工作填补了这一空白，通过提供无条件的从弱到标准非交互式零知识证明的转换，实现了这一目标。这些结果不仅为从弱条件到标准条件的NIZK证明提供了新的构建方法，还可能通过利用最坏情况的硬度来构造单向函数。因此，这项工作不仅解决了理论上的空白，还为实际应用提供了新的可能性。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>non-trivial zero-knowledge arguments require bounded completeness, soundness, and zero-knowledge errors.</li>
<li>non-trivial NIZK arguments imply the existence of one-way functions.</li>
<li>non-trivial interactive zero-knowledge arguments imply the existence of one-way functions.</li>
<li>the regime where zero-knowledge error plus square root of soundness error is at least 1 was previously open.</li>
<li>this work closes the gap in the regime where zero-knowledge error plus square root of soundness error is at least 1.</li>
<li>the results provide unconditional transformations from weak to standard NIZK proofs.</li>
<li>the techniques could be useful in constructing one-way functions from worst-case hardness.</li>
<li>one-way functions can be obtained from NIZKs with constant zero-knowledge error and soundness error satisfying certain conditions.</li>
<li>the work generalizes to the interactive setting, providing new implications.</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-73">正文（抓取，非 AI）</h3>
<p>[2602.17651v1] Non-Trivial Zero-Knowledge Implies One-Way Functions Computer Science &gt; Cryptography and Security arXiv:2602.17651v1 (cs) [Submitted on 19 Feb 2026] Title: Non-Trivial Zero-Knowledge Implies One-Way Functions Authors: Suvradip Chakraborty (1), James Hulett (2), Dakshita Khurana (2 and 3), Kabir Tomer (2) ((1) Visa Research, (2) UIUC, (3) NTT Research) View a PDF of the paper titled Non-Trivial Zero-Knowledge Implies One-Way Functions, by Suvradip Chakraborty (1) and 5 other authors View PDF HTML (experimental) Abstract: A recent breakthrough [Hirahara and Nanashima, STOC'2024] established that if $\mathsf{NP} \not \subseteq \mathsf{ioP/poly}$, the existence of zero-knowledge with negligible errors for $\mathsf{NP}$ implies the existence of one-way functions (OWFs). In this work, we obtain a characterization of one-way functions from the worst-case complexity of zero-knowledge {\em in the high-error regime}. We say that a zero-knowledge argument is {\em non-trivial} if the sum of its completeness, soundness and zero-knowledge errors is bounded away from $1$. Our results are as follows, assuming $\mathsf{NP} \not \subseteq \mathsf{ioP/poly}$: 1. {\em Non-trivial} Non-Interactive ZK (NIZK) arguments for $\mathsf{NP}$ imply the existence of OWFs. Using known amplification techniques, this result also provides an unconditional transformation from weak to standard NIZK proofs for all meaningful error parameters. 2. We also generalize to the interactive setting: {\em Non-trivial} constant-round public-coin zero-knowledge arguments for $\mathsf{NP}$ imply the existence of OWFs, and therefore also (standard) four-message zero-knowledge arguments for $\mathsf{NP}$. Prior to this work, one-way functions could be obtained from NIZKs that had constant zero-knowledge error $\epsilon_{zk}$ and soundness error $\epsilon_{s}$ satisfying $\epsilon_{zk} + \sqrt{\epsilon_{s}} &lt; 1$ [Chakraborty, Hulett and Khurana, CRYPTO'2025]. However, the regime where $\epsilon_{zk} + \sqrt{\epsilon_{s}} \geq 1$ remained open. This work closes the gap, and obtains new implications in the interactive setting. Our results and techniques could be useful stepping stones in the quest to construct one-way functions from worst-case hardness. Subjects: Cryptography and Security (cs.CR) Cite as: arXiv:2602.17651 [cs.CR] (or arXiv:2602.17651v1 [cs.CR] for this version) https://doi.org/10.48550/arXiv.2602.17651 Focus to learn more arXiv-issued DOI via DataCite (pending registration) Submission history From: Kabir Tomer [ view email ] [v1] Thu, 19 Feb 2026 18:56:24 UTC (41 KB) Full-text links: Access Paper: View a PDF of the paper titled Non-Trivial Zero-Knowledge Implies One-Way Functions, by Suvradip Chakraborty (1) and 5 other authors View PDF HTML (experimental) TeX Source view license Current browse context: cs.CR &lt; prev | next &gt; new | recent | 2026-02 Change to browse by: cs References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-74">38. What Makes a Good LLM Agent for Real-world Penetration Testing?</h2>
<ul>
<li>链接：https://arxiv.org/abs/2602.17622v1</li>
<li>来源：arxiv</li>
<li>摘要：LLM-based agents show promise for automating penetration testing, yet reported performance varies widely across systems and benchmarks. We analyze 28 LLM-based penetration testing systems and evaluate five representative implementations across three benchmarks of increasing complexity. Our analysis reveals two distinct failure modes: Type A failures stem from capability gaps (missing tools, inadequate prompts) that engineering readily addresses, while Type B failures persist regardless of tooling due to planning and state management limitations. We show that Type B failures share a root cause that is largely invariant to the underlying LLM: agents lack real-time task difficulty estimation. As a result, agents misallocate effort, over-commit to low-value branches, and exhaust context before completing attack chains.   Based on this insight, we present Excalibur, a penetration testing agent that couples strong tooling with difficulty-aware planning. A Tool and Skill Layer eliminates Type A failures through typed interfaces and retrieval-augmented knowledge. A Task Difficulty Assessment (TDA) mechanism addresses Type B failures by estimating tractability through four measurable dimens</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">首先，智能代理在执行任务时缺乏实时任务难度的估算能力，导致它们在攻击链中容易出现过度承诺于低价值分支的情况，从而耗尽上下文信息而无法完成攻击链。其次，Type A 失败源于能力缺口，而Type B 失败则归因于规划和状态管理的局限性。为了解决这些问题，Excalibur通过引入一个工具和技能层来消除Type A失败，并通过一种证据引导的攻击树搜索框架来应对Type B失败。该框架利用前景估计、证据置信度、上下文负载和历史成功来估算任务的可处理性。此外，Excalibur在GOAD Active Directory环境中成功攻破了4个目标中的3个，展示了其在复杂环境中的强大能力。因此，单纯依赖模型规模的扩展并不能解决由难度感知规划所揭示的局限性，这表明需要更精细的任务难度评估机制来提高攻击成功率。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>agents lack real-time task difficulty estimation</li>
<li>Type B failures persist regardless of tooling</li>
<li>agents misallocate effort, over-commit to low-value branches</li>
<li>agents exhaust context before completing attack chains</li>
<li>Excalibur achieves up to 91% task completion on CTF benchmarks</li>
<li>Type A failures stem from capability gaps</li>
<li>Type B failures are due to planning and state management limitations</li>
<li>a Tool and Skill Layer eliminates Type A failures</li>
<li>a Task Difficulty Assessment mechanism addresses Type B failures</li>
<li>horizon estimation, evidence confidence, context load, and historical success are used to estimate tractability</li>
<li>Excalibur uses an Evidence-Guided Attack Tree Search framework</li>
<li>Excalibur compromises 4 of 5 hosts on the GOAD Active Directory environment</li>
<li>model scaling alone does not eliminate the limitation addressed by difficulty-aware planning</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-75">正文（抓取，非 AI）</h3>
<p>[2602.17622v1] What Makes a Good LLM Agent for Real-world Penetration Testing? Computer Science &gt; Cryptography and Security arXiv:2602.17622v1 (cs) [Submitted on 19 Feb 2026] Title: What Makes a Good LLM Agent for Real-world Penetration Testing? Authors: Gelei Deng , Yi Liu , Yuekang Li , Ruozhao Yang , Xiaofei Xie , Jie Zhang , Han Qiu , Tianwei Zhang View a PDF of the paper titled What Makes a Good LLM Agent for Real-world Penetration Testing?, by Gelei Deng and 7 other authors View PDF HTML (experimental) Abstract: LLM-based agents show promise for automating penetration testing, yet reported performance varies widely across systems and benchmarks. We analyze 28 LLM-based penetration testing systems and evaluate five representative implementations across three benchmarks of increasing complexity. Our analysis reveals two distinct failure modes: Type A failures stem from capability gaps (missing tools, inadequate prompts) that engineering readily addresses, while Type B failures persist regardless of tooling due to planning and state management limitations. We show that Type B failures share a root cause that is largely invariant to the underlying LLM: agents lack real-time task difficulty estimation. As a result, agents misallocate effort, over-commit to low-value branches, and exhaust context before completing attack chains. Based on this insight, we present Excalibur, a penetration testing agent that couples strong tooling with difficulty-aware planning. A Tool and Skill Layer eliminates Type A failures through typed interfaces and retrieval-augmented knowledge. A Task Difficulty Assessment (TDA) mechanism addresses Type B failures by estimating tractability through four measurable dimensions (horizon estimation, evidence confidence, context load, and historical success) and uses these estimates to guide exploration-exploitation decisions within an Evidence-Guided Attack Tree Search (EGATS) framework. Excalibur achieves up to 91% task completion on CTF benchmarks with frontier models (39 to 49% relative improvement over baselines) and compromises 4 of 5 hosts on the GOAD Active Directory environment versus 2 by prior systems. These results show that difficulty-aware planning yields consistent end-to-end gains across models and addresses a limitation that model scaling alone does not eliminate. Subjects: Cryptography and Security (cs.CR) ; Software Engineering (cs.SE) Cite as: arXiv:2602.17622 [cs.CR] (or arXiv:2602.17622v1 [cs.CR] for this version) https://doi.org/10.48550/arXiv.2602.17622 Focus to learn more arXiv-issued DOI via DataCite (pending registration) Submission history From: Gelei Deng [ view email ] [v1] Thu, 19 Feb 2026 18:42:40 UTC (316 KB) Full-text links: Access Paper: View a PDF of the paper titled What Makes a Good LLM Agent for Real-world Penetration Testing?, by Gelei Deng and 7 other authors View PDF HTML (experimental) TeX Source view license Current browse context: cs.CR &lt; prev | next &gt; new | recent | 2026-02 Change to browse by: cs cs.SE References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-76">39. Cosmic voids evolution in modified gravity via hydrodynamics</h2>
<ul>
<li>链接：https://arxiv.org/abs/2602.17644v1</li>
<li>来源：arxiv</li>
<li>摘要：We present a hydrodynamical description of spherical void evolution in modified gravity (MG), extending the standard General Relativity (GR) and dynamical dark energy treatment by encoding gravity modifications into effective couplings that enter the Euler and Poisson equations. This yields a compact non-linear evolution equation for the Eulerian density contrast, controlled by a time- and density-dependent effective gravitational strength, and provides a direct map between model functions and void observables. We apply the framework to the luminal Galileon class of models, where derivative self-interactions generate Vainshtein screening and might lead to a breakdown of the physical branch in sufficiently underdense regions. Exploiting this feature, we apply the void-informed viability requirement that translates into bounds on the theory parameter space and, equivalently, on the minimum attainable void depth as a function of redshift. For viable parameters of a concrete model, we quantify the impact of MG on isolated void evolution, the Lagrangian to Eulerian mapping, and the shell-crossing threshold. Relative to GR, we find a clear hierarchy of MG effects, with ${\cal O}(10\%)$ m</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">MG理论中的引力修改（MG效应）不仅会影响有效引力强度的时间和密度依赖性，还通过密度对比的非线性演化方程明确反映出来。这一效应进一步影响了从Lagrangian到Eulerian的映射，导致壳交叉阈值的变化。在广义相对论（GR）中，不存在Vainshtein屏蔽，但在某些MG模型中却可能出现。理论可接受性要求对MG参数空间进行限制，这又导致了对最小空洞深度的限制。因此，空洞总是处于未屏蔽状态，这一结论在物理分支上成立。MG效应在空洞演化中表现为引力耦合的百分之一级变化，而空洞密度演化则会经历类似的百分之一级偏移。相比之下，映射和壳交叉阈值的变化幅度更小，仅为千分之一级。然而，空洞的物理分支在足够低密度区域可能会失效，这进一步揭示了MG效应在不同密度条件下的复杂性。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>MG理论中的引力修改会影响有效引力强度的时间和密度依赖性。</li>
<li>密度对比的非线性演化方程反映了MG效应。</li>
<li>Lagrangian到Eulerian的映射会受到MG效应的影响。</li>
<li>壳交叉阈值也会因MG效应而变化。</li>
<li>GR中不存在Vainshtein屏蔽，但在某些MG模型中可能出现。</li>
<li>MG理论中的参数空间受到理论可接受性的限制。</li>
<li>理论可接受性要求导致对最小空洞深度的限制。</li>
<li>空洞总是处于未屏蔽状态，这在物理分支上成立。</li>
<li>MG效应在空洞演化中表现为引力耦合的百分之一级变化。</li>
<li>空洞密度演化会经历百分之一级的偏移。</li>
<li>映射和壳交叉阈值的变化幅度更小，为千分之一级。</li>
<li>空洞的物理分支在足够低密度区域可能会失效。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-77">正文（抓取，非 AI）</h3>
<p>[2602.17644v1] Cosmic voids evolution in modified gravity via hydrodynamics Astrophysics &gt; Cosmology and Nongalactic Astrophysics arXiv:2602.17644v1 (astro-ph) [Submitted on 19 Feb 2026] Title: Cosmic voids evolution in modified gravity via hydrodynamics Authors: Tommaso Moretti , Noemi Frusciante , Giovanni Verza , Francesco Pace View a PDF of the paper titled Cosmic voids evolution in modified gravity via hydrodynamics, by Tommaso Moretti and 3 other authors View PDF HTML (experimental) Abstract: We present a hydrodynamical description of spherical void evolution in modified gravity (MG), extending the standard General Relativity (GR) and dynamical dark energy treatment by encoding gravity modifications into effective couplings that enter the Euler and Poisson equations. This yields a compact non-linear evolution equation for the Eulerian density contrast, controlled by a time- and density-dependent effective gravitational strength, and provides a direct map between model functions and void observables. We apply the framework to the luminal Galileon class of models, where derivative self-interactions generate Vainshtein screening and might lead to a breakdown of the physical branch in sufficiently underdense regions. Exploiting this feature, we apply the void-informed viability requirement that translates into bounds on the theory parameter space and, equivalently, on the minimum attainable void depth as a function of redshift. For viable parameters of a concrete model, we quantify the impact of MG on isolated void evolution, the Lagrangian to Eulerian mapping, and the shell-crossing threshold. Relative to GR, we find a clear hierarchy of MG effects, with ${\cal O}(10\%)$ modifications in the gravitational couplings, percent-level shifts in the void density evolution, and sub-percent deviations in both the mapping and the shell-crossing thresholds. Moreover, within the adopted parametrization, we show analytically that voids always lie in an unscreened regime on the physical branch. Overall, the formalism provides a self-consistent route to predict void dynamics and consistency constraints in a broad class of MG models. Comments: 30 pages, 13 figures Subjects: Cosmology and Nongalactic Astrophysics (astro-ph.CO) ; General Relativity and Quantum Cosmology (gr-qc) Cite as: arXiv:2602.17644 [astro-ph.CO] (or arXiv:2602.17644v1 [astro-ph.CO] for this version) https://doi.org/10.48550/arXiv.2602.17644 Focus to learn more arXiv-issued DOI via DataCite (pending registration) Submission history From: Tommaso Moretti [ view email ] [v1] Thu, 19 Feb 2026 18:54:23 UTC (2,521 KB) Full-text links: Access Paper: View a PDF of the paper titled Cosmic voids evolution in modified gravity via hydrodynamics, by Tommaso Moretti and 3 other authors View PDF HTML (experimental) TeX Source view license Current browse context: astro-ph.CO &lt; prev | next &gt; new | recent | 2026-02 Change to browse by: astro-ph gr-qc References &amp; Citations INSPIRE HEP NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) IArxiv recommender toggle IArxiv Recommender ( What is IArxiv? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-78">40. Exploring Novel Data Storage Approaches for Large-Scale Numerical Weather Prediction</h2>
<ul>
<li>链接：https://arxiv.org/abs/2602.17610v1</li>
<li>来源：arxiv</li>
<li>摘要：Driven by scientific and industry ambition, HPC and AI applications such as operational Numerical Weather Prediction (NWP) require processing and storing ever-increasing data volumes as fast as possible. Whilst POSIX distributed file systems and NVMe SSDs are currently a common HPC storage configuration providing I/O to applications, new storage solutions have proliferated or gained traction over the last decade with potential to address performance limitations POSIX file systems manifest at scale for certain I/O workloads.   This work has primarily aimed to assess the suitability and performance of two object storage systems -namely DAOS and Ceph- for the ECMWF's operational NWP as well as for HPC and AI applications in general. New software-level adapters have been developed which enable the ECMWF's NWP to leverage these systems, and extensive I/O benchmarking has been conducted on a few computer systems, comparing the performance delivered by the evaluated object stores to that of equivalent Lustre file system deployments on the same hardware. Challenges of porting to object storage and its benefits with respect to the traditional POSIX I/O approach have been discussed and, wher</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">POSIX文件系统在大规模IO工作负载中会表现出性能限制，这主要是由于其设计和实现的固有局限性。为了解决这一问题，新存储解决方案在过去十年中涌现或获得关注，这些解决方案可能提供更优的性能。DAOS和Ceph被评估作为ECMWF运营NWP的存储系统，新软件级适配器使ECMWF NWP能够利用这些系统。通过基准测试，比较了评估的对象存储系统的性能与等效的Lustre文件系统部署，结果显示DAOS在可扩展性和灵活性方面优于Ceph和Lustre。尽管POSIX I/O方法可能仍会继续使用，不一定会被完全取代，但DAOS和对象存储可能会在未来的HPC中心获得更广泛的应用。因此，这些新存储解决方案为大规模IO工作负载提供了更好的性能和灵活性，为未来HPC中心的应用提供了新的可能性。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>POSIX文件系统在大规模IO工作负载中会表现出性能限制。</li>
<li>新存储解决方案在过去十年中涌现或获得关注，可能解决POSIX文件系统在大规模IO工作负载中的性能限制。</li>
<li>DAOS和Ceph被评估作为ECMWF运营NWP的存储系统。</li>
<li>新软件级适配器使ECMWF NWP能够利用这些系统。</li>
<li>基准测试比较了评估的对象存储系统的性能与等效的Lustre文件系统部署。</li>
<li>DAOS在可扩展性和灵活性方面优于Ceph和Lustre。</li>
<li>POSIX I/O方法可能仍会继续使用，不一定会被完全取代。</li>
<li>DAOS和对象存储可能会在未来的HPC中心获得更广泛的应用。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-79">正文（抓取，非 AI）</h3>
<p>[2602.17610v1] Exploring Novel Data Storage Approaches for Large-Scale Numerical Weather Prediction Computer Science &gt; Distributed, Parallel, and Cluster Computing arXiv:2602.17610v1 (cs) [Submitted on 19 Feb 2026] Title: Exploring Novel Data Storage Approaches for Large-Scale Numerical Weather Prediction Authors: Nicolau Manubens Gil View a PDF of the paper titled Exploring Novel Data Storage Approaches for Large-Scale Numerical Weather Prediction, by Nicolau Manubens Gil View PDF Abstract: Driven by scientific and industry ambition, HPC and AI applications such as operational Numerical Weather Prediction (NWP) require processing and storing ever-increasing data volumes as fast as possible. Whilst POSIX distributed file systems and NVMe SSDs are currently a common HPC storage configuration providing I/O to applications, new storage solutions have proliferated or gained traction over the last decade with potential to address performance limitations POSIX file systems manifest at scale for certain I/O workloads. This work has primarily aimed to assess the suitability and performance of two object storage systems -namely DAOS and Ceph- for the ECMWF's operational NWP as well as for HPC and AI applications in general. New software-level adapters have been developed which enable the ECMWF's NWP to leverage these systems, and extensive I/O benchmarking has been conducted on a few computer systems, comparing the performance delivered by the evaluated object stores to that of equivalent Lustre file system deployments on the same hardware. Challenges of porting to object storage and its benefits with respect to the traditional POSIX I/O approach have been discussed and, where possible, domain-agnostic performance analysis has been conducted, leading to insight also of relevance to I/O practitioners and the broader HPC community. DAOS and Ceph have both demonstrated excellent performance, but DAOS stood out relative to Ceph and Lustre, providing superior scalability and flexibility for applications to perform I/O at scale as desired. This sets a promising outlook for DAOS and object storage, which might see greater adoption at HPC centres in the years to come, although not necessarily implying a shift away from POSIX-like I/O. Comments: PhD. thesis successfully defended at The University of Edinburgh on the 16th October 2025 Subjects: Distributed, Parallel, and Cluster Computing (cs.DC) ; Databases (cs.DB) Cite as: arXiv:2602.17610 [cs.DC] (or arXiv:2602.17610v1 [cs.DC] for this version) https://doi.org/10.48550/arXiv.2602.17610 Focus to learn more arXiv-issued DOI via DataCite (pending registration) Submission history From: Nicolau Manubens Gil [ view email ] [v1] Thu, 19 Feb 2026 18:35:41 UTC (26,979 KB) Full-text links: Access Paper: View a PDF of the paper titled Exploring Novel Data Storage Approaches for Large-Scale Numerical Weather Prediction, by Nicolau Manubens Gil View PDF TeX Source view license Current browse context: cs.DC &lt; prev | next &gt; new | recent | 2026-02 Change to browse by: cs cs.DB References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-80">41. What Breaks Embodied AI Security:LLM Vulnerabilities, CPS Flaws,or Something Else?</h2>
<ul>
<li>链接：https://arxiv.org/abs/2602.17345v1</li>
<li>来源：arxiv</li>
<li>摘要：Embodied AI systems (e.g., autonomous vehicles, service robots, and LLM-driven interactive agents) are rapidly transitioning from controlled environments to safety critical real-world deployments. Unlike disembodied AI, failures in embodied intelligence lead to irreversible physical consequences, raising fundamental questions about security, safety, and reliability. While existing research predominantly analyzes embodied AI through the lenses of Large Language Model (LLM) vulnerabilities or classical Cyber-Physical System (CPS) failures, this survey argues that these perspectives are individually insufficient to explain many observed breakdowns in modern embodied systems. We posit that a significant class of failures arises from embodiment-induced system-level mismatches, rather than from isolated model flaws or traditional CPS attacks. Specifically, we identify four core insights that explain why embodied AI is fundamentally harder to secure: (i) semantic correctness does not imply physical safety, as language-level reasoning abstracts away geometry, dynamics, and contact constraints; (ii) identical actions can lead to drastically different outcomes across physical states due to n</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">首先，语义正确性并不保证物理安全性，因为语言层面的推理会忽略几何、动力学和接触约束。其次，即使执行相同的动作，在不同的物理状态下也可能导致截然不同的结果，这主要是由于非线性动力学和状态不确定性。此外，小的错误会在紧密耦合的感知-决策-行动循环中传播并放大。因此，安全并不是在时间和系统层面上的组合，使得局部安全的决策可能会累积成全局不安全的行为。最终，确保具身AI的安全需要超越组件级别的防御，转向关于物理风险、不确定性和失败传播的系统级推理。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>semantic correctness does not imply physical safety, as language-level reasoning abstracts away geometry, dynamics, and contact constraints</li>
<li>identical actions can lead to drastically different outcomes across physical states due to nonlinear dynamics and state uncertainty</li>
<li>small errors propagate and amplify across tightly coupled perception-decision-action loops</li>
<li>safety is not compositional across time or system layers, enabling locally safe decisions to accumulate into globally unsafe behavior</li>
<li>securing embodied AI requires moving beyond component-level defenses toward system-level reasoning about physical risk, uncertainty, and failure propagation</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-81">正文（抓取，非 AI）</h3>
<p>[2602.17345v1] What Breaks Embodied AI Security:LLM Vulnerabilities, CPS Flaws,or Something Else? Computer Science &gt; Cryptography and Security arXiv:2602.17345v1 (cs) [Submitted on 19 Feb 2026] Title: What Breaks Embodied AI Security:LLM Vulnerabilities, CPS Flaws,or Something Else? Authors: Boyang Ma , Hechuan Guo , Peizhuo Lv , Minghui Xu , Xuelong Dai , YeChao Zhang , Yijun Yang , Yue Zhang View a PDF of the paper titled What Breaks Embodied AI Security:LLM Vulnerabilities, CPS Flaws,or Something Else?, by Boyang Ma and Hechuan Guo and Peizhuo Lv and Minghui Xu and Xuelong Dai and YeChao Zhang and Yijun Yang and Yue Zhang View PDF HTML (experimental) Abstract: Embodied AI systems (e.g., autonomous vehicles, service robots, and LLM-driven interactive agents) are rapidly transitioning from controlled environments to safety critical real-world deployments. Unlike disembodied AI, failures in embodied intelligence lead to irreversible physical consequences, raising fundamental questions about security, safety, and reliability. While existing research predominantly analyzes embodied AI through the lenses of Large Language Model (LLM) vulnerabilities or classical Cyber-Physical System (CPS) failures, this survey argues that these perspectives are individually insufficient to explain many observed breakdowns in modern embodied systems. We posit that a significant class of failures arises from embodiment-induced system-level mismatches, rather than from isolated model flaws or traditional CPS attacks. Specifically, we identify four core insights that explain why embodied AI is fundamentally harder to secure: (i) semantic correctness does not imply physical safety, as language-level reasoning abstracts away geometry, dynamics, and contact constraints; (ii) identical actions can lead to drastically different outcomes across physical states due to nonlinear dynamics and state uncertainty; (iii) small errors propagate and amplify across tightly coupled perception-decision-action loops; and (iv) safety is not compositional across time or system layers, enabling locally safe decisions to accumulate into globally unsafe behavior. These insights suggest that securing embodied AI requires moving beyond component-level defenses toward system-level reasoning about physical risk, uncertainty, and failure propagation. Subjects: Cryptography and Security (cs.CR) ; Artificial Intelligence (cs.AI) Cite as: arXiv:2602.17345 [cs.CR] (or arXiv:2602.17345v1 [cs.CR] for this version) https://doi.org/10.48550/arXiv.2602.17345 Focus to learn more arXiv-issued DOI via DataCite (pending registration) Submission history From: Yue Zhang [ view email ] [v1] Thu, 19 Feb 2026 13:29:00 UTC (14,783 KB) Full-text links: Access Paper: View a PDF of the paper titled What Breaks Embodied AI Security:LLM Vulnerabilities, CPS Flaws,or Something Else?, by Boyang Ma and Hechuan Guo and Peizhuo Lv and Minghui Xu and Xuelong Dai and YeChao Zhang and Yijun Yang and Yue Zhang View PDF HTML (experimental) TeX Source view license Current browse context: cs.CR &lt; prev | next &gt; new | recent | 2026-02 Change to browse by: cs cs.AI References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-82">42. When Vision Overrides Language: Evaluating and Mitigating Counterfactual Failures in VLAs</h2>
<ul>
<li>链接：https://arxiv.org/abs/2602.17659v1</li>
<li>来源：arxiv</li>
<li>摘要：Vision-Language-Action models (VLAs) promise to ground language instructions in robot control, yet in practice often fail to faithfully follow language. When presented with instructions that lack strong scene-specific supervision, VLAs suffer from counterfactual failures: they act based on vision shortcuts induced by dataset biases, repeatedly executing well-learned behaviors and selecting objects frequently seen during training regardless of language intent. To systematically study it, we introduce LIBERO-CF, the first counterfactual benchmark for VLAs that evaluates language following capability by assigning alternative instructions under visually plausible LIBERO layouts. Our evaluation reveals that counterfactual failures are prevalent yet underexplored across state-of-the-art VLAs. We propose Counterfactual Action Guidance (CAG), a simple yet effective dual-branch inference scheme that explicitly regularizes language conditioning in VLAs. CAG combines a standard VLA policy with a language-unconditioned Vision-Action (VA) module, enabling counterfactual comparison during action selection. This design reduces reliance on visual shortcuts, improves robustness on under-observed ta</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">VLAs在缺乏场景特定监督的情况下容易出现反事实失败，这种失败指的是它们基于视觉捷径执行行为，这些捷径由数据集偏见引起。反事实失败在最先进的VLAs中普遍存在，但尚未被充分探索。为了解决这一问题，LIBERO-CF是首个评估VLAs反事实能力的基准。为减少对视觉捷径的依赖，CAG通过结合标准VLAs策略和未受语言条件的VA模块设计而成。CAG的这种设计提高了对未观察任务的鲁棒性，而无需额外演示或修改现有架构或预训练模型。在LIBERO-CF上的表现上，CAG优于标准VLAs，特别是在未观察任务上。此外，CAG在真实世界评估中减少了9.4%的反事实失败，并提高了17.2%的任务成功率。因此，CAG为解决反事实失败问题提供了一种有效的方法。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>VLAs在缺乏场景特定监督的情况下容易出现反事实失败。</li>
<li>反事实失败指的是VLAs基于视觉捷径执行行为，这些捷径由数据集偏见引起。</li>
<li>LIBERO-CF是首个评估VLAs反事实能力的基准。</li>
<li>反事实失败在最先进的VLAs中普遍存在但尚未被充分探索。</li>
<li>CAG通过结合标准VLAs策略和未受语言条件的VA模块来减少对视觉捷径的依赖。</li>
<li>CAG设计提高了对未观察任务的鲁棒性，而无需额外演示或修改现有架构或预训练模型。</li>
<li>CAG在LIBERO-CF上的表现优于标准VLAs，特别是在未观察任务上。</li>
<li>CAG在真实世界评估中减少了9.4%的反事实失败，并提高了17.2%的任务成功率。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-83">正文（抓取，非 AI）</h3>
<p>[2602.17659v1] When Vision Overrides Language: Evaluating and Mitigating Counterfactual Failures in VLAs Computer Science &gt; Computer Vision and Pattern Recognition arXiv:2602.17659v1 (cs) [Submitted on 19 Feb 2026] Title: When Vision Overrides Language: Evaluating and Mitigating Counterfactual Failures in VLAs Authors: Yu Fang , Yuchun Feng , Dong Jing , Jiaqi Liu , Yue Yang , Zhenyu Wei , Daniel Szafir , Mingyu Ding View a PDF of the paper titled When Vision Overrides Language: Evaluating and Mitigating Counterfactual Failures in VLAs, by Yu Fang and 7 other authors View PDF HTML (experimental) Abstract: Vision-Language-Action models (VLAs) promise to ground language instructions in robot control, yet in practice often fail to faithfully follow language. When presented with instructions that lack strong scene-specific supervision, VLAs suffer from counterfactual failures: they act based on vision shortcuts induced by dataset biases, repeatedly executing well-learned behaviors and selecting objects frequently seen during training regardless of language intent. To systematically study it, we introduce LIBERO-CF, the first counterfactual benchmark for VLAs that evaluates language following capability by assigning alternative instructions under visually plausible LIBERO layouts. Our evaluation reveals that counterfactual failures are prevalent yet underexplored across state-of-the-art VLAs. We propose Counterfactual Action Guidance (CAG), a simple yet effective dual-branch inference scheme that explicitly regularizes language conditioning in VLAs. CAG combines a standard VLA policy with a language-unconditioned Vision-Action (VA) module, enabling counterfactual comparison during action selection. This design reduces reliance on visual shortcuts, improves robustness on under-observed tasks, and requires neither additional demonstrations nor modifications to existing architectures or pretrained models. Extensive experiments demonstrate its plug-and-play integration across diverse VLAs and consistent improvements. For example, on LIBERO-CF, CAG improves $\pi_{0.5}$ by 9.7% in language following accuracy and 3.6% in task success on under-observed tasks using a training-free strategy, with further gains of 15.5% and 8.5%, respectively, when paired with a VA model. In real-world evaluations, CAG reduces counterfactual failures of 9.4% and improves task success by 17.2% on average. Comments: Website: this https URL Subjects: Computer Vision and Pattern Recognition (cs.CV) ; Robotics (cs.RO) Cite as: arXiv:2602.17659 [cs.CV] (or arXiv:2602.17659v1 [cs.CV] for this version) https://doi.org/10.48550/arXiv.2602.17659 Focus to learn more arXiv-issued DOI via DataCite (pending registration) Submission history From: Yu Fang [ view email ] [v1] Thu, 19 Feb 2026 18:59:20 UTC (3,888 KB) Full-text links: Access Paper: View a PDF of the paper titled When Vision Overrides Language: Evaluating and Mitigating Counterfactual Failures in VLAs, by Yu Fang and 7 other authors View PDF HTML (experimental) TeX Source view license Current browse context: cs.CV &lt; prev | next &gt; new | recent | 2026-02 Change to browse by: cs cs.RO References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-84">43. Pushing the Frontier of Black-Box LVLM Attacks via Fine-Grained Detail Targeting</h2>
<ul>
<li>链接：https://arxiv.org/abs/2602.17645v1</li>
<li>来源：arxiv</li>
<li>摘要：Black-box adversarial attacks on Large Vision-Language Models (LVLMs) are challenging due to missing gradients and complex multimodal boundaries. While prior state-of-the-art transfer-based approaches like M-Attack perform well using local crop-level matching between source and target images, we find this induces high-variance, nearly orthogonal gradients across iterations, violating coherent local alignment and destabilizing optimization. We attribute this to (i) ViT translation sensitivity that yields spike-like gradients and (ii) structural asymmetry between source and target crops. We reformulate local matching as an asymmetric expectation over source transformations and target semantics, and build a gradient-denoising upgrade to M-Attack. On the source side, Multi-Crop Alignment (MCA) averages gradients from multiple independently sampled local views per iteration to reduce variance. On the target side, Auxiliary Target Alignment (ATA) replaces aggressive target augmentation with a small auxiliary set from a semantically correlated distribution, producing a smoother, lower-variance target manifold. We further reinterpret momentum as Patch Momentum, replaying historical crop gr</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">ViT翻译敏感性导致梯度尖峰，而结构不对称性则使得源和目标作物之间的梯度几乎正交，这增加了梯度方向的不一致性。为了解决这一问题，MCA通过迭代中多个独立采样的局部视图平均梯度来减少方差，从而使得目标 manifold 更平滑。同时，ATA通过使用语义相关的小辅助集来平滑目标 manifold，进一步减少梯度的不一致性。此外，Patch Momentum通过重播历史作物梯度来加强可转移方向，进一步提升模型的鲁棒性。M-Attack-V2通过上述技术的结合，显著提高了前沿LVLM的黑盒攻击成功率，并在多个模型上的成功率显著提高，超越了先前的黑盒LVLM攻击。Patch-size ensemble（PE+）与Patch Momentum结合，进一步增强了可转移方向，从而使得M-Attack-V2在多个模型上的成功率显著提高。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>ViT翻译敏感性导致梯度尖峰。</li>
<li>结构不对称性导致源和目标作物之间的梯度几乎正交。</li>
<li>MCA通过迭代中多个独立采样的局部视图平均梯度来减少方差。</li>
<li>ATA通过使用语义相关的小辅助集来平滑目标 manifold。</li>
<li>Patch Momentum通过重播历史作物梯度来加强可转移方向。</li>
<li>M-Attack-V2显著提高了前沿LVLM的黑盒攻击成功率。</li>
<li>MCA和ATA共同作用，使目标 manifold 更平滑、方差更低。</li>
<li>Patch-size ensemble（PE+）与Patch Momentum结合，增强了可转移方向。</li>
<li>M-Attack-V2在多个模型上的成功率显著提高，超越了先前的黑盒LVLM攻击。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-85">正文（抓取，非 AI）</h3>
<p>[2602.17645v1] Pushing the Frontier of Black-Box LVLM Attacks via Fine-Grained Detail Targeting Computer Science &gt; Machine Learning arXiv:2602.17645v1 (cs) [Submitted on 19 Feb 2026] Title: Pushing the Frontier of Black-Box LVLM Attacks via Fine-Grained Detail Targeting Authors: Xiaohan Zhao , Zhaoyi Li , Yaxin Luo , Jiacheng Cui , Zhiqiang Shen View a PDF of the paper titled Pushing the Frontier of Black-Box LVLM Attacks via Fine-Grained Detail Targeting, by Xiaohan Zhao and Zhaoyi Li and Yaxin Luo and Jiacheng Cui and Zhiqiang Shen View PDF HTML (experimental) Abstract: Black-box adversarial attacks on Large Vision-Language Models (LVLMs) are challenging due to missing gradients and complex multimodal boundaries. While prior state-of-the-art transfer-based approaches like M-Attack perform well using local crop-level matching between source and target images, we find this induces high-variance, nearly orthogonal gradients across iterations, violating coherent local alignment and destabilizing optimization. We attribute this to (i) ViT translation sensitivity that yields spike-like gradients and (ii) structural asymmetry between source and target crops. We reformulate local matching as an asymmetric expectation over source transformations and target semantics, and build a gradient-denoising upgrade to M-Attack. On the source side, Multi-Crop Alignment (MCA) averages gradients from multiple independently sampled local views per iteration to reduce variance. On the target side, Auxiliary Target Alignment (ATA) replaces aggressive target augmentation with a small auxiliary set from a semantically correlated distribution, producing a smoother, lower-variance target manifold. We further reinterpret momentum as Patch Momentum, replaying historical crop gradients; combined with a refined patch-size ensemble (PE+), this strengthens transferable directions. Together these modules form M-Attack-V2, a simple, modular enhancement over M-Attack that substantially improves transfer-based black-box attacks on frontier LVLMs: boosting success rates on Claude-4.0 from 8% to 30%, Gemini-2.5-Pro from 83% to 97%, and GPT-5 from 98% to 100%, outperforming prior black-box LVLM attacks. Code and data are publicly available at: this https URL . Comments: Code at: this https URL Subjects: Machine Learning (cs.LG) ; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV) Cite as: arXiv:2602.17645 [cs.LG] (or arXiv:2602.17645v1 [cs.LG] for this version) https://doi.org/10.48550/arXiv.2602.17645 Focus to learn more arXiv-issued DOI via DataCite (pending registration) Submission history From: Xiaohan Zhao [ view email ] [v1] Thu, 19 Feb 2026 18:54:32 UTC (4,395 KB) Full-text links: Access Paper: View a PDF of the paper titled Pushing the Frontier of Black-Box LVLM Attacks via Fine-Grained Detail Targeting, by Xiaohan Zhao and Zhaoyi Li and Yaxin Luo and Jiacheng Cui and Zhiqiang Shen View PDF HTML (experimental) TeX Source view license Current browse context: cs.LG &lt; prev | next &gt; new | recent | 2026-02 Change to browse by: cs cs.AI cs.CL cs.CV References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) IArxiv recommender toggle IArxiv Recommender ( What is IArxiv? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-86">44. Semi-Local Exchange-Correlation Approximations in Density Functional Theory</h2>
<ul>
<li>链接：https://arxiv.org/abs/2602.17333v1</li>
<li>来源：arxiv</li>
<li>摘要：Density functional theory is the workhorse of modern electronic structure calculations, with wide-ranging applications in chemistry, physics, materials science, and machine learning. At its heart lies the exchange-correlation functional, a quantity which exactly encapsulates the many-body effects stemming from the quantum mechanical interactions between the electrons. Yet, the exact functional is unknown, and computationally tractable approximations are therefore necessary for practical applications. Over the past six decades, hundreds of density functional approximations have been proposed with varying degrees of accuracy and computational efficiency.   This review surveys the theoretical foundations of semi-local functionals, including local density approximations, generalized gradient approximations, and meta-generalized gradient approximations. We provide a comprehensive, consistently organized discussion that consolidates both historical developments and recent advances in this field. Beginning with the essential concepts of Kohn-Sham density functional theory, we present the construction principles of semi-local exchange-correlation functionals. Special attention is given to </li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">易漏但重要的概念在于密度泛函理论（Density Functional Theory, DFT）中交换相关泛函的精确形式未知，因此需要计算上可行的近似方法。局部密度近似（Local Density Approximation, LDA）虽然简化了问题，但其准确性较低；而广义梯度近似（Generalized Gradient Approximation, GGA）则在计算成本增加的情况下提高了准确性。进一步地，元广义梯度近似（Meta-Generalized Gradient Approximation, MGGA）在准确性上有了更显著的提升。Kohn-Sham密度泛函理论是这一领域的基石，其发展受到物理动机和数学性质的双重指导。在实际应用中，还需要考虑系统的具体需求。半局部泛函是DFT的关键组成部分，这一理论在历史和近期的发展中得到了巩固。本文不仅为新入门者提供了一个全面的介绍，也是从业者的一个综合参考。因此，通过这些概念的梳理，读者可以更好地理解密度泛函理论的发展和应用。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>exact exchange-correlation functional is unknown</li>
<li>computational tractable approximations are necessary</li>
<li>local density approximations are simpler but less accurate</li>
<li>generalized gradient approximations improve accuracy at computational cost</li>
<li>meta-generalized gradient approximations further enhance accuracy</li>
<li>Kohn-Sham density functional theory is foundational</li>
<li>physical motivations guide functional development</li>
<li>mathematical properties influence functional construction</li>
<li>practical considerations determine applicability across systems</li>
<li>semi-local functionals are a key component of density functional theory</li>
<li>this review consolidates historical and recent advances</li>
<li>this work serves as an introduction for newcomers</li>
<li>this review is a comprehensive reference for practitioners</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-87">正文（抓取，非 AI）</h3>
<p>[2602.17333v1] Semi-Local Exchange-Correlation Approximations in Density Functional Theory Physics &gt; Chemical Physics arXiv:2602.17333v1 (physics) [Submitted on 19 Feb 2026] Title: Semi-Local Exchange-Correlation Approximations in Density Functional Theory Authors: Fabien Tran , Susi Lehtola , Stefano Pittalis , Miguel A. L. Marques View a PDF of the paper titled Semi-Local Exchange-Correlation Approximations in Density Functional Theory, by Fabien Tran and Susi Lehtola and Stefano Pittalis and Miguel A. L. Marques View PDF Abstract: Density functional theory is the workhorse of modern electronic structure calculations, with wide-ranging applications in chemistry, physics, materials science, and machine learning. At its heart lies the exchange-correlation functional, a quantity which exactly encapsulates the many-body effects stemming from the quantum mechanical interactions between the electrons. Yet, the exact functional is unknown, and computationally tractable approximations are therefore necessary for practical applications. Over the past six decades, hundreds of density functional approximations have been proposed with varying degrees of accuracy and computational efficiency. This review surveys the theoretical foundations of semi-local functionals, including local density approximations, generalized gradient approximations, and meta-generalized gradient approximations. We provide a comprehensive, consistently organized discussion that consolidates both historical developments and recent advances in this field. Beginning with the essential concepts of Kohn-Sham density functional theory, we present the construction principles of semi-local exchange-correlation functionals. Special attention is given to the physical motivations underlying functional development, the mathematical properties that guide their construction, and the practical considerations that determine their applicability across different chemical and physical systems. This work is intended to serve as both a introduction for newcomers to the field and a comprehensive reference for practitioners. By consolidating the extensive literature on semi-local functionals and providing a unified framework for understanding their construction and application, we aim to facilitate further developments in density functional approximations and their use in tackling the diverse challenges of modern computational chemistry and condensed matter physics. Comments: 194 pages, 2 figures Subjects: Chemical Physics (physics.chem-ph) ; Computational Physics (physics.comp-ph) Cite as: arXiv:2602.17333 [physics.chem-ph] (or arXiv:2602.17333v1 [physics.chem-ph] for this version) https://doi.org/10.48550/arXiv.2602.17333 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Susi Lehtola [ view email ] [v1] Thu, 19 Feb 2026 12:59:51 UTC (756 KB) Full-text links: Access Paper: View a PDF of the paper titled Semi-Local Exchange-Correlation Approximations in Density Functional Theory, by Fabien Tran and Susi Lehtola and Stefano Pittalis and Miguel A. L. Marques View PDF TeX Source view license Current browse context: physics.chem-ph &lt; prev | next &gt; new | recent | 2026-02 Change to browse by: physics physics.comp-ph References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-88">45. Conditional Flow Matching for Continuous Anomaly Detection in Autonomous Driving on a Manifold-Aware Spectral Space</h2>
<ul>
<li>链接：https://arxiv.org/abs/2602.17586v1</li>
<li>来源：arxiv</li>
<li>摘要：Safety validation for Level 4 autonomous vehicles (AVs) is currently bottlenecked by the inability to scale the detection of rare, high-risk long-tail scenarios using traditional rule-based heuristics. We present Deep-Flow, an unsupervised framework for safety-critical anomaly detection that utilizes Optimal Transport Conditional Flow Matching (OT-CFM) to characterize the continuous probability density of expert human driving behavior. Unlike standard generative approaches that operate in unstable, high-dimensional coordinate spaces, Deep-Flow constrains the generative process to a low-rank spectral manifold via a Principal Component Analysis (PCA) bottleneck. This ensures kinematic smoothness by design and enables the computation of the exact Jacobian trace for numerically stable, deterministic log-likelihood estimation. To resolve multi-modal ambiguity at complex junctions, we utilize an Early Fusion Transformer encoder with lane-aware goal conditioning, featuring a direct skip-connection to the flow head to maintain intent-integrity throughout the network. We introduce a kinematic complexity weighting scheme that prioritizes high-energy maneuvers (quantified via path tortuosity </li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">OT-CFM方法通过最优传输条件流匹配来表征专家人类驾驶行为的连续概率密度，而Deep-Flow框架则通过主成分分析瓶颈将生成过程约束在低秩谱流形上，从而解决复杂交叉口的多模态歧义。此外，Deep-Flow通过计算精确雅克比迹确保数值稳定和确定的对数似然估计，同时通过直接跳过连接保持网络中的意图完整性。在训练过程中，引入了动力学复杂性加权方案，使得模型能够识别出传统安全过滤器忽略的超出分布的行为，如车道边界违规和非规范交叉口行为。因此，Deep-Flow为定义统计安全门提供了数学严谨的基础，并揭示了动力学危险和语义不合规之间的根本区别。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>OT-CFM方法通过最优传输条件流匹配来表征专家人类驾驶行为的连续概率密度。</li>
<li>Deep-Flow框架通过主成分分析瓶颈将生成过程约束在低秩谱流形上。</li>
<li>Deep-Flow通过计算精确雅克比迹来确保数值稳定和确定的对数似然估计。</li>
<li>Early Fusion Transformer编码器通过车道感知目标条件化来解决复杂交叉口的多模态歧义。</li>
<li>Deep-Flow通过直接跳过连接保持网络中的意图完整性。</li>
<li>训练过程中引入了动力学复杂性加权方案。</li>
<li>Deep-Flow识别出传统安全过滤器忽略的超出分布的行为，如车道边界违规和非规范交叉口行为。</li>
<li>Deep-Flow为定义统计安全门提供了数学严谨的基础。</li>
<li>Deep-Flow揭示了动力学危险和语义不合规之间的根本区别。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-89">正文（抓取，非 AI）</h3>
<p>[2602.17586v1] Conditional Flow Matching for Continuous Anomaly Detection in Autonomous Driving on a Manifold-Aware Spectral Space Computer Science &gt; Robotics arXiv:2602.17586v1 (cs) [Submitted on 19 Feb 2026] Title: Conditional Flow Matching for Continuous Anomaly Detection in Autonomous Driving on a Manifold-Aware Spectral Space Authors: Antonio Guillen-Perez View a PDF of the paper titled Conditional Flow Matching for Continuous Anomaly Detection in Autonomous Driving on a Manifold-Aware Spectral Space, by Antonio Guillen-Perez View PDF HTML (experimental) Abstract: Safety validation for Level 4 autonomous vehicles (AVs) is currently bottlenecked by the inability to scale the detection of rare, high-risk long-tail scenarios using traditional rule-based heuristics. We present Deep-Flow, an unsupervised framework for safety-critical anomaly detection that utilizes Optimal Transport Conditional Flow Matching (OT-CFM) to characterize the continuous probability density of expert human driving behavior. Unlike standard generative approaches that operate in unstable, high-dimensional coordinate spaces, Deep-Flow constrains the generative process to a low-rank spectral manifold via a Principal Component Analysis (PCA) bottleneck. This ensures kinematic smoothness by design and enables the computation of the exact Jacobian trace for numerically stable, deterministic log-likelihood estimation. To resolve multi-modal ambiguity at complex junctions, we utilize an Early Fusion Transformer encoder with lane-aware goal conditioning, featuring a direct skip-connection to the flow head to maintain intent-integrity throughout the network. We introduce a kinematic complexity weighting scheme that prioritizes high-energy maneuvers (quantified via path tortuosity and jerk) during the simulation-free training process. Evaluated on the Waymo Open Motion Dataset (WOMD), our framework achieves an AUC-ROC of 0.766 against a heuristic golden set of safety-critical events. More significantly, our analysis reveals a fundamental distinction between kinematic danger and semantic non-compliance. Deep-Flow identifies a critical predictability gap by surfacing out-of-distribution behaviors, such as lane-boundary violations and non-normative junction maneuvers, that traditional safety filters overlook. This work provides a mathematically rigorous foundation for defining statistical safety gates, enabling objective, data-driven validation for the safe deployment of autonomous fleets. Subjects: Robotics (cs.RO) ; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) Cite as: arXiv:2602.17586 [cs.RO] (or arXiv:2602.17586v1 [cs.RO] for this version) https://doi.org/10.48550/arXiv.2602.17586 Focus to learn more arXiv-issued DOI via DataCite (pending registration) Submission history From: Antonio Guillen-Perez [ view email ] [v1] Thu, 19 Feb 2026 18:10:16 UTC (2,542 KB) Full-text links: Access Paper: View a PDF of the paper titled Conditional Flow Matching for Continuous Anomaly Detection in Autonomous Driving on a Manifold-Aware Spectral Space, by Antonio Guillen-Perez View PDF HTML (experimental) TeX Source view license Current browse context: cs.RO &lt; prev | next &gt; new | recent | 2026-02 Change to browse by: cs cs.AI cs.LG References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-90">46. Characterization of compressible fluctuations in solar wind streams dominated by balanced and imbalanced turbulence: Parker Solar Probe, Solar Orbiter and Wind observations</h2>
<ul>
<li>链接：https://arxiv.org/abs/2602.17606v1</li>
<li>来源：arxiv</li>
<li>摘要：Characterizing compressible fluctuations in the solar wind is essential for understanding their role in solar wind acceleration and heating, yet their origin and evolution across different turbulence regimes remain poorly understood. In this study, we carry out a statistical analysis of the properties of compressible fluctuations in solar wind dominated by balanced and imbalanced turbulence. Using in-situ measurements from Wind, Solar Orbiter, and Parker Solar Probe, we investigate the scale dependence of density and magnetic field fluctuations and their correlations with plasma beta and radial distance. Our results indicate that solar wind compressibility is likely affected by both expansion effects and compressible dynamics governed by local plasma conditions. The non-Alfvenic wind is dominated by anti-correlated fluctuations, whereas the Alfvenic wind contains a mixture of correlated and anti-correlated fluctuations, though the latter remain prevalent. While the anti-correlated component is consistent with MHD slow magnetosonic modes, the correlated (fast mode-like) component is not reproduced by predictions from either linear MHD theory or nonlinear models of forced compressibl</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">压缩波动在太阳风中的作用尚未完全理解，但已知非阿尔芬风主要由反相关波动支配，而阿尔芬风则包含相关和反相关波动的混合，但相关波动更为常见。反相关成分与MHD慢模子波一致，而相关成分则无法由线性MHD理论或非线性模型再现。慢模子波解释了观测到的β依赖性和帕克太阳探测器测量的增强密度波动，这表明慢模子波对压缩能量预算有显著贡献。因此，慢模子波可能在太阳附近对太阳风加热和加速起重要作用。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>压缩波动在太阳风中的作用尚未完全理解。</li>
<li>非阿尔芬风主要由反相关波动支配。</li>
<li>阿尔芬风包含相关和反相关波动的混合，但相关波动更为常见。</li>
<li>反相关成分与MHD慢模子波一致。</li>
<li>相关成分无法由线性MHD理论或非线性模型再现。</li>
<li>慢模子波解释了观测到的β依赖性和帕克太阳探测器测量的增强密度波动。</li>
<li>这表明慢模子波对压缩能量预算有显著贡献。</li>
<li>慢模子波可能在太阳附近对太阳风加热和加速起重要作用。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-91">正文（抓取，非 AI）</h3>
<p>[2602.17606v1] Characterization of compressible fluctuations in solar wind streams dominated by balanced and imbalanced turbulence: Parker Solar Probe, Solar Orbiter and Wind observations Physics &gt; Plasma Physics arXiv:2602.17606v1 (physics) [Submitted on 19 Feb 2026] Title: Characterization of compressible fluctuations in solar wind streams dominated by balanced and imbalanced turbulence: Parker Solar Probe, Solar Orbiter and Wind observations Authors: C.A. Gonzalez , C. Gonzalez , A. Tenerani View a PDF of the paper titled Characterization of compressible fluctuations in solar wind streams dominated by balanced and imbalanced turbulence: Parker Solar Probe, Solar Orbiter and Wind observations, by C.A. Gonzalez and 1 other authors View PDF HTML (experimental) Abstract: Characterizing compressible fluctuations in the solar wind is essential for understanding their role in solar wind acceleration and heating, yet their origin and evolution across different turbulence regimes remain poorly understood. In this study, we carry out a statistical analysis of the properties of compressible fluctuations in solar wind dominated by balanced and imbalanced turbulence. Using in-situ measurements from Wind, Solar Orbiter, and Parker Solar Probe, we investigate the scale dependence of density and magnetic field fluctuations and their correlations with plasma beta and radial distance. Our results indicate that solar wind compressibility is likely affected by both expansion effects and compressible dynamics governed by local plasma conditions. The non-Alfvenic wind is dominated by anti-correlated fluctuations, whereas the Alfvenic wind contains a mixture of correlated and anti-correlated fluctuations, though the latter remain prevalent. While the anti-correlated component is consistent with MHD slow magnetosonic modes, the correlated (fast mode-like) component is not reproduced by predictions from either linear MHD theory or nonlinear models of forced compressible fluctuations. Nevertheless, the dominant slow mode component explains the observed dependence on beta and the enhanced density fluctuations measured by Parker Solar Probe. This further suggests that slow mode waves contribute significantly to the compressible energy budget near the Sun and may play an important role in solar wind heating and acceleration close to the Sun. Subjects: Plasma Physics (physics.plasm-ph) ; Solar and Stellar Astrophysics (astro-ph.SR); Space Physics (physics.space-ph) Cite as: arXiv:2602.17606 [physics.plasm-ph] (or arXiv:2602.17606v1 [physics.plasm-ph] for this version) https://doi.org/10.48550/arXiv.2602.17606 Focus to learn more arXiv-issued DOI via DataCite (pending registration) Submission history From: Carlos González [ view email ] [v1] Thu, 19 Feb 2026 18:31:00 UTC (2,506 KB) Full-text links: Access Paper: View a PDF of the paper titled Characterization of compressible fluctuations in solar wind streams dominated by balanced and imbalanced turbulence: Parker Solar Probe, Solar Orbiter and Wind observations, by C.A. Gonzalez and 1 other authors View PDF HTML (experimental) TeX Source view license Current browse context: physics.plasm-ph &lt; prev | next &gt; new | recent | 2026-02 Change to browse by: astro-ph astro-ph.SR physics physics.space-ph References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-92">47. Do Hackers Dream of Electric Teachers?: A Large-Scale, In-Situ Evaluation of Cybersecurity Student Behaviors and Performance with AI Tutors</h2>
<ul>
<li>链接：https://arxiv.org/abs/2602.17448v1</li>
<li>来源：arxiv</li>
<li>摘要：To meet the ever-increasing demands of the cybersecurity workforce, AI tutors have been proposed for personalized, scalable education. But, while AI tutors have shown promise in introductory programming courses, no work has evaluated their use in hands-on exploration and exploitation of systems (e.g., ``capture-the-flag'') commonly used to teach cybersecurity. Thus, despite growing interest and need, no work has evaluated how students use AI tutors or whether they benefit from their presence in real, large-scale cybersecurity courses. To answer this, we conducted a semester-long observational study on the use of an embedded AI tutor with 309 students in an upper-division introductory cybersecurity course. By analyzing 142,526 student queries sent to the AI tutor across 396 cybersecurity challenges spanning 9 core cybersecurity topics and an accompanying set of post-semester surveys, we find (1) what queries and conversational strategies students use with AI tutors, (2) how these strategies correlate with challenge completion, and (3) students' perceptions of AI tutors in cybersecurity education. In particular, we identify three broad AI tutor conversational styles among users: Shor</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">AI tutors have shown promise in introductory programming courses but not in hands-on cybersecurity education, indicating their limited effectiveness in more complex and practical learning environments. A recent study evaluated the use of AI tutors in a large-scale, real-world cybersecurity course, revealing that students employed three broad conversational styles: short, reactive, and proactive. These conversational styles significantly predicted the completion of challenges, suggesting that the interaction patterns between students and AI tutors can greatly influence learning outcomes. However, student reports indicated that AI tutors became less useful for harder material, implying that the complexity of the subject matter may exceed the current capabilities of AI tutors. This study provides valuable insights for security educators and developers, offering practical suggestions on how to effectively utilize AI tutors in cybersecurity education.</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>AI tutors have shown promise in introductory programming courses but not in hands-on cybersecurity education.</li>
<li>The study evaluated the use of AI tutors in a large-scale, real-world cybersecurity course.</li>
<li>Students used three broad conversational styles with AI tutors: short, reactive, and proactive.</li>
<li>The use of conversational styles significantly predicted challenge completion.</li>
<li>AI tutors became less useful for harder material according to student reports.</li>
<li>The study provides suggestions for security educators and developers on practical AI tutor use.</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-93">正文（抓取，非 AI）</h3>
<p>[2602.17448v1] Do Hackers Dream of Electric Teachers?: A Large-Scale, In-Situ Evaluation of Cybersecurity Student Behaviors and Performance with AI Tutors Computer Science &gt; Human-Computer Interaction arXiv:2602.17448v1 (cs) [Submitted on 19 Feb 2026] Title: Do Hackers Dream of Electric Teachers?: A Large-Scale, In-Situ Evaluation of Cybersecurity Student Behaviors and Performance with AI Tutors Authors: Michael Tompkins , Nihaarika Agarwal , Ananta Soneji , Robert Wasinger , Connor Nelson , Kevin Leach , Rakibul Hasan , Adam Doupé , Daniel Votipka , Yan Shoshitaishvili , Jaron Mink View a PDF of the paper titled Do Hackers Dream of Electric Teachers?: A Large-Scale, In-Situ Evaluation of Cybersecurity Student Behaviors and Performance with AI Tutors, by Michael Tompkins and 10 other authors View PDF HTML (experimental) Abstract: To meet the ever-increasing demands of the cybersecurity workforce, AI tutors have been proposed for personalized, scalable education. But, while AI tutors have shown promise in introductory programming courses, no work has evaluated their use in hands-on exploration and exploitation of systems (e.g., ``capture-the-flag'') commonly used to teach cybersecurity. Thus, despite growing interest and need, no work has evaluated how students use AI tutors or whether they benefit from their presence in real, large-scale cybersecurity courses. To answer this, we conducted a semester-long observational study on the use of an embedded AI tutor with 309 students in an upper-division introductory cybersecurity course. By analyzing 142,526 student queries sent to the AI tutor across 396 cybersecurity challenges spanning 9 core cybersecurity topics and an accompanying set of post-semester surveys, we find (1) what queries and conversational strategies students use with AI tutors, (2) how these strategies correlate with challenge completion, and (3) students' perceptions of AI tutors in cybersecurity education. In particular, we identify three broad AI tutor conversational styles among users: Short (bounded, few-turn exchanges), Reactive (repeatedly submitting code and errors), and Proactive (driving problem-solving through targeted inquiry). We also find that the use of these styles significantly predicts challenge completion, and that this effect increases as materials become more advanced. Furthermore, students valued the tutor's availability but reported that it became less useful for harder material. Based on this, we provide suggestions for security educators and developers on practical AI tutor use. Comments: 33 pages, 7 figures Subjects: Human-Computer Interaction (cs.HC) ACM classes: K.3.2; K.3.1; H.1.2; K.6.5 Cite as: arXiv:2602.17448 [cs.HC] (or arXiv:2602.17448v1 [cs.HC] for this version) https://doi.org/10.48550/arXiv.2602.17448 Focus to learn more arXiv-issued DOI via DataCite (pending registration) Submission history From: Michael Tompkins [ view email ] [v1] Thu, 19 Feb 2026 15:13:43 UTC (440 KB) Full-text links: Access Paper: View a PDF of the paper titled Do Hackers Dream of Electric Teachers?: A Large-Scale, In-Situ Evaluation of Cybersecurity Student Behaviors and Performance with AI Tutors, by Michael Tompkins and 10 other authors View PDF HTML (experimental) TeX Source view license Current browse context: cs.HC &lt; prev | next &gt; new | recent | 2026-02 Change to browse by: cs References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-94">48. [干货]深入浅出LSTM及其Python代码实现</h2>
<ul>
<li>链接：https://www.zhihu.com/tardis/zm/art/104475016</li>
<li>来源：bing</li>
<li>摘要：2022年5月12日 · 3. 长短时间记忆网络（LSTM） 3.1 LSTM与RNN的关系 长短期记忆（Long Short Term Memory，LSTM）网络是一种特殊的RNN模型，其特殊的结构设计使得它可以避免长期依赖问题， …</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">首先，神经网络的输出信号只与输入信号有关，而不受输入信号的先后顺序影响，这使得神经网络在处理需要理解上下文信息的任务时显得力不从心。其次，RNN通过内部循环允许信息在神经元之间传递，从而保留时间序列的上下文信息，理论上可以保留过去任意时刻的信息，但在实际使用中，由于信息传递会逐渐衰减，这种能力往往受到限制。因此，RNN对于信息的长期依赖问题没有很好的处理办法。为了解决这一问题，LSTM应运而生，作为一种特殊形式的RNN，LSTM通过特殊的结构设计避免了长期依赖问题。具体而言，LSTM的隐藏状态和细胞状态可以分别存储和传递信息，从而保留时间序列的上下文。此外，LSTM通过门机制（输入门、遗忘门、输出门）来控制信息的流动，其中遗忘门决定哪些信息应该被保留，哪些应该被丢弃；输入门决定哪些新的信息应该被添加到细胞状态中；输出门决定哪些细胞状态中的信息应该被输出。因此，LSTM通过细胞状态和门机制来实现长期依赖问题的解决。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>神经网络的输出信号只与输入信号有关，而与输入信号的先后顺序无关。</li>
<li>神经网络的“记忆”能力不足，无法处理需要理解上下文意思的信号。</li>
<li>RNN通过内部循环允许信息在神经元之间传递，从而保留时间序列的上下文信息。</li>
<li>RNN理论上可以保留过去任意时刻的信息，但在实际使用时信息传递会逐渐衰减。</li>
<li>RNN对于信息的长期依赖问题没有很好的处理办法。</li>
<li>LSTM能够有效处理时间序列数据，是RNN的一种特殊形式。</li>
<li>LSTM通过特殊的结构设计避免了长期依赖问题。</li>
<li>LSTM的隐藏状态可以存储和传递信息，从而保留时间序列的上下文。</li>
<li>LSTM的细胞状态可以存储长期信息，而门机制可以控制信息的流入和流出。</li>
<li>LSTM通过门机制（输入门、遗忘门、输出门）来控制信息的流动。</li>
<li>LSTM的遗忘门可以决定哪些信息应该被保留，哪些应该被丢弃。</li>
<li>LSTM的输入门可以决定哪些新的信息应该被添加到细胞状态中。</li>
<li>LSTM的输出门可以决定哪些细胞状态中的信息应该被输出。</li>
<li>LSTM通过细胞状态和门机制来实现长期依赖问题的解决。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-95">正文（抓取，非 AI）</h3>
<p>人工神经网络在近年来大放异彩，在图像识别、语音识别、自然语言处理与大数据分析领域取得了巨大的成功。本文将由浅入深介绍循环神经网络RNN和长短期记忆网络LSTM的基本原理，并基于Pytorch实现一个例子，完整代码在文章最后。 获取更多技术干货： AI与机器人 1. 神经网络简介 1.1 神经网络起源 人工神经网络（Aritificial Neural Networks, ANN）是一种仿生的网络结构，起源于对人类大脑的研究。人工神经网络（Aritificial Neural Networks）也常被简称为神经网络（Neural Networks, NN），基本思想是通过大量简单的神经元之间的相互连接来构造复杂的网络结构，信号（数据）可以在这些神经元之间传递，通过激活不同的神经元和对传递的信号进行加权来使得信号被放大或衰减，经过多次的传递来改变信号的强度和表现形式。 神经网络最早起源于20世纪40年代，神经科学家和控制论专家Warren McCulloch和逻辑学家Walter Pitts基于数学和阈值逻辑算法创造了最早的神经网络计算模型。由于当时的计算资源有限，无法构建层数太多的神经网络（3层以内），因此神经网络的应用范围很局限。随着计算机技术的发展，神经网络层数的增加带来的计算负担已经可以被现代计算机解决，各位前辈大牛对于神经网络的理解也进一步加深。历史上神经网络的发展大致经历了三次高潮：20世纪40年代的控制论、20世纪80年代到90年代中期的联结主义和2006年以来的深度学习。深度学习的出现直接引爆了一部分应用市场，这里有太多的案例可以讲，想了解的读者可参考下面的链接： 极海·GeoHey：深度学习(deep learning)发展史 CDA经管之家：一文带你看遍深度学习发展的成就历程(一) 1.2 传统神经网络的缺陷 本文假设读者已经了解神经网络的基本原理了，如果有读者是初次接触神经网络的知识，这里分享一篇个人觉得非常适合初学者的文章： 神经网络浅讲：从神经元到深度学习 - 计算机的潜意识 - 博客园 1.2.1 传统神经网络的原理回顾 传统的神经网络结构可以用下面这张图表示： 其中： 输入层：可以包含多个神经元，可以接收多维的信号输入（特征信息）； 输出层：可以包含多个神经元，可以输出多维信号； 隐含层：可以包含多个神经网络层，每一层包含多个神经元。 每层的神经元与上一层神经元和下一层神经元连接（类似生物神经元的突触），这些连接通路用于信号传递。每个神经元接收来自上一层的信号输入，使用一定的加和规则将所有的信号输入汇聚到一起，并使用 激活函数 将输入信号激活为输出信号，再将信号传递到下一层。 神经网络为什么要使用激活函数？不同的激活函数有什么不同的作用？读者可参考： SIGAI：神经网络的激活函数总结 神经网络激励函数的作用是什么？有没有形象的解释？ 所以，影响神经网络表现能力的主要因素有神经网络的层数、神经元的个数、神经元之间的连接方式以及神经元所采用的激活函数。神经元之间以不同的连接方式（全连接、部分连接）组合，可以构成不同神经网络，对于不同的信号处理效果也不一样。但是，目前依旧没有一种通用的方法可以根据信号输入的特征来决定神经网络的结构，这也是神经网络模型被称为黑箱的原因之一，带来的问题也就是模型的参数不容易调整，也不清楚其中到底发生了什么。因此，在不断的探索当中，前辈大牛们总结得到了许多经典的神经网络结构：MLP、BP、FFNN、CNN、RNN等。详细的介绍见以下链接： The Neural Network Zoo 多图｜一文看懂25个神经网络模型 神经网络优点很明显，给我们提供了构建模型的便利，你大可不用顾及模型本身是如何作用的，只需要按照规则构建网络，然后使用训练数据集不断调整参数，在许多问题上都能得到一个比较“能接受”的结果，然而我们对其中发生了什么是未可知的。在深度学习领域，许多问题都可以通过构建深层的神经网络模型来解决。这里，我们不对神经网络的优点做过多阐述。 1.2.2 传统神经网络结构的缺陷 从传统的神经网络结构我们可以看出，信号流从输入层到输出层依次流过，同一层级的神经元之间，信号是不会相互传递的。这样就会导致一个问题，输出信号只与输入信号有关，而与输入信号的先后顺序无关。并且神经元本身也不具有存储信息的能力，整个网络也就没有“记忆”能力，当输入信号是一个跟时间相关的信号时，如果我们想要通过这段信号的“上下文”信息来理解一段时间序列的意思，传统的神经网络结构就显得无力了。与我们人类的理解过程类似，我们听到一句话时往往需要通过这句话中词语出现的顺序以及我们之前所学的关于这些词语的意思来理解整段话的意思，而不是简单的通过其中的几个词语来理解。 例如，在自然语言处理领域，我们要让神经网络理解这样一句话：“地球上最高的山是珠穆朗玛峰”，按照传统的神经网络结构，它可能会将这句话拆分为几个单独的词（地球、上、最高的、山、是、珠穆朗玛峰），分别输入到模型之中，而不管这几个词之间的顺序。然而，直观上我们可以看到，这几个词出现的顺序是与最终这句话要表达的意思是密切相关的，但传统的神经网络结构无法处理这种情况。 因此，我们需要构建具有“记忆”能力的神经网络模型，用来处理需要理解上下文意思的信号，也就是时间序列数据。循环神经网络（RNN）就是用来处理这类信号的，RNN之所以能够有效的处理时间序列数据，主要是基于它比较特殊的运行原理。下面将介绍RNN的构建过程和基本运行原理，然后引入长短期记忆网络（LSTM）。 2. 循环神经网络RNN 本章将介绍循环神经网络的基本原理，参考了下文： Understanding LSTM Networks 以及YouTue上的一个视频： Illustrated Guide to LSTM's and GRU's: A step by step explanation 2.1 RNN的构造过程 RNN是一种特殊的神经网路结构，其本身是包含循环的网络，允许信息在神经元之间传递，如下图所示： 图示是一个RNN结构示意图，图中的 表示神经网络模型， 表示模型的输入信号， 表示模型的输出信号，如果没有 的输出信号传递到 的那个箭头， 这个网络模型与普通的神经网络结构无异。那么这个箭头做了什么事情呢？它允许 将信息传递给 ，神经网络将自己的输出作为输入了！这怎么理解啊？作者第一次看到这个图的时候也是有点懵，读者可以思考一分钟。 关键在于输入信号是一个时间序列，跟时间 有关。也就是说，在 时刻，输入信号 作为神经网络 的输入， 的输出分流为两部分，一部分输出给 ，一部分作为一个隐藏的信号流被输入到 中，在下一次时刻输入信号 时，这部分隐藏的信号流也作为输入信号输入到了 中。此时神经网络 就同时接收了 时刻和 时刻的信号输入了，此时的输出信号又将被传递到下一时刻的 中。如果我们把上面那个图根据时间 展开来看，就是： 看到了吗？ 时刻的信息输出给 时刻的模型 了， 时刻的信息输出给 时刻的模型 了， 。这样，相当于RNN在时间序列上把自己复制了很多遍，每个模型都对应一个时刻的输入，并且当前时刻的输出还作为下一时刻的模型的输入信号。 这样链式的结构揭示了RNN本质上是与序列相关的，是对于时间序列数据最自然的神经网络架构。并且理论上，RNN可以保留以前任意时刻的信息。RNN在语音识别、自然语言处理、图片描述、视频图像处理等领域已经取得了一定的成果，而且还将更加大放异彩。在实际使用的时候，用得最多的一种RNN结构是LSTM，为什么是LSTM呢？我们从普通RNN的局限性说起。 2.2 RNN的局限性 RNN利用了神经网络的“内部循环”来保留时间序列的上下文信息，可以使用过去的信号数据来推测对当前信号的理解，这是非常重要的进步，并且理论上RNN可以保留过去任意时刻的信息。但实际使用RNN时往往遇到问题，请看下面这个例子。 假如我们构造了一个语言模型，可以通过当前这一句话的意思来预测下一个词语。现在有这样一句话：“我是一个中国人，出生在普通家庭，我最常说汉语，也喜欢写汉字。我喜欢妈妈做的菜”。我们的语言模型在预测“我最常说汉语”的“汉语”这个词时，它要预测“我最长说”这后面可能跟的是一个语言，可能是英语，也可能是汉语，那么它需要用到第一句话的“我是中国人”这段话的意思来推测我最常说汉语，而不是英语、法语等。而在预测“我喜欢妈妈做的菜”的最后的词“菜”时并不需要“我是中国人”这个信息以及其他的信息，它跟我是不是一个中国人没有必然的关系。 这个例子告诉我们，想要精确地处理时间序列，有时候我们只需要用到最近的时刻的信息。例如预测“我喜欢妈妈做的菜”最后这个词“菜”，此时信息传递是这样的： “菜”这个词与“我”、“喜欢”、“妈妈”、“做”、“的”这几个词关联性比较大，距离也比较近，所以可以直接利用这几个词进行最后那个词语的推测。 而有时候我们又需要用到很早以前时刻的信息，例如预测“我最常说汉语”最后的这个词“汉语”。此时信息传递是这样的： 此时，我们要预测“汉语”这个词，仅仅依靠“我”、“最”、“常”、“说”这几个词还不能得出我说的是汉语，必须要追溯到更早的句子“我是一个中国人”，由“中国人”这个词语来推测我最常说的是汉语。因此，这种情况下，我们想要推测“汉语”这个词的时候就比前面那个预测“菜”这个词所用到的信息就处于更早的时刻。 而RNN虽然在理论上可以保留所有历史时刻的信息，但在实际使用时，信息的传递往往会因为时间间隔太长而逐渐衰减，传递一段时刻以后其信息的作用效果就大大降低了。因此，普通RNN对于信息的长期依赖问题没有很好的处理办法。 为了克服这个问题，Hochreiter等人在1997年改进了RNN，提出了一种特殊的RNN模型——LSTM网络，可以学习长期依赖信息，在后面的20多年被改良和得到了广泛的应用，并且取得了极大的成功。 3. 长短时间记忆网络（LSTM） 3.1 LSTM与RNN的关系 长短期记忆（Long Short Term Memory，LSTM）网络是一种特殊的RNN模型，其特殊的结构设计使得它可以避免长期依赖问题，记住很早时刻的信息是LSTM的默认行为，而不需要专门为此付出很大代价。 普通的RNN模型中，其重复神经网络模块的链式模型如下图所示，这个重复的模块只有一个非常简单的结构，一个单一的神经网络层（例如tanh层），这样就会导致信息的处理能力比较低。 而LSTM在此基础上将这个结构改进了，不再是单一的神经网络层，而是4个，并且以一种特殊的方式进行交互。 粗看起来，这个结构有点复杂，不过不用担心，接下来我们会慢慢解释。在解释这个神经网络层时我们先来认识一些基本的模块表示方法。图中的模块分为以下几种： 黄色方块：表示一个神经网络层（Neural Network Layer）； 粉色圆圈：表示按位操作或逐点操作（pointwise operation），例如向量加和、向量乘积等； 单箭头：表示信号传递（向量传递）； 合流箭头：表示两个信号的连接（向量拼接）； 分流箭头：表示信号被复制后传递到2个不同的地方。 下面我们将分别介绍这些模块如何在LSTM中作用。 3.2 LSTM的基本思想 LSTM的关键是细胞状态（直译：cell state），表示为 ，用来保存当前LSTM的状态信息并传递到下一时刻的LSTM中，也就是RNN中那根“自循环”的箭头。当前的LSTM接收来自上一个时刻的细胞状态 ，并与当前LSTM接收的信号输入 共同作用产生当前LSTM的细胞状态 ，具体的作用方式下面将详细介绍。 在LSTM中，采用专门设计的“门”来引入或者去除细胞状态 中的信息。门是一种让信息选择性通过的方法。有的门跟信号处理中的滤波器有点类似，允许信号部分通过或者通过时被门加工了；有的门也跟数字电路中的逻辑门类似，允许信号通过或者不通过。这里所采用的门包含一个 神经网络层和一个按位的乘法操作，如下图所示： 其中黄色方块表示 神经网络层，粉色圆圈表示按位乘法操作。 神经网络层可以将输入信号转换为 到 之间的数值，用来描述有多少量的输入信号可以通过。 表示“不允许任何量通过”， 表示“允许所有量通过”。 神经网络层起到类似下图的 函数所示的作用： 其中，横轴表示输入信号，纵轴表示经过sigmoid函数以后的输出信号。 LSTM主要包括三个不同的门结构：遗忘门、记忆门和输出门。这三个门用来控制LSTM的信息保留和传递，最终反映到细胞状态 和输出信号 。如下图所示： 图中标示了LSTM中各个门的构成情况和相互之间的关系，其中： 遗忘门由一个 神经网络层和一个按位乘操作构成； 记忆门由输入门（input gate）与tanh神经网络层和一个按位乘操作构成； 输出门（output gate）与 函数（注意：这里不是 神经网络层）以及按位乘操作共同作用将细胞状态和输入信号传递到输出端。 3.3 遗忘门 顾名思义，遗忘门的作用就是用来“忘记”信息的。在LSTM的使用过程中，有一些信息不是必要的，因此遗忘门的作用就是用来选择这些信息并“忘记”它们。遗忘门决定了细胞状态 中的哪些信息将被遗忘。那么遗忘门的工作原理是什么呢？看下面这张图。 左边高亮的结构就是遗忘门了，包含一个 神经网络层（黄色方框，神经网络参数为 ），接收 时刻的输入信号 和 时刻LSTM的上一个输出信号 ，这两个信号进行拼接以后共同输入到 神经网络层中，然后输出信号 ， 是一个 到 之间的数值，并与 相乘来决定 中的哪些信息将被保留，哪些信息将被舍弃。可能看到这里有的初学者还是不知道具体是什么意思，我们用一个简单的例子来说明。 假设 , , , 那么遗忘门的输入信号就是 和 的组合，即 , 然后通过 神经网络层输出每一个元素都处于 到 之间的向量 ，注意，此时 是一个与 维数相同的向量，此处为3维。如果看到这里还没有看懂的读者，可能会有这样的疑问：输入信号明明是6维的向量，为什么 就变成了3维呢？这里可能是将 神经网络层当成了 激活函数了，两者不是一个东西，初学者在这里很容易混淆。下文所提及的 神经网络层和 神经网络层而是类似的道理，他们并不是简单的 激活函数和 激活函数，在学习时要注意区分。 3.4 记忆门 记忆门的作用与遗忘门相反，它将决定新输入的信息 和 中哪些信息将被保留。 如图所示，记忆门包含2个部分。第一个是包含 神经网络层（输入门，神经网络网络参数为 ）和一个 神经网络层（神经网络参数为 ）。 神经网络层的作用很明显，跟遗忘门一样，它接收 和 作为输入，然后输出一个 到 之间的数值 来决定哪些信息需要被更新； Tanh神经网络层的作用是将输入的 和 整合，然后通过一个 神经网络层来创建一个新的状态候选向量 ， 的值范围在 到 之间。 记忆门的输出由上述两个神经网络层的输出决定， 与 相乘来选择哪些信息将被新加入到 时刻的细胞状态 中。 3.5 更新细胞状态 有了遗忘门和记忆门，我们就可以更新细胞状态 了。 这里将遗忘门的输出 与上一时刻的细胞状态 相乘来选择遗忘和保留一些信息，将记忆门的输出与从遗忘门选择后的信息加和得到新的细胞状态 。这就表示 时刻的细胞状态 已经包含了此时需要丢弃的 时刻传递的信息和 时刻从输入信号获取的需要新加入的信息 。 将继续传递到 时刻的LSTM网络中，作为新的细胞状态传递下去。 3.6 输出门 前面已经讲了LSTM如何来更新细胞状态 ， 那么在 时刻我们输入信号 以后，对应的输出信号该如何计算呢？ 如上面左图所示，输出门就是将 时刻传递过来并经过了前面遗忘门与记忆门选择后的细胞状态 ， 与 时刻的输出信号 和 时刻的输入信号 整合到一起作为当前时刻的输出信号。整合的过程如上图所示， 和 经过一个 神经网络层（神经网络参数为 ）输出一个 到 之间的数值 。 经过一个 函数（注意：这里不是 神经网络层）到一个在 到 之间的数值，并与 相乘得到输出信号 ，同时 也作为下一个时刻的输入信号传递到下一阶段。 其中， 函数是激活函数的一种，函数图像为： 至此，基本的LSTM网络模型就介绍完了。如果对LSTM模型还没有理解到的，可以看一下 这个视频 ，作者是一个外国小哥，英文讲解的，有动图，方便理解。 3.7 LSTM的一些变体 前面已经介绍了基本的LSTM网络模型，而实际应用时，我们常常会采用LSTM的一些变体，虽然差异不大，这里不再做详细介绍，有兴趣的读者可以自行了解。 3.7.1 在门上增加窥视孔 这是2000年Gers和Schemidhuber教授提出的一种LSTM变体。图中，在传统的LSTM结构基础上，每个门（遗忘门、记忆门和输出门）增加了一个“窥视孔”（Peephole），有的学者在使用时也选择只对部分门加入窥视孔。 3.7.2 整合遗忘门和输入门 与传统的LSTM不同的是，这个变体不需要分开来确定要被遗忘和记住的信息，采用一个结构搞定。在遗忘门的输出信号值（ 到 之间）上，用 减去该数值来作为记忆门的状态选择，表示只更新需要被遗忘的那些信息的状态。 3.7.3 GRU 改进比较大的一个LSTM变体叫Gated Recurrent Unit (GRU)，目前应用较多。结构图如下 GRU主要包含2个门：重置门和更新门。GRU混合了细胞状态 和隐藏状态 为一个新的状态，使用 来表示。 该模型比传统的标准LSTM模型简单。 4. 基于Pytorch的LSTM代码实现 Pytorch 是Python的一个机器学习包，与Tensorflow类似，Pytorch非常适合用来构建神经网络模型，并且已经提供了一些常用的神经网络模型包，用户可以直接调用。下面我们就用一个简单的小例子来说明如何使用Pytorch来构建LSTM模型。 我们使用正弦函数和余弦函数来构造时间序列，而正余弦函数之间是成导数关系，所以我们可以构造模型来学习正弦函数与余弦函数之间的映射关系，通过输入正弦函数的值来预测对应的余弦函数的值。 正弦函数和余弦函数对应关系图如下图所示： 可以看到，每一个函数曲线上，每一个正弦函数的值都对应一个余弦函数值。但其实如果只关心正弦函数的值本身而不考虑当前值所在的时间，那么正弦函数值和余弦函数值不是一一对应关系。例如，当 和 时， ，但在这两个不同的时刻， 的值却不一样，也就是说如果不考虑时间，同一个正弦函数值可能对应了不同的几个余弦函数值。对于传统的神经网络来说，它仅仅基于当前的输入来预测输出，对于这种同一个输入可能对应多个输出的情况不再适用。 我们取正弦函数的值作为LSTM的输入，来预测余弦函数的值。基于Pytorch来构建LSTM模型，采用1个输入神经元，1个输出神经元，16个隐藏神经元作为LSTM网络的构成参数，平均绝对误差（LMSE）作为损失误差，使用Adam优化算法来训练LSTM神经网络。基于Anaconda和Python3.6的完整代码如下： 训练的过程如下： 该模型在训练集和测试集上的结果如下： LSTM在训练集和测试集上的表现 图中，红色虚线的左边表示该模型在训练数据集上的表现，右边表示该模型在测试数据集上的表现。可以看到，使用LSTM构建训练模型，我们可以仅仅使用正弦函数在 时刻的值作为输入来准确预测 时刻的余弦函数值，不用额外添加当前的时间信息、速度信息等。 5. 参考 Understanding LSTM Networks 推荐看这个Youtube视频 循环神经网络 获取更多技术干货： AI与机器人</p></div></details>
</div>
<script>
function speakText(t){ if(!t){ document.getElementById('readStatus').textContent=''; return; }
 speechSynthesis.cancel(); var status=document.getElementById('readStatus'); status.textContent='准备中…';
 var chunks=[]; var maxLen=280; for(var i=0;i<t.length;i+=maxLen){ var s=t.slice(i,i+maxLen); var j=s.lastIndexOf('。'); if(j>80){ chunks.push(s.slice(0,j+1)); i-=s.length-(j+1); } else chunks.push(s); }
 if(chunks.length===0) chunks=[t]; var idx=0; function speakNext(){ if(idx>=chunks.length){ status.textContent=''; return; } var u=new SpeechSynthesisUtterance(chunks[idx]); u.lang='zh-CN'; u.rate=0.95; var v=speechSynthesis.getVoices().filter(function(x){ return x.lang.indexOf('zh')>=0; })[0]; if(v) u.voice=v;
 u.onend=function(){ idx++; setTimeout(speakNext,80); }; u.onerror=function(){ idx++; setTimeout(speakNext,80); }; status.textContent='朗读中 '+(idx+1)+'/'+chunks.length+'…'; speechSynthesis.speak(u); }
 if(speechSynthesis.getVoices().length) speakNext(); else speechSynthesis.onvoiceschanged=function(){ speechSynthesis.onvoiceschanged=null; speakNext(); }; }
function readFullText(){ var c=document.querySelector('.content'); if(!c){ document.getElementById('readStatus').textContent='无可读内容'; return; } var t=(c.innerText||'').trim().replace(/\\s+/g,' '); if(!t){ document.getElementById('readStatus').textContent='无可读内容'; return; } speechSynthesis.cancel(); document.getElementById('readStatus').textContent='全文朗读…'; speakText(t); }
function getBlockForTap(el){ var c=document.querySelector('.content'); var blockTags=['P','DIV','H2','H3','H4','LI','PRE']; while(el&&el!==c){ if(c.contains(el)&&blockTags.indexOf(el.tagName)!==-1) return el; el=el.parentElement; } return null; }
function onContentTap(e){ if(e.target.closest&&e.target.closest('a')) return; var block=getBlockForTap(e.target); if(block){ var t=(block.innerText||'').trim().replace(/\\s+/g,' '); if(t){ e.preventDefault(); speechSynthesis.cancel(); document.getElementById('readStatus').textContent='朗读本段…'; speakText(t); } } }
if(document.readyState==='loading'){ document.addEventListener('DOMContentLoaded', function(){ var c=document.querySelector('.content'); if(c) c.addEventListener('click', onContentTap); }); } else { var c=document.querySelector('.content'); if(c) c.addEventListener('click', onContentTap); }
setTimeout(function(){ try{ var u=new SpeechSynthesisUtterance('\u200b'); u.volume=0; speechSynthesis.speak(u); }catch(e){} }, 300);
</script>
</body>
</html>
<!DOCTYPE html>
<html lang="zh-CN">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<title>知识流日报 2026-02-22：谐振抑制、多阶段优化规划、互联新能源电力系统、光伏直流升压汇集系统、新型电力系统平衡机理、EEMD-LSTM网络、有源和无源阻尼协同控制、大规模先进压缩空气储能、基于时空多视图学习算法、PMU电压数据重构方法、iTransformer、基于改进型主动深度学习框架、区内AGC机组分布式协同控制、跟网型和构网型变流器混合系统、新型电力系统形态量化推演方法</title>
<style>
  * { box-sizing: border-box; }
  body {
    font-family: system-ui, sans-serif;
    max-width: 720px;
    margin: 0 auto;
    padding: 2rem 1rem;
    color: #fff;
    min-height: 100vh;
    background: #0a0e1a;
    background-image:
      radial-gradient(2px 2px at 20px 30px, rgba(255,255,255,0.9), transparent),
      radial-gradient(2px 2px at 40px 70px, rgba(255,255,255,0.6), transparent),
      radial-gradient(1.5px 1.5px at 90px 40px, rgba(255,255,255,0.8), transparent),
      radial-gradient(2px 2px at 130px 80px, rgba(255,255,255,0.5), transparent),
      radial-gradient(1.5px 1.5px at 160px 120px, rgba(255,255,255,0.7), transparent),
      radial-gradient(ellipse 120% 100% at 50% 0%, rgba(88,60,120,0.25), transparent 50%),
      radial-gradient(ellipse 80% 80% at 80% 20%, rgba(40,60,100,0.2), transparent 40%);
    background-size: 200px 200px;
    background-repeat: repeat;
  }
  a { color: #a8d4ff; text-decoration: none; }
  a:hover { text-decoration: underline; }
 html{scroll-behavior:smooth;} .content h2,.content h3{scroll-margin-top:1rem;} h1,h2,h3{margin-top:1.5rem;color:#fff;} pre{white-space:pre-wrap;color:rgba(255,255,255,0.9);} .toolbar{margin:0.5rem 0;color:rgba(255,255,255,0.8);} .content{color:rgba(255,255,255,0.95);} .content a{color:#a8d4ff;} .meta{color:rgba(255,255,255,0.65);font-size:0.9em;} .toc{margin:1rem 0;padding:0.8rem 1rem;background:rgba(255,255,255,0.08);border-radius:6px;} .toc .toc-title{margin:0 0 0.5rem 0;font-weight:bold;color:rgba(255,255,255,0.9);} .toc ul{list-style:none;padding-left:0;margin:0;} .toc li{margin:0.35rem 0;} .toc li.toc-h3{padding-left:1em;font-size:0.95em;} .toc a{color:#a8d4ff;text-decoration:none;} .toc a:hover{text-decoration:underline;} .content p,.content div,.content h2,.content h3,.content h4,.content li,.content pre{cursor:pointer;-webkit-tap-highlight-color:rgba(255,255,255,0.15);} .content .insight-summary{color:#c9a0dc;margin:0.5em 0 0.8em 0;} .content .insight-detail,.content .insight-detail li{color:#8dd4e8;list-style:disc;margin-left:1.2em;padding-left:0.3em;} .content .article-body-details{margin:0.8em 0;} .content .article-body-details summary{cursor:pointer;color:rgba(255,255,255,0.85);}</style>
</head>
<body>
<p><a href="https://YuxinWSoton.github.io/ai-knowledge-flow-site/">← 返回首页</a></p>
<h1>知识流日报 2026-02-22：谐振抑制、多阶段优化规划、互联新能源电力系统、光伏直流升压汇集系统、新型电力系统平衡机理、EEMD-LSTM网络、有源和无源阻尼协同控制、大规模先进压缩空气储能、基于时空多视图学习算法、PMU电压数据重构方法、iTransformer、基于改进型主动深度学习框架、区内AGC机组分布式协同控制、跟网型和构网型变流器混合系统、新型电力系统形态量化推演方法</h1>
<p class="meta" style="margin-top:0;">最后更新：2026-02-23 22:23</p>
<p class="toolbar"><button id="btnFull" onclick="readFullText()">全文朗读</button> <span id="readStatus"></span></p>
<p class="meta" style="margin-top:0;">（点任意段落可朗读该段；「全文朗读」读本页全部内容）</p>
<nav class="toc" aria-label="目录">
<p class="toc-title">目录</p>
<ul>
  <li><a href="#toc-0">1. 电路中的谐振和振荡有什么具体区别吗？ - 知乎</a></li>
  <li class="toc-h3"><a href="#toc-1">正文</a></li>
  <li><a href="#toc-2">2. 互联网的10大主要用途有哪些？ - 知乎</a></li>
  <li class="toc-h3"><a href="#toc-3">正文</a></li>
  <li><a href="#toc-4">3. “万物互联”到底是个什么概念？ - 知乎</a></li>
  <li class="toc-h3"><a href="#toc-5">正文</a></li>
  <li><a href="#toc-6">4. 互联网+是什么意思？ - 知乎</a></li>
  <li class="toc-h3"><a href="#toc-7">正文</a></li>
  <li><a href="#toc-8">5. 互联网到底是啥？ - 知乎</a></li>
  <li class="toc-h3"><a href="#toc-9">正文</a></li>
  <li><a href="#toc-10">6. 光猫改桥接之后无法进入路由器的管理页面了，怎么办？</a></li>
  <li class="toc-h3"><a href="#toc-11">正文</a></li>
  <li><a href="#toc-12">7. 为什么EMD (EEMD)分解可以用于预测？ - 知乎</a></li>
  <li class="toc-h3"><a href="#toc-13">正文</a></li>
  <li><a href="#toc-14">8. EMD和EEMD对原始数据分解之后再进行重构的意义是什么 ...</a></li>
  <li class="toc-h3"><a href="#toc-15">正文</a></li>
  <li><a href="#toc-16">9. 集合经验模态分解EEMD如何做有效改进？ - 知乎</a></li>
  <li class="toc-h3"><a href="#toc-17">正文</a></li>
  <li><a href="#toc-18">10. EMD分解信号后，怎么确定其IMF分量的对应因素？</a></li>
  <li class="toc-h3"><a href="#toc-19">正文</a></li>
  <li><a href="#toc-20">11. 时间序列预测，采用EEMD_LSTM模型，如何建模？</a></li>
  <li class="toc-h3"><a href="#toc-21">正文</a></li>
  <li><a href="#toc-22">12. EMD算法之Hilbert-Huang Transform原理详解和案例分析 - 知乎</a></li>
  <li class="toc-h3"><a href="#toc-23">正文</a></li>
  <li><a href="#toc-24">13. 同一段信号，EEMD分解出的层数小于CEEMD，这是为什么呢？</a></li>
  <li class="toc-h3"><a href="#toc-25">正文</a></li>
  <li><a href="#toc-26">14. EMD过程跟去噪有关系么？ - 知乎</a></li>
  <li class="toc-h3"><a href="#toc-27">正文</a></li>
  <li><a href="#toc-28">15. 豆包大模型 Seed-2.0 正式发布，带来哪些新功能和体验升级？</a></li>
  <li class="toc-h3"><a href="#toc-29">正文</a></li>
  <li><a href="#toc-30">16. 谷歌 Gemini 3 Deep Think 深度思考大模型升级，相比前代 ...</a></li>
  <li class="toc-h3"><a href="#toc-31">正文</a></li>
  <li><a href="#toc-32">17. 关于论文题目中的基于和面向的区别？ - 知乎</a></li>
  <li class="toc-h3"><a href="#toc-33">正文</a></li>
  <li><a href="#toc-34">18. “基于”这两个字可以这样用吗？ - 知乎</a></li>
  <li class="toc-h3"><a href="#toc-35">正文</a></li>
  <li><a href="#toc-36">19. 在写研究课题时，“基于……的研究”和“……视角下的研究 ...</a></li>
  <li class="toc-h3"><a href="#toc-37">正文</a></li>
  <li><a href="#toc-38">20. 有基于 C/C++ 的 Web 开发框架吗？ - 知乎</a></li>
  <li class="toc-h3"><a href="#toc-39">正文</a></li>
  <li><a href="#toc-40">21. 如何通俗地介绍基于模型的系统工程（MBSE）？ - 知乎</a></li>
  <li class="toc-h3"><a href="#toc-41">正文</a></li>
  <li><a href="#toc-42">22. ATE中PMU和PPMU的具体工作原理是什么？ - 知乎</a></li>
  <li class="toc-h3"><a href="#toc-43">正文</a></li>
  <li><a href="#toc-44">23. 自动化测试 | PMU与PMU有什么区别吗？ - 知乎</a></li>
  <li class="toc-h3"><a href="#toc-45">正文</a></li>
  <li><a href="#toc-46">24. Revisions | OpenReview</a></li>
  <li class="toc-h3"><a href="#toc-47">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-48">25. 突破瓶颈！我国科学家打造出新型电池-观察者网</a></li>
  <li class="toc-h3"><a href="#toc-49">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-50">26. EEMD，集合经验模态分解EEMD的信号分解（Matlab完整 ...</a></li>
  <li class="toc-h3"><a href="#toc-51">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-52">27. TimeXer: Empowering Transformers for Time Series Forecasting …</a></li>
  <li class="toc-h3"><a href="#toc-53">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-54">28. FreDF: Learning to Forecast in the Frequency Domain</a></li>
  <li class="toc-h3"><a href="#toc-55">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-56">29. Crossformer: Transformer Utilizing Cross-Dimension Dependency …</a></li>
  <li class="toc-h3"><a href="#toc-57">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-58">30. A Time Series is Worth 64 Words: Long-term Forecasting with...</a></li>
  <li class="toc-h3"><a href="#toc-59">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-60">31. 电脑或者笔记本怎么投屏到电视或者投影仪或者大屏幕？</a></li>
  <li class="toc-h3"><a href="#toc-61">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-62">32. Forum | OpenReview</a></li>
  <li class="toc-h3"><a href="#toc-63">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-64">33. EMAformer: Enhancing Transformer through Embedding Armor for Time Series Forecasting</a></li>
  <li class="toc-h3"><a href="#toc-65">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-66">34. Data Augmentation in Time Series Forecasting through Inverted Framework</a></li>
  <li class="toc-h3"><a href="#toc-67">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-68">35. An Optimization Method for Autoregressive Time Series Forecasting</a></li>
  <li class="toc-h3"><a href="#toc-69">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-70">36. Selective Learning for Deep Time Series Forecasting</a></li>
  <li class="toc-h3"><a href="#toc-71">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-72">37. TwinFormer: A Dual-Level Transformer for Long-Sequence Time-Series Forecasting</a></li>
  <li class="toc-h3"><a href="#toc-73">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-74">38. Quantum Neural Network Architectures for Multivariate Time-Series Forecasting</a></li>
  <li class="toc-h3"><a href="#toc-75">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-76">39. Efficient Deep Learning for Short-Term Solar Irradiance Time Series Forecasting: A Benchmark Study in Ho Chi Minh City</a></li>
  <li class="toc-h3"><a href="#toc-77">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-78">40. Lossless Compression: A New Benchmark for Time Series Model Evaluation</a></li>
  <li class="toc-h3"><a href="#toc-79">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-80">41. Transformer-Based Modeling of User Interaction Sequences for Dwell Time Prediction in Human-Computer Interfaces</a></li>
  <li class="toc-h3"><a href="#toc-81">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-82">42. The Procrustean Bed of Time Series: The Optimization Bias of Point-wise Loss</a></li>
  <li class="toc-h3"><a href="#toc-83">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-84">43. InvDec: Inverted Decoder for Multivariate Time Series Forecasting with Separated Temporal and Variate Modeling</a></li>
  <li class="toc-h3"><a href="#toc-85">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-86">44. 完全弄懂X射线光电子能谱（XPS）</a></li>
  <li class="toc-h3"><a href="#toc-87">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-88">45. Learning from Brain Topography: A Hierarchical Local-Global Graph-Transformer Network for EEG Emotion Recognition</a></li>
  <li class="toc-h3"><a href="#toc-89">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-90">46. Beyond MSE: Ordinal Cross-Entropy for Probabilistic Time Series Forecasting</a></li>
  <li class="toc-h3"><a href="#toc-91">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-92">47. Benchmarking M-LTSF: Frequency and Noise-Based Evaluation of Multivariate Long Time Series Forecasting Models</a></li>
  <li class="toc-h3"><a href="#toc-93">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-94">48. 知乎盐选 | 3.4 基于 EEMD-SVR 的股票指数预测建模</a></li>
  <li class="toc-h3"><a href="#toc-95">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-96">49. ITRANSFORMER: INVERTED TRANSFORMERS ARE EFFECTIVE …</a></li>
  <li class="toc-h3"><a href="#toc-97">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-98">50. Parsimony or Capability? Decomposition Delivers Both in</a></li>
  <li class="toc-h3"><a href="#toc-99">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-100">51. 知乎盐选 | 2.3 光引发剂</a></li>
  <li class="toc-h3"><a href="#toc-101">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-102">52. AC ASE STUDY WITHP TST AND VARYING INPUT LENGTH</a></li>
  <li class="toc-h3"><a href="#toc-103">正文（抓取，非 AI）</a></li>
</ul>
</nav>
<div class="content">
<h1>知识流日报 2026-02-22：谐振抑制、多阶段优化规划、互联新能源电力系统、光伏直流升压汇集系统、新型电力系统平衡机理、EEMD-LSTM网络、有源和无源阻尼协同控制、大规模先进压缩空气储能、基于时空多视图学习算法、PMU电压数据重构方法、iTransformer、基于改进型主动深度学习框架、区内AGC机组分布式协同控制、跟网型和构网型变流器混合系统、新型电力系统形态量化推演方法</h1>
<p>共 52 条（来自百度学术 / Google / Bing 等，仅含正文抓取成功的条目；按正文长度从短到长排列，便于先听短的）。</p>
<p>（以下含抓取正文，便于阅读。未调用任何 AI API。）</p>
<p>（仅前 50 条含模型提取的「易漏细节」关键点，页面上以颜色区分；第 51 条及之后未调用模型，故无易漏细节。可在 config 中调大 extract_insights_max_items 以增加条数。）</p>
<h2 id="toc-0">1. 电路中的谐振和振荡有什么具体区别吗？ - 知乎</h2>
<ul>
<li>链接：https://www.zhihu.com/question/265398672</li>
<li>来源：bing</li>
<li>摘要：2018年1月10日 · 简单说一下。首先说下谐振。所谓谐振是指电路中 容抗 与 感抗 在某个频率处互相抵消。此时的 输入阻抗 表现为纯阻性，常用来选择某个频率（当然 Q值 有限，只能选择一定带宽的信 …</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">谐振时，电路中的容抗与感抗互相抵消，使得输入阻抗在谐振点表现为纯阻性，这一特性使得电路能够选择性地放大或通过特定频率的信号。由于谐振具有选择性，它只能有效地处理一定带宽内的信号，从而限制了电路对其他频率信号的响应。因此，谐振不仅能够优化电路对特定频率信号的响应，还能有效抑制其他不必要的频率，从而提高信号处理的精确度和效率。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>谐振时容抗与感抗互相抵消。</li>
<li>输入阻抗在谐振时表现为纯阻性。</li>
<li>谐振只能选择一定带宽的信号。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-1">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-2">2. 互联网的10大主要用途有哪些？ - 知乎</h2>
<ul>
<li>链接：https://www.zhihu.com/question/650602330</li>
<li>来源：bing</li>
<li>摘要：未来，互联网的发展趋势将更加多元化和智能化。 一方面，随着5G、物联网等技术的普及，互联网将连接更多的设备和服务，实现万物互联。 另一方面，人工智能、大数据等技术将与互联网深度融合， …</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">首先，互联网将连接更多的设备和服务，这不仅意味着网络覆盖范围的扩大，也意味着连接的设备种类将更加多样，从传统的电脑、手机到各种智能家电、可穿戴设备乃至汽车等。其次，万物互联需要5G、物联网等技术支持，这些技术不仅提升了网络的传输速度和稳定性，还使得设备间的连接更加便捷和高效。此外，人工智能和大数据将深度融合互联网，通过分析海量数据，人工智能可以提供更加个性化的服务和解决方案，使互联网的应用更加智能化。因此，互联网的发展将更加多元化和智能化，这不仅改变了人们的生活方式，也为各行各业带来了前所未有的机遇。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>互联网将连接更多的设备和服务。</li>
<li>万物互联需要5G、物联网等技术支持。</li>
<li>人工智能和大数据将深度融合互联网。</li>
<li>互联网的发展将更加多元化和智能化。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-3">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-4">3. “万物互联”到底是个什么概念？ - 知乎</h2>
<ul>
<li>链接：https://www.zhihu.com/question/355892352</li>
<li>来源：bing</li>
<li>摘要：2019年11月19日 · 万物互联 - 万物智能互联! 6G是以5G为基础，全力支持全社会的数字化转型。而且6G在时-频-空间资源利用上将具有超灵活的优势。 在频率维度上，6G传输将使用高频段，如毫米波 …</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">首先，G技术将支持全社会的数字化转型，这不仅是因为它能够提供更广泛的应用场景和更高的数据处理能力，还因为它在时-频-空间资源利用上具有超灵活的优势，能够根据不同的需求灵活调整资源分配，从而提高整体效率。其次，G传输将使用高频段，如毫米波，这使得数据传输速度显著提升，进一步推动了数字化转型的进程。因此，通过高频段的使用，G不仅能够满足现代社会对高速、大容量数据传输的需求，还能在支持数字化转型的同时，提升整个社会的信息化水平。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>G将支持全社会的数字化转型。</li>
<li>G在时-频-空间资源利用上具有超灵活的优势。</li>
<li>G传输将使用高频段，如毫米波。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-5">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-6">4. 互联网+是什么意思？ - 知乎</h2>
<ul>
<li>链接：https://www.zhihu.com/question/299532137</li>
<li>来源：bing</li>
<li>摘要：2018年10月25日 · 和互联网有什么联系？ 【互联网+】 就是 互联网+“某个垂直行业” 就好比如： 淘宝=互联网+商品买卖领域 美团=互联网+外卖 滴滴=互联网+搭车 微信=互联网+通讯/社交 共享自行车=互联 …</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">互联网+强调的是互联网技术与传统行业的融合，这种融合涉及特定垂直行业的应用。首先，互联网+中的“互联网”指的是互联网技术，这些技术包括但不限于大数据、云计算、人工智能等。其次，互联网+可以应用于多个不同的行业领域，如制造业、农业、医疗、教育等，通过将互联网技术融入这些行业，可以实现效率的提升和创新的推动。因此，互联网+不仅是一种技术手段，更是一种促进传统行业转型升级的重要方式。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>互联网+涉及特定垂直行业的应用。</li>
<li>互联网+强调的是互联网技术与传统行业的融合。</li>
<li>互联网+中的“互联网”指的是互联网技术。</li>
<li>互联网+可以应用于多个不同的行业领域。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-7">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-8">5. 互联网到底是啥？ - 知乎</h2>
<ul>
<li>链接：https://www.zhihu.com/question/347500330</li>
<li>来源：bing</li>
<li>摘要：2019年10月2日 · 互联网的前身 互联网的创始应该追溯到1969年美国资助开发的一个名叫“ARPAnet”的研究项目。 美国政府认为，在不同网络之间通过转换实现互联互通，效率是非常低的。 只有用一套系 …</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">首先，互联网的前身是ARPAnet，这是一个由美国国防部高级研究计划局（ARPA）资助的项目。政府的资助是该项目启动的关键因素，为互联网的早期发展提供了必要的资金支持。其次，随着ARPAnet的发展，互联效率逐渐成为了一个问题，这促使人们开始寻求更高效的网络连接方式。因此，统一网络协议的开发成为了必然趋势，旨在提高互联效率，从而推动互联网的进一步发展。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>互联网的前身是ARPAnet；政府资助是项目启动的关键；互联效率低促使统一网络协议的开发。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-9">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-10">6. 光猫改桥接之后无法进入路由器的管理页面了，怎么办？</h2>
<ul>
<li>链接：https://www.zhihu.com/question/1900726834187974599</li>
<li>来源：bing</li>
<li>摘要：2025年5月28日 · 看上去，可能是因为你的光猫和路由器指定为同地址（或同网段）了。 也就是，光猫的地址是192.168.1.1，路由器的地址也就被设为了192.168.1.1，形成了冲突，造成了不能访问。 把它 …</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">首先，设置网络设备时应避免将光猫和路由器设为同一地址，因为这会导致网络冲突，影响设备的正常运行。其次，访问路由器的管理页面时，务必注意不要与光猫的地址发生冲突，以确保能够顺利进行网络设置。因此，设置时应确保光猫和路由器不在同一网段，以避免潜在的网络问题。通过合理规划网络地址，可以有效避免因地址冲突带来的网络故障，确保网络环境的稳定与高效。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>光猫和路由器设为同一地址会导致冲突。</li>
<li>访问路由器管理页面时要注意避免与光猫地址冲突。</li>
<li>设置时应确保光猫和路由器不在同一网段。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-11">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-12">7. 为什么EMD (EEMD)分解可以用于预测？ - 知乎</h2>
<ul>
<li>链接：https://www.zhihu.com/question/422172882</li>
<li>来源：bing</li>
<li>摘要：2020年9月22日 · 1.3 怎么实现EEMD? Rlibeemd是R语言下的一个EEMD算法包，它提供了多种EEMD算法的实现，并支持分解结果可视化。 该包的优点在于它提供了多种EEMD算法的选择，标准的EMD …</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">EMD (EEMD)分解因其独特的优势，在预测领域中扮演着重要角色。首先，EEMD算法包提供了多种分解算法的选择，使得用户可以根据具体需求灵活调整，以获得更准确的预测结果。其次，通过EEMD分解，可以将复杂的时间序列信号分解为一系列较为简单的、具有物理意义的本征模态函数（IMFs），这不仅有助于深入理解信号的内在结构，也为后续的预测分析提供了坚实的基础。此外，EEMD分解的结果可以通过可视化工具进行展示，这不仅有助于直观地理解分解后的各个IMFs，还能帮助研究人员和实践者更好地评估预测模型的效果，从而进一步优化预测策略。因此，EMD (EEMD)分解不仅是预测过程中的一个重要步骤，更是提升预测准确性和可靠性的重要手段。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>EMD (EEMD)分解可以用于预测。</li>
<li>EEMD算法包支持多种分解算法选择。</li>
<li>EEMD分解结果可以通过可视化工具进行展示。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-13">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-14">8. EMD和EEMD对原始数据分解之后再进行重构的意义是什么 ...</h2>
<ul>
<li>链接：https://www.zhihu.com/question/301240606</li>
<li>来源：bing</li>
<li>摘要：2020年3月7日 · 某信号分解得到的低频分量 观察上边两张高频分量和低频分量图，应该更容易理解高频分量均值趋近于0，低频分量更不易趋近于0。 举例说明，还是以上篇文章中的2012-2020年原油期货 …</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">EMD和EEMD分解方法能够有效分离信号的不同频率成分，进而实现信号的重构。首先，通过EMD和EEMD分解，可以提取出信号的高频分量和低频分量。高频分量的均值趋近于0，表明这些分量主要包含噪声；而低频分量不易趋近于0，说明它们主要包含有用信息。因此，EMD和EEMD分解有助于分离信号中的噪声与有用信息。进一步地，原始数据经过分解后再重构，可以更有效地分离和提取信号的特征，从而提高信号分析的精度和可靠性。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>EMD和EEMD分解后的重构可以提取信号的不同频率成分。</li>
<li>高频分量的均值趋近于0，意味着高频分量主要包含噪声。</li>
<li>低频分量不易趋近于0，表明低频分量主要包含有用信息。</li>
<li>EMD和EEMD分解有助于分离信号中的噪声与有用信息。</li>
<li>原始数据分解后再重构，是为了更好地分离和提取信号特征。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-15">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-16">9. 集合经验模态分解EEMD如何做有效改进？ - 知乎</h2>
<ul>
<li>链接：https://www.zhihu.com/question/598584478</li>
<li>来源：bing</li>
<li>摘要：综合 来说，EEMD 算法会有计算量大和参数设置具有主观性这两个缺陷 [1]。 1 快速集合经验模态分解FEEMD介绍 EEMD简介: FEEMD（Fast Ensemble Empirical Mode Decomposition）是 EEMD 的一 …</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">EEMD算法虽然在信号处理中表现出色，但由于其计算量大和参数设置具有主观性，实际应用中存在一定的局限性。为了解决这些问题，快速集合经验模态分解（FEEMD）应运而生。首先，FEEMD旨在通过优化算法减少EEMD的计算量，从而提高处理效率。其次，FEEMD通过改进参数设置方法，减少主观性的影响，使算法更加稳定和可靠。因此，FEEMD不仅提高了处理速度，还增强了算法的实用性和可靠性。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>EEMD算法计算量大。</li>
<li>EEMD算法的参数设置具有主观性。</li>
<li>快速集合经验模态分解FEEMD旨在解决EEMD的计算量大的问题。</li>
<li>快速集合经验模态分解FEEMD旨在解决EEMD参数设置主观性的问题。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-17">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-18">10. EMD分解信号后，怎么确定其IMF分量的对应因素？</h2>
<ul>
<li>链接：https://www.zhihu.com/question/479290083</li>
<li>来源：bing</li>
<li>摘要：2022年4月25日 · 之前我们有了十几篇文章讲述了EMD算法的基础理论、IMF的含义、EMD的MATLAB实现方法， EEMD 、 CEEMD 、 CEEMDAN 、 VMD 、 ICEEMDAN 、 LMD 、 EWT 的理论及代码实 …</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">确定IMF分量对应因素的方法可能需要结合具体应用背景，而EMD分解后的IMF分量不一定直接对应特定因素，这表明分解结果的解释依赖于具体问题和数据特性。进一步分析EMD分解后的IMF分量以确定其物理意义是必要的，因为这些分量可能需要与其他分析方法结合使用，以确保其对应因素的唯一性和准确性。因此，EMD分解后的IMF分量的解释和应用需要综合考虑多种因素，以确保分析结果的可靠性和实用性。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>确定IMF分量对应因素的方法可能需要结合具体应用背景。</li>
<li>EMD分解后的IMF分量不一定直接对应特定因素。</li>
<li>EMD分解结果的解释依赖于具体问题和数据特性。</li>
<li>EMD分解后的IMF分量可能需要进一步分析以确定其物理意义。</li>
<li>EMD分解后的IMF分量对应因素可能不唯一。</li>
<li>EMD分解后的IMF分量可能需要与其他分析方法结合使用。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-19">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-20">11. 时间序列预测，采用EEMD_LSTM模型，如何建模？</h2>
<ul>
<li>链接：https://www.zhihu.com/question/470976344</li>
<li>来源：bing</li>
<li>摘要：2021年7月9日 · 利用EEMD方法对时间序列进行分解后，建议采用合适的模态筛选指标对合适的IMF分量进行筛选并重构，将重构后的时间序列输入LSTM模型进行预测。 以旋转机械故障诊断为例：比如采 …</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">EEMD方法能够有效分解非线性非平稳时间序列，为后续分析提供基础。然而，筛选出合适的IMF分量是建模的关键步骤，因为这些分量能够更好地反映信号的内在特性。因此，经过筛选重构的时间序列可以被输入到LSTM模型中进行处理，LSTM模型因其对时间序列数据的处理能力而适用于这一场景。此外，旋转机械故障诊断可以作为实际应用案例，展示上述方法和技术在实际工程中的应用效果。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>EEMD方法能有效分解非线性非平稳时间序列。</li>
<li>筛选合适的IMF分量是建模的关键步骤。</li>
<li>LSTM模型适用于处理经过筛选重构的时间序列。</li>
<li>旋转机械故障诊断可作为实际应用案例。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-21">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-22">12. EMD算法之Hilbert-Huang Transform原理详解和案例分析 - 知乎</h2>
<ul>
<li>链接：https://www.zhihu.com/column/p/150706759</li>
<li>来源：bing</li>
<li>摘要：2020年6月25日 · 文章由博客园网友 Mario-Chao的授权转载于"脑机接口社区" EMD算法之Hilbert-Huang Transform原理详解和案例分析在我们正式开始讲解Hilbert-Huang Transform之前，不 …</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">HHT方法结合了Hilbert变换和经验模式分解，能够处理非线性和非平稳信号，适用于分析复杂信号，如生物医学信号中的EEG和EMG信号。Hilbert变换用于获得信号的瞬时频率和瞬时相位，而经验模式分解（EMD）是一种自适应的多尺度分析方法。HHT可以提供瞬时幅值和瞬时相位，从而给出信号的详细时频信息，但其结果依赖于初始条件和噪声水平，且瞬时频率并非真正的物理频率，而是一种描述信号局部特征的方式。此外，HHT需要原始信号作为输入，不能处理离散数据，因此在解释HHT结果时需要专业知识和经验，且HHT可能受噪声影响。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>HHT方法结合了Hilbert变换和经验模式分解。</li>
<li>Hilbert变换用于获得信号的瞬时频率和瞬时相位。</li>
<li>经验模式分解（EMD）是一种自适应的多尺度分析方法。</li>
<li>HHT可以处理非线性和非平稳信号。</li>
<li>HHT适用于分析复杂信号，但可能受噪声影响。</li>
<li>瞬时频率不是真正的物理频率，而是一种描述信号局部特征的方式。</li>
<li>EMD分解结果依赖于初始条件和噪声水平。</li>
<li>HHT需要原始信号作为输入，不能处理离散数据。</li>
<li>瞬时幅值和瞬时相位可以提供信号的详细时频信息。</li>
<li>HHT适用于分析生物医学信号，如EEG和EMG信号。</li>
<li>HHT结果的解释需要专业知识和经验。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-23">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-24">13. 同一段信号，EEMD分解出的层数小于CEEMD，这是为什么呢？</h2>
<ul>
<li>链接：https://www.zhihu.com/question/590555751</li>
<li>来源：bing</li>
<li>摘要：EEMD和CEEMD都是一种基于EMD（经验模态分解）的信号 分解方法，它们的不同之处在于CEEMD中加入了 旋转因子 以减少模态混叠现象。因此，CEEMD通常比EEMD更能有效地解决 模态混叠 问 …</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">EEMD分解出的层数较少是因为CEEMD加入了旋转因子以减少模态混叠现象，而模态混叠是导致信号分解不完全的重要因素。此外，CEEMD通常比EEMD更能有效地解决模态混叠问题，因此在实际应用中，CEEMD往往能提供更准确的信号分解结果。因此，选择CEEMD作为信号处理工具可以显著提高信号分析的精度和可靠性。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>EEMD分解出的层数较少是因为CEEMD加入了旋转因子以减少模态混叠现象。</li>
<li>CEEMD通常比EEMD更能有效地解决模态混叠问题。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-25">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-26">14. EMD过程跟去噪有关系么？ - 知乎</h2>
<ul>
<li>链接：https://www.zhihu.com/question/299061804</li>
<li>来源：bing</li>
<li>摘要：2018年4月3日 · 这是两个中心频率分别为50和100Hz的正弦波叠加并叠加高斯噪声的时域波形，信噪比为0dB EMD分解的前5个IMF分量 EEMD分解的前5个IMF分量 请问，从这些分解结果中怎么降噪呢?也许 …</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">EMD（经验模式分解）和EEMD（ ensemble经验模式分解）可以有效地提取信号的内在模式函数，从而实现去噪。IMF（经验模式分解的分量）代表信号的不同频率成分，通过分析IMF分量可以识别噪声。信噪比为0dB意味着信号和噪声的幅度相同，这要求在处理过程中需要特别小心，以避免信号失真。高斯噪声的存在显著影响了信号的纯净度，因此必须通过EMD或EEMD等方法进行处理，以确保信号的准确性和可靠性。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>EMD和EEMD分解可以提取信号的内在模式函数，从而实现去噪。</li>
<li>IMF分量代表信号的不同频率成分，通过分析IMF分量可以识别噪声。</li>
<li>信噪比为0dB意味着信号和噪声的幅度相同，需要仔细处理以避免失真。</li>
<li>高斯噪声的存在影响了信号的纯净度，需要通过EMD或EEMD进行处理。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-27">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-28">15. 豆包大模型 Seed-2.0 正式发布，带来哪些新功能和体验升级？</h2>
<ul>
<li>链接：https://www.zhihu.com/question/2006012406770455579</li>
<li>来源：bing</li>
<li>摘要：2026年2月14日 · 豆包大模型 Seed-2.0 正式发布，带来哪些新功能和体验升级？ 2 月 14 日，豆包大模型2.0正式发布。 现在大家打开豆包 App、电脑客户端或网页版，点击「专家模式」，即可第一时间体 …</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">专家模式提供了一系列新功能和体验升级，旨在为用户提供更加深入和定制化的服务。然而，为了体验这些新功能，用户必须首先点击进入「专家模式」。值得注意的是，对于普通用户而言，这些新功能和体验升级可能并不明显，因此他们可能不会主动启用「专家模式」。因此，「专家模式」可能成为访问这些新功能的唯一途径，这也意味着用户如果不开启「专家模式」，就无法享受到这些升级带来的好处。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>专家模式提供新功能和体验升级。</li>
<li>用户需点击「专家模式」才能体验新功能。</li>
<li>新功能和体验升级对普通用户可能不明显。</li>
<li>「专家模式」可能是访问新功能的唯一途径。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-29">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-30">16. 谷歌 Gemini 3 Deep Think 深度思考大模型升级，相比前代 ...</h2>
<ul>
<li>链接：https://www.zhihu.com/question/2005579925827130176</li>
<li>来源：bing</li>
<li>摘要：2026年2月13日 · 首先要明确一点， Gemini 3 Deep Think 并不是面向普通用户的通用大模型，它的定位是科学、研究与工程场景的“ 深度推理模式 ”（之前的DeepSeek-V3.2-Speciale也是类似），目标是 …</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">Gemini 3 Deep Think 并非面向普通用户的通用大模型，而是专为科学、研究与工程场景设计的“深度推理模式”。这一模式强调深度分析与推理能力，类似于其前身DeepSeek-V3.2-Speciale，同样专注于提供更为专业和深入的支持。因此，Gemini 3 Deep Think 的设计初衷和使用场景与普通用户的需求有所不同，更适合那些需要进行复杂推理和深度分析的专业用户。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>Gemini 3 Deep Think 不是面向普通用户的通用大模型。</li>
<li>Gemini 3 Deep Think 的定位是科学、研究与工程场景的“深度推理模式”。</li>
<li>之前的DeepSeek-V3.2-Speciale也是类似定位。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-31">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-32">17. 关于论文题目中的基于和面向的区别？ - 知乎</h2>
<ul>
<li>链接：https://www.zhihu.com/question/53396928</li>
<li>来源：bing</li>
<li>摘要：2018年5月9日 · 关于论文题目中的基于和面向的区别？ 个人理解，基于应该是基于某种新技术，面相应该是面相的一种特定的情况。 不知道是不是这样，老师敷衍的给了个论文题目是，基于标 …</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">基于通常指依托某种新技术或理论，而面向则指针对特定情况或需求。这种依托新技术或理论的基于方法，往往能够提供更高效、更精准的解决方案。然而，这种基于方法的成功实施，关键在于能否准确地面向特定的情况或需求。因此，只有将基于与面向相结合，才能确保技术或理论的应用真正发挥其应有的效果，从而实现预期的目标。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>基于通常指依托某种新技术或理论。</li>
<li>面向则指针对特定情况或需求。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-33">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-34">18. “基于”这两个字可以这样用吗？ - 知乎</h2>
<ul>
<li>链接：https://www.zhihu.com/question/507368602</li>
<li>来源：bing</li>
<li>摘要：“基于”这两个字可以这样用吗？ 看到一本书上这样写：“从名称可以看出，它直接基于我们之前介绍过的自助采样法”。 老觉得不太顺口，文章这样写可以吗？ 显示全部 关注者 7 被浏览</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">基于可以用于表示新概念与已有概念之间的关系，这不仅有助于清晰地界定新概念的内涵，还能让读者更容易理解其与现有知识体系的联系。文章中使用“基于”是正确的，尽管可能感觉不顺口，但这种表达方式有助于增强论述的逻辑性和连贯性。此外，基于还可以用于介绍新方法或理论的来源，这有助于读者了解这些新概念或理论的背景和合理性。因此，基于的使用取决于语境和读者的接受程度，选择合适的表达方式可以更好地服务于文章的整体目标。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>基于可以用于表示新概念与已有概念的关系。</li>
<li>文章中使用“基于”是正确的，尽管可能感觉不顺口。</li>
<li>基于可以用于介绍新方法或理论的来源。</li>
<li>基于的使用取决于语境和读者的接受程度。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-35">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-36">19. 在写研究课题时，“基于……的研究”和“……视角下的研究 ...</h2>
<ul>
<li>链接：https://www.zhihu.com/question/588333573</li>
<li>来源：bing</li>
<li>摘要：2023年3月8日 · 如题，不同的题目，省略号在课题中所起的作用有什么不同 1.确定研究问题 确定研究问题是进行研究的前提。教师的叙事研究虽然已明确了总的框架是教师研究，但是，教师 …</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">确定研究问题的明确性对于整个研究至关重要，这不仅有助于研究方向的清晰界定，还能够确保研究过程的有序进行。对于教师的叙事研究而言，具体明确的研究问题更是不可或缺，它能够为研究提供明确的指导。此外，基于特定视角的研究可以提供独特的分析视角，使得研究更具深度和广度。值得注意的是，不同的题目中省略号所代表的内容可能各不相同，这要求研究者在设计研究问题时要特别注意其明确性和具体性，以确保研究的有效性和针对性。因此，明确的研究问题不仅能够提升研究的质量，还能为后续的研究工作奠定坚实的基础。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>确定研究问题的明确性对于整个研究至关重要。</li>
<li>教师的叙事研究需要具体明确的研究问题来指导研究方向。</li>
<li>基于特定视角的研究可以提供独特的分析视角。</li>
<li>不同的题目中省略号所代表的内容可能各不相同。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-37">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-38">20. 有基于 C/C++ 的 Web 开发框架吗？ - 知乎</h2>
<ul>
<li>链接：https://www.zhihu.com/question/11084761501</li>
<li>来源：bing</li>
<li>摘要：有一个轻量级的web库叫crow，它是基于boost::asio的一个二次封装，特点是本身只有一个头文件，所以使用起来相对简单，而且它有超高的性能，这个库有7.5k的star（新的地址还有3.6k …</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">crow库是一个轻量且高效的C++网络库，其设计简洁，仅包含一个头文件，使得集成和使用变得极为便捷。首先，crow库基于boost::asio进行二次封装，继承了boost::asio的强大功能和灵活性，同时简化了网络编程的复杂性。其次，crow库具有较高的性能，这得益于其对底层boost::asio的优化和封装，使得开发者能够高效地处理网络请求，提高应用的响应速度和处理能力。因此，crow库不仅易于集成，还具备出色的性能，是进行网络编程的理想选择。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>crow库只有一个头文件。</li>
<li>crow库基于boost::asio进行二次封装。</li>
<li>crow库具有较高的性能。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-39">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-40">21. 如何通俗地介绍基于模型的系统工程（MBSE）？ - 知乎</h2>
<ul>
<li>链接：https://www.zhihu.com/question/31958675</li>
<li>来源：bing</li>
<li>摘要：03 基于模型的系统工程MBSE 在传统的系统工程应用过程中，不管是需求，还是架构都是以文档为中心，每个阶段工作输出结果的载体都是文档，不同团队和阶段的交流也都是基于文档。</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">文档是传统系统工程各阶段的主要输出形式，不同团队之间的交流主要依赖于文档。然而，随着基于模型的系统工程（MBSE）的引入，情况发生了变化。MBSE 强调以模型而非文档为中心，这不仅改变了系统工程的实践方式，还改善了不同团队之间的沟通。通过使用模型，团队可以更直观地理解系统的设计和功能，从而减少误解和沟通障碍，提升整体项目的效率和质量。因此，MBSE 的应用不仅能够提高工程的透明度，还能促进跨团队的合作，使得整个系统工程过程更加高效和可靠。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>文档是传统系统工程各阶段的主要输出形式。</li>
<li>不同团队之间的交流主要依赖于文档。</li>
<li>基于模型的系统工程（MBSE）强调以模型而非文档为中心。</li>
<li>MBSE 能够改善不同团队之间的沟通。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-41">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-42">22. ATE中PMU和PPMU的具体工作原理是什么？ - 知乎</h2>
<ul>
<li>链接：https://www.zhihu.com/question/8932154222</li>
<li>来源：bing</li>
<li>摘要：呀，ATE中PMU和PPMU的工作原理呀，让我来给你讲讲吧 ( ‿ )！ PMU工作原理 PMU，就是精密测量单元啦，它的工作原理主要是通过驱动电流或电压进入被测器件（DUT），然后采集被测器件的输出 …</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">PMU（电源测量单元）通过驱动电流或电压进入被测器件来工作，这一过程确保了被测器件能够处于实际工作状态，从而能够准确模拟其在实际应用中的表现。PMU随后采集被测器件的输出以进行测量，这一步骤使得可以全面评估被测器件的性能指标，包括电压、电流、功率等参数，确保其符合设计要求和行业标准。因此，PMU通过这两个关键步骤，不仅能够驱动被测器件，还能对其输出进行全面的测量和分析，从而保证测试结果的准确性和可靠性。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>PMU通过驱动电流或电压进入被测器件来工作。</li>
<li>PMU采集被测器件的输出以进行测量。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-43">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-44">23. 自动化测试 | PMU与PMU有什么区别吗？ - 知乎</h2>
<ul>
<li>链接：https://www.zhihu.com/question/594851077</li>
<li>来源：bing</li>
<li>摘要：在自动化测试中，PMU和PMI是两个不同的概念，它们分别代表着“性能监测单元”和“性能监测接口”。 PMU（Performance Monitoring Unit）指的是一种硬件模块或者芯片，通常用于CPU、GPU等设备 …</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">PMU是硬件模块或芯片，而非软件接口，其主要功能是收集系统的实时性能数据。与此不同，PMI指的是“性能监测接口”，它是一个软件接口，用于获取PMU提供的硬件数据。因此，PMU与PMI是两个不同的概念，前者是硬件设备，后者是软件接口，两者共同作用于系统性能的监测与优化。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>PMU是硬件模块或芯片，而非软件接口。</li>
<li>PMU与PMI是两个不同的概念。</li>
<li>PMI指的是“性能监测接口”，而非硬件模块。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-45">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-46">24. Revisions | OpenReview</h2>
<ul>
<li>链接：https://openreview.net/revisions?id=NfbBdPzcbp</li>
<li>来源：bing</li>
<li>摘要：2025年5月11日 · Title: iTransformer: Inverted Transformers Are Effective for Time Series Forecasting. Authors: Yong Liu, Tengge Hu, Haoran Zhang, Haixu Wu, Shiyu Wang 0001, Lintao Ma, Mingsheng …</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">首先，iTransformer模型通过反转Transformer结构在时间序列预测中表现出有效性。这一研究专注于时间序列forecasting任务，而非其他领域，这表明时间序列预测是一个专门的应用领域，与其他任务有着显著的区别。其次，模型的名称为iTransformer，而非其他变体，这进一步强调了其独特性。此外，反转的Transformer结构可能带来新的视角和性能提升，这为时间序列预测提供了新的可能性。因此，iTransformer模型不仅在时间序列预测中表现出色，而且其独特的结构设计也为该领域的研究带来了新的启示。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>iTransformer模型通过反转Transformer结构在时间序列预测中表现出有效性。</li>
<li>该研究关注时间序列 forecasting 而非其他任务。</li>
<li>模型名称为iTransformer，而非其他变体。</li>
<li>反转的Transformer结构可能带来新的视角和性能提升。</li>
<li>时间序列预测是一个专门的应用领域，与其他领域不同。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-47">正文（抓取，非 AI）</h3>
<p>Revisions | OpenReview Loading</p>
</div></details><h2 id="toc-48">25. 突破瓶颈！我国科学家打造出新型电池-观察者网</h2>
<ul>
<li>链接：https://m.guancha.cn/politics/2026_02_19_807511.shtml</li>
<li>来源：bing</li>
<li>摘要：3 天之前 · 天津大学许运华教授团队联合华南理工大学黄飞教授团队等单位，成功研制出一种新型有机正极材料，突破了传统有机锂电池“电量低”“难以实用化”等瓶颈。 相关研究成果于北京时间2月19日在线 …</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">新型有机正极材料突破了传统有机锂电池“电量低”“难以实用化”等瓶颈，这主要得益于有机电极材料的分子结构可灵活设计且自身柔韧，但制成的电池往往“电量”不足或充电缓慢。为解决这一问题，研究团队通过调控材料中电子与锂离子的“协同传输”效率，成功研制出新型有机正极材料。基于这种新型材料，团队制备出能量密度超过250瓦时/公斤的有机软包电池。这种有机软包电池不仅能在-70℃到80℃的温度下正常工作，还具有良好的柔韧性和安全性。实验表明，即使在弯折、拉伸、外力挤压等情况下，有机软包电池也无破损，且电池容量不减。此外，团队研制的软包电池还成功通过了严格的针刺安全测试，进一步验证了其安全性。这些成果为未来开发“绿色电池”奠定了关键材料基础，也为柔性电子、可穿戴设备等领域提供了储能解决方案。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>新型有机正极材料突破了传统有机锂电池“电量低”“难以实用化”等瓶颈。</li>
<li>有机电极材料的分子结构可灵活设计且自身柔韧，但制成的电池往往“电量”不足或充电缓慢。</li>
<li>研究团队通过调控材料中电子与锂离子的“协同传输”效率，成功研制出新型有机正极材料。</li>
<li>基于新型有机正极材料，团队制备出能量密度超过250瓦时/公斤的有机软包电池。</li>
<li>有机软包电池能在-70℃到80℃的温度下正常工作，且具有良好的柔韧性和安全性。</li>
<li>实验表明，有机软包电池在弯折、拉伸、外力挤压等情况下无破损，且电池容量不减。</li>
<li>团队研制的软包电池成功通过了严格的针刺安全测试，安全性得到验证。</li>
<li>相关成果为未来开发“绿色电池”奠定了关键材料基础，也为柔性电子、可穿戴设备等领域提供了储能解决方案。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-49">正文（抓取，非 AI）</h3>
<p>突破瓶颈！我国科学家打造出新型电池-观察者网 我国科学家打造出安全、抗冻、耐热新型电池 来源：央视新闻客户端 2026-02-19 12:29 天津大学许运华教授团队联合华南理工大学黄飞教授团队等单位，成功研制出一种新型有机正极材料，突破了传统有机锂电池“电量低”“难以实用化”等瓶颈。相关研究成果于北京时间2月19日在线发表于国际学术期刊《自然》。 在科技革命与能源转型浪潮中，锂电池犹如现代社会的“能量心脏”，其重要性日益凸显。目前，主流锂电池正极材料大多使用钴、镍等无机矿物，这类材料面临资源短缺、成本较高及柔性不足等多重问题。相比之下，有机电极材料取材广泛，其分子结构可灵活设计且自身柔韧。然而，这类材料制成的电池往往“电量”不足或充电缓慢，严重阻碍其实用化进程。 △能量密度超过250瓦时/公斤的有机软包电池。 为解决这一困境，研究团队在一种新型导电聚合物材料基础上，系统调控了材料中电子与锂离子的“协同传输”效率，成功研制出一种兼具优异电子导电性、锂离子快速传输能力和高储能容量的有机正极材料。 基于此材料，团队制备出一款能量密度超过250瓦时/公斤的有机软包电池，这一数值已超越目前广泛使用的磷酸铁锂电池。这款电池展现出卓越的温度适应能力，不仅能在-70℃到80℃的温度下正常工作，还兼具良好的柔韧性与安全性。 △有机软包电池性能图。 实验表明，其电极在弯折、拉伸、外力挤压等情况下无破损，且电池容量不减。团队研制的软包电池成功通过了严格的针刺安全测试，安全性得到验证。 许运华表示，相关成果为未来开发“绿色电池”奠定了关键材料基础，其柔性特质也为未来柔性电子、可穿戴设备等领域提供了全新的储能解决方案。 据悉，团队正加快推进这项技术的成果转化与产业化进程，致力于建设有机软包电池生产线，积极探索其商业化应用前景。 责任编辑：胡致 观察者APP，更好阅读体验 举报 取消 请选择举报理由 违反法律法规 垃圾信息广告 色情、淫秽内容 人身攻击 谣言、不实信息 冒充、冒用信息 其它 涉未成年人有害信息 提交 Copyright©2021观察者 沪ICP备1021382-2号 互联网信息许可证：3112014003 气疯了，“10%不够，加到15%！” 大军压境，特朗普却对伊朗“松口”？ 特朗普关税被推翻，日本咬牙：对美投资承诺不变 官宣！取代美国，中国再次成为德国最大贸易伙伴 “这下好了，中国怎么可能再买美国大豆？”</p>
</div></details><h2 id="toc-50">26. EEMD，集合经验模态分解EEMD的信号分解（Matlab完整 ...</h2>
<ul>
<li>链接：https://bbs.csdn.net/topics/617492795</li>
<li>来源：bing</li>
<li>摘要：2023年10月31日 · 以下内容是CSDN社区关于EEMD，集合经验模态分解EEMD的信号分解（Matlab完整源码和数据) 下载相关内容，如果想了解更多关于下载资源悬赏专区社区其他内容，请访问CSDN社 …</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">首先，EEMD（经验模态分解）的实现需要Matlab 2023及以上版本，这确保了其高效性和兼容性。其次，参数化编程使得EEMD的参数可以方便更改，进一步增强了其灵活性。代码思路清晰，注释详细，便于理解，这使得学习和使用EEMD变得更加容易。此外，EEMD适用于计算机、电子信息工程、数学等专业的学生，直接替换数据即可使用，特别适合新手。因此，附赠案例数据，用户可以一键运行出图，进一步简化了操作过程。EEMD的分解效果图完全满足需求，使得用户能够直观地看到分解结果。值得注意的是，EMD经验模态分解与EEMD不同，适用场景和效果可能不同，而VMD变分模态分解同样适用于不同场景。此外，C EEMD AN完全自适应噪声集合经验模态分解与EEMD类似，但有其特定应用。最后，EEMD的源码和数据可以直接下载使用，方便用户进行研究和实践。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>EEMD的实现需要Matlab 2023及以上版本。</li>
<li>参数化编程使得EEMD的参数可以方便更改。</li>
<li>代码思路清晰，注释详细，便于理解。</li>
<li>适用对象包括计算机、电子信息工程、数学等专业的学生。</li>
<li>直接替换数据即可使用，适合新手。</li>
<li>附赠案例数据，可一键运行出图。</li>
<li>EEMD的分解效果图可完全满足需求。</li>
<li>EMD经验模态分解与EEMD不同，适用场景和效果可能不同。</li>
<li>VMD变分模态分解与EEMD不同，适用于不同场景。</li>
<li>C EEMD AN完全自适应噪声集合经验模态分解与EEMD类似，但有其特定应用。</li>
<li>EEMD的源码和数据可直接下载使用。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-51">正文（抓取，非 AI）</h3>
<p>EEMD，集合经验模态分解EEMD的信号分解（Matlab完整源码和数据) 下载-CSDN社区 社区 下载资源悬赏专区 帖子详情 EEMD，集合经验模态分解EEMD的信号分解（Matlab完整源码和数据) 下载 weixin_39820535 2023-10-31 10:01:06 EEMD，集合经验模态分解EEMD的信号分解（Matlab完整源码和数据) EEMD，集合经验模态分解EEMD的信号分解（Matlab完整源码和数据) EEMD，集合经验模态分解EEMD的信号分解（Matlab完整源码和数据) , 相关下载链接： https://download.csdn.net/download/m0_57362105/88430301?utm_source=bbsseo ...全文 8 回复 打赏 收藏 EEMD，集合经验模态分解EEMD的信号分解（Matlab完整源码和数据) 下载 EEMD，集合经验模态分解EEMD的信号分解（Matlab完整源码和数据) EEMD，集合经验模态分解EEMD的信号分解（Matlab完整源码和数据) EEMD，集合经验模态分解EEMD的信号分解（Matlab完整源码和数据) , 相关下载链接：ht 复制链接 扫一扫 分享 转发到动态 举报 写回复 配置赞助广告 取 消 确 定 用AI写文章 请发表友善的回复… 发表回复 打赏红包 需支付: 0.00 元 取 消 确 定 EEMD 集合 经验模态 分解 （ Matlab 完整 源码 和 数据 ） 1. EEMD 集合 经验模态 分解 （ Matlab 完整 源码 和 数据 ），运行环境为 Matlab 2023及以上。 2.代码特点：参数化编程、参数可方便更改、代码编程思路清晰、注释明细。 3.适用对象：计算机，电子信息工程、数学等专业的大学生... EEMD ， 集合 经验模态 分解 EEMD 的 信号 分解 （ Matlab 完整 源码 和 数据 ) EEMD ， 集合 经验模态 分解 EEMD 的 信号 分解 （ Matlab 完整 源码 和 数据 ) EEMD ， 集合 经验模态 分解 EEMD 的 信号 分解 （ Matlab 完整 源码 和 数据 ) EEMD ， 集合 经验模态 分解 EEMD 的 信号 分解 （ Matlab 完整 源码 和 数据 ) Matlab 实现 EEMD 集合 经验模态 分解 时间序列 信号 分解 （ 完整 源码 和 数据 ） 1. Matlab 实现 EEMD 集合 经验模态 分解 时间序列 信号 分解 （ 完整 源码 和 数据 ） 2. 分解 效果图 ，效果如图所示，可完全满足您的需求～ 3.直接替换 数据 即可用 适合新手小白 注释清晰～ 4.附赠案例 数据 直接运行main一键出图～ ... Matlab 实现EMD 经验模态 分解 时间序列 信号 分解 （ 完整 源码 和 数据 ) Matlab 实现EMD 经验模态 分解 时间序列 信号 分解 （ 完整 源码 和 数据 ) 1. Matlab 实现EMD 经验模态 分解 时间序列 信号 分解 Matlab 语言 2.算法新颖小众，用的人很少，包含 分解 图～ 3.适用对象：计算机，电子信息工程、数学等专业... 【数字 信号 分解 】基于 matlab 数字 信号 集成 经验模态 分解 EEMD 【含 Matlab 源码 期】.zip 3. EEMD （集成 经验模态 分解 ，Ensemble Empirical Mode Decomposition） 4. VMD（变分模态 分解 ，Variational Mode Decomposition） 5. C EEMD AN（完全自适应噪声 集合 经验模态 分解 ，Complementary Ensemble Empirical ... 下载资源悬赏专区 13,654 社区成员 12,578,697 社区内容 发帖 与我相关 我的任务 下载资源悬赏专区 CSDN 下载资源悬赏专区 复制链接 扫一扫 分享 确定 社区描述 CSDN 下载资源悬赏专区 其他 技术论坛（原bbs） 社区管理员 加入社区 获取链接或二维码 近7日 近30日 至今 加载中 查看更多榜单 社区公告 暂无公告 试试用AI创作助手写篇文章吧 + 用AI写文章</p>
</div></details><h2 id="toc-52">27. TimeXer: Empowering Transformers for Time Series Forecasting …</h2>
<ul>
<li>链接：https://openreview.net/forum?id=INAeUQ04lT</li>
<li>来源：bing</li>
<li>摘要：2024年9月25日 · Essentially, TimeXer and iTransformer share a similar model structure, yet TimeXer consumes less memory. What do you believe accounts for TimeXer being more memory-efficient …</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">首先，为了准确预测 endogenous variables，需要考虑外部信息。其次，exogenous variables 能够提供有助于 endogenous variables 的外部信息，因此它们在预测过程中扮演着重要角色。此外，TimeXer 通过设计嵌入层来增强 Transformer 的能力，使得模型能够更好地处理 endogenous 和 exogenous variables。进一步地，patch-wise 自注意力和 variate-wise 交叉注意力同时使用，有助于模型捕捉不同尺度的信息。因此，全局 endogenous token 被学习以有效传递外部信息，从而提升预测的准确性。因此，TimeXer 在多个实际预测基准上表现出色，并且其代码可以在指定的 GitHub 仓库中找到。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>endogenous variables 的准确预测需要考虑外部信息。</li>
<li>exogenous variables 可以提供有助于 endogenous variables 的外部信息。</li>
<li>TimeXer 通过设计嵌入层来增强 Transformer 的能力。</li>
<li>patch-wise 自注意力和 variate-wise 交叉注意力同时使用。</li>
<li>全局 endogenous token 被学习以有效传递外部信息。</li>
<li>TimeXer 在多个实际预测基准上表现出色。</li>
<li>TimeXer 的代码可以在指定的 GitHub 仓库中找到。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-53">正文（抓取，非 AI）</h3>
<p>TimeXer: Empowering Transformers for Time Series Forecasting with Exogenous Variables | OpenReview Go to NeurIPS 2024 Conference homepage TimeXer: Empowering Transformers for Time Series Forecasting with Exogenous Variables Yuxuan Wang , Haixu Wu , Jiaxiang Dong , Guo Qin , Haoran Zhang , Yong Liu , Yunzhong Qiu , Jianmin Wang , Mingsheng Long Published: 25 Sept 2024, Last Modified: 31 Jan 2025 NeurIPS 2024 poster Everyone Revisions BibTeX CC BY 4.0 Keywords : Transformer, Time Series Forecasting, Exogenous Variable Abstract : Deep models have demonstrated remarkable performance in time series forecasting. However, due to the partially-observed nature of real-world applications, solely focusing on the target of interest, so-called endogenous variables, is usually insufficient to guarantee accurate forecasting. Notably, a system is often recorded into multiple variables, where the exogenous variables can provide valuable external information for endogenous variables. Thus, unlike well-established multivariate or univariate forecasting paradigms that either treat all the variables equally or ignore exogenous information, this paper focuses on a more practical setting: time series forecasting with exogenous variables. We propose a novel approach, TimeXer, to ingest external information to enhance the forecasting of endogenous variables. With deftly designed embedding layers, TimeXer empowers the canonical Transformer with the ability to reconcile endogenous and exogenous information, where patch-wise self-attention and variate-wise cross-attention are used simultaneously. Moreover, global endogenous tokens are learned to effectively bridge the causal information underlying exogenous series into endogenous temporal patches. Experimentally, TimeXer achieves consistent state-of-the-art performance on twelve real-world forecasting benchmarks and exhibits notable generality and scalability. Code is available at this repository: https://github.com/thuml/TimeXer. Supplementary Material : zip Primary Area : Deep learning architectures Submission Number : 3309 Loading OpenReview is a long-term project to advance science through improved peer review with legal nonprofit status. We gratefully acknowledge the support of the OpenReview Sponsors . © 2026 OpenReview</p>
</div></details><h2 id="toc-54">28. FreDF: Learning to Forecast in the Frequency Domain</h2>
<ul>
<li>链接：https://openreview.net/forum?id=4A9IdSa1ul</li>
<li>来源：bing</li>
<li>摘要：2025年1月22日 · For FreDF, we fix the hyperparameters associated with the forecast models, such as the iTransformer and TimesNet, and only fine-tune the frequency loss strength α and the learning rate …</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">首先，当前的预报模型在微调过程中固定了超参数，仅调整频率损失强度和学习率，而忽视了未来序列标签之间的关联性。其次，FreDF通过在频率域中学习来解决由标签关联性引起的学习目标偏差问题，从而提高了预报性能。此外，FreDF兼容多种预报模型，如iTransformer和TimesNet，而这些模型在处理标签关联性时并未考虑。因此，FreDF在实验中表现出对标签关联性的估计偏差减少，并且在现有最先进的方法中表现出色。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>forecast models' hyperparameters are fixed while fine-tuning frequency loss strength and learning rate</li>
<li>label correlations among future sequences are often overlooked in current forecasting models</li>
<li>FreDF addresses the bias in learning objectives caused by label correlations</li>
<li>FreDF improves forecasting performance by learning in the frequency domain</li>
<li>FreDF is compatible with various forecast models like iTransformer and TimesNet</li>
<li>label correlations are not considered in the Direct Forecast paradigm</li>
<li>FreDF reduces estimation bias in the presence of label correlations</li>
<li>FreDF outperforms existing state-of-the-art methods in experiments</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-55">正文（抓取，非 AI）</h3>
<p>FreDF: Learning to Forecast in the Frequency Domain | OpenReview Go to ICLR 2025 Conference homepage FreDF: Learning to Forecast in the Frequency Domain Hao Wang , Lichen Pan , Yuan Shen , Zhichao Chen , Degui Yang , Yifei Yang , Sen Zhang , Xinggao Liu , Haoxuan Li , Dacheng Tao Published: 22 Jan 2025, Last Modified: 02 Apr 2025 ICLR 2025 Poster Everyone Revisions BibTeX CC BY 4.0 Keywords : Time series, Long-term Forecast TL;DR : Learning to forecast in the frequency domain improves forecasting performance. Abstract : Time series modeling presents unique challenges due to autocorrelation in both historical data and future sequences. While current research predominantly addresses autocorrelation within historical data, the correlations among future labels are often overlooked. Specifically, modern forecasting models primarily adhere to the Direct Forecast (DF) paradigm, generating multi-step forecasts independently and disregarding label correlations over time. In this work, we demonstrate that the learning objective of DF is biased in the presence of label correlation. To address this issue, we propose the Frequency-enhanced Direct Forecast (FreDF), which mitigates label correlation by learning to forecast in the frequency domain, thereby reducing estimation bias. Our experiments show that FreDF significantly outperforms existing state-of-the-art methods and is compatible with a variety of forecast models. Code is available at https://github.com/Master-PLC/FreDF. Supplementary Material : pdf Primary Area : learning on time series and dynamical systems Code Of Ethics : I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics. Submission Guidelines : I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide. Anonymous Url : I certify that there is no URL (e.g., github page) that could be used to find authors’ identity. No Acknowledgement Section : I certify that there is no acknowledgement section in this submission for double blind review. Submission Number : 13602 Loading OpenReview is a long-term project to advance science through improved peer review with legal nonprofit status. We gratefully acknowledge the support of the OpenReview Sponsors . © 2026 OpenReview</p>
</div></details><h2 id="toc-56">29. Crossformer: Transformer Utilizing Cross-Dimension Dependency …</h2>
<ul>
<li>链接：https://openreview.net/forum?id=vSVLM2j9eie</li>
<li>来源：bing</li>
<li>摘要：2023年2月1日 · We propose Crossformer, a Transformer-based model that explicitly utilizes cross-dimension dependency for multivariate time series forecasting.</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">首先，Crossformer能够捕捉时间维度和变量维度的依赖关系，这是其在多变量时间序列预测中表现出色的关键因素之一。其次，Crossformer通过DSW嵌入保留时间和变量信息，确保了在预测过程中能够高效地利用这些信息。此外，TSA层进一步增强了Crossformer在时间维度和变量维度上的依赖关系捕捉能力。因此，Crossformer通过层次编码器-解码器结构，利用不同尺度的信息进行预测，从而在多变量时间序列预测中取得了优异的性能。相比之下，现有的Transformer模型往往忽略了变量维度的依赖关系，导致预测效果不如Crossformer。实验结果也证实了Crossformer的优越性。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>Crossformer可以捕捉时间维度和变量维度的依赖关系。</li>
<li>Crossformer通过DSW嵌入保留时间和变量信息。</li>
<li>TSA层有助于高效捕捉时间维度和变量维度的依赖关系。</li>
<li>Crossformer通过层次编码器-解码器结构利用不同尺度的信息进行预测。</li>
<li>现有Transformer模型往往忽略了变量维度的依赖关系。</li>
<li>Crossformer在多变量时间序列预测中表现出色。</li>
<li>实验结果表明Crossformer优于现有方法。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-57">正文（抓取，非 AI）</h3>
<p>Crossformer: Transformer Utilizing Cross-Dimension Dependency for Multivariate Time Series Forecasting | OpenReview Go to ICLR 2023 Conference homepage Crossformer: Transformer Utilizing Cross-Dimension Dependency for Multivariate Time Series Forecasting Yunhao Zhang , Junchi Yan Published: 01 Feb 2023, Last Modified: 02 Mar 2023 ICLR 2023 notable top 5% Readers: Everyone Keywords : Transformer, multivariate time series forecasting, deep learning Abstract : Recently many deep models have been proposed for multivariate time series (MTS) forecasting. In particular, Transformer-based models have shown great potential because they can capture long-term dependency. However, existing Transformer-based models mainly focus on modeling the temporal dependency (cross-time dependency) yet often omit the dependency among different variables (cross-dimension dependency), which is critical for MTS forecasting. To fill the gap, we propose Crossformer, a Transformer-based model utilizing cross-dimension dependency for MTS forecasting. In Crossformer, the input MTS is embedded into a 2D vector array through the Dimension-Segment-Wise (DSW) embedding to preserve time and dimension information. Then the Two-Stage Attention (TSA) layer is proposed to efficiently capture the cross-time and cross-dimension dependency. Utilizing DSW embedding and TSA layer, Crossformer establishes a Hierarchical Encoder-Decoder (HED) to use the information at different scales for the final forecasting. Extensive experimental results on six real-world datasets show the effectiveness of Crossformer against previous state-of-the-arts. Anonymous Url : I certify that there is no URL (e.g., github page) that could be used to find authors’ identity. No Acknowledgement Section : I certify that there is no acknowledgement section in this submission for double blind review. Code Of Ethics : I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics Submission Guidelines : Yes Please Choose The Closest Area That Your Submission Falls Into : Applications (eg, speech processing, computer vision, NLP) TL;DR : We propose Crossformer, a Transformer-based model that explicitly utilizes cross-dimension dependency for multivariate time series forecasting. 21 Replies Loading OpenReview is a long-term project to advance science through improved peer review with legal nonprofit status. We gratefully acknowledge the support of the OpenReview Sponsors . © 2026 OpenReview</p>
</div></details><h2 id="toc-58">30. A Time Series is Worth 64 Words: Long-term Forecasting with...</h2>
<ul>
<li>链接：https://openreview.net/forum?id=Jbdc0vTOcol</li>
<li>来源：bing</li>
<li>摘要：2023年2月1日 · Channel-independent patch time series transformer works very well for long-term forecasting and representation learning.</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">patch时间序列Transformer通过分割时间序列来保留局部语义信息，同时减少了注意力图的计算和内存使用，从而能够关注更长的历史。这种设计不仅提高了模型的效率，还增强了其对长期预测的准确性，使其在长期预测任务上优于当前最先进的SOTA Transformer模型。此外，patch时间序列Transformer在自监督预训练任务中表现出色，并且在不同数据集上的转移预训练也能达到SOTA预测准确性，进一步证明了其在实际应用中的强大潜力。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>patch时间序列Transformer通过分割时间序列来保留局部语义信息。</li>
<li>patch时间序列Transformer通过分割时间序列减少了注意力图的计算和内存使用。</li>
<li>patch时间序列Transformer通过分割时间序列能够关注更长的历史。</li>
<li>channel独立性意味着每个通道包含一个单一的单变量时间序列。</li>
<li>patch时间序列Transformer在长期预测准确性上优于SOTA Transformer模型。</li>
<li>patch时间序列Transformer在自监督预训练任务中表现出色。</li>
<li>patch时间序列Transformer在不同数据集上的转移预训练也能达到SOTA预测准确性。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-59">正文（抓取，非 AI）</h3>
<p>A Time Series is Worth 64 Words: Long-term Forecasting with Transformers | OpenReview Go to ICLR 2023 Conference homepage A Time Series is Worth 64 Words: Long-term Forecasting with Transformers Yuqi Nie , Nam H Nguyen , Phanwadee Sinthong , Jayant Kalagnanam Published: 01 Feb 2023, Last Modified: 14 Jan 2026 ICLR 2023 poster Readers: Everyone Keywords : time series, transformer, forecasting, channel-independence, self-supervised learning, representation learning TL;DR : Channel-independent patch time series transformer works very well for long-term forecasting and representation learning. Abstract : We propose an efficient design of Transformer-based models for multivariate time series forecasting and self-supervised representation learning. It is based on two key components: (i) segmentation of time series into subseries-level patches which are served as input tokens to Transformer; (ii) channel-independence where each channel contains a single univariate time series that shares the same embedding and Transformer weights across all the series. Patching design naturally has three-fold benefit: local semantic information is retained in the embedding; computation and memory usage of the attention maps are quadratically reduced given the same look-back window; and the model can attend longer history. Our channel-independent patch time series Transformer (PatchTST) can improve the long-term forecasting accuracy significantly when compared with that of SOTA Transformer-based models. We also apply our model to self-supervised pre-training tasks and attain excellent fine-tuning performance, which outperforms supervised training on large datasets. Transferring of masked pre-training performed on one dataset to other datasets also produces SOTA forecasting accuracy. Anonymous Url : I certify that there is no URL (e.g., github page) that could be used to find authors’ identity. No Acknowledgement Section : I certify that there is no acknowledgement section in this submission for double blind review. Code Of Ethics : I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics Submission Guidelines : Yes Please Choose The Closest Area That Your Submission Falls Into : Applications (eg, speech processing, computer vision, NLP) Community Implementations : <a href="https://www.catalyzex.com/paper/a-time-series-is-worth-64-words-long-term/code"><img alt="CatalyzeX" src="/images/catalyzex_icon.svg"/> 4 code implementations</a> 21 Replies Loading OpenReview is a long-term project to advance science through improved peer review with legal nonprofit status. We gratefully acknowledge the support of the OpenReview Sponsors . © 2026 OpenReview</p>
</div></details><h2 id="toc-60">31. 电脑或者笔记本怎么投屏到电视或者投影仪或者大屏幕？</h2>
<ul>
<li>链接：https://www.zhihu.com/tardis/bd/ans/3157494202</li>
<li>来源：bing</li>
<li>摘要：2023年8月9日 · 一、Windows 电脑投屏方法 微软自Windows 8.1开始就在系统内置 Miracast 的投屏功能（官方功能称“无线投影”）。这个协议可以提供给用户镜像复制电脑屏幕的功能；也能实现扩展延 …</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">Miracast投屏需要电脑支持无线WiFi功能，而Windows 8.1及更高版本系统已内置支持。然而，如果电脑主板没有集成无线网卡，可能无法实现Miracast投屏。在Windows 11系统中，投屏功能默认位于「系统」-「屏幕」中，而在Windows 10和Windows 8.1系统中，分别可以在屏幕右下角的「操作中心」和「设备」-「投影」中找到。相比之下，Windows 7系统则需要借助第三方软件才能实现投屏。对于macOS用户，Big Sur 11.0及以上版本中，屏幕镜像功能位于控制中心面板中，而Catalina 10.15及以下版本中，则在菜单栏中找到。此外，AWDL协议支持P2P Airplay投屏，无需连接同一网络，而无线HDMI投屏设备则可以解决多人频繁使用的问题，实现即插即用、无需安装软件、传输范围更大、更灵活和简单上手、无需繁琐配置的便捷体验。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>Miracast投屏需要电脑支持无线WiFi功能。</li>
<li>Windows 8.1及更高版本系统支持Miracast投屏。</li>
<li>电脑主板如果没有集成无线网卡，可能不支持Miracast。</li>
<li>Windows 11系统中，投屏功能默认在「系统」-「屏幕」中。</li>
<li>Windows 10系统中，投屏功能在屏幕右下角的「操作中心」中。</li>
<li>Windows 8.1系统中，投屏功能在「设备」-「投影」中。</li>
<li>Windows 7系统中，需要借助第三方软件实现投屏。</li>
<li>macOS Big Sur 11.0及以上版本中，屏幕镜像功能在控制中心面板中。</li>
<li>macOS Catalina 10.15及以下版本中，屏幕镜像功能在菜单栏中。</li>
<li>AWDL协议支持P2P Airplay投屏，无需连接同一网络。</li>
<li>无线HDMI投屏设备可以解决多人频繁使用的问题。</li>
<li>无线HDMI投屏设备即插即用，无需安装软件。</li>
<li>无线HDMI投屏设备传输范围更大，更灵活。</li>
<li>无线HDMI投屏设备简单上手，无需繁琐配置。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-61">正文（抓取，非 AI）</h3>
<p>一、Windows 电脑投屏方法 微软自Windows 8.1开始就在系统内置 Miracast 的投屏功能（官方功能称“无线投影”）。这个协议可以提供给用户镜像复制电脑屏幕的功能；也能实现扩展延伸，将主副屏连接起来共同作为一块大屏幕使用。 Miracast投屏前置条件： 操作系统： Windows 8.1及更高 版本系统； 显卡、显卡驱动（WDDM 1.3及以上）均支持Miracast； 无线网卡、无线网卡驱动 均支持Miracast。（电脑本身要有无线WiFi功能） 常见的问题就是以前很多台式电脑的主板都没有集成无线网卡 ，所以不能用 Miracast。 当然，如果电脑不支持 Miracast 也没关系，网上也有很多软件是在电脑安装一个软件，然后通过私有投屏协议实现相关效果，用网线连接也能投屏。 （请参考下方第4节 Windows 7 投屏的文章内容） <em>如何确认自己的电脑当前是否支持 Miracast 无线投屏？ 最简单的方法，就是点开系统的设置界面，找到「设备」-「蓝牙和其他设备」-「添加蓝牙或其他设备」，选择「无线显示扩展坞」。 如果能正常显示搜索设备的界面，表示当前设备支持Miracast投屏；反之如果不支持，就会有相关的错误提示。 </em>Windows系统通用快捷键 微软 Windows 系统已经提前为我们设计好快捷方式，这样我们也不需要自己找半天。 投屏快捷键：Win+K 屏幕模式快捷键：Win + P Win + K 的作用是调用系统自带的无线投屏功能 ，用于连接到可投屏的电视机、投影仪、电视盒子、投屏器。 Win + P 则是在成功连接投屏设备开始投屏后， 用于切换屏幕复制、扩展模式的快捷键。 1、Windows 11 如果不方便使用键盘，我们也能用最传统的鼠标点击进行操作。微软的 Windows 11 系统简化了主题界面和选项，可能有人一时找不到位置， 默认情况下我们需要到系统的设置菜单里面找到相关功能 。 在 「系统」 - 「屏幕」 的界面中，找到 「连接到无线显示器」 选项。 点击 「连接」 进入 「投放」 窗口界面。 选择支持 Miracast 无线投屏功能的电视机/投影仪/投屏器。 2、Windows 10 可能还有很多人的电脑依然是 Windows 10 系统，这边也给大家简单介绍下具体的图形界面。 点击屏幕右下角的 「操作中心」 选项。 点击 「连接」 选项。 选择支持Miracast无线投屏功能的电视机/投影仪/投屏器。 3、Windows 8.1 2023 年 1 月，微软也正式宣布停止维护 Windows 8.1 了，这算是介于 Win 7 到 Win 10 的过渡系统，甚至比 Win7 的装机率还要低。 将鼠标移动至屏幕右下角，打开 「超级按钮」 菜单，然后单击或点击 「设备」。 单击或点击 「投影」 ，然后单击或点击 「添加无线显示」 。 选择支持Miracast无线投屏功能的电视机/投影仪/投屏器。 4、Windows 7 Win 7 没办法使用 Miracast 投屏协议，我们需要借助第三方工具的私有投屏协议实现投屏功能。主要就是在电视端装一个软件，在电脑端也装一个软件，然后两端都使用同一家投屏协议，就可以实现投屏效果了。 这里有两种方法： （1）购买投屏器、电视盒子之类的外置设备，并利用厂商配套的投屏软件进行投屏。适合不能安装软件的非智能电视机、智能投影仪等显示终端使用。 （2）如果电视机或投影仪是近几年购置的产品，一般都是可以安装第三方软件的，甚至有些会内置投屏软件。这时候就不需要另外购买外置设备也能使用投屏功能。 无论选择哪种方式，只要根据厂商的教程进行操作，一般都能顺利投屏。 二、macOS 电脑投屏方法 苹果电脑出厂使用的是 macOS 操作系统，是通过自家的 Airplay 协议实现投屏功能。 这个协议最重要的特点就是，无论使用的是网线连接还是无线WiFi连接，投屏的电脑和被投屏的显示器必须处于同一个网络中。 对于苹果电脑用户来说，可能刚开始上手的时候还真的一时半会儿找不到投屏的图标和位置，特别是有些人一开始发现上方菜单栏里面找不到 「屏幕镜像」/「Screen Mirroring」 的图标。 1、macOS Big Sur 11.0及以上 苹果在 macOS Big Sur 11.0 版本之后，改变了选项设计， 将屏幕镜像功能内置到控制中心面板 ，像是手机端一样集成多个功能到一个窗口中。 确保mac电脑与被投屏显示屏处于同一个网络。 打开桌面上方的 「控制中心」 面板。 找到 「屏幕镜像」 选项 选择附近支持Airplay的电视机/投影仪/投屏器。 2、 macOS Catalina 10.15及以下 在过去的老版本 macOS 操作系统下， 请认准以下这个重要的图标符号！ 这就是以前版本中被称为“隔空播放”的图标。 确保 mac 电脑与被投屏显示屏处于同一个网络。 如果找不到 「隔空播放」 图标，可以在电脑的 「显示器」 界面下勾选显示图标选项。 点按菜单栏中的 「隔空播放」 状态图标。 选择附近支持 Airplay 的电视机/投影仪/投屏器。 *基于 P2P Airplay 的屏幕投屏方法 传统使用 Airplay 投屏的时候，mac 电脑需要提前连接到与投屏电视或投屏盒子相同的网络，才能正常使用。 但还有一种更便捷的情况，如果我们使用的设备支持基于 AWDL 的 P2P Airplay 投屏功能，我们可以在免联网的情况直接进行屏幕镜像。 支持 P2P AirPlay 的设备： 三、通过HDMI接口进行无线投屏 上面基于无线投屏协议的方法是比较简单快捷的，而且一般不需要使用额外的配件就能投屏。 但有时候存在以下情况： 企业会议室或学校教室，经常有不同的使用者和访客轮流使用无线投屏，每次投屏前都要花时间配置投屏功能。 家里有老人或小孩，不是很会操作电脑的无线投屏。 电脑是台式机，不方便移动位置但又想使用无线投屏。 以上情况我会更偏向选择使用无线HDMI类型的投屏设备。无线HDMI设备相当于将传统的HDMI高清线从实体有线传输的方式转变成无线传输，通过发射器+接收器的组合方式实现无线投屏。 有几个优点： 与HDMI有线传输一样，一样是即插即用，也不需要安装什么软件。 相对有线传输来说，范围更大更灵活，设备不管放在哪个位置都能投屏。 简单上手，无论是不是第一次用，不用繁琐配置网络都能开始投屏。 可以自己去搜索相关类型的产品，这边我只是给出适合企业用的参考型号和款式。 码字不易！如果觉得有帮助到你就帮忙双击一下屏幕！ 有更多投屏问题也可以直接告诉我！</p>
</div></details><h2 id="toc-62">32. Forum | OpenReview</h2>
<ul>
<li>链接：https://openreview.net/forum?id=JePfAI8fah</li>
<li>来源：bing</li>
<li>摘要：Promoting openness in scientific communication and the peer-review process</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">首先，iTransformer 的设计基于对 Transformer 组件功能的反思，无需修改基本组件，从而保持了模型的简洁性和高效性。其次，iTransformer 通过将注意力机制和前馈网络应用于逆向维度来捕捉多变量间的关联，这种设计使得模型能够更好地理解变量之间的复杂关系。此外，iTransformer 能够处理具有较大回看窗口的时间序列预测，且性能优于传统方法，这表明它在处理长序列数据时具有更强的能力。因此，iTransformer 能够学习变量中心化的表示，从而生成有意义的注意力图，进一步增强了模型的解释性和实用性。此外，iTransformer 在多种变元上具有更好的泛化能力，使其在不同场景下都能表现出色。因此，iTransformer 为时间序列预测提供了灵活的框架，能够利用任意大小的回看窗口，是时间序列预测的一个很好的基础模型替代品。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>iTransformer 的设计基于对 Transformer 组件功能的反思，无需修改基本组件。</li>
<li>iTransformer 通过将注意力机制和前馈网络应用于逆向维度来捕捉多变量间的关联。</li>
<li>iTransformer 能够处理具有较大回看窗口的时间序列预测，且性能优于传统方法。</li>
<li>iTransformer 能够学习变量中心化的表示，从而生成有意义的注意力图。</li>
<li>iTransformer 在多种变元上具有更好的泛化能力。</li>
<li>iTransformer 为时间序列预测提供了灵活的框架，能够利用任意大小的回看窗口。</li>
<li>iTransformer 是时间序列预测的一个很好的基础模型替代品。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-63">正文（抓取，非 AI）</h3>
<p>iTransformer: Inverted Transformers Are Effective for Time Series Forecasting | OpenReview Go to ICLR 2024 Conference homepage iTransformer: Inverted Transformers Are Effective for Time Series Forecasting Yong Liu , Tengge Hu , Haoran Zhang , Haixu Wu , Shiyu Wang , Lintao Ma , Mingsheng Long Published: 16 Jan 2024, Last Modified: 14 Mar 2024 ICLR 2024 spotlight Everyone Revisions BibTeX Code Of Ethics : I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics. Keywords : Time Series Forecasting, Transformer Submission Guidelines : I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2024/AuthorGuide. TL;DR : Based on the reflection on the duties of Transformer components, we propose inverted Transformer for time series forecasting, which achieves the SOTA in real-world applications and shows powerful strength on framework generalization. Abstract : The recent boom of linear forecasting models questions the ongoing passion for architectural modifications of Transformer-based forecasters. These forecasters leverage Transformers to model the global dependencies over temporal tokens of time series, with each token formed by multiple variates of the same timestamp. However, Transformers are challenged in forecasting series with larger lookback windows due to performance degradation and computation explosion. Besides, the embedding for each temporal token fuses multiple variates that represent potential delayed events and distinct physical measurements, which may fail in learning variate-centric representations and result in meaningless attention maps. In this work, we reflect on the competent duties of Transformer components and repurpose the Transformer architecture without any modification to the basic components. We propose iTransformer that simply applies the attention and feed-forward network on the inverted dimensions. Specifically, the time points of individual series are embedded into variate tokens which are utilized by the attention mechanism to capture multivariate correlations; meanwhile, the feed-forward network is applied for each variate token to learn nonlinear representations. The iTransformer model achieves state-of-the-art on challenging real-world datasets, which further empowers the Transformer family with promoted performance, generalization ability across different variates, and better utilization of arbitrary lookback windows, making it a nice alternative as the fundamental backbone of time series forecasting. Code is available at this repository: https://github.com/thuml/iTransformer. Anonymous Url : I certify that there is no URL (e.g., github page) that could be used to find authors' identity. No Acknowledgement Section : I certify that there is no acknowledgement section in this submission for double blind review. Primary Area : representation learning for computer vision, audio, language, and other modalities Submission Number : 632 Loading OpenReview is a long-term project to advance science through improved peer review with legal nonprofit status. We gratefully acknowledge the support of the OpenReview Sponsors . © 2026 OpenReview</p>
</div></details><h2 id="toc-64">33. EMAformer: Enhancing Transformer through Embedding Armor for Time Series Forecasting</h2>
<ul>
<li>链接：https://arxiv.org/abs/2511.08396v1</li>
<li>来源：arxiv</li>
<li>摘要：Multivariate time series forecasting is crucial across a wide range of domains. While presenting notable progress for the Transformer architecture, iTransformer still lags behind the latest MLP-based models. We attribute this performance gap to unstable inter-channel relationships. To bridge this gap, we propose EMAformer, a simple yet effective model that enhances the Transformer with an auxiliary embedding suite, akin to armor that reinforces its ability. By introducing three key inductive biases, i.e., \textit{global stability}, \textit{phase sensitivity}, and \textit{cross-axis specificity}, EMAformer unlocks the further potential of the Transformer architecture, achieving state-of-the-art performance on 12 real-world benchmarks and reducing forecasting errors by an average of 2.73\% in MSE and 5.15\% in MAE. This significantly advances the practical applicability of Transformer-based approaches for multivariate time series forecasting. The code is available on https://github.com/PlanckChang/EMAformer.</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">performance gap 通常归因于不稳定通道间关系，而 global stability 通过引入增强通道间关系的机制得以提升。phase sensitivity 有助于捕捉时间序列的相位变化，而 cross-axis specificity 则进一步提高了不同通道间特定关系的建模能力。为了实现这些改进，EMAformer 通过辅助嵌入套件显著提升了 Transformer 的性能，其 inductive biases 成为了成功的关键。因此，EMAformer 在 12 个实际基准上的表现堪称 state-of-the-art，MSE 和 MAE 错误分别平均降低了 2.73% 和 5.15%，相关代码已开源在 GitHub 上。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>performance gap 通常归因于不稳定通道间关系。</li>
<li>global stability 通过引入增强通道间关系的机制。</li>
<li>phase sensitivity 有助于捕捉时间序列的相位变化。</li>
<li>cross-axis specificity 提高了不同通道间特定关系的建模能力。</li>
<li>EMAformer 通过辅助嵌入套件显著提升了 Transformer 的性能。</li>
<li>state-of-the-art 表现在 12 个实际基准上。</li>
<li>MSE 和 MAE 错误分别平均降低了 2.73% 和 5.15%。</li>
<li>代码可在 GitHub 上获取。</li>
<li>inductive biases 是 EMAformer 成功的关键。</li>
<li>Transformer 架构的增强依赖于这些偏置。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-65">正文（抓取，非 AI）</h3>
<p>[2511.08396v1] EMAformer: Enhancing Transformer through Embedding Armor for Time Series Forecasting Computer Science &gt; Machine Learning arXiv:2511.08396v1 (cs) [Submitted on 11 Nov 2025] Title: EMAformer: Enhancing Transformer through Embedding Armor for Time Series Forecasting Authors: Zhiwei Zhang , Xinyi Du , Xuanchi Guo , Weihao Wang , Wenjuan Han View a PDF of the paper titled EMAformer: Enhancing Transformer through Embedding Armor for Time Series Forecasting, by Zhiwei Zhang and 4 other authors View PDF HTML (experimental) Abstract: Multivariate time series forecasting is crucial across a wide range of domains. While presenting notable progress for the Transformer architecture, iTransformer still lags behind the latest MLP-based models. We attribute this performance gap to unstable inter-channel relationships. To bridge this gap, we propose EMAformer, a simple yet effective model that enhances the Transformer with an auxiliary embedding suite, akin to armor that reinforces its ability. By introducing three key inductive biases, i.e., \textit{global stability}, \textit{phase sensitivity}, and \textit{cross-axis specificity}, EMAformer unlocks the further potential of the Transformer architecture, achieving state-of-the-art performance on 12 real-world benchmarks and reducing forecasting errors by an average of 2.73\% in MSE and 5.15\% in MAE. This significantly advances the practical applicability of Transformer-based approaches for multivariate time series forecasting. The code is available on this https URL . Comments: 14 pages, 9 figures, 6 tables, accepted by AAAI2026 Subjects: Machine Learning (cs.LG) Cite as: arXiv:2511.08396 [cs.LG] (or arXiv:2511.08396v1 [cs.LG] for this version) https://doi.org/10.48550/arXiv.2511.08396 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Zhiwei Zhang [ view email ] [v1] Tue, 11 Nov 2025 16:12:44 UTC (402 KB) Full-text links: Access Paper: View a PDF of the paper titled EMAformer: Enhancing Transformer through Embedding Armor for Time Series Forecasting, by Zhiwei Zhang and 4 other authors View PDF HTML (experimental) TeX Source view license Current browse context: cs.LG &lt; prev | next &gt; new | recent | 2025-11 Change to browse by: cs References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) IArxiv recommender toggle IArxiv Recommender ( What is IArxiv? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-66">34. Data Augmentation in Time Series Forecasting through Inverted Framework</h2>
<ul>
<li>链接：https://arxiv.org/abs/2507.11439v2</li>
<li>来源：arxiv</li>
<li>摘要：Currently, iTransformer is one of the most popular and effective models for multivariate time series (MTS) forecasting. Thanks to its inverted framework, iTransformer effectively captures multivariate correlation. However, the inverted framework still has some limitations. It diminishes temporal interdependency information, and introduces noise in cases of nonsignificant variable correlation. To address these limitations, we introduce a novel data augmentation method on inverted framework, called DAIF. Unlike previous data augmentation methods, DAIF stands out as the first real-time augmentation specifically designed for the inverted framework in MTS forecasting. We first define the structure of the inverted sequence-to-sequence framework, then propose two different DAIF strategies, Frequency Filtering and Cross-variation Patching to address the existing challenges of the inverted framework. Experiments across multiple datasets and inverted models have demonstrated the effectiveness of our DAIF.</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">首先，iTransformer的倒置框架设计虽然能够提高模型的效率，但会减弱时间依赖性信息，这使得模型在处理时间序列数据时面临挑战。其次，DAIF作为首个专门为倒置框架设计的实时数据增强方法，通过频率过滤和交叉变异补丁两种策略来解决这一问题。频率过滤策略通过调整数据的频率特性，增强时间依赖性信息；而交叉变异补丁则通过生成多样化的补丁，进一步提升模型对时间序列数据的处理能力。因此，DAIF的有效性已在多个数据集和倒置模型上得到验证，证明了其在增强倒置框架下的模型性能方面的显著效果。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>iTransformer的倒置框架会减弱时间依赖性信息。</li>
<li>DAIF是首个专门为倒置框架设计的实时数据增强方法。</li>
<li>DAIF通过频率过滤和交叉变异补丁两种策略来解决倒置框架的挑战。</li>
<li>DAIF的有效性已在多个数据集和倒置模型上得到验证。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-67">正文（抓取，非 AI）</h3>
<p>[2507.11439v2] Data Augmentation in Time Series Forecasting through Inverted Framework Computer Science &gt; Machine Learning arXiv:2507.11439v2 (cs) [Submitted on 15 Jul 2025 ( v1 ), last revised 16 Jul 2025 (this version, v2)] Title: Data Augmentation in Time Series Forecasting through Inverted Framework Authors: Hongming Tan , Ting Chen , Ruochong Jin , Wai Kin Chan View a PDF of the paper titled Data Augmentation in Time Series Forecasting through Inverted Framework, by Hongming Tan and 3 other authors View PDF HTML (experimental) Abstract: Currently, iTransformer is one of the most popular and effective models for multivariate time series (MTS) forecasting. Thanks to its inverted framework, iTransformer effectively captures multivariate correlation. However, the inverted framework still has some limitations. It diminishes temporal interdependency information, and introduces noise in cases of nonsignificant variable correlation. To address these limitations, we introduce a novel data augmentation method on inverted framework, called DAIF. Unlike previous data augmentation methods, DAIF stands out as the first real-time augmentation specifically designed for the inverted framework in MTS forecasting. We first define the structure of the inverted sequence-to-sequence framework, then propose two different DAIF strategies, Frequency Filtering and Cross-variation Patching to address the existing challenges of the inverted framework. Experiments across multiple datasets and inverted models have demonstrated the effectiveness of our DAIF. Comments: The paper is under consideration at Pattern Recognition Letters Subjects: Machine Learning (cs.LG) Cite as: arXiv:2507.11439 [cs.LG] (or arXiv:2507.11439v2 [cs.LG] for this version) https://doi.org/10.48550/arXiv.2507.11439 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Hongming Tan [ view email ] [v1] Tue, 15 Jul 2025 16:01:58 UTC (502 KB) [v2] Wed, 16 Jul 2025 11:40:43 UTC (502 KB) Full-text links: Access Paper: View a PDF of the paper titled Data Augmentation in Time Series Forecasting through Inverted Framework, by Hongming Tan and 3 other authors View PDF HTML (experimental) TeX Source view license Current browse context: cs.LG &lt; prev | next &gt; new | recent | 2025-07 Change to browse by: cs References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) IArxiv recommender toggle IArxiv Recommender ( What is IArxiv? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-68">35. An Optimization Method for Autoregressive Time Series Forecasting</h2>
<ul>
<li>链接：https://arxiv.org/abs/2602.02288v1</li>
<li>来源：arxiv</li>
<li>摘要：Current time-series forecasting models are primarily based on transformer-style neural networks. These models achieve long-term forecasting mainly by scaling up the model size rather than through genuinely autoregressive (AR) rollout. From the perspective of large language model training, the traditional training process for time-series forecasting models ignores temporal causality. In this paper, we propose a novel training method for time-series forecasting that enforces two key properties: (1) AR prediction errors should increase with the forecasting horizon. Any violation of this principle is considered random guessing and is explicitly penalized in the loss function, and (2) the method enables models to concatenate short-term AR predictions for forming flexible long-term forecasts. Empirical results demonstrate that our method establishes a new state-of-the-art across multiple benchmarks, achieving an MSE reduction of more than 10% compared to iTransformer and other recent strong baselines. Furthermore, it enables short-horizon forecasting models to perform reliable long-term predictions at horizons over 7.5 times longer. Code is available at https://github.com/LizhengMathAi/A</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">AR预测误差应随预测horizon增加，否则视为随机猜测并惩罚，这表明AR模型的预测能力随时间的推移而减弱。为了解决这一问题，该方法通过惩罚违反原则的模型来强制执行AR属性，确保预测误差随horizon增加。相比之下，传统时间序列模型训练忽略了时间因果性，导致短期预测模型难以进行可靠的长期预测。因此，该方法旨在通过强制执行AR属性，使短期预测模型能够进行可靠长期预测，从而提高预测的准确性。代码可在指定的GitHub仓库中找到，为实现这一目标提供了技术支持。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>AR预测误差应随预测 horizon 增加，否则视为随机猜测并惩罚。</li>
<li>该方法通过惩罚违反原则的模型来强制执行 AR 属性。</li>
<li>传统时间序列模型训练忽略了时间因果性。</li>
<li>该方法使短期预测模型能够进行可靠长期预测。</li>
<li>代码可在指定的 GitHub 仓库中找到。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-69">正文（抓取，非 AI）</h3>
<p>[2602.02288v1] An Optimization Method for Autoregressive Time Series Forecasting Computer Science &gt; Machine Learning arXiv:2602.02288v1 (cs) [Submitted on 2 Feb 2026] Title: An Optimization Method for Autoregressive Time Series Forecasting Authors: Zheng Li , Jerry Cheng , Huanying Gu View a PDF of the paper titled An Optimization Method for Autoregressive Time Series Forecasting, by Zheng Li and 2 other authors View PDF HTML (experimental) Abstract: Current time-series forecasting models are primarily based on transformer-style neural networks. These models achieve long-term forecasting mainly by scaling up the model size rather than through genuinely autoregressive (AR) rollout. From the perspective of large language model training, the traditional training process for time-series forecasting models ignores temporal causality. In this paper, we propose a novel training method for time-series forecasting that enforces two key properties: (1) AR prediction errors should increase with the forecasting horizon. Any violation of this principle is considered random guessing and is explicitly penalized in the loss function, and (2) the method enables models to concatenate short-term AR predictions for forming flexible long-term forecasts. Empirical results demonstrate that our method establishes a new state-of-the-art across multiple benchmarks, achieving an MSE reduction of more than 10% compared to iTransformer and other recent strong baselines. Furthermore, it enables short-horizon forecasting models to perform reliable long-term predictions at horizons over 7.5 times longer. Code is available at this https URL Comments: 10 pages, 2 figures, 2 tables Subjects: Machine Learning (cs.LG) ; Artificial Intelligence (cs.AI) Cite as: arXiv:2602.02288 [cs.LG] (or arXiv:2602.02288v1 [cs.LG] for this version) https://doi.org/10.48550/arXiv.2602.02288 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Zheng Li [ view email ] [v1] Mon, 2 Feb 2026 16:28:00 UTC (242 KB) Full-text links: Access Paper: View a PDF of the paper titled An Optimization Method for Autoregressive Time Series Forecasting, by Zheng Li and 2 other authors View PDF HTML (experimental) TeX Source view license Current browse context: cs.LG &lt; prev | next &gt; new | recent | 2026-02 Change to browse by: cs cs.AI References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) IArxiv recommender toggle IArxiv Recommender ( What is IArxiv? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-70">36. Selective Learning for Deep Time Series Forecasting</h2>
<ul>
<li>链接：https://arxiv.org/abs/2510.25207v1</li>
<li>来源：arxiv</li>
<li>摘要：Benefiting from high capacity for capturing complex temporal patterns, deep learning (DL) has significantly advanced time series forecasting (TSF). However, deep models tend to suffer from severe overfitting due to the inherent vulnerability of time series to noise and anomalies. The prevailing DL paradigm uniformly optimizes all timesteps through the MSE loss and learns those uncertain and anomalous timesteps without difference, ultimately resulting in overfitting. To address this, we propose a novel selective learning strategy for deep TSF. Specifically, selective learning screens a subset of the whole timesteps to calculate the MSE loss in optimization, guiding the model to focus on generalizable timesteps while disregarding non-generalizable ones. Our framework introduces a dual-mask mechanism to target timesteps: (1) an uncertainty mask leveraging residual entropy to filter uncertain timesteps, and (2) an anomaly mask employing residual lower bound estimation to exclude anomalous timesteps. Extensive experiments across eight real-world datasets demonstrate that selective learning can significantly improve the predictive performance for typical state-of-the-art deep models, inc</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">时间序列预测容易受到噪声和异常的影响，导致过拟合。当前的深度学习（DL）方法倾向于学习所有时间步长，包括不确定和异常的时间步长，这可能导致模型性能下降。为了解决这一问题，引入了选择性学习策略，该策略通过双重掩码机制优化时间步长。首先，不确定性掩码利用残差熵来过滤不确定的时间步长，其次，异常掩码使用残差下界估计来排除异常的时间步长。因此，选择性学习可以显著提高预测性能，减少MSE误差，具体而言，可以减少37.4%的MSE误差，减少8.4%的MSE误差，以及减少6.5%的MSE误差。此外，该方法在八个真实世界数据集上进行了广泛实验，证明了其在优化时间步长方面的有效性，从而指导模型关注可泛化的时间步长，忽略不可泛化的非一般性时间步长，显著改善了最先进的深度模型的预测性能。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>MSE损失会导致模型学习所有时间步长，包括不确定和异常的时间步长。</li>
<li>不确定性掩码利用残差熵来过滤不确定的时间步长。</li>
<li>异常掩码使用残差下界估计来排除异常的时间步长。</li>
<li>选择性学习策略可以显著提高预测性能。</li>
<li>该方法在八个真实世界数据集上进行了广泛实验。</li>
<li>选择性学习可以减少37.4%的MSE误差。</li>
<li>选择性学习可以减少8.4%的MSE误差。</li>
<li>选择性学习可以减少6.5%的MSE误差。</li>
<li>选择性学习指导模型关注可泛化的时间步长。</li>
<li>选择性学习帮助模型忽略不可泛化的非一般性时间步长。</li>
<li>时间序列容易受到噪声和异常的影响，导致过拟合。</li>
<li>当前的DL方法没有区分地学习所有时间步长。</li>
<li>选择性学习通过引入双重掩码机制来优化时间步长。</li>
<li>选择性学习可以显著改善最先进的深度模型的预测性能。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-71">正文（抓取，非 AI）</h3>
<p>[2510.25207v1] Selective Learning for Deep Time Series Forecasting Computer Science &gt; Machine Learning arXiv:2510.25207v1 (cs) [Submitted on 29 Oct 2025] Title: Selective Learning for Deep Time Series Forecasting Authors: Yisong Fu , Zezhi Shao , Chengqing Yu , Yujie Li , Zhulin An , Qi Wang , Yongjun Xu , Fei Wang View a PDF of the paper titled Selective Learning for Deep Time Series Forecasting, by Yisong Fu and 7 other authors View PDF Abstract: Benefiting from high capacity for capturing complex temporal patterns, deep learning (DL) has significantly advanced time series forecasting (TSF). However, deep models tend to suffer from severe overfitting due to the inherent vulnerability of time series to noise and anomalies. The prevailing DL paradigm uniformly optimizes all timesteps through the MSE loss and learns those uncertain and anomalous timesteps without difference, ultimately resulting in overfitting. To address this, we propose a novel selective learning strategy for deep TSF. Specifically, selective learning screens a subset of the whole timesteps to calculate the MSE loss in optimization, guiding the model to focus on generalizable timesteps while disregarding non-generalizable ones. Our framework introduces a dual-mask mechanism to target timesteps: (1) an uncertainty mask leveraging residual entropy to filter uncertain timesteps, and (2) an anomaly mask employing residual lower bound estimation to exclude anomalous timesteps. Extensive experiments across eight real-world datasets demonstrate that selective learning can significantly improve the predictive performance for typical state-of-the-art deep models, including 37.4% MSE reduction for Informer, 8.4% for TimesNet, and 6.5% for iTransformer. Comments: Accepted by NeurIPS 2025 Subjects: Machine Learning (cs.LG) Cite as: arXiv:2510.25207 [cs.LG] (or arXiv:2510.25207v1 [cs.LG] for this version) https://doi.org/10.48550/arXiv.2510.25207 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Yisong Fu [ view email ] [v1] Wed, 29 Oct 2025 06:18:52 UTC (2,743 KB) Full-text links: Access Paper: View a PDF of the paper titled Selective Learning for Deep Time Series Forecasting, by Yisong Fu and 7 other authors View PDF TeX Source view license Current browse context: cs.LG &lt; prev | next &gt; new | recent | 2025-10 Change to browse by: cs References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) IArxiv recommender toggle IArxiv Recommender ( What is IArxiv? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-72">37. TwinFormer: A Dual-Level Transformer for Long-Sequence Time-Series Forecasting</h2>
<ul>
<li>链接：https://arxiv.org/abs/2512.12301v1</li>
<li>来源：arxiv</li>
<li>摘要：TwinFormer is a hierarchical Transformer for long-sequence time-series forecasting. It divides the input into non-overlapping temporal patches and processes them in two stages: (1) a Local Informer with top-$k$ Sparse Attention models intra-patch dynamics, followed by mean pooling; (2) a Global Informer captures long-range inter-patch dependencies using the same top-$k$ attention. A lightweight GRU aggregates the globally contextualized patch tokens for direct multi-horizon prediction. The resulting architecture achieves linear $O(kLd)$ time and memory complexity. On eight real-world benchmarking datasets from six different domains, including weather, stock price, temperature, power consumption, electricity, and disease, and forecasting horizons $96-720$, TwinFormer secures $27$ positions in the top two out of $34$. Out of the $27$, it achieves the best performance on MAE and RMSE at $17$ places and $10$ at the second-best place on MAE and RMSE. This consistently outperforms PatchTST, iTransformer, FEDformer, Informer, and vanilla Transformers. Ablations confirm the superiority of top-$k$ Sparse Attention over ProbSparse and the effectiveness of GRU-based aggregation. Code is avail</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-73">正文（抓取，非 AI）</h3>
<p>[2512.12301v1] TwinFormer: A Dual-Level Transformer for Long-Sequence Time-Series Forecasting Computer Science &gt; Machine Learning arXiv:2512.12301v1 (cs) [Submitted on 13 Dec 2025] Title: TwinFormer: A Dual-Level Transformer for Long-Sequence Time-Series Forecasting Authors: Mahima Kumavat , Aditya Maheshwari View a PDF of the paper titled TwinFormer: A Dual-Level Transformer for Long-Sequence Time-Series Forecasting, by Mahima Kumavat and Aditya Maheshwari View PDF HTML (experimental) Abstract: TwinFormer is a hierarchical Transformer for long-sequence time-series forecasting. It divides the input into non-overlapping temporal patches and processes them in two stages: (1) a Local Informer with top-$k$ Sparse Attention models intra-patch dynamics, followed by mean pooling; (2) a Global Informer captures long-range inter-patch dependencies using the same top-$k$ attention. A lightweight GRU aggregates the globally contextualized patch tokens for direct multi-horizon prediction. The resulting architecture achieves linear $O(kLd)$ time and memory complexity. On eight real-world benchmarking datasets from six different domains, including weather, stock price, temperature, power consumption, electricity, and disease, and forecasting horizons $96-720$, TwinFormer secures $27$ positions in the top two out of $34$. Out of the $27$, it achieves the best performance on MAE and RMSE at $17$ places and $10$ at the second-best place on MAE and RMSE. This consistently outperforms PatchTST, iTransformer, FEDformer, Informer, and vanilla Transformers. Ablations confirm the superiority of top-$k$ Sparse Attention over ProbSparse and the effectiveness of GRU-based aggregation. Code is available at this repository: this https URL . Comments: 14 pages, 4 figures Subjects: Machine Learning (cs.LG) Cite as: arXiv:2512.12301 [cs.LG] (or arXiv:2512.12301v1 [cs.LG] for this version) https://doi.org/10.48550/arXiv.2512.12301 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Aditya Maheshwari [ view email ] [v1] Sat, 13 Dec 2025 11:50:18 UTC (964 KB) Full-text links: Access Paper: View a PDF of the paper titled TwinFormer: A Dual-Level Transformer for Long-Sequence Time-Series Forecasting, by Mahima Kumavat and Aditya Maheshwari View PDF HTML (experimental) TeX Source view license Current browse context: cs.LG &lt; prev | next &gt; new | recent | 2025-12 Change to browse by: cs References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) IArxiv recommender toggle IArxiv Recommender ( What is IArxiv? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-74">38. Quantum Neural Network Architectures for Multivariate Time-Series Forecasting</h2>
<ul>
<li>链接：https://arxiv.org/abs/2510.21168v1</li>
<li>来源：arxiv</li>
<li>摘要：In this paper, we address the challenge of multivariate time-series forecasting using quantum machine learning techniques. We introduce adaptation strategies that extend variational quantum circuit models, traditionally limited to univariate data, toward the multivariate setting, exploring both purely quantum and hybrid quantum-classical formulations. First, we extend and benchmark several VQC-based and hybrid architectures to systematically evaluate their capacity to model cross-variable dependencies. Second, building upon these foundations, we introduce the iQTransformer, a novel quantum transformer architecture that integrates a quantum self-attention mechanism within the iTransformer framework, enabling a quantum-native representation of inter-variable relationships. Third, we provide a comprehensive empirical evaluation on both synthetic and real-world datasets, showing that quantum-based models may achieve competitive or superior forecasting accuracy with fewer trainable parameters and faster convergence than state-of-the-art classical and quantum baselines in some cases. These contributions highlight the potential of quantum-enhanced architectures as efficient and scalable t</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">量子模型传统上仅限于单变量数据，这使得跨变量依赖关系的系统评估变得困难。为了解决这一问题，iQTransformer引入了量子自注意力机制，从而减少了可训练参数的数量，提高了模型的准确性。此外，一些研究表明，这种量子增强架构不仅更高效，而且更具可扩展性。在探索混合量子-经典形式的同时，实证评估涵盖了合成和真实世界数据集，结果显示量子原生表示能够更好地捕捉变量间的相互关系。因此，与最先进的经典和量子基准相比，iQTransformer展示了具有竞争力或优越的预测准确性。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>quantum models traditionally limited to univariate data</li>
<li>cross-variable dependencies require systematic evaluation</li>
<li>iQTransformer integrates quantum self-attention mechanism</li>
<li>fewer trainable parameters can lead to superior accuracy</li>
<li>faster convergence is observed in some cases</li>
<li>quantum-enhanced architectures are efficient and scalable</li>
<li>hybrid quantum-classical formulations are explored</li>
<li>empirical evaluation includes both synthetic and real-world datasets</li>
<li>quantum-native representation enables better inter-variable relationships</li>
<li>competitive or superior forecasting accuracy is shown</li>
<li>state-of-the-art classical and quantum baselines are considered</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-75">正文（抓取，非 AI）</h3>
<p>[2510.21168v1] Quantum Neural Network Architectures for Multivariate Time-Series Forecasting Quantum Physics arXiv:2510.21168v1 (quant-ph) [Submitted on 24 Oct 2025] Title: Quantum Neural Network Architectures for Multivariate Time-Series Forecasting Authors: Sandra Ranilla-Cortina , Diego A. Aranda , Jorge Ballesteros , Jesus Bonilla , Nerea Monrio , Elías F. Combarro , Jose Ranilla View a PDF of the paper titled Quantum Neural Network Architectures for Multivariate Time-Series Forecasting, by Sandra Ranilla-Cortina and 6 other authors View PDF HTML (experimental) Abstract: In this paper, we address the challenge of multivariate time-series forecasting using quantum machine learning techniques. We introduce adaptation strategies that extend variational quantum circuit models, traditionally limited to univariate data, toward the multivariate setting, exploring both purely quantum and hybrid quantum-classical formulations. First, we extend and benchmark several VQC-based and hybrid architectures to systematically evaluate their capacity to model cross-variable dependencies. Second, building upon these foundations, we introduce the iQTransformer, a novel quantum transformer architecture that integrates a quantum self-attention mechanism within the iTransformer framework, enabling a quantum-native representation of inter-variable relationships. Third, we provide a comprehensive empirical evaluation on both synthetic and real-world datasets, showing that quantum-based models may achieve competitive or superior forecasting accuracy with fewer trainable parameters and faster convergence than state-of-the-art classical and quantum baselines in some cases. These contributions highlight the potential of quantum-enhanced architectures as efficient and scalable tools for advancing multivariate time-series forecasting. Subjects: Quantum Physics (quant-ph) Cite as: arXiv:2510.21168 [quant-ph] (or arXiv:2510.21168v1 [quant-ph] for this version) https://doi.org/10.48550/arXiv.2510.21168 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Sandra Ranilla-Cortina [ view email ] [v1] Fri, 24 Oct 2025 05:44:41 UTC (708 KB) Full-text links: Access Paper: View a PDF of the paper titled Quantum Neural Network Architectures for Multivariate Time-Series Forecasting, by Sandra Ranilla-Cortina and 6 other authors View PDF HTML (experimental) TeX Source view license Current browse context: quant-ph &lt; prev | next &gt; new | recent | 2025-10 References &amp; Citations INSPIRE HEP NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-76">39. Efficient Deep Learning for Short-Term Solar Irradiance Time Series Forecasting: A Benchmark Study in Ho Chi Minh City</h2>
<ul>
<li>链接：https://arxiv.org/abs/2512.23898v1</li>
<li>来源：arxiv</li>
<li>摘要：Reliable forecasting of Global Horizontal Irradiance (GHI) is essential for mitigating the variability of solar energy in power grids. This study presents a comprehensive benchmark of ten deep learning architectures for short-term (1-hour ahead) GHI time series forecasting in Ho Chi Minh City, leveraging high-resolution NSRDB satellite data (2011-2020) to compare established baselines (e.g. LSTM, TCN) against emerging state-of-the-art architectures, including Transformer, Informer, iTransformer, TSMixer, and Mamba. Experimental results identify the Transformer as the superior architecture, achieving the highest predictive accuracy with an R^2 of 0.9696. The study further utilizes SHAP analysis to contrast the temporal reasoning of these architectures, revealing that Transformers exhibit a strong "recency bias" focused on immediate atmospheric conditions, whereas Mamba explicitly leverages 24-hour periodic dependencies to inform predictions. Furthermore, we demonstrate that Knowledge Distillation can compress the high-performance Transformer by 23.5% while surprisingly reducing error (MAE: 23.78 W/m^2), offering a proven pathway for deploying sophisticated, low-latency forecasting o</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">首先，传统架构如 LSTM 和 TCN 在对比中表现不如新兴的 Transformer 架构。实验结果表明，Transformer 在短期全球水平辐射（GHI）预测中表现出色，R² 值高达 0.9696。其次，SHAP 分析揭示了不同架构的时序推理差异，其中 Transformer 侧重于近期气象条件，这可能是其在预测中表现优异的原因之一。此外，Mamba 架构通过利用 24 小时周期性依赖关系来进一步改进预测，增强了模型的准确性。因此，知识蒸馏可以压缩高性能的 Transformer 模型，同时减少误差，使得这些模型更适合资源受限的边缘设备。最终，经过压缩的复杂低延迟预测模型可以在这些设备上部署，从而提高预测效率和准确性。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>LSTM 和 TCN 等传统架构在对比中表现不如新兴的 Transformer 架构。</li>
<li>实验结果表明，Transformer 在短期 GHI 预测中表现最佳，R² 达到 0.9696。</li>
<li>SHAP 分析揭示了不同架构的时序推理差异，Transformer 侧重于近期气象条件。</li>
<li>Mamba 架构通过利用 24 小时周期性依赖关系来改进预测。</li>
<li>知识蒸馏可以压缩高性能的 Transformer 模型，同时减少误差。</li>
<li>资源受限的边缘设备上可以部署经过压缩的复杂低延迟预测模型。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-77">正文（抓取，非 AI）</h3>
<p>[2512.23898v1] Efficient Deep Learning for Short-Term Solar Irradiance Time Series Forecasting: A Benchmark Study in Ho Chi Minh City Computer Science &gt; Machine Learning arXiv:2512.23898v1 (cs) [Submitted on 29 Dec 2025] Title: Efficient Deep Learning for Short-Term Solar Irradiance Time Series Forecasting: A Benchmark Study in Ho Chi Minh City Authors: Tin Hoang View a PDF of the paper titled Efficient Deep Learning for Short-Term Solar Irradiance Time Series Forecasting: A Benchmark Study in Ho Chi Minh City, by Tin Hoang View PDF HTML (experimental) Abstract: Reliable forecasting of Global Horizontal Irradiance (GHI) is essential for mitigating the variability of solar energy in power grids. This study presents a comprehensive benchmark of ten deep learning architectures for short-term (1-hour ahead) GHI time series forecasting in Ho Chi Minh City, leveraging high-resolution NSRDB satellite data (2011-2020) to compare established baselines (e.g. LSTM, TCN) against emerging state-of-the-art architectures, including Transformer, Informer, iTransformer, TSMixer, and Mamba. Experimental results identify the Transformer as the superior architecture, achieving the highest predictive accuracy with an R^2 of 0.9696. The study further utilizes SHAP analysis to contrast the temporal reasoning of these architectures, revealing that Transformers exhibit a strong "recency bias" focused on immediate atmospheric conditions, whereas Mamba explicitly leverages 24-hour periodic dependencies to inform predictions. Furthermore, we demonstrate that Knowledge Distillation can compress the high-performance Transformer by 23.5% while surprisingly reducing error (MAE: 23.78 W/m^2), offering a proven pathway for deploying sophisticated, low-latency forecasting on resource-constrained edge devices. Comments: preprint, 40 pages Subjects: Machine Learning (cs.LG) ; Artificial Intelligence (cs.AI) Cite as: arXiv:2512.23898 [cs.LG] (or arXiv:2512.23898v1 [cs.LG] for this version) https://doi.org/10.48550/arXiv.2512.23898 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Tin Hoang [ view email ] [v1] Mon, 29 Dec 2025 23:22:25 UTC (3,287 KB) Full-text links: Access Paper: View a PDF of the paper titled Efficient Deep Learning for Short-Term Solar Irradiance Time Series Forecasting: A Benchmark Study in Ho Chi Minh City, by Tin Hoang View PDF HTML (experimental) TeX Source view license Current browse context: cs.LG &lt; prev | next &gt; new | recent | 2025-12 Change to browse by: cs cs.AI References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) IArxiv recommender toggle IArxiv Recommender ( What is IArxiv? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-78">40. Lossless Compression: A New Benchmark for Time Series Model Evaluation</h2>
<ul>
<li>链接：https://arxiv.org/abs/2509.21002v1</li>
<li>来源：arxiv</li>
<li>摘要：The evaluation of time series models has traditionally focused on four canonical tasks: forecasting, imputation, anomaly detection, and classification. While these tasks have driven significant progress, they primarily assess task-specific performance and do not rigorously measure whether a model captures the full generative distribution of the data. We introduce lossless compression as a new paradigm for evaluating time series models, grounded in Shannon's source coding theorem. This perspective establishes a direct equivalence between optimal compression length and the negative log-likelihood, providing a strict and unified information-theoretic criterion for modeling capacity. Then We define a standardized evaluation protocol and metrics. We further propose and open-source a comprehensive evaluation framework TSCom-Bench, which enables the rapid adaptation of time series models as backbones for lossless compression. Experiments across diverse datasets on state-of-the-art models, including TimeXer, iTransformer, and PatchTST, demonstrate that compression reveals distributional weaknesses overlooked by classic benchmarks. These findings position lossless compression as a principle</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">首先，lossless compression 作为一项原则性任务，补充并扩展现有时间序列模型评估，确保模型能够完全捕捉数据生成分布。其次，optimal compression length 直接等同于负对数似然，而负对数似然提供了一种严格统一的信息论标准。此外，TSCom-Bench 评估框架使时间序列模型能够快速适应无损压缩，从而提高模型的适应性和准确性。然而，经典基准可能忽略分布中的弱点，这表明现有的评估方法可能未能全面捕捉模型的性能。因此，引入lossless compression 作为评估标准，可以更全面地评估时间序列模型的性能，确保模型不仅在整体上表现良好，还能在细节上准确无误。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>lossless compression 评估模型是否完全捕捉数据生成分布。</li>
<li>optimal compression length 直接等同于负对数似然。</li>
<li>negative log-likelihood 提供严格统一的信息论标准。</li>
<li>TSCom-Bench 评估框架使时间序列模型快速适应无损压缩。</li>
<li>经典基准可能忽略分布中的弱点。</li>
<li>lossless compression 作为原则性任务补充并扩展现有时间序列模型评估。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-79">正文（抓取，非 AI）</h3>
<p>[2509.21002v1] Lossless Compression: A New Benchmark for Time Series Model Evaluation Computer Science &gt; Machine Learning arXiv:2509.21002v1 (cs) [Submitted on 25 Sep 2025] Title: Lossless Compression: A New Benchmark for Time Series Model Evaluation Authors: Meng Wan , Benxi Tian , Jue Wang , Cui Hui , Ningming Nie , Tiantian Liu , Zongguo Wang , Cao Rongqiang , Peng Shi , Yangang Wang View a PDF of the paper titled Lossless Compression: A New Benchmark for Time Series Model Evaluation, by Meng Wan and 9 other authors View PDF HTML (experimental) Abstract: The evaluation of time series models has traditionally focused on four canonical tasks: forecasting, imputation, anomaly detection, and classification. While these tasks have driven significant progress, they primarily assess task-specific performance and do not rigorously measure whether a model captures the full generative distribution of the data. We introduce lossless compression as a new paradigm for evaluating time series models, grounded in Shannon's source coding theorem. This perspective establishes a direct equivalence between optimal compression length and the negative log-likelihood, providing a strict and unified information-theoretic criterion for modeling capacity. Then We define a standardized evaluation protocol and metrics. We further propose and open-source a comprehensive evaluation framework TSCom-Bench, which enables the rapid adaptation of time series models as backbones for lossless compression. Experiments across diverse datasets on state-of-the-art models, including TimeXer, iTransformer, and PatchTST, demonstrate that compression reveals distributional weaknesses overlooked by classic benchmarks. These findings position lossless compression as a principled task that complements and extends existing evaluation for time series modeling. Comments: 24 pages Subjects: Machine Learning (cs.LG) ; Artificial Intelligence (cs.AI) Cite as: arXiv:2509.21002 [cs.LG] (or arXiv:2509.21002v1 [cs.LG] for this version) https://doi.org/10.48550/arXiv.2509.21002 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Meng Wan [ view email ] [v1] Thu, 25 Sep 2025 10:52:48 UTC (3,041 KB) Full-text links: Access Paper: View a PDF of the paper titled Lossless Compression: A New Benchmark for Time Series Model Evaluation, by Meng Wan and 9 other authors View PDF HTML (experimental) TeX Source view license Current browse context: cs.LG &lt; prev | next &gt; new | recent | 2025-09 Change to browse by: cs cs.AI References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) IArxiv recommender toggle IArxiv Recommender ( What is IArxiv? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-80">41. Transformer-Based Modeling of User Interaction Sequences for Dwell Time Prediction in Human-Computer Interfaces</h2>
<ul>
<li>链接：https://arxiv.org/abs/2512.17149v1</li>
<li>来源：arxiv</li>
<li>摘要：This study investigates the task of dwell time prediction and proposes a Transformer framework based on interaction behavior modeling. The method first represents user interaction sequences on the interface by integrating dwell duration, click frequency, scrolling behavior, and contextual features, which are mapped into a unified latent space through embedding and positional encoding. On this basis, a multi-head self-attention mechanism is employed to capture long-range dependencies, while a feed-forward network performs deep nonlinear transformations to model the dynamic patterns of dwell time. Multiple comparative experiments are conducted with BILSTM, DRFormer, FedFormer, and iTransformer as baselines under the same conditions. The results show that the proposed method achieves the best performance in terms of MSE, RMSE, MAPE, and RMAE, and more accurately captures the complex patterns in interaction behavior. In addition, sensitivity experiments are carried out on hyperparameters and environments to examine the impact of the number of attention heads, sequence window length, and device environment on prediction performance, which further demonstrates the robustness and adaptabi</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">Transformer模型能够捕捉长距离依赖关系，这得益于其嵌入和位置编码机制，这些机制将多种特征映射到统一的潜在空间，使得模型能够处理复杂的交互模式。多头自注意力机制进一步增强了模型的复杂交互能力，使其在多个指标上表现出色。然而，超参数和环境变化可能会影响预测性能，因此需要进行敏感性实验来证明方法的鲁棒性和适应性。这些实验不仅验证了新方法在理论和方法论上的优势，还展示了其在实际应用中的稳定性和可靠性。因此，Transformer方法不仅提供了一种新的解决方案，还展示了其在处理长距离依赖关系和复杂交互模式方面的强大能力。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>Transformer模型能够捕捉长距离依赖关系。</li>
<li>嵌入和位置编码将多种特征映射到统一的潜在空间。</li>
<li>多头自注意力机制有助于模型复杂交互模式。</li>
<li>比较实验表明新方法在多个指标上表现最佳。</li>
<li>超参数和环境变化会影响预测性能。</li>
<li>Transformer方法在理论和方法论上提供了新解决方案。</li>
<li>敏感性实验进一步证明了方法的鲁棒性和适应性。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-81">正文（抓取，非 AI）</h3>
<p>[2512.17149v1] Transformer-Based Modeling of User Interaction Sequences for Dwell Time Prediction in Human-Computer Interfaces Computer Science &gt; Human-Computer Interaction arXiv:2512.17149v1 (cs) [Submitted on 19 Dec 2025] Title: Transformer-Based Modeling of User Interaction Sequences for Dwell Time Prediction in Human-Computer Interfaces Authors: Rui Liu , Runsheng Zhang , Shixiao Wang View a PDF of the paper titled Transformer-Based Modeling of User Interaction Sequences for Dwell Time Prediction in Human-Computer Interfaces, by Rui Liu and 2 other authors View PDF Abstract: This study investigates the task of dwell time prediction and proposes a Transformer framework based on interaction behavior modeling. The method first represents user interaction sequences on the interface by integrating dwell duration, click frequency, scrolling behavior, and contextual features, which are mapped into a unified latent space through embedding and positional encoding. On this basis, a multi-head self-attention mechanism is employed to capture long-range dependencies, while a feed-forward network performs deep nonlinear transformations to model the dynamic patterns of dwell time. Multiple comparative experiments are conducted with BILSTM, DRFormer, FedFormer, and iTransformer as baselines under the same conditions. The results show that the proposed method achieves the best performance in terms of MSE, RMSE, MAPE, and RMAE, and more accurately captures the complex patterns in interaction behavior. In addition, sensitivity experiments are carried out on hyperparameters and environments to examine the impact of the number of attention heads, sequence window length, and device environment on prediction performance, which further demonstrates the robustness and adaptability of the method. Overall, this study provides a new solution for dwell time prediction from both theoretical and methodological perspectives and verifies its effectiveness in multiple aspects. Subjects: Human-Computer Interaction (cs.HC) Cite as: arXiv:2512.17149 [cs.HC] (or arXiv:2512.17149v1 [cs.HC] for this version) https://doi.org/10.48550/arXiv.2512.17149 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Rui Liu [ view email ] [v1] Fri, 19 Dec 2025 00:55:14 UTC (362 KB) Full-text links: Access Paper: View a PDF of the paper titled Transformer-Based Modeling of User Interaction Sequences for Dwell Time Prediction in Human-Computer Interfaces, by Rui Liu and 2 other authors View PDF view license Current browse context: cs.HC &lt; prev | next &gt; new | recent | 2025-12 Change to browse by: cs References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-82">42. The Procrustean Bed of Time Series: The Optimization Bias of Point-wise Loss</h2>
<ul>
<li>链接：https://arxiv.org/abs/2512.18610v2</li>
<li>来源：arxiv</li>
<li>摘要：Optimizing time series models via point-wise loss functions (e.g., MSE) relying on a heuristic point-wise i.i.d. assumption disregards the causal temporal structure. Focusing on the core independence issue under covariance stationarity, this paper aims to provide a first-principles analysis of the Expectation of Optimization Bias (EOB). Our analysis reveals a fundamental paradigm paradox: The more deterministic and structured the time series, the more severe the bias incurred by point-wise loss function. We derive the first closed-form quantification for the non-deterministic EOB across linear and non-linear systems, and prove EOB is an intrinsic data property, governed exclusively by sequence length and the defined Structural Signal-to-Noise Ratio. This theoretical discovery motivates our principled debiasing program that eliminates the bias through sequence length reduction and structural orthogonalization. We present a concrete solution via DFT or DWT, and propose a novel harmonized $\ell_p$ norm framework to rectify gradient optimization pathologies of high-variance sequences. Extensive experiments validate EOB Theory's generality and the superior performance of debiasing progr</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">点-wise损失函数假设时间序列数据是独立同分布的，忽略了因果时间结构，这在协方差平稳性下是核心问题。这种假设导致了在时间序列越确定和结构化时偏见越严重的现象。点-wise损失函数的非确定性期望偏见首次在线性和非线性系统中得到了闭式量化，而期望偏见是一个由序列长度和结构信噪比决定的内在数据属性。因此，通过减少序列长度和结构正交化可以消除偏见，而DFT或DWT提供了一个具体的解决方案来减少偏见。此外，提出了一个新的协调的$\ell_p$范式框架来纠正高方差序列的梯度优化病态。实验验证了期望偏见理论的普遍性和去偏见程序的优越性能，去偏见程序在iTransformer上实现了5.2%和5.1%的平均MSE和MAE改进。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>point-wise loss函数假设时间序列数据是独立同分布的，忽略了因果时间结构。</li>
<li>点-wise损失函数假设下的独立性问题在协方差平稳性下是核心问题。</li>
<li>点-wise损失函数导致的偏见在时间序列越确定和结构化时越严重。</li>
<li>点-wise损失函数的非确定性期望偏见在线性和非线性系统中首次得到了闭式量化。</li>
<li>期望偏见是一个内在数据属性，由序列长度和结构信噪比决定。</li>
<li>通过减少序列长度和结构正交化可以消除偏见。</li>
<li>DFT或DWT提供了一个具体的解决方案来减少偏见。</li>
<li>提出了一个新的协调的$\ell_p$范式框架来纠正高方差序列的梯度优化病态。</li>
<li>实验验证了期望偏见理论的普遍性和去偏见程序的优越性能。</li>
<li>去偏见程序在iTransformer上实现了5.2%和5.1%的平均MSE和MAE改进。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-83">正文（抓取，非 AI）</h3>
<p>[2512.18610v2] The Procrustean Bed of Time Series: The Optimization Bias of Point-wise Loss Computer Science &gt; Machine Learning arXiv:2512.18610v2 (cs) [Submitted on 21 Dec 2025 ( v1 ), last revised 1 Feb 2026 (this version, v2)] Title: The Procrustean Bed of Time Series: The Optimization Bias of Point-wise Loss Authors: Rongyao Cai , Yuxi Wan , Kexin Zhang , Ming Jin , Hao Wang , Zhiqiang Ge , Daoyi Dong , Yong Liu , Qingsong Wen View a PDF of the paper titled The Procrustean Bed of Time Series: The Optimization Bias of Point-wise Loss, by Rongyao Cai and 8 other authors View PDF HTML (experimental) Abstract: Optimizing time series models via point-wise loss functions (e.g., MSE) relying on a heuristic point-wise i.i.d. assumption disregards the causal temporal structure. Focusing on the core independence issue under covariance stationarity, this paper aims to provide a first-principles analysis of the Expectation of Optimization Bias (EOB). Our analysis reveals a fundamental paradigm paradox: The more deterministic and structured the time series, the more severe the bias incurred by point-wise loss function. We derive the first closed-form quantification for the non-deterministic EOB across linear and non-linear systems, and prove EOB is an intrinsic data property, governed exclusively by sequence length and the defined Structural Signal-to-Noise Ratio. This theoretical discovery motivates our principled debiasing program that eliminates the bias through sequence length reduction and structural orthogonalization. We present a concrete solution via DFT or DWT, and propose a novel harmonized $\ell_p$ norm framework to rectify gradient optimization pathologies of high-variance sequences. Extensive experiments validate EOB Theory's generality and the superior performance of debiasing program, achieving 5.2% and 5.1% average improvement of MSE and MAE conducted on the iTransformer across 11 datasets, respectively. Comments: 54 pages Subjects: Machine Learning (cs.LG) Cite as: arXiv:2512.18610 [cs.LG] (or arXiv:2512.18610v2 [cs.LG] for this version) https://doi.org/10.48550/arXiv.2512.18610 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Rongyao Cai [ view email ] [v1] Sun, 21 Dec 2025 06:08:22 UTC (27,328 KB) [v2] Sun, 1 Feb 2026 14:34:29 UTC (34,944 KB) Full-text links: Access Paper: View a PDF of the paper titled The Procrustean Bed of Time Series: The Optimization Bias of Point-wise Loss, by Rongyao Cai and 8 other authors View PDF HTML (experimental) TeX Source view license Current browse context: cs.LG &lt; prev | next &gt; new | recent | 2025-12 Change to browse by: cs References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) IArxiv recommender toggle IArxiv Recommender ( What is IArxiv? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-84">43. InvDec: Inverted Decoder for Multivariate Time Series Forecasting with Separated Temporal and Variate Modeling</h2>
<ul>
<li>链接：https://arxiv.org/abs/2510.20302v1</li>
<li>来源：arxiv</li>
<li>摘要：Multivariate time series forecasting requires simultaneously modeling temporal patterns and cross-variate dependencies. Channel-independent methods such as PatchTST excel at temporal modeling but ignore variable correlations, while pure variate-attention approaches such as iTransformer sacrifice temporal encoding. We proposeInvDec (Inverted Decoder), a hybrid architecture that achieves principled separation between temporal encoding and variate-level decoding. InvDec combines a patch-based temporal encoder with an inverted decoder operating on the variate dimension through variate-wise self-attention. We introduce delayed variate embeddings that enrich variable-specific representations only after temporal encoding, preserving temporal feature integrity. An adaptive residual fusion mechanism dynamically balances temporal and variate information across datasets of varying dimensions. Instantiating InvDec with PatchTST yields InvDec-PatchTST. Extensive experiments on seven benchmarks demonstrate significant gains on high-dimensional datasets: 20.9% MSE reduction on Electricity (321 variables), 4.3% improvement on Weather, and 2.7% gain on Traffic compared to PatchTST, while maintainin</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">InvDec 能够同时处理时间模式和变量依赖性，通过分离时间编码和变量解码实现了原理上的区分，而 PatchTST 专注于时间编码但忽视了变量间的关联，iTransformer 则专注于变量编码但牺牲了时间编码。InvDec 通过在时间编码后丰富延迟变量嵌入，实现了对变量特定表示的优化，而自适应残差融合机制则在不同维度的数据集上动态平衡时间和变量信息。InvDec-PatchTST 组合体在高维数据集上表现出显著改进，而交叉变量建模在变量数量增加时变得至关重要。消融研究进一步验证了每个组件的有效性，特别是 InvDec 的优势随着数据集维度的增加而更加明显。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>InvDec 能够同时处理时间模式和变量依赖性。</li>
<li>PatchTST 专注于时间编码但忽视了变量间的关联。</li>
<li>iTransformer 专注于变量编码但牺牲了时间编码。</li>
<li>InvDec 通过分离时间编码和变量解码实现了原理上的区分。</li>
<li>延迟变量嵌入仅在时间编码后才丰富变量特定表示。</li>
<li>自适应残差融合机制在不同维度的数据集上动态平衡时间和变量信息。</li>
<li>InvDec-PatchTST 组合体在高维数据集上表现出显著改进。</li>
<li>交叉变量建模在变量数量增加时变得至关重要。</li>
<li>消融研究验证了每个组件的有效性。</li>
<li>InvDec 的优势随着数据集维度的增加而增加。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-85">正文（抓取，非 AI）</h3>
<p>[2510.20302v1] InvDec: Inverted Decoder for Multivariate Time Series Forecasting with Separated Temporal and Variate Modeling Computer Science &gt; Machine Learning arXiv:2510.20302v1 (cs) [Submitted on 23 Oct 2025] Title: InvDec: Inverted Decoder for Multivariate Time Series Forecasting with Separated Temporal and Variate Modeling Authors: Yuhang Wang View a PDF of the paper titled InvDec: Inverted Decoder for Multivariate Time Series Forecasting with Separated Temporal and Variate Modeling, by Yuhang Wang View PDF HTML (experimental) Abstract: Multivariate time series forecasting requires simultaneously modeling temporal patterns and cross-variate dependencies. Channel-independent methods such as PatchTST excel at temporal modeling but ignore variable correlations, while pure variate-attention approaches such as iTransformer sacrifice temporal encoding. We proposeInvDec (Inverted Decoder), a hybrid architecture that achieves principled separation between temporal encoding and variate-level decoding. InvDec combines a patch-based temporal encoder with an inverted decoder operating on the variate dimension through variate-wise self-attention. We introduce delayed variate embeddings that enrich variable-specific representations only after temporal encoding, preserving temporal feature integrity. An adaptive residual fusion mechanism dynamically balances temporal and variate information across datasets of varying dimensions. Instantiating InvDec with PatchTST yields InvDec-PatchTST. Extensive experiments on seven benchmarks demonstrate significant gains on high-dimensional datasets: 20.9% MSE reduction on Electricity (321 variables), 4.3% improvement on Weather, and 2.7% gain on Traffic compared to PatchTST, while maintaining competitive performance on low-dimensional ETT datasets. Ablation studies validate each component, and analysis reveals that InvDec's advantage grows with dataset dimensionality, confirming that cross-variate modeling becomes critical as the number of variables increases. Comments: 23pages, 3 figures Subjects: Machine Learning (cs.LG) Cite as: arXiv:2510.20302 [cs.LG] (or arXiv:2510.20302v1 [cs.LG] for this version) https://doi.org/10.48550/arXiv.2510.20302 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Yuhang Wang [ view email ] [v1] Thu, 23 Oct 2025 07:42:01 UTC (475 KB) Full-text links: Access Paper: View a PDF of the paper titled InvDec: Inverted Decoder for Multivariate Time Series Forecasting with Separated Temporal and Variate Modeling, by Yuhang Wang View PDF HTML (experimental) TeX Source view license Current browse context: cs.LG &lt; prev | next &gt; new | recent | 2025-10 Change to browse by: cs References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) IArxiv recommender toggle IArxiv Recommender ( What is IArxiv? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-86">44. 完全弄懂X射线光电子能谱（XPS）</h2>
<ul>
<li>链接：https://www.zhihu.com/tardis/bd/art/613989503</li>
<li>来源：bing</li>
<li>摘要：2025年11月12日 · XPS谱图一般包括 光电子谱线，卫星峰（伴峰），俄歇电子谱线，自旋-轨道分裂 (SOS) 等。 ① 光电子谱线：每一种元素都有自己特征的光电子线，它是元素定性分析的主要依据。 谱 …</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">XPS是一种表面分析手段，探测深度通常为3λ，主要用于探测元素的种类和所电离激发的原子轨道。光电子的能量仅与元素的种类和所电离激发的原子轨道有关，因此，氧化作用会使内层电子结合能上升，氧化中失电子愈多，上升幅度愈大；而还原作用会使内层电子结合能下降，还原中得电子愈多，下降幅度愈大。然而，XPS无法检测H和He，因为它们的光电离界面小，信号太弱。此外，荷电效应将使样品表面出现一稳定的电势Vs，对电子的逃离有一定束缚作用。C1s作为基准峰来校正谱中其他元素的结合能，而某种元素失去电子，其结合能会向高场方向偏移；某种元素得到电子，其结合能会向低场方向偏移。因此，XPS是一种半定量分析手段，因为光电子的强度还与光电子的平均自由程、样品的表面光洁度、元素所处的化学状态、X射线源强度以及仪器的状态有关。XPS主谱线是强度最大、峰宽最小、对称性最好的谱峰，而伴峰是由于常规X射线源并非是单色的，存在一些能量略高的小伴线。俄歇电子谱线总是伴随着XPS，但具有比XPS更宽更复杂的结构。此外，自旋-轨道分裂使l＞0的内壳层轨道能级发生分裂，p, d, f等双峰谱线的双峰间距及峰高比一般为一定值。鬼峰有时会出现在XPS谱图中，这可能是由于仪器的不完美或样品的不均匀性引起的。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>XPS是一种表面分析手段，探测深度通常为3λ。</li>
<li>光电子的能量仅与元素的种类和所电离激发的原子轨道有关。</li>
<li>氧化作用使内层电子结合能上升，氧化中失电子愈多,上升幅度愈大。</li>
<li>还原作用使内层电子结合能下降，还原中得电子愈多，下降幅度愈大。</li>
<li>XPS无法检测H, He是因为它们的光电离界面小，信号太弱。</li>
<li>荷电效应将使样品表面出现一稳定的电势Vs，对电子的逃离有一定束缚作用。</li>
<li>C1s作为基准峰来校正谱中其他元素的结合能。</li>
<li>某种元素失去电子，其结合能会向高场方向偏移。</li>
<li>某种元素得到电子，其结合能会向低场方向偏移。</li>
<li>XPS是一种半定量分析手段，因为光电子的强度还与光电子的平均自由程、样品的表面光洁度，元素所处的化学状态，X射线源强度以及仪器的状态有关。</li>
<li>XPS主谱线是强度最大、峰宽最小、对称性最好的谱峰。</li>
<li>伴峰是由于常规Ｘ射线源并非是单色的，存在一些能量略高的小伴线。</li>
<li>俄歇电子谱线总是伴随着XPS，但具有比XPS更宽更复杂的结构。</li>
<li>自旋-轨道分裂使l＞0的内壳层轨道能级发生分裂。</li>
<li>p, d, f等双峰谱线的双峰间距及峰高比一般为一定值。</li>
<li>鬼峰有时会出现在XPS谱图中。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-87">正文（抓取，非 AI）</h3>
<p>X射线光电子能谱（XPS） 收藏之余，点个赞吧！今天加个鸡腿！ 最新的专栏文章，精华文章都会整理好放到里面，目前特价，早订优惠中，欢迎大家订阅~ 必看！半导体制造技术入门到精通必备（更新ing） 半导体基础知识学习必备（精华必看版） 半导体制造技术知识学习宝典（精华必看版） - 知乎 材料及电化学知识学习宝典（精华必看版） 1、XPS简介 （1）XPS, 全称为X-ray Photoelectron Spectroscopy（X射线光电子能谱）,是一种收集和利用 X-射线光子辐照样品表面时所激发出的光电子和俄歇电子能量分布 的方法。 XPS可用于定性分析以及半定量分析, 一般从XPS图谱的 峰位和峰形获得样品表面元素成分、化学态和分子结构 等信息，从 峰强可获得样品表面元素含量或浓度（不常用） 。 （2）XPS是一种典型的 表面分析手段 。其根本原因在于：尽管X射线可穿透样品很深, 但 只有样品近表面一薄层发射出的光电子可逃逸出来 。 样品的探测深度（d）由电子的逃逸深度（λ受X射线波长和样品状态等因素影响）决定，通常，取样深度d = 3λ。对于金属而言λ为0.5-3 nm;无机非金属材料为2-4 nm; 有机物和高分子为4-10 nm。 2、XPS基本原理 XPS可以定性分析： 样品表面元素组成；样品表面元素的化学态和分子结构 。 （1）XPS定性分析元素组成 基本原理—— 光电离作用 ：当一束光子辐照到样品表面时，光子可以被样品中某一元素的原子轨道上的电子所吸收，使得 该电子脱离原子核的束缚，以一定的动能从原子内部发射出来，变成自由的光电子 ，而原子本身则变成一个激发态的离子。 根据爱因斯坦光电发射定律有： 式中，Ek为出射的光电子动能；hν为X射线源光子的能量；EB为特定原子轨道上的结合能（不同原子轨道具有不同的结合能）。从式中可以看出，对于特定的单色激发源和特定的原子轨道，其光电子的能量是特征的。 当固定激发源能量时，其光电子的能量仅与元素的种类和所电离激发的原子轨道有关。 因此，我们可以根据光电子的结合能定性分析物质的元素种类。 （2）XPS定性分析元素的化学态与分子结构 基本原理： 原子因所处化学环境不同，其内壳层电子结合能会发生变化 ，这种变化在谱图上表现为谱峰的位移（化学位移）。这种化学环境的不同可以是与原子相结合的元素种类或者数量不同，也可能是原子具有不同的化学价态。 注：一般规律：1) 氧化作用使内层电子结合能上升，氧化中失电子愈多,上升幅度愈大; 2)还原作用使内层电子结合能下降，还原中得电子愈多，下降幅度愈大；3) 对于给定价壳层结构的原子，所有内层电子结合能的位移几乎相同。 这可以理解为因为氧化作用失去电子后，剩下电子收到原子核的吸引作用增强，电子结合能增强；反之，还原作用得到电子，所有电子收到的吸引减弱，结合能减弱。 3、分析元素范围 XPS常用Al 或者Mg X射线为激发源，能检测周期表中 除氢、氦以外 的所有元素，一般检测限为0.1%（原子百分数）。 XPS之所以无法检测H, He是因为：1) H和He的光电离界面小，信号太弱；2) H1s电子很容易转移，在大多数情况下会转移到其他原子附近，检测起来非常困难; 3) H和He没有内层电子，其外层电子用于成键，H以原子核形式存在。所以用X射线去激发时，没有光电子可以被激发出来。 4、XPS定性分析的具体方法 （1） 全谱分析 ：化合物中元素种类的分析 对于一个化学成分未知的样品，首先应作全谱扫描,以初步判定表面的化学成分 。全谱能量扫描范围一般取0∼1200 eV, 因为几乎所有元素的最强峰都在这一范围之内。由于组成元素的光电子线和俄歇线的特征能量值具唯一性，与XPS标准谱图手册和数据库的结合能进行对比，可以用来鉴别某特定元素的存在。 鉴定顺序：1) 鉴别总是存在的元素谱线，如C、O的谱线;2) 鉴别样品中主要元素的强谱线和有关的次强谱线;3) 鉴别剩余的弱谱线假设它们是未知元素的最强谱线。 XPS表征手册一般采用：Chastain, Jill, andRoger C. King, eds. Handbook of X-ray photoelectron spectroscopy: a reference book of standard spectra for identification and interpretation of XPS data. Eden Prairie, MN: Physical Electronics, 1995. XPS数据库一般采用NIST XPS database: 1） http:// srdata.nist.gov/xps/Elm SpectralSrch.aspx?selEnergy=PE 2） https://www. thermofisher.cn/cn/zh/h ome/materials-science/learning-center/periodic-table/other-metal/tin.html （2） 窄区扫描 （也叫高分辨谱）：特定元素化学态与结构分析 ① 荷电校正 当用XPS测量绝缘体或者半导体时，由于光电子的连续发射而得不到电子补充，使得样品表面出现电子亏损，这种现象称为“荷电效应”。 荷电效应将使样品表面出现一稳定的电势Vs，对电子的逃离有一定束缚作用 。因此荷电效应将引起能量的位移，使得测量的结合能偏离真实值，造成测试结果的偏差。在用XPS测量绝缘体或者半导体时，需要对荷电效应所引起的偏差进行校正（荷电校正的目的），称之为“荷电校正”。 注：一般来说样品都需要进行荷电校正，用于后期判定峰偏移以及电子转移。 ② 荷电校正方法 一般采用外来污染碳的 C1s作为基准峰 来进行校准。以测量值和参考值(284.8 eV)之差作为荷电校正值(Δ)来矫正谱中其他元素的结合能。 具体操作：1) 求取荷电校正值：C单质的标准峰位(一般采用284.8 eV)-实际测得的C单质峰位=荷电校正值Δ；2)采用荷电校正值对其他谱图进行校正：将要 分析元素的XPS图谱的结合能加上Δ ，即得到校正后的峰位（整个过程中XPS谱图强度不变）。将校正后的峰位和强度作图得到的就是校正后的XPS谱图。 ③ 高分辨谱判定样品中某种元素的价态 高分辨谱定性分析元素的价态主要看两个点：1）可以对照标准谱图值（NIST数据库或者文献值）来确定谱线的化合态；2）对于p,d,f等具有双峰谱线的（自旋裂分），双峰间距也是判断元素化学状态的一个重要指标。 ④ 高分辨谱的其他常见用途 实际上，多数情况下，人们关心的不仅仅是表面某个元素呈几价，更多的是对比处理前后样品表面元素的化学位移变化，通过这种位移的变化来说明样品的表面化学状态或者是样品表面元素之间的电子相互作用。 一般， 某种元素失去电子，其结合能会向高场方向偏移，某种元素得到电子，其结合能会向低场方向偏移 ，对于给定价壳层结构的原子,所有内层电子结合能的位移几乎相同. 这种电子的偏移偏向可以给出元素之间电子相互作用的关系。 5、XPS定量分析方法与原理<em>（不常用） A. 基本原理： 经X射线辐照后，从样品表面出射的光电子的强度（I，指特征峰的峰面积）与样品中该原子的浓度（n）有线性关系，因此可以利用它进行元素的半定量分析。 简单的可以表示为：I = n</em>S, S称为灵敏度因子（有经验标准常数可查，但有时需校正）。 对于对某一固体试样中两个元素i和j, 如已知它们的灵敏度因子 和 ,并测出各自特定谱线强度 和 ，则它们的原子浓度之比为: ,因此可以求得相对含量。 B. 为什么XPS是一种 半定量分析手段 ？ 鉴于光电子的强度不仅与原子的浓度有关，还与光电子的平均自由程、样品的表面光洁度，元素所处的化学状态，X射线源强度以及仪器的状态有关。因此，XPS技术一般不能给出所分析元素的绝对含量，仅能提供各元素的相对含量。 注：由于元素的灵敏度因子不仅与元素种类有关，还与元素在物质中的存在状态，仪器的状态有一定的关系，因此不经校准测得的相对含量也会存在很大的误差。在实际分析中也可用对照标样校正，测量元素的相对含量。 6、XPS谱图 （1） XPS谱图中重要的谱线结构 XPS谱图一般包括 光电子谱线，卫星峰（伴峰），俄歇电子谱线，自旋-轨道分裂(SOS) 等。 ① 光电子谱线 ：每一种元素都有自己特征的光电子线，它是元素定性分析的主要依据。谱图中强度最大、峰宽最小、对称性最好的谱峰，称为XPS的主谱线。 ② 卫星峰（伴峰） ：常规Ｘ射线源(Al/Mg )并非是单色的，而是还存在一些能量略高的小伴线（ 和 等），所以导致XPS中，除 所激发的主谱外，还有一些小的伴峰。 ③ 俄歇电子谱线（KLL） ：电子电离后，芯能级出现空位，弛豫过程中若使另一电子激发成为自由电子，该电子即为俄歇电子。俄歇电子谱线总是伴随着XPS，但具有比XPS更宽更复杂的结构，多以谱线群的方式出现。特征：其动能与入射光ｈν无关。 ④ 自旋-轨道分裂(SOS) ：由于电子的轨道运动和自旋运动发生耦合后使轨道能级发生分裂。对于l＞0的内壳层来说，用内量子数j（j=｜l± ｜）表示自旋轨道分裂。即若l=0 则j=1/2；若l=1则j=1/2或3/2。除s亚壳层不发生分裂外，其余亚壳层都将分裂成两个峰。 对于某一特定价态的元素而言，其p, d, f 等双峰谱线的双峰间距及峰高比一般为一定值。 p峰的强度比为1:2；d线为2:3；f线为3:4 。对于p峰，特别是4p线，其强度比可能小于1:2。双峰间距也是判断元素化学状态的一个重要指标。 ⑤鬼峰：有时，由于X射源的阳极可能不纯或被污染，则产生的X射线不纯。因非阳极材料X射线所激发出的光电子谱线被称为“鬼峰”。 （2）XPS全谱分析与EDS异同 EDS与XPS的相同点： 两者均可以用于元素的定性和定量检测 。 EDS与XPS的不同点： 基本原理不同 : XPS是用X射线打出电子，检测的是电子；EDS则是用电子打出X射线，检测的是X射线 。 灵敏度和获得信息不同 ：EDS只能检测元素的组成与含量，不能测定元素的价态，且EDX的检测限较高（含量＞2%），即其灵敏度较低。而XPS既可以测定表面元素和含量，又可以测定表其价态。XPS的灵敏度更高，最低检测浓度＞0.１％。 用法不同：EDS常与SEM，TEM联用，可以对样品进行点扫，线扫，面扫等，能够比较方便地知道样品的表面（和SEM联用）或者体相（和TEM联用）的 元素分布情况 ；而XPS则一般独立使用，对样品表面信息进行检测，可以判定 元素的组成，化学态，分子结构 信息等。 注：EDS或EDX指能量色散X射线光谱仪，全称为Energy Dispersive X-ray Spectroscopy。 7、XPS原始数据处理（含分峰拟合） （1）荷电校正 具体步骤见上 （2）分峰拟合 https:// mp.weixin.qq.com/s/oKK9 Pnb2wZCSqgT_N8FnfQ 内容主要参考整理自研之成理、 科研共进社 ，仅作整理分享。 不会还有人没有关注我吧！？快来交个朋友！发现更多精彩内容！ 更多可关注主页以及查看更多文章： 必看！电化学学习入门与精通必备(已完结） 材料分析与表征（更新ing) 必看！最全材料学学科分享——搞懂材料科学基础篇（已完结) 必看！心理学全逻辑简单学（更新中ing) 最后的最后，欢迎大家点赞收藏和关注！每一篇内容都是在大家的鼓励中更新的！大家查看里面的小文章或者回答时，欢迎顺手点个赞！</p>
</div></details><h2 id="toc-88">45. Learning from Brain Topography: A Hierarchical Local-Global Graph-Transformer Network for EEG Emotion Recognition</h2>
<ul>
<li>链接：https://arxiv.org/abs/2601.10525v1</li>
<li>来源：arxiv</li>
<li>摘要：Understanding how local neurophysiological patterns interact with global brain dynamics is essential for decoding human emotions from EEG signals. However, existing deep learning approaches often overlook the brain's intrinsic spatial organization, failing to simultaneously capture local topological relations and global dependencies. To address these challenges, we propose Neuro-HGLN, a Neurologically-informed Hierarchical Graph-Transformer Learning Network that integrates biologically grounded priors with hierarchical representation learning. Neuro-HGLN first constructs a spatial Euclidean prior graph based on physical electrode distances to serve as an anatomically grounded inductive bias. A learnable global dynamic graph is then introduced to model functional connectivity across the entire brain. In parallel, to capture fine-grained regional dependencies, Neuro-HGLN builds region-level local graphs using a multi-head self-attention mechanism. These graphs are processed synchronously through local-constrained parallel GCN layers to produce region-specific representations. Subsequently, an iTransformer encoder aggregates these features to capture cross-region dependencies under a </li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">脑的固有空间组织在深度学习方法中往往被忽视，这可能导致对局部神经生理模式与全局脑动态之间相互作用的误解。局部神经生理模式与全局脑动态之间的相互作用对于从EEG信号解码人类情绪至关重要，因此，神经-结构化先验图有助于捕捉局部拓扑关系和全局依赖性，而全局动态图则有助于建模整个大脑的功能连接。为了实现这一目标，神经-HGLN网络采用了多头自注意力机制来构建区域级局部图，并通过局部约束并行GCN层生成区域特定表示。此外，iTransformer编码器用于聚合特征以捕捉区域间依赖性，从而增强了解释性。因此，神经-HGLN网络在多个基准上实现了最先进的性能，并且通过统一局部拓扑学习与区域间依赖性建模，提供了稳健的EEG情绪识别。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>脑的固有空间组织常常被现有的深度学习方法忽略。</li>
<li>局部神经生理模式与全局脑动态之间的相互作用对于从EEG信号解码人类情绪至关重要。</li>
<li>神经-结构化先验图有助于捕捉局部拓扑关系和全局依赖性。</li>
<li>全局动态图有助于建模整个大脑的功能连接。</li>
<li>多头自注意力机制用于构建区域级局部图。</li>
<li>局部约束并行GCN层用于生成区域特定表示。</li>
<li>iTransformer编码器用于聚合特征以捕捉区域间依赖性。</li>
<li>神经-HGLN网络在多个基准上实现了最先进的性能。</li>
<li>神经-HGLN网络提供了基于神经生理结构的增强解释性。</li>
<li>局部拓扑学习与区域间依赖性建模的统一对于稳健的EEG情绪识别是有效的。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-89">正文（抓取，非 AI）</h3>
<p>[2601.10525v1] Learning from Brain Topography: A Hierarchical Local-Global Graph-Transformer Network for EEG Emotion Recognition Computer Science &gt; Human-Computer Interaction arXiv:2601.10525v1 (cs) [Submitted on 15 Jan 2026] Title: Learning from Brain Topography: A Hierarchical Local-Global Graph-Transformer Network for EEG Emotion Recognition Authors: Yijin Zhou , Fu Li , Yi Niu , Boxun Fu , Huaning Wang , Lijian Zhang View a PDF of the paper titled Learning from Brain Topography: A Hierarchical Local-Global Graph-Transformer Network for EEG Emotion Recognition, by Yijin Zhou and 5 other authors View PDF HTML (experimental) Abstract: Understanding how local neurophysiological patterns interact with global brain dynamics is essential for decoding human emotions from EEG signals. However, existing deep learning approaches often overlook the brain's intrinsic spatial organization, failing to simultaneously capture local topological relations and global dependencies. To address these challenges, we propose Neuro-HGLN, a Neurologically-informed Hierarchical Graph-Transformer Learning Network that integrates biologically grounded priors with hierarchical representation learning. Neuro-HGLN first constructs a spatial Euclidean prior graph based on physical electrode distances to serve as an anatomically grounded inductive bias. A learnable global dynamic graph is then introduced to model functional connectivity across the entire brain. In parallel, to capture fine-grained regional dependencies, Neuro-HGLN builds region-level local graphs using a multi-head self-attention mechanism. These graphs are processed synchronously through local-constrained parallel GCN layers to produce region-specific representations. Subsequently, an iTransformer encoder aggregates these features to capture cross-region dependencies under a dimension-as-token formulation. Extensive experiments demonstrate that Neuro-HGLN achieves state-of-the-art performance on multiple benchmarks, providing enhanced interpretability grounded in neurophysiological structure. These results highlight the efficacy of unifying local topological learning with cross-region dependency modeling for robust EEG emotion recognition. Subjects: Human-Computer Interaction (cs.HC) Cite as: arXiv:2601.10525 [cs.HC] (or arXiv:2601.10525v1 [cs.HC] for this version) https://doi.org/10.48550/arXiv.2601.10525 Focus to learn more arXiv-issued DOI via DataCite Submission history From: YiJin Zhou [ view email ] [v1] Thu, 15 Jan 2026 15:52:05 UTC (10,475 KB) Full-text links: Access Paper: View a PDF of the paper titled Learning from Brain Topography: A Hierarchical Local-Global Graph-Transformer Network for EEG Emotion Recognition, by Yijin Zhou and 5 other authors View PDF HTML (experimental) TeX Source view license Current browse context: cs.HC &lt; prev | next &gt; new | recent | 2026-01 Change to browse by: cs References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-90">46. Beyond MSE: Ordinal Cross-Entropy for Probabilistic Time Series Forecasting</h2>
<ul>
<li>链接：https://arxiv.org/abs/2511.10200v2</li>
<li>来源：arxiv</li>
<li>摘要：Time series forecasting is an important task that involves analyzing temporal dependencies and underlying patterns (such as trends, cyclicality, and seasonality) in historical data to predict future values or trends. Current deep learning-based forecasting models primarily employ Mean Squared Error (MSE) loss functions for regression modeling. Despite enabling direct value prediction, this method offers no uncertainty estimation and exhibits poor outlier robustness. To address these limitations, we propose OCE-TS, a novel ordinal classification approach for time series forecasting that replaces MSE with Ordinal Cross-Entropy (OCE) loss, preserving prediction order while quantifying uncertainty through probability output. Specifically, OCE-TS begins by discretizing observed values into ordered intervals and deriving their probabilities via a parametric distribution as supervision signals. Using a simple linear model, we then predict probability distributions for each timestep. The OCE loss is computed between the cumulative distributions of predicted and ground-truth probabilities, explicitly preserving ordinal relationships among forecasted values. Through theoretical analysis usin</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">时间序列预测涉及分析历史数据中的模式，而当前的深度学习模型主要使用MSE损失，但MSE损失无法提供不确定性估计且对异常值不鲁棒。为解决这些问题，OCE-TS通过离散化观测值来量化不确定性，使用简单线性模型预测概率分布，并通过概率输出提供不确定性估计。OCE-TS采用Ordinal Cross-Entropy损失函数，该函数能够保持预测值的顺序关系，从而在多个时间序列数据集上优于基准模型。此外，理论分析表明交叉熵损失比MSE损失更稳定，因此OCE-TS在多个应用场景中表现出色。因此，OCE-TS不仅能够提供更可靠的预测，还能保留预测值的顺序关系，从而在实际应用中具有显著优势。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>MSE损失无法提供不确定性估计。</li>
<li>OCE-TS通过离散化观测值来量化不确定性。</li>
<li>OCE损失能够保持预测值的顺序关系。</li>
<li>理论分析表明交叉熵损失比MSE损失更稳定。</li>
<li>实验证明OCE-TS在多个时间序列数据集上优于基准模型。</li>
<li>OCE-TS使用简单线性模型预测概率分布。</li>
<li>OCE损失是通过累积分布计算的。</li>
<li>MSE损失对异常值不鲁棒。</li>
<li>OCE-TS通过概率输出提供不确定性估计。</li>
<li>Ordinal Cross-Entropy损失函数是为时间序列预测设计的。</li>
<li>时间序列预测涉及分析历史数据中的模式。</li>
<li>当前的深度学习模型主要使用MSE损失。</li>
<li>预测顺序在OCE-TS中得到保留。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-91">正文（抓取，非 AI）</h3>
<p>[2511.10200v2] Beyond MSE: Ordinal Cross-Entropy for Probabilistic Time Series Forecasting Computer Science &gt; Machine Learning arXiv:2511.10200v2 (cs) [Submitted on 13 Nov 2025 ( v1 ), last revised 27 Nov 2025 (this version, v2)] Title: Beyond MSE: Ordinal Cross-Entropy for Probabilistic Time Series Forecasting Authors: Jieting Wang , Huimei Shi , Feijiang Li , Xiaolei Shang View a PDF of the paper titled Beyond MSE: Ordinal Cross-Entropy for Probabilistic Time Series Forecasting, by Jieting Wang and 3 other authors View PDF HTML (experimental) Abstract: Time series forecasting is an important task that involves analyzing temporal dependencies and underlying patterns (such as trends, cyclicality, and seasonality) in historical data to predict future values or trends. Current deep learning-based forecasting models primarily employ Mean Squared Error (MSE) loss functions for regression modeling. Despite enabling direct value prediction, this method offers no uncertainty estimation and exhibits poor outlier robustness. To address these limitations, we propose OCE-TS, a novel ordinal classification approach for time series forecasting that replaces MSE with Ordinal Cross-Entropy (OCE) loss, preserving prediction order while quantifying uncertainty through probability output. Specifically, OCE-TS begins by discretizing observed values into ordered intervals and deriving their probabilities via a parametric distribution as supervision signals. Using a simple linear model, we then predict probability distributions for each timestep. The OCE loss is computed between the cumulative distributions of predicted and ground-truth probabilities, explicitly preserving ordinal relationships among forecasted values. Through theoretical analysis using influence functions, we establish that cross-entropy (CE) loss exhibits superior stability and outlier robustness compared to MSE loss. Empirically, we compared OCE-TS with five baseline models-Autoformer, DLinear, iTransformer, TimeXer, and TimeBridge-on seven public time series datasets. Using MSE and Mean Absolute Error (MAE) as evaluation metrics, the results demonstrate that OCE-TS consistently outperforms benchmark models. The codeis publicly available at: this https URL . Subjects: Machine Learning (cs.LG) Cite as: arXiv:2511.10200 [cs.LG] (or arXiv:2511.10200v2 [cs.LG] for this version) https://doi.org/10.48550/arXiv.2511.10200 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Feijiang Li [ view email ] [v1] Thu, 13 Nov 2025 11:14:24 UTC (10,056 KB) [v2] Thu, 27 Nov 2025 11:46:13 UTC (10,059 KB) Full-text links: Access Paper: View a PDF of the paper titled Beyond MSE: Ordinal Cross-Entropy for Probabilistic Time Series Forecasting, by Jieting Wang and 3 other authors View PDF HTML (experimental) TeX Source view license Current browse context: cs.LG &lt; prev | next &gt; new | recent | 2025-11 Change to browse by: cs References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) IArxiv recommender toggle IArxiv Recommender ( What is IArxiv? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-92">47. Benchmarking M-LTSF: Frequency and Noise-Based Evaluation of Multivariate Long Time Series Forecasting Models</h2>
<ul>
<li>链接：https://arxiv.org/abs/2510.04900v1</li>
<li>来源：arxiv</li>
<li>摘要：Understanding the robustness of deep learning models for multivariate long-term time series forecasting (M-LTSF) remains challenging, as evaluations typically rely on real-world datasets with unknown noise properties. We propose a simulation-based evaluation framework that generates parameterizable synthetic datasets, where each dataset instance corresponds to a different configuration of signal components, noise types, signal-to-noise ratios, and frequency characteristics. These configurable components aim to model real-world multivariate time series data without the ambiguity of unknown noise. This framework enables fine-grained, systematic evaluation of M-LTSF models under controlled and diverse scenarios. We benchmark four representative architectures S-Mamba (state-space), iTransformer (transformer-based), R-Linear (linear), and Autoformer (decomposition-based). Our analysis reveals that all models degrade severely when lookback windows cannot capture complete periods of seasonal patters in the data. S-Mamba and Autoformer perform best on sawtooth patterns, while R-Linear and iTransformer favor sinusoidal signals. White and Brownian noise universally degrade performance with l</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">real-world datasets introduce unknown noise properties that complicate evaluation, necessitating the use of synthetic datasets for controlled evaluation of M-LTSF models. To ensure accurate performance, lookback windows must capture complete periods of seasonal patterns, avoiding model degradation. Different models exhibit varying performance based on specific signal characteristics: S-Mamba and Autoformer excel with sawtooth patterns, while R-Linear and iTransformer perform better with sinusoidal signals. White and Brownian noise reduce performance at lower signal-to-noise ratios, and both Trend-noise and Seasonal-noise affect S-Mamba and iTransformer performance, respectively. Notably, S-Mamba and iTransformer achieve superior frequency reconstruction, highlighting their strengths in handling complex noise conditions. Therefore, model selection should consider signal characteristics and noise conditions, as benchmarking reveals model-specific strengths and limitations through MSE scores.</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>real-world datasets introduce unknown noise properties that complicate evaluation.</li>
<li>synthetic datasets allow controlled evaluation of M-LTSF models.</li>
<li>lookback windows must capture complete periods of seasonal patterns to avoid model degradation.</li>
<li>S-Mamba and Autoformer excel with sawtooth patterns.</li>
<li>R-Linear and iTransformer perform better with sinusoidal signals.</li>
<li>White and Brownian noise reduce performance at lower signal-to-noise ratios.</li>
<li>Trend-noise affects S-Mamba performance.</li>
<li>Seasonal-noise impacts iTransformer performance.</li>
<li>S-Mamba and iTransformer achieve superior frequency reconstruction.</li>
<li>Model selection should consider signal characteristics and noise conditions.</li>
<li>Benchmarking reveals model-specific strengths and limitations through MSE scores.</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-93">正文（抓取，非 AI）</h3>
<p>[2510.04900v1] Benchmarking M-LTSF: Frequency and Noise-Based Evaluation of Multivariate Long Time Series Forecasting Models Computer Science &gt; Machine Learning arXiv:2510.04900v1 (cs) [Submitted on 6 Oct 2025] Title: Benchmarking M-LTSF: Frequency and Noise-Based Evaluation of Multivariate Long Time Series Forecasting Models Authors: Nick Janßen , Melanie Schaller , Bodo Rosenhahn View a PDF of the paper titled Benchmarking M-LTSF: Frequency and Noise-Based Evaluation of Multivariate Long Time Series Forecasting Models, by Nick Jan{\ss}en and 2 other authors View PDF HTML (experimental) Abstract: Understanding the robustness of deep learning models for multivariate long-term time series forecasting (M-LTSF) remains challenging, as evaluations typically rely on real-world datasets with unknown noise properties. We propose a simulation-based evaluation framework that generates parameterizable synthetic datasets, where each dataset instance corresponds to a different configuration of signal components, noise types, signal-to-noise ratios, and frequency characteristics. These configurable components aim to model real-world multivariate time series data without the ambiguity of unknown noise. This framework enables fine-grained, systematic evaluation of M-LTSF models under controlled and diverse scenarios. We benchmark four representative architectures S-Mamba (state-space), iTransformer (transformer-based), R-Linear (linear), and Autoformer (decomposition-based). Our analysis reveals that all models degrade severely when lookback windows cannot capture complete periods of seasonal patters in the data. S-Mamba and Autoformer perform best on sawtooth patterns, while R-Linear and iTransformer favor sinusoidal signals. White and Brownian noise universally degrade performance with lower signal-to-noise ratio while S-Mamba shows specific trend-noise and iTransformer shows seasonal-noise vulnerability. Further spectral analysis shows that S-Mamba and iTransformer achieve superior frequency reconstruction. This controlled approach, based on our synthetic and principle-driven testbed, offers deeper insights into model-specific strengths and limitations through the aggregation of MSE scores and provides concrete guidance for model selection based on signal characteristics and noise conditions. Comments: Number of pages: 13 Number of figures: 16 Number of Tables: 1 Submitted to: IEEE Transactions on Signal Processing Subjects: Machine Learning (cs.LG) ; Systems and Control (eess.SY) Cite as: arXiv:2510.04900 [cs.LG] (or arXiv:2510.04900v1 [cs.LG] for this version) https://doi.org/10.48550/arXiv.2510.04900 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Nick Janßen [ view email ] [v1] Mon, 6 Oct 2025 15:16:52 UTC (441 KB) Full-text links: Access Paper: View a PDF of the paper titled Benchmarking M-LTSF: Frequency and Noise-Based Evaluation of Multivariate Long Time Series Forecasting Models, by Nick Jan{\ss}en and 2 other authors View PDF HTML (experimental) TeX Source view license Current browse context: cs.LG &lt; prev | next &gt; new | recent | 2025-10 Change to browse by: cs cs.SY eess eess.SY References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) IArxiv recommender toggle IArxiv Recommender ( What is IArxiv? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-94">48. 知乎盐选 | 3.4 基于 EEMD-SVR 的股票指数预测建模</h2>
<ul>
<li>链接：https://www.zhihu.com/market/pub/120326947/manuscript/1536837782323884032</li>
<li>来源：bing</li>
<li>摘要：结合 EEMD 对时序信号的自适应分解功能与 ε-不敏感 SVR 突出的非线性预测能力，本书提出基于 EEMD 与 ε-不敏感 SVR 的沪深 300 指数预测方法 EEMD-SVRP，以提高股票指数预测的准确度，为股票交 …</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">EEMD-SVRP 方法通过结合EEMD和ε-不敏感SVR的优点，有效降低了预测误差和预测滞后性，特别适用于非线性、非平稳的复杂时间序列建模。该方法通过加和集成各分量序列预测值获得指数的集成预测值，从而显著提高经济金融复杂时序建模的预测能力。因此，EEMD-SVRP 方法不仅能提供更准确的股票指数预测信息，还能有效提高股票交易决策过程中的价格预测信息和能力，增强投资者把握市场行情的能力，有效抵抗市场风险，获取超额收益。此外，EEMD-SVRP 方法通过ε-不敏感损失函数降低了回归模型的过拟合风险，有效避免了模型收敛于局部最优值的问题，进一步提高了模型的预测能力和有效性。因此，EEMD-SVRP 方法在提高股票指数预测准确度和预测能力方面具有显著优势。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>EEMD-SVRP 方法能有效降低预测误差和预测滞后性。</li>
<li>EEMD-SVRP 方法结合了EEMD和ε-不敏感SVR的优点。</li>
<li>EEMD-SVRP 方法适用于非线性、非平稳的复杂时间序列建模。</li>
<li>EEMD-SVRP 方法能提供更准确的股票指数预测信息。</li>
<li>EEMD-SVRP 方法通过加和集成各分量序列预测值获得指数的集成预测值。</li>
<li>EEMD-SVRP 方法能有效提高股票交易决策过程中的价格预测能力。</li>
<li>EEMD-SVRP 方法通过ε-不敏感损失函数降低了回归模型的过拟合风险。</li>
<li>EEMD-SVRP 方法能有效避免模型收敛于局部最优值的问题。</li>
<li>EEMD-SVRP 方法能显著提高经济金融复杂时序建模的预测能力。</li>
<li>EEMD-SVRP 方法能有效增强投资者把握市场行情的能力。</li>
<li>EEMD-SVRP 方法能有效抵抗市场风险的能力。</li>
<li>EEMD-SVRP 方法能有效获取超额收益的能力。</li>
<li>EEMD-SVRP 方法能有效提高模型的预测能力。</li>
<li>EEMD-SVRP 方法能有效提高股票指数预测的准确度。</li>
<li>EEMD-SVRP 方法能有效提高股票交易决策过程中的价格预测信息。</li>
<li>EEMD-SVRP 方法能有效提高模型的预测有效性。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-95">正文（抓取，非 AI）</h3>
<p>知乎盐选 | 3.4 基于 EEMD-SVR 的股票指数预测建模 注册或登录 3.4 基于 EEMD-SVR 的股票指数预测建模 3.4 基于 EEMD-SVR 的股票指数预测建模 将集合经验模态分解（EEMD）引入股票指数预测研究中，并充分考虑 ε-不敏感支持向量回归（SVR）出色的非线性建模能力，提出基于 EEMD 与 SVR 建模的沪深 300 指数预测方法 EEMD-SVRP。首先，对沪深 300 指数时间序列进行 EEMD 分解，获得多个本征模函数和趋势项，并根据序列的均值特征将本征模函数重组为高频、低频分量；其次，运用 ε-不敏感 SVR 分别建立各分量序列的预测模型，进而对各分量序列预测值进行加和，获得指数的集成预测值；最后，为评估模型的预测有效性，对 EEMD-SVRP 与 ARIMA、MLP-ANN 和 SVR 三种典型指数预测方法进行对比实验，结果表明，EEMD-SVRP 具有更低的预测误差和预测滞后性，是一种更加有效的沪深 300 指数预测方法。 3.4.1 引言及文献综述 股票是金融市场最重要的工具之一，其价格波动能否预测以及如何有效预测是金融领域研究的核心课题。股价预测建模通过深入分析宏观经济、市场运行、企业经营状况和投资者预期等数据，以获取选股、择时以及组合构建等投资实践环节所期望的股市趋势或状态超额信息，进一步优化投资决策，获得超额收益（ 傅中杰 、 吴清强 ，2018）。股价预测是股市投资实践中提高收益和规避风险的重要环节之一，其建模问题得到了金融投资理论和实务界的高度重视。从时间序列角度对股票价格进行预测建模是股价预测建模中最重要的方式，也是金融计量、量化投资以及信息科学等领域研究者长期关注并深入研究的核心课题之一（ 李斌 等，2019）。 现有的股票价格与指数的预测建模方法可以分为技术分析法、统计建模方法和机器学习方法三大类（ 丁鹏 ，2016）。技术分析法以道氏理论为理论基础，其优点是直观、可操作性强，但存在时效性不强、生成买卖信号不准确等缺陷。统计建模方法具有坚实的统计学理论基础，典型方法包括 ARIMA、GARCH 及其改进方法。该方法预测结果在统计意义上可靠，但过于严格的应用假设条件限制了其适用的情形与范围。非线性机器学习方法通常具有强大的非线性复杂关系建模与表示能力，而用于股价预测建模并取得显著成效的典型方法包括 ANN、SVR 和 HMM 等（ 贺毅岳 、 李萍 等，2019）。其中，ε-不敏感 SVR 通过引入 ε-不敏感损失函数，极大地降低了回归模型的过拟合风险，且结构风险最小化构建原则能使其有效避免模型收敛于局部最优值的问题，近年来日渐成为一种重要的经济金融复杂序列预测建模方法（ 丁鹏 ，2016）。 股票价格时间序列的波动具有高度的非平稳、非线性的复杂特性，使得直接运用 ANN 和 SVR 等方法很难高效地提取股价时序的复杂变化模式，进而难以建立投资决策所需的高精度股价预测模型。因而，在金融投资尤其是主动型股票投资中，进一步分析股价序列所具有的随机性、复杂性，提出更有效的股票价格预测方法，是获取最优投资决策所需的超额信息，进而获得超额收益过程中亟须解决的关键问题（ 杨青 、 王晨蔚 ，2019）。 针对上述非线性、非平稳复杂时间序列建模的难题，Huang 等（1998）提出了经验模态分解（EMD）方法。该方法理论上能将任意复杂的时间序列信号自适应地分解为不同频率尺度下的本征模函数（IMF），其中每一个 IMF 都具有特定的频率特征和波动模态。在气象科学、故障诊断等应用领域的大量研究证实，EMD 方法具有对非线性、非平稳时间序列建模的突出优势，引起了经济金融研究者的关注，并进一步将其引入和运用于具体的研究问题。阮连法等（2012）运用 EMD 对杭州市房价数据进行分解，并根据本征模函数的特征进行重组，深入分析了各因素对价格周期波动造成的影响程度；李祥飞等（2014）利用 EMD 分解提取 6 种指数收益序列数据的多时间尺度分量，进而通过各分量的 STSA 分析获得原始序列的变化模式；邸浩等（2018）提出一种 EEMD 与 LSTM 相结合的商品价格预测方法，并通过沪金期货数据证实该方法比已有方法具有更强的预测能力。上述研究结果表明，通过 EEMD 分解再运用计量或机器学习方法进行时序建模，能显著降低经济金融复杂时序建模的难度，同时可有效提高模型的预测能力。 结合 EEMD 对时序信号的自适应分解功能与 ε-不敏感 SVR 突出的非线性预测能力，本书提出基于 EEMD 与 ε-不敏感 SVR 的沪深 300 指数预测方法 EEMD-SVRP，以提高股票指数预测的准确度，为股票交易决策过程提供至关重要的价格预测信息，从而有效增强投资者把握市场行情、抵抗市场风险以及获取超额收益的能力。 3.4.2 沪深 300 指数预测建模 3.4.2.1 EEMD-SVRP 建模思路及流程 在图 3-21 所示基于 EEMD-SVRP 的指数预测建模中，以沪深 300 指数时间序列为输入数据，包括指数序列 EEMD 分解、IMF 重组、各分量序列的预测建模、加和集成四个主要步骤。 加载中... 图 3-21 基于 EEMD-SVRP 的指数预测建模流程 具体建模过程包括如下四个步骤（ 贺毅岳 、 韩进博 等，2020）： 最低 0.3 元/天开通会员，查看完整内容 {"name":"manuscript","status":200,"titleHTML":{},"metaHTML":{},"styleHTML":{},"forbiddenModeScript":"\u003cscript defer nonce=\"6HAL6wwYs-ZpMGo38-sbJ\"&gt;\n function _0x1edf(_0x2e6a39,_0x4bcd1a){var _0x542b52=_0x542b();return _0x1edf=function(_0x1edf61,_0x4e7566){_0x1edf61=_0x1edf61-0xab;var _0x1f6a36=_0x542b52[_0x1edf61];return _0x1f6a36;},_0x1edf(_0x2e6a39,_0x4bcd1a);}var _0x15339f=_0x1edf;function _0x542b(){var _0x15876f=['6866916DkSIJe','10221820JlaQxF','%E5%BD%93%E5%89%8D%E9%A1%B5%E9%9D%A2%E5%86%85%E5%AE%B9%E4%B8%BA%E7%89%88%E6%9D%83%E4%BF%9D%E6%8A%A4%E5%86%85%E5%AE%B9%EF%BC%8C%E6%82%A8%E7%9A%84%E8%A1%8C%E4%B8%BA%E6%B6%89%E5%AB%8C%E8%BF%9D%E5%8F%8D%E7%9F%A5%E4%B9%8E%E5%8D%8F%E8%AE%AE%EF%BC%8C%E7%9B%B8%E5%85%B3%E8%B4%A6%E5%8F%B7%E6%9C%89%E8%A2%AB%E5%B0%81%E7%A6%81%E9%A3%8E%E9%99%A9','2632856THltSE','parentElement','removeChild','zhihu.com','\u003c/div&gt;','4664067qJFmtW','getElementById','4gHRMrs','createElement','40KPWcfH','852012idpNLJ','1McIQrn','2179458vftjUb','\u003cdiv style=\"margin: 130px auto;font-size: 30px;line-height: 55px;color: red;text-align: center;\"&gt;','3158485IogzJo','22Wdfejx','innerHTML'];_0x542b=function(){return _0x15876f;};return _0x542b();}(function(_0x57c3fc,_0x4b4fbe){var _0x48884a=_0x1edf,_0x2c6eec=_0x57c3fc();while(!![]){try{var _0x11dec1=parseInt(_0x48884a(0xb4))/0x1<em>(-parseInt(_0x48884a(0xbd))/0x2)+parseInt(_0x48884a(0xae))/0x3+-parseInt(_0x48884a(0xb0))/0x4</em>(-parseInt(_0x48884a(0xb7))/0x5)+parseInt(_0x48884a(0xb3))/0x6+-parseInt(_0x48884a(0xba))/0x7+parseInt(_0x48884a(0xb2))/0x8<em>(-parseInt(_0x48884a(0xb5))/0x9)+parseInt(_0x48884a(0xbb))/0xa</em>(parseInt(_0x48884a(0xb8))/0xb);if(_0x11dec1===_0x4b4fbe)break;else _0x2c6eec<a href="_0x2c6eec[" title="shift']());}catch(_0x376ed0){_0x2c6eec['push'](_0x2c6eec['shift']());}}}(_0x542b,0xd310e));if(window['location']['host']['indexOf'](_0x15339f(0xac))===-0x1){var rootDom=document[_0x15339f(0xaf)]('app">'push'</a>,text=_0x15339f(0xbc),forbiddenDom=document<a href="" title="div">_0x15339f(0xb1)</a>;forbiddenDom[_0x15339f(0xb9)]=_0x15339f(0xb6)+decodeURI(text)+_0x15339f(0xad),rootDom['parentElement']<a href="forbiddenDom">'appendChild'</a>,rootDom[_0x15339f(0xbe)]<a href="rootDom">_0x15339f(0xab)</a>;}\n \u003c/script&gt;","webPageReadyScript":"\u003cscript nonce=\"6HAL6wwYs-ZpMGo38-sbJ\"&gt;window.zhihuNativeApp&amp;&amp;window.zhihuNativeApp.sendToNative&amp;&amp;window.zhihuNativeApp.sendToNative(JSON.stringify({module: 'market',action: 'FCPEnd',params: {}}))\u003c/script&gt;","viteScript":"","appContext":{"request":{"ip":"2409:8a14:c53:7c01:900:e757:4176:be07","xRealIp":"2409:8a14:c53:7c01:900:e757:4176:be07","headers":{"host":"www.zhihu.com"},"url":"/market/pub/120326947/manuscript/1536837782323884032","href":"https://www.zhihu.com/market/pub/120326947/manuscript/1536837782323884032","path":"/market/pub/120326947/manuscript/1536837782323884032","params":{"0":"manuscript","productType":"pub","productId":"120326947","manuscriptId":"1536837782323884032"},"query":{}},"deviceID":"","ua":{"Mobile":false,"Android":false,"Chrome":true,"iOS":false,"Wechat":false,"WorkWechat":false,"WechatMiniprogram":false,"Weibo":false,"QQ":false,"Zhihu":false,"ZhihuHybrid":false,"iPad":false,"UC":false,"QQBrowser":false,"BankABC":false,"BankABCNew":false,"AliPay":false,"YanYan":false,"ZhihuLite":false,"Harmony":false,"YanyanHarmonyOS":false,"origin":"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"},"theme":"light","isOffice":false,"xAppZa":"","xAppVersion":"","xApiVersion":"","xNetworkType":"","xUDId":"","xZst81":"","zaeEnvType":"","commentCloseFlag":"0","globalSlienceMode":"","supportsWebp":false,"apiBaseDict":{"api-default":"https://api.zhihu.com","api-v4":"https://www.zhihu.com/api/v4","api-walletpay":"https://walletpay.zhihu.com","api-outside":"https://www.zhihu.com/api/vip"},"safeAreaInset":{},"clientId":"41a7e2fa-c68c-43d7-84de-0ab477898cb1","vipPrivilegesUrl":"https://www.zhihu.com/xen/market/vip-privileges","nonce":"6HAL6wwYs-ZpMGo38-sbJ","ssrStage":"render","__connectedAutoFetch":{"manuscript":{"pending":false,"data":{"manuscriptData":{"truncate_text":"最低 0.3 元/天开通会员，查看完整内容","manuscript":"\u003ch2 class=\"txtheading-1-1\"&gt;3.4 基于 EEMD-SVR 的股票指数预测建模\u003c/h2&gt;\u003cp class=\"bodyContent-1\"&gt;将集合经验模态分解（EEMD）引入股票指数预测研究中，并充分考虑 ε-不敏感支持向量回归（SVR）出色的非线性建模能力，提出基于 EEMD 与 SVR 建模的沪深 300 指数预测方法 EEMD-SVRP。首先，对沪深 300 指数时间序列进行 EEMD 分解，获得多个本征模函数和趋势项，并根据序列的均值特征将本征模函数重组为高频、低频分量；其次，运用 ε-不敏感 SVR 分别建立各分量序列的预测模型，进而对各分量序列预测值进行加和，获得指数的集成预测值；最后，为评估模型的预测有效性，对 EEMD-SVRP 与 ARIMA、MLP-ANN 和 SVR 三种典型指数预测方法进行对比实验，结果表明，EEMD-SVRP 具有更低的预测误差和预测滞后性，是一种更加有效的沪深 300 指数预测方法。\u003c/p&gt;\u003ch3 class=\"txtheading-2-1 sigil_not_in_toc\"&gt;3.4.1 引言及文献综述\u003c/h3&gt;\u003cp class=\"bodyContent-1\"&gt;股票是金融市场最重要的工具之一，其价格波动能否预测以及如何有效预测是金融领域研究的核心课题。股价预测建模通过深入分析宏观经济、市场运行、企业经营状况和投资者预期等数据，以获取选股、择时以及组合构建等投资实践环节所期望的股市趋势或状态超额信息，进一步优化投资决策，获得超额收益（\u003cspan class=\"kaiti\"&gt;傅中杰\u003c/span&gt;、\u003cspan class=\"kaiti\"&gt;吴清强\u003c/span&gt;，2018）。股价预测是股市投资实践中提高收益和规避风险的重要环节之一，其建模问题得到了金融投资理论和实务界的高度重视。从时间序列角度对股票价格进行预测建模是股价预测建模中最重要的方式，也是金融计量、量化投资以及信息科学等领域研究者长期关注并深入研究的核心课题之一（\u003cspan class=\"kaiti\"&gt;李斌\u003c/span&gt;等，2019）。\u003c/p&gt;\u003cp class=\"bodyContent-1\"&gt;现有的股票价格与指数的预测建模方法可以分为技术分析法、统计建模方法和机器学习方法三大类（\u003cspan class=\"kaiti\"&gt;丁鹏\u003c/span&gt;，2016）。技术分析法以道氏理论为理论基础，其优点是直观、可操作性强，但存在时效性不强、生成买卖信号不准确等缺陷。统计建模方法具有坚实的统计学理论基础，典型方法包括 ARIMA、GARCH 及其改进方法。该方法预测结果在统计意义上可靠，但过于严格的应用假设条件限制了其适用的情形与范围。非线性机器学习方法通常具有强大的非线性复杂关系建模与表示能力，而用于股价预测建模并取得显著成效的典型方法包括 ANN、SVR 和 HMM 等（\u003cspan class=\"kaiti\"&gt;贺毅岳\u003c/span&gt;、\u003cspan class=\"kaiti\"&gt;李萍\u003c/span&gt;等，2019）。其中，ε-不敏感 SVR 通过引入 ε-不敏感损失函数，极大地降低了回归模型的过拟合风险，且结构风险最小化构建原则能使其有效避免模型收敛于局部最优值的问题，近年来日渐成为一种重要的经济金融复杂序列预测建模方法（\u003cspan class=\"kaiti\"&gt;丁鹏\u003c/span&gt;，2016）。\u003c/p&gt;\u003cp class=\"bodyContent-1\"&gt;股票价格时间序列的波动具有高度的非平稳、非线性的复杂特性，使得直接运用 ANN 和 SVR 等方法很难高效地提取股价时序的复杂变化模式，进而难以建立投资决策所需的高精度股价预测模型。因而，在金融投资尤其是主动型股票投资中，进一步分析股价序列所具有的随机性、复杂性，提出更有效的股票价格预测方法，是获取最优投资决策所需的超额信息，进而获得超额收益过程中亟须解决的关键问题（\u003cspan class=\"kaiti\"&gt;杨青\u003c/span&gt;、\u003cspan class=\"kaiti\"&gt;王晨蔚\u003c/span&gt;，2019）。\u003c/p&gt;\u003cp class=\"bodyContent-1\"&gt;针对上述非线性、非平稳复杂时间序列建模的难题，Huang 等（1998）提出了经验模态分解（EMD）方法。该方法理论上能将任意复杂的时间序列信号自适应地分解为不同频率尺度下的本征模函数（IMF），其中每一个 IMF 都具有特定的频率特征和波动模态。在气象科学、故障诊断等应用领域的大量研究证实，EMD 方法具有对非线性、非平稳时间序列建模的突出优势，引起了经济金融研究者的关注，并进一步将其引入和运用于具体的研究问题。阮连法等（2012）运用 EMD 对杭州市房价数据进行分解，并根据本征模函数的特征进行重组，深入分析了各因素对价格周期波动造成的影响程度；李祥飞等（2014）利用 EMD 分解提取 6 种指数收益序列数据的多时间尺度分量，进而通过各分量的 STSA 分析获得原始序列的变化模式；邸浩等（2018）提出一种 EEMD 与 LSTM 相结合的商品价格预测方法，并通过沪金期货数据证实该方法比已有方法具有更强的预测能力。上述研究结果表明，通过 EEMD 分解再运用计量或机器学习方法进行时序建模，能显著降低经济金融复杂时序建模的难度，同时可有效提高模型的预测能力。\u003c/p&gt;\u003cp class=\"bodyContent-1\"&gt;结合 EEMD 对时序信号的自适应分解功能与 ε-不敏感 SVR 突出的非线性预测能力，本书提出基于 EEMD 与 ε-不敏感 SVR 的沪深 300 指数预测方法 EEMD-SVRP，以提高股票指数预测的准确度，为股票交易决策过程提供至关重要的价格预测信息，从而有效增强投资者把握市场行情、抵抗市场风险以及获取超额收益的能力。\u003c/p&gt;\u003ch3 class=\"txtheading-2-1 sigil_not_in_toc\"&gt;3.4.2 沪深 300 指数预测建模\u003c/h3&gt;\u003ch4 class=\"txtheading-3-kaiti sigil_not_in_toc\"&gt;3.4.2.1 EEMD-SVRP 建模思路及流程\u003c/h4&gt;\u003cp class=\"bodyContent-1\"&gt;在图 3-21 所示基于 EEMD-SVRP 的指数预测建模中，以沪深 300 指数时间序列为输入数据，包括指数序列 EEMD 分解、IMF 重组、各分量序列的预测建模、加和集成四个主要步骤。\u003c/p&gt;\u003cdiv class=\"chatu_img\"&gt;\n\u003cimg alt=\"\" src=\"https://pic1.zhimg.com/v2-a0a252d52d36f623bc2836134e44ef35.jpg?source=f11ebe26\" style=\"width:45%;\"/&gt;\n\u003cp class=\"tuti\"&gt;图 3-21 基于 EEMD-SVRP 的指数预测建模流程\u003c/p&gt;\n\u003c/div&gt;\u003cp class=\"bodyContent-1\"&gt;具体建模过程包括如下四个步骤（\u003cspan class=\"kaiti\"&gt;贺毅岳\u003c/span&gt;、\u003cspan class=\"kaiti\"&gt;韩进博\u003c/span&gt;等，2020）：\u003c/p&gt;","title":"3.4 基于 EEMD-SVR 的股票指数预测建模","is_fold":true,"ebook":{"sku_id":"1536813021078364160","description":"基于机器学习的量化投资建模是金融科技和量化投资研究的新热点。以深度强化学习为标志的机器学习取得突破性进展，激起了金融投资领域开展人工智能与机器学习研究的热潮。如何将前沿的机器学习方法深度应用于金融数据建模与量化投资研究中，进而提出新的主动型量化投资模型与方法，是一项极具吸引力和挑战性的研究。本书从机器学习与金融投资交叉的视角，运用人工智能与机器学习领域的多种前沿方法，深入研究量化投资研究与实务中涉及到的重要建模问题，主要包括股票价格与市场指数的预测建模、行业板块指数互动关系建模、量化选股与择时策略建模以及高频算法交易策略设计等方面。","title":"基于机器学习的量化投资建模研究","has_interested":false,"price":"35.20","authors":[{"name":"贺毅岳","url":"","type":2,"user_type":"people","headline":"","avatar_url":"https://picx.zhimg.com/da8e974dc.jpg?source=f11ebe26","member_hash":"","url_token":"","id":362470}],"version":1,"ownership":false,"artwork":"https://picx.zhimg.com/v2-d975b3d775182636623da009b102d0d8.jpg?source=f11ebe26","tab_artwork":"https://picx.zhimg.com/v2-d975b3d775182636623da009b102d0d8_l.jpg?source=f11ebe26","id":"120326947"},"share":{"url":"https://www.zhihu.com/pub/book/120326947","title":"基于机器学习的量化投资建模研究","description":"基于机器学习的量化投资建模是金融科技和量化投资研究的新热点。以深度强化学习为标志的机器学习取得突破性进展，激起了金融投资领域开展人工智能与机器学习研究的热潮。如何将前沿的机器学习方法深度应用于金融数据建模与量化投资研究中，进而提出新的主动型量化投资模型与方法，是一项极具吸引力和挑战性的研究。本书从机器学习与金融投资交叉的视角，运用人工智能与机器学习领域的多种前沿方法，深入研究量化投资研究与实务中涉及到的重要建模问题，主要包括股票价格与市场指数的预测建模、行业板块指数互动关系建模、量化选股与择时策略建模以及高频算法交易策略设计等方面。","artwork":"https://picx.zhimg.com/v2-d975b3d775182636623da009b102d0d8.jpg?source=f11ebe26"},"recommend_card":{"category":"投融资交易 财商","product_type":"ebook","title":"基于机器学习的量化投资建模研究","price":"35.20","icons":{"night":"https://pic1.zhimg.com/v2-f64d738c3edcddd3ca4b95db8fc5d148.png?source=6a64a727","normal":"https://pic1.zhimg.com/v2-f64d738c3edcddd3ca4b95db8fc5d148.png?source=6a64a727"},"source":"盐书刊","chapter_count":52,"score":"","jump_url":"https://www.zhihu.com/pub/reader/120326947/chapter/1536837821600210944?continue=1","chapter_uid":1536837782323884000,"producer_name":"电子书","tab_artwork":"https://picx.zhimg.com/v2-d975b3d775182636623da009b102d0d8_l.jpg?source=f11ebe26"},"comment_count":0,"like_count":0,"has_like":false,"sku_right_type":"svip_free","svip_right_card":{"day":"https://pic3.zhimg.com/v2-4ed2fd42766dbac1ba5cffa1d7a01638.png","night":"https://pic4.zhimg.com/v2-1b83a176355a5370778dc7c5c14b9c62.png"},"can_comment":{"status":false,"reason":"购买付费内容的用户可以发布评论。"},"badge":{"content":"会员专享","icon":{"night":"https://pic1.zhimg.com/v2-f64d738c3edcddd3ca4b95db8fc5d148.png?source=6a64a727","normal":"https://pic1.zhimg.com/v2-f64d738c3edcddd3ca4b95db8fc5d148.png?source=6a64a727"}},"id":"1536837782323884032","pTagList":[],"nospan_manuscript":[]},"annotationsData":{},"ttsInfo":{},"authorArticleInfo":{},"annotationComment":{},"workInfo":{},"abInfo":{},"useFontAntiNewFamily":false},"error":null}},"self":null,"readSetting":{},"isKocPage":false}}</p>
</div></details><h2 id="toc-96">49. ITRANSFORMER: INVERTED TRANSFORMERS ARE EFFECTIVE …</h2>
<ul>
<li>链接：https://openreview.net/pdf?id=JePfAI8fah</li>
<li>来源：bing</li>
<li>摘要：The iTransformer model achieves state-of-the-art on challenging real-world datasets, which further empowers the Transformer family with promoted performance, generalization ability across differ-ent …</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">首先，iTransformer模型在挑战性的现实世界数据集上达到了最先进的性能，这表明其在处理复杂任务时具有强大的能力。其次，iTransformer模型增强了Transformer家族的推广性能和泛化能力，这意味着它不仅在特定数据集上表现出色，而且在不同数据集之间也能保持良好的性能。此外，不同的数据集之间具有推广性能和泛化能力，这进一步说明了iTransformer模型的适应性和稳健性。因此，iTransformer模型的性能和泛化能力在不同数据集上表现良好，使其成为处理各种任务的可靠选择。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>iTransformer模型在挑战性的现实世界数据集上达到了最先进的性能。</li>
<li>iTransformer模型增强了Transformer家族的推广性能和泛化能力。</li>
<li>不同的数据集之间具有推广性能和泛化能力。</li>
<li>iTransformer模型的性能和泛化能力在不同数据集上表现良好。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-97">正文（抓取，非 AI）</h3>
<p>%PDF-1.5 %���� 857 0 obj &lt;&lt; /Linearized 1 /L 6180839 /H [ 3693 608 ] /O 861 /E 641968 /N 25 /T 6175425 &gt;&gt; endobj 858 0 obj &lt;&lt; /Type /XRef /Length 176 /Filter /FlateDecode /DecodeParms &lt;&lt; /Columns 5 /Predictor 12 &gt;&gt; /W [ 1 3 1 ] /Index [ 857 431 ] /Info 373 0 R /Root 859 0 R /Size 1288 /Prev 6175426 /ID [&lt;502b702c9d8e436f3bfaec62934eb6be&gt;&lt;9e0b606ce671310a498649f07e25f851&gt;] &gt;&gt; stream x�cbd<code>�g</code>b<code>8 "y@$c+�dJ �� �] D��E&gt;�H�Y �SD�����`���?0�Hr JD��L���{ R� ����r,��E)�"��A�,'�4P��NoAl?�{��z!$3+�"_101~w��/�(9J�O2.x7��C�� ME�$�H���Qr�� � F endstream endobj 859 0 obj &lt;&lt; /Names 1188 0 R /OpenAction 1252 0 R /Outlines 1158 0 R /PageMode /UseOutlines /Pages 1157 0 R /Type /Catalog &gt;&gt; endobj 860 0 obj &lt;&lt; /Filter /FlateDecode /S 427 /O 600 /Length 520 &gt;&gt; stream x�c```b`��e`c</code>�ce�0�$���8���� ��=�����A�w� 9+�3B�s.�/g��õwA�]30�8�D �-a�7��v j� gdO 0p��C�Y�㸂�5&amp; ��k�Z����9�L�V/���Yy !a��l{�!Z,<em><em>��v]�� �A���N�u�nL]��r�����7I�.[Uv��Lej��n��|ߝu �a7ff�?��\΋o MJ]�<code>&amp;���0��M�S;�=hK�\�[w����6�!� ̌z��ǆȲٗs&lt;=�Ж��Mm�-Ϋ�Ԣn�P[zt͗�?_q^R%ڹB#L-��K�s�����}jQ�y�M����f?���cf�&gt; 7)���c``�c���*Vbkp&lt; dfh� a�</code>� ��I�^G��/ �5@�U _ ��d�S� 0�5�)� <code>Itx�Aƃ��A� j'��(�I1 vR</code>ra�h��i��A$���!�AȀ��a�~ ���+��OPPe�y�p�ѼA����a�pS�� �T<code>�g�\g� N_�R T�� endstream endobj 861 0 obj &lt;&lt; /Annots [ 1253 0 R 1254 0 R 1255 0 R 1256 0 R 1257 0 R 1258 0 R 1259 0 R 1260 0 R 1261 0 R 1262 0 R 1263 0 R 1264 0 R 1265 0 R 1266 0 R 1267 0 R 1276 0 R 1268 0 R 1269 0 R 1277 0 R 1270 0 R 1271 0 R 1272 0 R ] /Contents 863 0 R /MediaBox [ 0 0 612 792 ] /Parent 1130 0 R /Resources 904 0 R /Type /Page &gt;&gt; endobj 862 0 obj &lt;&lt; /BBox [ 181.2772 91.58253 536.0453 449.0422 ] /Filter /FlateDecode /FormType 1 /PTEX.FileName (./pic/radar.pdf) /PTEX.InfoDict 1278 0 R /PTEX.PageNumber 1 /Resources &lt;&lt; /ColorSpace &lt;&lt; /Cs1 1279 0 R &gt;&gt; /Font &lt;&lt; /TT1 1280 0 R /TT2 1281 0 R /TT3 1282 0 R &gt;&gt; /ProcSet [ /PDF /Text ] /XObject &lt;&lt; /Fm1 864 0 R &gt;&gt; &gt;&gt; /Subtype /Form /Type /XObject /Length 1420 &gt;&gt; stream x�XMOI�ϯ��9����.!�U�Z�� � ��J@D�������� ������q{\���U�+?�=�%k,^,6�⩄bR��-}����@�Ξ�6O���i����;�w�����3�wtK7��f��  6q7�63���V�Av��P2��Lb�</code>��d ^��B��2��I%e�9�&amp;%�(Fî����|7:t���mF��F�� oT��S� fRf\��5��f�]�S�H�&gt;��o��݇{��ߐ���qʵ&gt;b �A�����HF � ��A@a⏵�-�&gt;��F$�a ��Z鳾�/�:</em>�G+\���O:_7</em>���ڈ�)Z�����&gt;F�M�ޓ8JHexy�lg'$����l�/�u ]����v��� 7S0�V��7����F����� hH6���ߥd$��g�V�1n&gt;����  ��=j&gt;����� �dAB�EŴ��[MH��$6!dV�r�r��.�����Ѹ� s��/�o�e����;��c0C$�H ��R�����7�.k��=qQ� �s� biu9h��s^�����F1��l�+�� ��^v eb�5 J�ry�+c��6.A;F5� �&amp;�x�rHr6�b�G ��|[���=<em>&lt;ګ2Ό���C����"��5�ćCg=���+�E��@rH:�X� ���ENqr@�?u D�o����� �� .L���U��i�� �����q2�7�c"G��Bhp�t����<code>�*��p�y&amp;�8.�_.�J)� ����1���@�*�����Um</code>�����p��{r�{���T (Z��=��{� �w�E���-e;o{��]�z�����������5)xe Ί._��Zwg" C�d���̀�9$ ?�ӧ�N�.��˳�NuxI9y�M}V߶K�b"GWEw�U</em>ZH �K�{��i� .L���� �&gt; f���qI| ��9 T1��#�� ]��}�+w�M�+�͔�fC��%���l-����YTP��U˱/�сpW�di,�dD � ��=I�:3\T�gcC� ��t� �V���h  0 �MC1���i: ���(Y�Oz�6�Z<em>냪��~�c������/�� �Ӣ�n���PS0[�ܨ�0�� �8��m/�վ�(��a�;ߛ��FQ� ����<code>���</code>t\V_N �����Vi���(.�3I�SJ���F~F �r&lt;���KM+F&amp;(�v$�U�q���8�Fv��<em>RpV�ؚ^�,r��Uu��[�6�z{\�Jئ��/���?v V7S�I��O��</em>���HϘW^�X46�A��H�<em>������i�%~]���ݖ�C�51e���[7v�e��(�"!<code>"G*cI**j*�zY�/D諃��4�2q��&gt;�O �[U/W�]�^�,��� endstream endobj 863 0 obj &lt;&lt; /Filter /FlateDecode /Length 3846 &gt;&gt; stream x��ZYs�6~����Ueq���%s8����1N�6�&gt;�lq�"5&lt;����� &lt;$:�$�* �F���� ��U������ų/�V���:^�ܭT���(�c�V7��/�w�mY�{�[otxy+O~l���6��Z� �mdF���W���- �p����/�n.&gt;\(</code>!X)YQ�i����</em>��v���U��,]=д��d1&lt;�ջ��/�?8݇2~�h#�C��3x���W����U��&amp; �q3�1��|��)��X{/���R� ���F��6�yC��h��ťU��4֎�������� � %���+�D��ܚ� ^/.�?N� �7���y��U����ċe&amp;��Ox�Z��1a��t�Q��:���� �� "�W���w=Ȗ6c��d��G��</em>���<code>�gB["j�����e��qs�¿ �㩃c? F� ������2�n_&lt;�q�-/��}4[����!�RK�ST]^s�M�,� �}�7EuqgP���# &amp;�$���*/G-���? �I��FC1����K�_߉���a� ~5V8y��������׾���cU�u�}\�ȳM[t�(r��-�[8�Ui(o^�jIfp��E\��������ս�$�~���qЎ���}ݟ/��U �b��B�&lt;�$J�ɲ</code>��Dp��|ʢ����r����6�F��O:XXD�<em>�&amp;r_�?EpD� �l�ND��]�o�%�U��i�g���H���eIz� �n�s��&amp;���y���1����t|14�t�L0��Y��� <em>�<code>�f5�%����,�f~� 'B� B��!�+r����t=F�m��� &lt;��v�.4~%�� l|us���'�z��Z��л�AR�b�@�*����p����yۉ)�ޡ�ٲ������n�V ����y�� � 9��)�i �" ��1�7�}��m�79��$ƅ ��z��%�����7k˫8 ց���[�0�����"KQ��o'/ o�S3�T��:a�J;q;9E�0C� �</code>�z]�OOd1��}Y���G[� �V����,"Z�3;�M�</em>)=g���#��E��=:W[�K�1%�i'q6'X�=yvb�ʺ�7讵 l�<code>l�����ͷ��� � o���G~ ��+�����ϛ"�HR�D����ym�8@^�.? }Ǻ:�=�%AƼU#�8�p�}PL ��#�1Vqc���������܆</code>���m ��  ��r8�����|��8��S����O�<code>�jǼ�1jo�[n��rCd;�tfص���&amp;ߑE�</code>^��.��w�7���X�h�hE</em>� �;&gt;a\ �/�f�BN��d�y��[�ۉ�0��!z-V]R[�M���- �̀�L3�.s� ��sZ�� ����;� � X� �cD���C� �Mz�@��Z 3,C|�+�з ���u�=������}c��<em>�8��/P&amp;4;�� �B���ᢥ͛�<code>�v��g���)�W��G�o����ASl� ��l,jAo[�9�I��3� W.m�fw$�ڽϏ�vc�Z��}�z}��G3%Q��</code>ݼG3-�bl \�(Z�c�V�a� 0��k;&gt;<code>��  ν�10�x2h��+&gt;J�����}s�[��5��N � d&gt;�@׶�</code>\U�,MÜ|���;�@6�ȝy��1�m ��Ạs��VP�A I�V�,T�n@�b�����f���Iʕ̿-n։�4���]�7SB�����9!g��k�ñ|&lt;�i�W�cY8w @�02Ω��v��� C]e;�fO�nC�+��-�A !T.^t��at��jɆN��$�� �V����� ����%�B����ڤ����;H�b�c���W»!�@�4�a���L�A"�ݲ����%�1��)?�̜�Gq+l�� 8&amp;� �@���I��CB��z<code>�e�c��h����a�</code>!&amp;WE{p��m~$�[ �$�23��b�ɣ?p�,���By�� �'�KN^'�xI�BQ �k/�G \�4��C�^�'0 y�  </em>ih�s��M~ j� �|��Z:r�w�� �p&amp;�0�u<em>At :S�t9�� lG��, =(g�0��-�&amp;o ����� ��� � �^9w�bk�L���|�].X_U�M�=kJ��:d�P���.G8�K����B� m� ��02�l���?�u�w���Q�,ĢdំQ�U�X"X~[J��]��X@�n �����9�oE$e�}� ��Ϟ݃V��X�x�C�����₏��X����\���ʖ���� ���Fd�ah�6�PL;)=�U��R�k��"��R�^�n\� wT�ٌ�V�@.�ExO��d�RU+XM���}�Q)�m��6 �(�����A�^����0�-����t;7z��%C"F�EqO(rqO= e6)h�B��idf��&amp;hH���P�؜�с�3�&lt;]�"(r��:ѳzؖ ��;��A�yw�� 8,</em>��b�Ƃy@W��d v�c�֊ �b�� ��? �� �Aǰ�4 pD(z|x ?A4'�r�� ~c�ؕ��%��@%�W��sv9h9/#�mW�-Oo���9�Q�#T&amp;.�� ��p�C�ϥ���e^��|Ѓ��Id�ǭ�ljL� * �=�����@���W�n�4|!O&amp;��(�����B�r���^�-W���}<em>&lt;� �  �E�2.n䁧���� �\[��!��A�o�"(�U{,+���1퀣&gt; �� +;��Tr��a^0�f z�m�s��<code>��Հ�H��k�Ltf\��I @��/�B���'$�I��;{,��.�zǼh � ���$�A�NQh7�a *F�&amp;��7太V��ZC�H� �</code>?���7,uQ���:�P옲�ݸ�X!w��<code>���f�m����sWNS�7�3n�-���2s}�2'~h&amp;�Ƙ��c���� ��X �B#�)� ER�'� �[��}_q��0��k�*�M�|�,v�3�8�b?t/�K���i�= ���i�h����{&lt;��R��� yg��I��*�#}��P�I2��e�А p�� 0��D-.���E���P�QUx��D�MebB@@� ��fk� f���s�ۆ'�i� A5�)�EN���T�ɜz ���lrBPR?������MF�㊟�S</code>r�U�F�}��W��$\�$4��B�-�H��@8<code>���V�Ôz1T(�����t��� ��%�O�����-�&lt;��\�L]��ՙ,�%&gt;+�!+��cwU��</code>Vv8p��$<code>9�['d��S�I�;�}Q ��R� �������X41�Bi������V���������n螸��Ŀ���@ �3��� &lt;�t��ඦ��t�0�� � ˡ�G��U9#�"?[��Ӌ���I�G���:����~ƃ�s�ky;���fV�1hU�(r�'���^���J� ����C��l</code>� ��x��@R�;?��,3�߬��(MI�<em>�$�,Ee~��</em>�!w =�iK0��6 7��������D��B��b�����K��g@�;'�'�� )Xx� endstream endobj 864 0 obj &lt;&lt; /BBox [ 0 0 781 718 ] /Filter /FlateDecode /FormType 1 /Group &lt;&lt; /CS 1287 0 R /I true /K false /S /Transparency &gt;&gt; /Resources 1286 0 R /Subtype /Form /Type /XObject /Length 278 &gt;&gt; stream x���N�@ E�|� ^?�㞆����j%�6�F��8�@�Eӌ ��=׹��06�h<code>��Y&amp;� '��@�T�@tBWSິ�V3��$����0,���� �@u�s:��utx:ϧ������zZyy:�#1��� ����!��6rm�~k��i������ ��KĂ �+uc��4H� ;��U��gA�4� i�1X�,�t� �k�a_1�6�T/�"��ѩN*�è��l�ޒ�\�Q���[&lt; ��t�D � B��o%�� {��5F֊c��In��z��� endstream endobj 865 0 obj &lt;&lt; /Alternate /DeviceRGB /Filter /FlateDecode /N 3 /Length 2612 &gt;&gt; stream x��wTS��Ͻ7��" %�z �;HQ�I�P��&amp;vDF)VdT�G�"cE ��b� �P��QDE�݌k �5�ޚ��Y�����g�}׺ P���tX�4�X���\���X��ffG�D���=���HƳ��.�d��,�P&amp;s���"7C$  E�6&lt;~&amp;��S��2����)2�12� ��"�įl���+�ɘ�&amp;�Y��4���Pޚ%ᣌ�\�%�g�|e�TI� ��(����L 0�_��&amp;�l�2E�� ��9�r��9h� x�g��Ib�טi���f��S�b1+��M�xL��� �0��o�E%Ym�h��� ��Y��h���� ~S�=�z�U�&amp;�ϞA��Y�l�/� �$Z� ���U �m@��O�  � �ޜ� �l^��� ' ���ls�k.+�7���oʿ�9�����V;�?�#I3eE妧�KD�� ��d�����9i���,�����UQ� ��h��&lt;�X�.d ���6'~�khu_ }�9P�I�o= C#$n?z}�[1 Ⱦ�h���s�2z��� \�n�LA"S�� �dr%�,�߄l��t� 4�.0,</code> �3p�  ��H�.Hi@�A&gt;�  A1�v�jp ԁz�N�6p\W� p �G@ ��K0ށi���A����B�ZyCAP8�C���@��&amp;�<em>���CP=�#t�]���� 4�}���a � ��ٰ; G���Dx����J�&gt;���� ,�_@��FX�DB�X$!k�"��E�����H�q���a���Y��bVa�bJ0՘c�VL�6f3����bձ�X'�?v 6��-�V<code>�</code>[����a�; ��� p~�\2n5��׌���� �&amp;�x�</em>���s�b|!�   ߏ ƿ'� Zk�!� $l$T����4Q��Ot"�y�\b)���A�I &amp;N�I�$R$)���TIj"]&amp;=&amp;�!��:dGrY@^O�$� </em>%�?P�(&amp;OJ EB�N9J�@y@yC�R �n�X����ZO�D}J}/G�3���ɭ���k��{%O�חw�<em>.�'</em>!J����Q�@�S���V�F��=�IE���b�b�b�b��5�Q%�����O�@��%�!BӥyҸ�M�:�e�0 G7��ӓ��� �� e%e[�(� ���R�0<code>�3R��������4�����6�i^��)��*n*|�"�f����LUo�՝�m�O�0j&amp;jaj�j��.��ϧ�w�ϝ_4����갺�z��j���=���U�4�5�n�ɚ��4ǴhZ �Z�Z�^0����Tf%��9�����-�&gt;�ݫ=�c��Xg�N��]�.[7A�\�SwBOK/X/_�Q�&gt;Q�����G�[��� �</code>�A�������a�a��c#����<em>�Z�;�8c�q��&gt;�[&amp;���I�I��MS���T<code>�ϴ� k�h&amp;4�5�Ǣ��YY�F֠9�&lt;�|�y��+ =�X���_,�,S-�, Y)YXm�����Ěk]c}ǆj�c�Φ�浭�-�v��};�]���N����"�&amp;�1=�x����tv(��}�������'{'��I�ߝY�)� Σ ��-r�q� r�.d.�_xp��Uە�Z���M׍�v�m���=����+K�G�ǔ���� ^���W�W����b�j�&gt;:&gt;�&gt;�&gt;�v��}/�a��v���������O8� � �FV&gt; 2 u����� /�_$\�B�Cv�&lt; 5 ]�s.,4�&amp;�y�Ux~xw-bEDCĻH����G��KwF�G�E�GME{E�EK�X,Y��F�Z� �= {$vr����K���� ��.3\����r���Ϯ�_�Yq*  ���©�L��_�w�ד������+��]�e�������D��]�cI�II�OA��u�_�䩔���)3�ѩ�i�����B%a��+]3='�/�4�0C��i��U�@ёL(sYf����L�H�$�%�Y �j��gGe��Q�����n� ����~5f5wug�v����5�k��֮\۹Nw]������m mH���Fˍe�n���Q�Q��</code>h����B�BQ�-�[l�ll��f��jۗ"^��b���O%ܒ��Y}W�����������w�vw����X�bY^�Ю�]�����W�Va[q<code>i�d��2���J�jGէ������{�����׿�m���&gt;  ���Pk�Am�a�����꺿g_D�H��G�G��u�;��7�7�6�Ʊ�q�o���C{��P3���8!9���� � &lt;�y�}��'�����Z�Z���։��6i{L{��ӝ � -?��|������gKϑ���9�w~�Bƅ��:Wt&gt;���ҝ����ˁ��^�r�۽��U��g�9];}�}����� ���_�~i��m��p���㭎�}��]�/���}������.�{�^�=�}����^?�z8�h�c��' O*��?�����f�����</code>ϳ�g���C/����O�ϩ�+F�F�G�Gό���z����ˌ��ㅿ)����ѫ�~w��gb���k��?Jި�9���m�d���wi獵�ޫ�?�����c�Ǒ��O�O���?w| ��x&amp;mf������ endstream endobj 866 0 obj &lt;&lt; /Filter /FlateDecode /Length1 3264 /Length 2251 &gt;&gt; stream x�VYl��c���&amp;wIJ�H.�K��R�KS"iQWt��O�.�H�;�l�� E�6I��)� ��@[@/-z&lt;�Z���&gt;�%̓[�F�a�A � ��,��Jӧvf��=�o�����K�� �DyO,�/#��U�n&lt;�x�� �L"�N��?ٱѿ�7N�@��e�ӧ�.&gt;�n�A�c�����$vvi�����?��87�t z(�]h���W.�&amp;�C&gt;�ς�� �A����6uy�{��{� b;}�^���q����}Ծ�~L �7�=���o���(B �o�}�����1</em>�� �I���<em>�I�y��&gt;߄<code>&lt; 55��� �+XDu��p��/��N(x݃�,� أ�d�tf���Bg���Z���mf������D�.�6 {��k�&gt;h�杈���x���y���F�^�m�7.���np��-�M</code>� Q�'}ɀ/��o�o⛶�2�W� ��#y�r����e��j�S���D/ŉ��唛��qj��D.�2{��W��ZK��&gt;����=Fvv�77z .�置�|�w��N�h���;"z��7=��g��x���ـ�ɵ6�-�1��@ʊ�zɨZBAQҨ�ơ&amp;�B �"aX�� ��s�bm���q����J�#$6�</em>;9�:S��� �aZ�0H�t� o(��d�8|g߭�(�F�~'�8R]����;l&gt;�=1�h�-V����[0��}��<code>4 x��ʓx�8��gc;�q�l$���[�?�JI |ԞU��-�Q� ���B��y[ �CV�J@����p�y�a��2�,Ъ��^�v ��(���=���_1"~_�!��=� �B�$��9���pgk�j��2!?���P�o\�\Y��*� |��%B�yt���+����+�#�bJׇt���� �j{����S�@�:�ˬ}���3T P��9��@� � �JY�: @4��o&amp;k���]ceb�n  n&gt;����x��7�k�/Ӝ-�1���!�A#�)*�&gt;U+��|�</code>JT�zN��ȥ�Zy�� ��|��$���,3s��@<code>�� ���y��~O_�,������bqt[��Y��@��M�G�� �P�� *�; �/T&gt;�w�i�1�8ࢿ\�</code>��Uar���� "ӛtC{�i��<code>Y��o���#�V��Jݮ�˙��)QN�Aa���tQ��:]a\�K&amp;��$Xȥ��S���;�&amp;f �E�ś���/~��� l��_�����o�zEf�8}���\r���_�Ä/}���B�}�S�a���� �]�!˕���BF̴ �&amp;ɍI��!7,Eem' V����U�*�HO4��07ߟ �V��e��&amp;H M1doH5R���U/���.���|�'x���QK��J��f���rsaAH�J0S '�S�����]i�� 8Y ��O�9 ���;��L�� �bI�V��� S=�n����Qm�d#�_��v�� � ե���1�� �����Q�zlL����QX�I��;�׍2U�l_��z� gU��gޤ�712Q�U��҉3�%�TŸߎ f'�X��- u5���� N\|6d���#��P �� "�ж�y=�H6�D�b��  �Gn�nl""љ��Y�_:�\^-�0lG�� ���v��.��vh^ߥ&amp;fdBʯ �&gt;�k&lt;�lo�M�-S���6�G��PD�lt@�\7vM��&amp;���RגI�U��nE*v��.U�7,���gZ���Y҃q��m,�*+-6�����{�Gt+ [\4���7[k��VGc� P \� v���,�\�|�O�*)��t���c���n�[G�¡�&lt;���/��&amp;+�GO�&amp;_|��0� �&gt; a �E�WT �f�t�85t��O��؈���D�u��Q�펈�P�� ���'�[ؓ�6��f�؆��Е5����r�(y ����+*�5��dT��¶�JG�� v ^ IS:G(Ƽ �~{� {�M�&lt;�bI��{ 0����</code>y8%�|��}'6�s&amp;�Qv���{|Vo�8 ���S��]v&amp;���1eQ���@U8c�� 8�O�i}ڍ����+<em>+vm�2�߽|���|a����� 1��' endstream endobj 867 0 obj &lt;&lt; /Filter /FlateDecode /Length1 2196 /Length 1590 &gt;&gt; stream x�UIl���YDs�l���3҈ R�!ǔHJ��P�Wʭ��M$��F� �i I�Kz�!h�&amp;��EQ��H� z(zi��PE��{�!N� �7$�io}�����������n�x�� DQ��A�~ �ɽ���N�q�kW�όu�/��k�0�w@� nݞ�@:�ן&gt;���Ġ{b ����������}��7��pk�W@6ǿ��o w��4�摧=N� ����~{)�������nǋ��O�߇�����&gt;D�U �C�&gt;Dq�<code>x���p닶��!�� Ge�#ET�,��p xx �^Y [ �A� ��l{ ��E b�&lt;����#ȏ�mɨ���7Ї��_� �:�F~GS� ��.S��p6���(����35���9� ��Ũw��K��G�u�U[�DN�齜��&lt;.�D:��|��c��{H@B��&amp;UE$1D��E\1z�� �)��'�uw !Ѹ��d/�_��_̇%�n���&gt;������������� O{/�����o� ���n���UB8�EY�s���6�E_� ��9&lt;P�a)I�^qU�s�����[��_��Q�c$�͟C ���ƇA�!�W���I��v��� � &amp;���̅ec�������Z�K��&amp;�ֿR�?W_��T��R�JE(��V�";���� �92K�pH$$������R�Ć���d\�������?R���Ս6^�)�^΋�� �?���3�k�;)ը�ꢝ- ��p�3 �9�����%S� N� �a 1��&gt;!��撟W�3��t���o̩ G��b�:u4%� a�HL Ɗ��\C�0����a��z B�$�jV]m���^u�l+ol]�F�S��O^�[˺�b��� �3e�����t4����� {{)�\ ��~s�Ƙd&lt;|� �A�S�p� e��9��� #���%H|U�����hy�rO-��.Ø��9���Tr����#d�{�^�Ȕ����G�QG� 8|9���)��� �/�ة�5�zP�����[�"� A��i[���M&amp;��@@�K$gg� ?-qfam��ZJÍڥ��:g�{��O �R&lt;^j&amp;TK�$�R�eO���n]UyF�*�L���gk�ʹi� F۲�6� ���w��� Ml/�0{F-�S0�</code>�j!Ȳ�(o�ꆅ ��f���ih��0��d'�</em> Q�4w����ε �'��~������V�Y �$�������]�N7��5H�7��&gt;Ɵ�'<code>� |���P�(ԣ�D$� ��\��+6����4�G �c �?s�ֳ�~�we����M�oWJP� endstream endobj 868 0 obj &lt;&lt; /Filter /FlateDecode /Length1 3632 /Length 2286 &gt;&gt; stream x�W[l���.�Zr)r���|-ɥ(&gt;$&gt;%��EIT$ˑ%K�帰L�NeG�e[FmA� � 4 ���p��'@�"�G��G��)P�h��h݇�q�"���%#ˏ��g��s��s��ݺp�4��k�"�ɍ�&amp;�� W��~�Y�&amp;K���N7O6� ��,6.�]�غܶ�a\Z?w�}�$�Nm4/�� ����͍������湋[��� �������^4��� ���� ����|u�Y�7��;솷�K����?f&gt;i]oݣw�p���?�o蛭{���i]��6������{(Gmh��v_ {��t;#��} z�AnX+b Uh �� z��*��4�$��ɁLh �I4+���8���M��Yv.=0��m�nG��.$��;� ��܍����*��Ǩ���K�}"�U�.�GSt���~ƭqoq����ۀ'P�Sd�YX w�a�Ӻ��YZ��?-t�O��[��p�"Be�G�p&gt;W*2D��� �B�B\�� �z�W��\�Żؙ �9\Y*D�2~���DO������W^xj��֓9�eЌ�7o�V1J̃�J�o-^��������J����E8-F��-�+�TA���w��� U��2|�W=�,�Jm;�� 3�*���rz�EG�Ҩ� ����� �1 �0J�� hT㱴l�8��� %.��m' G��e�E��"����P�b2��.�ۄ��/3����&gt;���a|"D�J��' �/^������ ����"</code>]����};���B(%u8��Y�! 0� � {�=�|� �r��4fT�q5b�k:� ��0kt��}�ط��&amp;Ɣ�)o��+���&amp;;|vY&amp;&amp;3���8 ��+�r�9�2?j��c_���˄|��2ت,���XVo|��AC��D��!�lO�M�F����9I�������L<em>�+�&lt;�a�SL��~����F���C�x��p�� v�Cwㅠ���2e �� Y{=������o�3 i�b��� �wtT� �7��cJ]�Ԉ&gt;�X�O�f</em> ͌�'�&amp;��f�tH� |7'E �FR�s�Xm!]ZP�]6'� .Ecٸ���-� ʢ:���XI�cjnf�0_����@�x�'x��g��k� ��N<code>��S�}U��8S��Ĵ)�Ld��� �&lt;���� ]�2h�P�v� LX�lX�Lx���iGǟ2����?�B^ C(V( ����'��u$� 晰�H�0��5 �Uvk؛�z��p��ds011�K͂�Tr,㐫�xIuy�R$9�3��b4�k6�&gt;� N9�i9\�D�Z-5�Q�� ���FBqO,� Fd���twE�VW���� </code>�<code>���RKY���|��s�~ג�T*�g����l㥉�̩�?�X�� �:�� J�7�ǚ��&amp;����&gt;�) ���^Ϩ 8 �~ �~}(����+˻w��7k ��#W8 i� R�������N�����&amp;�V|71�W���n���2^?1�:����3�� ���r�| Ua#k���&amp;�@9pd�;$�e\a�|������)J�l��}%��b���S�������a6u����Bn� Ď.��V���?����&gt;yZ%��x&gt;! ��ݹ�+3�H�B�i&amp; �a��v�� "������D37����E2~}����ҭU�� i��ЬJH�Ʊ�{� �����Qס.T�n��� P�A݅� �کj]zb�&lt;�T2�p�u典���)�4Y� %7�k�֫��JN�窉�:��z��/�ݢN���M���Dd�Ɛ�����֌�_6��7;�����] %㾠۴�0�L'w,ʻU�[�p��df(�z�1IM��;���q</code>�c%�R2��W��p.V_��k �� ~l����| �c �G�Ɂ@�Ξ��r1#o9=C�R�L�&amp;��N���?y4{�Z��L)}dj ��������/&amp;NM�<em>&gt;</em>]mD|#<em>�� ����o(�9s^� &lt;�Q���:���</em>�7s�b��Ǖ.�PQ�C��F ;#r�b�Z,&amp;˞�C�<em>ezR ���ٓ�#�&lt;���<code>�r�a����j&lt;���#���</code>� �:h |</em>�_-� j�Ԉ����sť�B�+�ɾ�p<em>Y� �=ɡ�ŐߧH�:�J��w���A�V&amp;��X�=�i�.��W</em>륀�����@�PQe���˄b�;$��m�T�!�Y"D��� ��2��dC�Jm�S�8x��<em>���&lt;=2�P����5� �����m�d�܏h^X�P��,d�&lt;</em>�2�5L�i4�f� k -���:�����a��f���69p<code>s��F3=~n��� } endstream endobj 869 0 obj &lt;&lt; /BitsPerComponent 8 /ColorSpace 1279 0 R /Filter /FlateDecode /Height 1 /Interpolate true /SMask 875 0 R /Subtype /Image /Type /XObject /Width 1 /Length 11 &gt;&gt; stream x���? �� endstream endobj 870 0 obj &lt;&lt; /BitsPerComponent 8 /ColorSpace 1279 0 R /Filter /FlateDecode /Height 2400 /Interpolate true /SMask 876 0 R /Subtype /Image /Type /XObject /Width 1 /Length 54 &gt;&gt; stream x�Ё    � � �Pa�� 0</code>�� 0<code>�� 0</code>���� ��� endstream endobj 871 0 obj &lt;&lt; /BitsPerComponent 8 /ColorSpace 1279 0 R /Filter /FlateDecode /Height 1 /Interpolate true /Subtype /Image /Type /XObject /Width 2000 /Length 50 &gt;&gt; str</p>
</div></details><h2 id="toc-98">50. Parsimony or Capability? Decomposition Delivers Both in</h2>
<ul>
<li>链接：https://openreview.net/pdf?id=wiEHZSV15I</li>
<li>来源：bing</li>
<li>摘要：In contrast, iTransformer and Crossformer achieve advanta-geous performance with shorter input ranges but exhibit diminished gains from extended historical data.</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-99">正文（抓取，非 AI）</h3>
<p>%PDF-1.5 %���� 1591 0 obj &lt;&lt; /Linearized 1 /L 10010015 /H [ 2885 627 ] /O 1595 /E 136444 /N 26 /T 10000195 &gt;&gt; endobj 1592 0 obj &lt;&lt; /Type /XRef /Length 123 /Filter /FlateDecode /DecodeParms &lt;&lt; /Columns 5 /Predictor 12 &gt;&gt; /W [ 1 3 1 ] /Index [ 1591 315 ] /Info 999 0 R /Root 1593 0 R /Size 1906 /Prev 10000196 /ID [&lt;182bc637560c9ce991c88cfeb28b6401&gt;&lt;0ad39b991b2085280ce95a628f81e095&gt;] &gt;&gt; stream x�cbd<code>�g</code>b<code>`8 "���H�6�T &amp;A$��� "��@�Z��1�r�@��� ��r D-��L �E�&gt;fCȵ</code>68�(�������T�KG�Q�X����a� ~$�܁w�(9�I �,- endstream endobj 1593 0 obj &lt;&lt; /Names 1590 0 R /OpenAction 1855 0 R /Outlines 1783 0 R /PageMode /UseOutlines /Pages 1782 0 R /Type /Catalog &gt;&gt; endobj 1594 0 obj &lt;&lt; /Filter /FlateDecode /S 523 /O 656 /Length 538 &gt;&gt; stream x�c<code>``b</code>�ve�2V3H0@��� ,  ��a�� �<code>*Yf2�g��&lt;�Aw7��*�P?#�)��]� � 0�]a�</code>���ٽ�ɡ��3��kg�I.O ���m�' �N��E�Na�H�PH�4���r�sJ �G%)'�iY^v�=��(@-���s��b�ପ,�C���w ���%�����Ʊ Mi󺲴N{�g.�euJ]v�����z��u��� �"�).����#�s٢5[v��tS���E4B%��){�1N!N[�&amp;�"�)�/�L=�R�j�3�S��'F1<em>�S�l� � j��6]zo.|!F��% 33A}.�</em>z�G�Z�צ�bԢrřk&lt;�f.<em>4i��g�2&amp;�� E���=�wl��0��^0C���cT$E�g<code>&gt;�0�F�Z��� � �x��"Y����R�d�zf;�3nd��T�1�[� ��</code>�����O�ϖS����4ٓ�TĤ���� VP����a��x�� �s�� @x�q ДZ�j&amp;+ƭ���������98]�, #��j endstream endobj 1595 0 obj &lt;&lt; /Annots [ 1856 0 R 1857 0 R 1858 0 R 1859 0 R 1860 0 R 1861 0 R 1862 0 R 1863 0 R 1864 0 R 1865 0 R 1866 0 R 1867 0 R 1868 0 R 1869 0 R 1870 0 R 1871 0 R 1872 0 R 1873 0 R 1874 0 R 1875 0 R 1876 0 R 1877 0 R 1878 0 R ] /Contents 1596 0 R /MediaBox [ 0 0 612 792 ] /Parent 1738 0 R /Resources 1901 0 R /Type /Page &gt;&gt; endobj 1596 0 obj &lt;&lt; /Filter /FlateDecode /Length 3292 &gt;&gt; stream x��ZY��~�_1/�"�vGs �C$˖���(�:v|T ;�r<code>�A������h</code>8��Vŕh4�&gt;���s6��|}� =���� ���I�����)� � �� �&lt;��N:w������� ԩ�q���9~�I8I��y :7k�����-D׫�mv˫ �m��b+nU���</em>�W!Ⱦ�E[o�^ �m��KY)��+�Ȯ_�t�� b7�3����&lt;�y�/ڡ�)�L}�6��Av5��,�l�j�o+�)�s��2�I(��B�j6���W7G:I��MB�� ~�5^;��C7����z��Y�T)� ������ ��%��J �i6̫?y�N�0��1�4�$~��|� �q'�5��l7LK0���� B�j��V�T�I�.}�<em>��ča3������r��Ô�!��s�8���αX7 c���j!t� ��Q� ��:n���G�;�����Ls����[���ŹM$i4�D�8Ϳs�N�QO�z���z�G�';����s�Q"r�\�w�,&lt;�x�~����&gt;M|x�Z���2��۷�e#;1(2rh���ϯ��A�Rt��-�Y[7�O��m-��5�dwʀ]/�ਜ਼Z� #���� ��0F����6l���m�0^�h�| ڦ���D�ծ d�</em>�Hz�{n���{9/�jǡ���o��nǇ��ګBɦ�G��Y���EٴU�13~�bo����տ ��x�����G�� eN����;�"��L��0t�,= ]�^&gt;�l ��h�[9��&lt;�1������������x�(�w��ܝtǡ�������~� �iv(�ztw�y66��+�"������E鱻O8�g?W�� ^4��@P��<em>��v��x�Yq':W\�q?�E��� _ �q �CǸ�" f?��N����EǑ�PVfw� C�n �pS�� ��b<code>&lt; @.�� �&amp; ���,�hq�zI��d��vW��3���C�Q�(�λ�m%; ���Ժt�d�Jp�Zvшj׫���KE)H+��Y��}�w��'A�ʧA6=&lt;'�</code> ��f;L��_Fr �P�L�nQB1�x1��Y} x@��,��a�}���Xz/vSrx�~�m�V��]7J8׵.���n�V��]�t��RU��<em>��V$ړ��G5�8�j7�Gĸ�ԝ��� ��c2/a1U�ӯ�) �_�bhȦXq�o8��</em>�E{�����1����;/� J ��q�G��Yq�mìN�-���򔅽���e� c��oݮe�CTC�&gt;�1�Q��(a�7�U�  1E��y�е��0AjC����Z!I�3+�%Nv�ۃ��vk ;�Ƈ�{��bv���Rݪ ^GZ� �0 )��L���h{�� ӯe !ao�F'-a�Ksc�e׎��0�g�ہ0�� ��ӈN84 ����R[�Z��98��ׇ4]�&lt;�q�/ �����������@��a�� p� A��8��5�0�� c���Є#<em>6߉��(r�h�ݏ[ �"D��/]{�D�C�&lt;���s�s|���E�����@��?���D/:�8$�X���a�ǅ�v ��:��ӂ�c�E�C�ΉP��ml�� �l]�]#j�&amp; MQ��{� �Cvc�-hL�mb��x&lt; 8Œ��l&gt;O5].����( Ll�ᕤ ��7�� �&lt;�=y�'n�i����{?���-)&amp;�� Ԩ���4�2�����o���z�̌U�vM'���o&amp;�#}��9�2��G�R@Lꆉ��P�� �J�~�.��m�{3�EO�L<code>�: ꆝ�&lt;�gi�O�PU���9DݪAT� nPP��u �g��l�#.� �  ���Cf������� �����Y�.�a���ɓ���Vi����}�Z�x���q�{��y� ] �a6ݞ�{�J���z,tdy�]�F�-p�&amp;�� ��5}}�bn�gH�C$Zm4 �Q�]��F����p��VV랇�d�8ʭZ�K �)�7f&amp;&lt;���|[X��6R(��&amp;�&lt;��0�=�\7&gt;r��v��CM</code>��}h1�D�eo�-�f� �������&amp;,LM�Ć���t�f�P�S௑����؃��q���Q�x�in9���T���U;b��ݢr/�x���<em>�qj��C�P��K�.}�.!B�)w~���O�˄s?7s^���фS��2��q$�� �Z�i�]fM��Pj�ިߦ˞�����}����P�$�k흠��2�O � #ܗ�s_Q&lt;�Kh�i��?��f�D0��mO8��K30�+Ո�&amp;�{ҫ�b6�%�ai B�P�Q1ؔ��%Lk�1Y���� �;���~��Ԟ���zO2I�����&gt;���+�+-��[�ؽO��okʌjDOAEl���G���7� :�piA�d�B����$~�[٘55&amp;��]�;�%��%��Тqm�&lt;��W��G�� �fm���Ή9�[jeSH���O.</em>Jҟ׆S�j�𝠰,�Rݙ&amp;'�܀��t�k� д�z�P�c(Oی"7�9�D�l�x �� �c���7=aC��I����H�Q ��� &gt; stream x��\�r Ǳ}�W����UO����%Y�6�j�C 8�"1�@���7���F� $��l�</em>����5+3+�du7l�a0�M6��7 �fd��E� ѰI R��qf(%�5��s��ݜ � Fra�!8�$ ���%�V���+2�縒Ԃ � �f��vp!� �P�&gt;E��a��~WX��+f�y��(yYCN E� �3� j� R6�W�͙�3r1��E�F�U�Ґ��x!K��� �<code>�</code>RQ�"8�1�����d����$�(.����9[���l�@af�y(9�V� ��m��H.�� ��q8�&lt;��z�3I���DX�8��C&amp;7��IY� e�aI�0ޡA6�rJ��r1;d��� �gY�a��eF�#��1[&amp;�3f+� 0[��d�VI-Xe<em>ܕ�1�C�� :�q�$�S#�V�-җ�$ΐ�� [���Yv�\�8���&gt;<code>$,Ň����gI|�Ѕ��J ŊƐ�Vf�Eǭ�W�f�Bﳑ�rp,��D�"�2L��JV���l�5�m��Xa6Y���?��������L2�W�����J�8bs�^&lt;}:�0��ol��� �y�:�8_��Gg���tt�ְx0,&gt;\����������\602���X���G���a���x+�.�т�!�U��R �5���������-�ŧ�� ���̿z�޾��zʼ #�M��[;Vh�Qv��.���҉ď�ʐ�+��h�p�n�")������7#��4:l�F�p�bO�9Q�&lt;9:=Y�����������[���D</code>�q�ؕ� s# �(�s !ބ��/@�v�l�9]no@��n P1�F ;^���</em>B�Mi�a��ҍ6g� n@��5��g��7���a��u޹1�{!ϻ(�K�m�ɼ{��- ����UOr�3 � ���|}�<code>�Y_ܽ'C-�o1�w�?��� ?T7�!�����]ଧ�8�_Owi�˞�� �=ݕ�{�4W�t'��{�TW�|'�ԙ1 ��m9m��8Ow1�E�Ԯ���f@�u@Ӑ��d){.ys�� �9 �}�\�&lt;�J��ݥּP�����d#�Vqݾ�~.S�'���� c�&lt; �|v�L}���_���ݻ� }s���t#�� =] �z�t�i� =�v�h��GF�]&gt;��( ���ζ�sY�L_#�áL��b(�� �$�c�6�?�l��R�M�:</code>���x�<em>�� �/��6i?�l�J�m�ӡLB&gt;]�J�&amp;��z p�,���g���Z�����f�,Zb�6�f�|���.B2m����Rn۟�r�h��l#���9</em>�-�'Q 췼�v�A�"� �<em>��=ܿV�B��z�����+1 � r�-.�1]�[ ���@�9#"s�I��K� H� E�A��Y S����/��8%k��}.c���� �\ ��, W��}c4m i+�2/p_�^QB�V�lP���F�N�#���ձ.�oA�,��<code>�#�Kfcn0� sF[�)���9 nLn���:9���d6�!P�� f�rgR������,����.�(�2���a�vz��6 ��tAT�^�u-ԉ��tb��+M'�/�ż��/ꅁ\�\s�(�za��5��t�.4�ػ(멾�bU���Z�;��#�9w���᝻,0�$��.�wM��1��ډ�}������z�t�p�ɏ���gx�ݞm��X&lt;������-��G���^g� ��\�&gt;j#ܚL���͒X�:�q���9 �F�]h�&gt;8=^?Z��(�Y=~�&lt;_� /7 �i6�"$5�4´��~�uW�OS��os j�E�d �K�k�,^E��x+�V܂���� ��[</code>ӭ��b�� �0y��<code>��{���˂�;Щv���P ��B8|�Bo3���eN:z��r�/��3X��w�Â^z�g]͟�!�C5i��� ?TS&amp;�PM��C�f��� ?T�:�P���C�a��� ?T�&amp;�Pm��C�e���K�P����� ?T�z�P���� 3~pq?����g��J� \}� �⇷��-~x ��m���</code>��}</em>����<code>�X���?LgW_�I، ���Ä���N(�ᇆ�~�����a���I���7�6�܌ ���C��!�?��� B��C(3~u?��d�!�?D����⇷��-~x� ��� �w��5�C����� ݏx~HSc�"F����d&gt;X:��\#^��m�k2' Μ�� Fo�i�����kj� �����MS�2�!M��x� ��</code>X��W ���[�� ����O. ���g��zwyz�x���g��z�S1{����cd'Hs���aqؗ��^��d?:�0��ׂhH&lt; :&lt;�7 ���O����/���I9��|���{���?����<code>}� �RŐ�F�V4a�k3�F'����tu���g�-N.V���ŝ���L�l��H�/�R���_Ipf�XA�� w�VK����G+�atbCB�#�̚W"����x�G�xS+O��W0@�pT���y� ���68Y��t[G �!�Y^�������C0�D}���k��k%��J�M ˁF�?f0�9�</code>�6E�%^7^ ����:��Bh ���&gt; y� ώ$���,��X�Y����h�ܾ��$dU3&amp;A����Ǝ<code>�o,��_b�k � G�!M��b� E���#� �Oُ��8b�i�^��!-��������I�"�/c�r)�^�#��e'iU�G��q��J��б �~r/&gt;��;������T��r���&lt;��9I��� �1�4�#�C�m�Fل� KY���&lt;ˌ���-�8�V�J���mR�k�MR �Ou� y�R-��</code>=.� ��- �.I ^��굝�8᪥���Q�B?��~:�����k�k;��H��)J �gC��ð�@��YJ��I���$�pD�\2�� �CJ���m/�K� E D���xs�\���W��2��+9g<em>��z��F���L�W�ҵ�M �B�r@Q~�; �V�qP�\�ȭ�5(��%��zc�:�ө����<code>�t��� �],�SK����Qv�oi_��E�b ��ʔ""h��{h+Ө Դ�O3�F� �]�a���Rˑ���-��ڮ�֑��S,���b�p���&lt;-�s ���m�yΨi!/��Mc;ۊlԟ��v�©؆�Q_����� �ӽ�^Ӌ_��pWKvt�ׁ.���}�p� K'�r�J�� #� �e��V����f!��5�����y�� ��� EN[���BO�[z &lt;@E��r!DQ�P�����f]�d%(�l'v��v,n�ܕ� G�� x��*�&gt;�䊢 �W���}�#:CN��J�} G� D�A�%� �'&lt;</code>���~�z����^�34�,S�l�m��X)a�� ~����t�Y�������;O�<em>��X�ڦ��-</em>�pp\а Sm����v�� ���]HPc�&amp;��� 2J��+� �&amp; '�_�j�bv��T �T��� H��|</em>(i-:MMpv��gY3 ���Q�S-)�Q��4�iiɕ} 1�������e��3�D���M e 3o[�$Ǹ[C �o�o1����� ��������W�Oi������W���N| �&amp;�%�0���<a href='���|�W�����E��!����ӯ�4߳ ���|����L�D#�~��M/gL���Y x�Ɵ� ��Ņ:"ƽ�E�^t �w�j���/��_ڏ�[�u�-ɉ��� ��ĠJ v"rS �g�@��1SmӶw� a:��ւ袨���J5i�S2c��q6!^6�a3�b��gY���ˊoAm�� �3% �&amp;� �G�&lt;�'>k�����q���+�k�TPF��Kx�rO|Y yϷ����|�:�o��g�?��0�����0p�r9&gt;�͔�����,n�i �1</a>qZlng���8����w1���I$�� � PZ��� ?��� �| ��U ��u�9� � ����J ����/@��:�� mv�;v}��� �m{��O6����w���)�WO��Tg ����� 3�-��Y�Dcϓ�W�����o ����b'ޡ�L�k��O�/�f��  Sx�=� �d��Iۙ�v{�� �~^\�Ͻ[��u��Ej�</em>��;��'�h��5����B�B��4v�P���"t�V{ � �A�Z�Z[x�@ǖ �01��dɗ�� /O�xn"���%�%�u&lt;�B0�i��6֨ ��y�� h���&lt;��a����:]���&gt;�{ ��4�-�:(��OoX&amp;Ƹ{�X:�3�ln  �p~� {3����� � � =U+��A �h���у�n� ��bw#����߾|������O;y�������i��=ݷi�t���M�w -�Z ��ݽ/߽s��w�\p�S6�v�����5�p��W-�[,��5Ŷ&amp;��ޚ���&gt;����w�����K�?h�" endstream endobj 1598 0 obj &lt;&lt; /Filter /FlateDecode /Length1 1643 /Length2 9661 /Length3 0 /Length 10738 &gt;&gt; stream x���P��-����0hpwww�$�  3��� ,��{p��@��� 99��{��W�WS5��ս�{��jh(�4����M@2�vPfv6���� 7�w{S'����n��w��)KۙI��ڂ����I��@��cwg�s��v��v�s�����&amp;̜ X����� y��)�&amp;�l (������� r��L-Y��rw ��d�m~������<code>���lz�A� ]@ ��3���? ��P��f</code>S(�d�C�'��d�?<em>�� ��y��l�?�~z��^f�v6�����_V)q)�? ��'!a��df0sp���y������E �� �B�����l�} ӿ<em>v�{��=࿓��?o-@�ϒ�c�f3}�b�^��B��6�w��ے�oA2�66������@[���߄�u�&gt; @��Yv�K���2� �l��^y(�Y�v�����������Ȁ�@fj<code>��埕�c��-5� H� ��ny�bc� ߳�L��� ����B����cг���i;S{�ߺ��� ����(�W������5���� V;{�s�go�����k�����6���� �Z��g��?� ���o��</code>����&amp;��g���o���ؿ�f5�&gt; j���'�� ��<code>���|���!���� /�3v���̅�-��t��l�ϥC���k���NNϣ�K�c������LQf�M�jۮ��I\��ƅ�S��8��� _@��'�~�if�/*��,���Zu�H8^g~�;�ܬ%�s�d&amp;��iAn3�t ; �yEJ9�����$�Z�ЩF�,��KQ3�&gt;ML�75=�񮴸E2� Am* _��ҕ�I�u�5��&amp;�Z[��o��m�6&gt;� KV�D�(�q��Z��60&lt;���K��A���:�--8H*Ņ\����q�U��镧�,�� ��I������&gt;�}ߐ�a���m�\�n?����t��:2 L�tc�������� ��OÄ� �ڢ+8O�2_ ���kc#���-� h�� k�ڱ �z9G��EQ67\��5�L6 ��ͳ|L{�Ga���'��"~�j�&amp;?&lt;�b��Qˌ@�Λ�0L��z�h�F���i�(s�x���|K �Q$n3+?�b�F#9�ͩyn5��{��e����}$�F�|�W�3�n��p�ob�?����Y�B�~ҽ��/��&gt;��s�X� dz�(/�����D�����&lt;Ï���ao�_m�;fj὜V���� �B���� � 2��) ���1�_W��d��C���v��i� ����9�Z��N�Đ19��Qƕ�"Me�S�����H� ��� ���j\/�U  �D���}YS��YYɻԀ�UO�&amp;� 67��i�K� �v��_mty��#�/����Mt=(!�g����1�W�� xĴp��.����F� p�w���~Ň#+�a��,oz����B��;��� ��x�U��K A�k]箪Ǖz�1C- b�ؽ��R�E#]TjA��u���B�&gt;�9%�czk�2)��D$���U��!�i�T�� F��p���"�*����e H&gt; �W֕ےt�Q�Xe����4F赣��f��߯¨��I����bq� ��z݇��)��� _2� $. ��������N��ZUf]�� � ��[��� 躌� -S&gt;��</code>�[�dO���F�O��ʦ 5�E�"�˨����(#G���=���9&gt;��J�-�L��}O�c� �n�r .��J�|�I� s'�QۜF;r��֒�v,�^���Nl� E'�K���Q�d栓�&lt;�"��M05F�fW��}~����_E�q�!��������P� � �</em>Ws���z�Z#��՜��kY{�� 4�9&gt;�L\���<code>�5��O:� E ����xT��G��� �v�3� ��J���Ι�����X��$� *ݨ�M��O �� }��Q�/��hpW9�t��E�&amp;\A� -�ԧ����(�x��x�S�03�:unF��������"�n����� �X7!</code>NIWG��X܇+la w;8���K=��ȵ ��2�X<em>��9�����K�5���/���-AUы��[A.�f,����</em>H�P�p�-bcqD���ZUp 4��-�KB��1�?����б9�T�s�.�P �����3�2�S� ��r�1.�aq���l���D�y�� ���=r��]4���ɭ�:�N�ٺ fv����� �ĄRVA�kd�i��N��KE�v���v_Y���[�<code>_$VRG)h�ǨX����D�7[��|9�'��J�MN~bM؎6�/�=�KC"��e��{H�/oa���tc�ƖX����*�tTaTX�O�Ɔ!z+A�8e U� E}�E����|�$5C5 ��5�H3q#&gt;a�N�X���-��&gt;ٞ*@���\� qL�@Ʃ����)�i��ѿ�k���ȍ�箍� �]Ue�3ih ��0S��iWT��Ϧ�Ә�y���|ɿ��&amp;HT ;������ �DcU)�� ~�J̔2'Hz"�&amp;&lt;\ ;:�%�� ;�S� U~�Z�@vٌ�~�� �ɴS�_�m�a$N8�u�_��&gt;��q-��U����=�R[ȶ@�v �'mm��� R��p�%q�;� Xn��ؑ^H� pU�?ōmWaB1o2��V�gĨq���x1j������Q��ז\eA� Y�IQ�gZ S��6� �/|V9�� ���g;�5;b�x��ޚA�P�D� .?g=�'6I~�5=_�+��I��:W#�FD"�h��m}�03 ��K�K��ŭt �� �㳊k�Q��)��GgT\/�ƹf���m�*\n&lt;Cܢ��1�f8�ϟvAC�/z����*�R&amp;3��O���d��K &lt;��ܝ � �_��|m:��FϚ�� &lt;8l�� T�n Ϙ�Ķ�Fb�6*��b���4�@��A}��_ [�d�u��-� i���%��V^BD&gt;��w]���Fl� �S���F�:T;YpZ�VI~���P�</code>�V��zo � ��m'l�z\�e���e����KBM�Os��D���}��&gt;l�Ji'�YN�<em>��+�[��̿�W�|A��j���GlJy4m�#�����fd�eղ�4��WrHg�r,YCȐd}� �| F��~��~�R�|�}\� jo�O;&lt;�yY�[��1�7��t?���g��A� . &gt;7D�:-vi� ɘ��Qk���n2�M����a;źL�̷T�j&gt;��Y���B:z���TJ��5�ob�}��q�j�&lt;�<em>7��Kz���d���������FY�P���Y�s����ʫ|�R 5Lכ4̥����g}�&amp;r�&lt;��G��V ����u�|�� ��+��w6��Y�p��P^����9%��T�u�Ԭ�!���U��Z |����O�/�K$�g����L+5���8\T^���|O�&amp;Ӵ��=, �{���IrOV�/c�ٸ��% =,���|h�cI ��6$o�l(�<code>-� ��;Ϛ�})f�!��o-AA+ީ_I��i���6�Xњg��s z���=d�R�i�w&gt;* �XT�,f����!�!�+^R�u�ޜ�u ��b�V�#�kb��l6�*�����"�x7KA��.�6n1Iӥ��D1Y}/|�</code>;׷����?��}&amp;O�����%=�����[�����o���;�L�k�7k3:w}</em>|�0Ʈ�ΐo7U����6H��<code>h4�|��c�K�l�i�P�����۷���dY�Ō�tq�n�l�� v{ ��͵C��&gt;C��c �(�X���� ��g_���.^ܠ���x�Hu�}r.L�x&amp;BEL�ƭrnQ�/���X 3���� �|$0� -x��ԾC�C��n�E�Z&gt;��~|��/�����y�=ٴ����D�5�Hr��5����-X�;�m��Ms]��c"f�k��И{U^]����C+���cm }Ƕ��Ý�(�.Ҥ���1\!� �j��H��b��1e�%uX���I���/R*�8�E�]�� ��P��&amp; �K��ܭP�s]��"�1���=, # '�Ck��&lt;6�&gt;��+�t��I�8�oa�H����eK����C8�-&gt;wE��.�����Zabۘ�Z5WKL�;' � �u�~z%��o��n�~��&amp;�nQs��?|cS�uau]u� ��8�7�h�nMK��'���ᤛF</code>2=�'�� /����-י�K�z�E���3h0P�M���w-.�cwˇ̣1���� ��}�D�Rv �e%</em>4�'���<em>���/U�O�Eg���;�F�[�C�</em>L z՘�lAFyrx���Q�c�R�1h�"�{ݾv�q�� ~�����D3�5 ߆�Rݞ�S�������\�����hr��F���r�2�p�~�� 5J}g__W�Cf��;�� Ի�B/���ܘg~榴)������P�N=|qc��� ��ʋ��5badZZT����3:�a%�'\�&amp;� 79a��ݟiU��Wq��F]�Ydc~[[�W�</em>���0:&gt;��v��<code>u87��mO����Õ��Ua �KD�A(dc��O*�\� �*c�'�-֦��� )���H i�������&gt;�G��.z������h��yx�M͈5v�p_t��9��0��=OP�c �r|f~;+&lt;)�҄�N%���L�|I򃘆���7��ek�7�hD'���2ȔE0���QS8*)��5�ҁfp�צ����KgJB�Q������0\�����"V��؅�Z]GJ;�}��u8Ӟ/ �}p�V�0t]�ť� 4��l}�,D�����*���D�g ������o� nM(酒C(���y L�(��ܷBȕ�� N_w��h-a�\���~X�/ �-���Ɨ�����O��N �M���&gt;~p�5R���� z�D���l��&lt;;7J�ֲ03�N�=ǂl/�n�;;v��ܫ1!�%�P�F�ztH�GzʌoՂ�x[�8�^��%j�,V~u�W�G�� ��(*�xLS�ڒ*��R��C �"� iG�m�N̝�k ����MM��e�zFjŠ�����ż�H��5����Wz�R�P�Q� �@���c�\ˤ�����]��!.O�}O�3��5�f��+�leJ#!89m�����Ӊ�����e6�T%FV��W�#� z�s(5�i���� u�]{�OJԥ��������%P&gt;L&gt;hg�qw}R�����z ��m}��b�W�5*��3� �u��#�� �Q9T�ǁDY�V��)fTEu7�o� Rv��{���E�:f8�ǭwgC���&gt;J�?Dz,�z�� ٶ��l(-p�l���K: �M[A���W��So�y�m v�N,f�W� z[Ճ��ˢWi�*v�y�g���ܤ �Nہ�m�G�ְ �rS������ V�&gt;##i&gt;xWs�S��V�6�݊��R�B����,�'�</code>���TL��|u�t��� ���<code>�l�W7B�Ҵ.�gbG���C�2^�'��u^I�H�$</code>��� JzV=ԡ��,�'��C�����(6}Lч 5_<em>�Zj앒OQ�8<code>�I��lr�.�ҍѫ0�T�o�����P�"�w^*�l�}�/If2���⮷��Ll�O�95�Qd�s�Q^Xv���39&amp;n�a�:_�g����d87��RK���:1Yn .Pvv�]�)3.� $�2�%9�z�e�����wE� ��,���%�T�˂ˑ_��� +��P �6n�j��C�Q���Muj݈vT��_v�SdFY+�Z/�|�U�+*�N]u9z.��걯�j�Y�4:Վ���|#��}��t��jj�Yb���QG � � �Ұ�,��5��+�q4&lt;�aY,}��´-��vw U��TqB������ YOb� ��)Os� ��޸��e;Ǫ�ϛn����#����=/j!����A ��w"�*k]ⵝ���[k���� �](�*���K[7=����� Jc�8i��0V���e� @ UK0Nz����� �޸�|�ӱ�v�{��Y��D�n�\�-�y�K�:v��~.��*��ߣ�������Ru^V e����v��#�</code>ԧ0�'2|�J��2�����g}�K CI�H8&gt;Me� , ��5 T�$)Z��z��2[�9 �0�k^�W�ވ��y&gt;����-'�ъ�5��K�KN��Ƙ�����rH�͙�N�JO����g�B,� 'lS�J� �G���f�},�V#$Acb�����.}r�A�� V��Fz I�v����X1���������Kn�?�G����2�~:���5v?�/_&lt; �s�/}E�{�d;mo�uX誄��' ���x<code>�(��������N���8�s�r%�6u��mL����xq�Xi��K��N5� +��_K� ��埩�^���p\�&gt;l��&amp;"�7t^����f&amp;�-.���u��U��q�.�KFUc�fC��_��}F�;?��J?@���f����</code>�u�� ��4z ��/e�$&amp;��NC��7ȭ5��kUb�7���@)��~���?�۾ I�0��ǡ8H� �5�+C�&amp;����t3I^���~��A��l�� ������P_Y����_0�==�v|,��3h��<code>C c+eiP����TvU�ܙ�g���/��q)=�.�j%�6U����Y#�w�� � �87g\RFrKY��KЄu����_vź2�V�)7 JRG�� � JCV�t���ce�P�x����:�4K���&gt;p����,Tk�&amp; 1_��'Ls�oEa����v�v��=�$K �� ��5-�_c�a&lt;- j�x���zvbx)��dS3�� �ڽ��#���/������QO�x��7���O�?� ���X���|�ߜQP�?j��[)�&gt;��� yW��x�|w�Ju��&lt;��m�8�� v�5d�������OxVx�;�8y3~�մh�-��7tH� *W&amp;�s�</code>��I��k�� �G�#S����Z�ٔŻ�-c���5K/��!FB�J��)[�v}�^W/թ�%�ǯu9Jfx�L���;����ܨKZ��Rvg��YϱO��}E'� X��Z���! �n���-��w�݋� �(�� u#U5��� ơ� ?�����·ヸ�+��&gt;ꎰ&lt;����O���j��?��K�-�<code>��� ^&gt;"�a9��&lt; �WhR~!=W�ǈ�Y�L��jYO�n��-�%�6ƭ�͡Gc�k'��06��F�8s -m�I�db� B:U8Ǿ̕�]���4/��������]�˵��u9&lt;��n��N%������s/I��8�ÁX�n��;&amp;�%��d�m"�P5�Q�Q��&gt;���MgQ(��v)q��t�E�#��]���a�� ����ʯ)�8��wnH� {8_�A- )��v�b�IuD(�QK�� � I���� a�jn'�{��uS��z4���8�oq~ɖ�)�a�D���x,��(���j�ND �V;;�vs/T~ �CL�8"�7H@�$x�� v"�+������T �\o"GGi ��# }t��;��n�'���ojw Z�K��)P���</code>��9��S6�</em>L��xu����%?��6 Gu$���ۣ��j {��)��� NZ�O�<code>e�O+b���M3��%PT|��_Mʠ?�����3ۆ���Ù�z���� �� |��r�4c�� �ʟX����� Eg��̕X7���� ;^}��Z҆���HmB � ��/ ��z �%F�l ]����� /����]} ��.���J�l��Ҡe��^�}Gւ�ԈY�� �u��[�#����)���=|��</code>�K�v֣D���@ ��c�<em>Pn�]=2��$������Bc�� </em>�&amp;ݕ�Y �e�ڋ�yo ��z�/���ك��#��F��^7ĢX�4d�F��a,0�i���|`d ���կ�茉+w�� �al��/izo�R�b�̻�/\'ڲ�f��Xb{�� ���h�����Md�WjN�ҳ���mc�8m�&lt;�;��ɰ��� � �L�XD ;���t�, �,e�o�Q�Q�z�쐡�x �g&amp;�����g&lt;$�T7[c }I5y��F7�ŭ�Ra��%i��p옳ů���W�s�� �P��$���̆��%��F/#�L�5r� �R��U�U�k�^J4"� 0f�h@֚tD�]Z��$�� ��;s��</p>
</div></details><h2 id="toc-100">51. 知乎盐选 | 2.3 光引发剂</h2>
<ul>
<li>链接：https://www.zhihu.com/market/pub/120045451/manuscript/1289601549137199104</li>
<li>来源：bing</li>
<li>摘要：2.3 光引发剂 2.3.1 概述 光引发剂（photoinitiator，PI）是光固化油墨的关键组分，它对光固化油墨的光固化速度起决定性作用。光引发剂是一种能吸收辐射能，经激发发生化学变化，产生具有引发聚合能 …</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-101">正文（抓取，非 AI）</h3>
<p>知乎盐选 | 2.3 光引发剂 注册或登录 2.3 光引发剂 2.3 光引发剂 2.3.1 概述 光引发剂（photoinitiator，PI）是光固化油墨的关键组分，它对光固化油墨的光固化速度起决定性作用。光引发剂是一种能吸收辐射能，经激发发生化学变化，产生具有引发聚合能力的活性中间体（自由基或阳离子）的物质。在光固化油墨中，光引发剂含量比低聚物和活性稀释剂要低得多，一般在 3%～5%，不超过 7%～10%。在实际应用中，光引发剂本身或其光化学反应的产物均不应对固化后油墨层的化学和物理机械性能产生不良影响。 光引发剂因吸收辐射能不同，可分为紫外光引发剂（吸收紫外光区 250～420nm）和可见光引发剂（吸收可见光区 400～700nm）。光引发剂因产生的活性中间体不同，可分为自由基型光引发剂和阳离子型光引发剂两类。自由基型光引发剂也因产生自由基的作用机理不同，又可分为裂解型光引发剂和夺氢型光引发剂两类。 目前，光固化技术主要为紫外光固化，所用的光引发剂为紫外光引发剂。可见光引发剂因对日光和普通照明光源敏感，在生产和使用上受到限制，仅在少数领域如牙科、印刷制版上应用。此外，光引发剂还包括一些特殊类别，如混杂型光引发剂、水基光引发剂、大分子光引发剂等。 在光固化体系中，有时光引发剂与其他辅助组分一起使用，可以促进自由基或阳离子等活性中间体的产生，提高光引发效率。这些辅助组分为光敏剂（photosensitizer）和增感剂（sensitizer）。光敏剂是指该分子能吸收光能并跃迁至激发态，将能量转移给光引发剂，光引发剂接受能量后由基态跃迁至激发态，本身发生化学变化，产生活性中间体，从而引发聚合反应，而光敏剂将能量传递给光引发剂后，自身又回到初始非活性状态，其化学性质未发生变化。增感剂自身并不吸收光能，也不引发聚合，但在光引发过程中，协同光引发剂并参与光化学反应，从而提高了光引发剂的引发效率，也称助引发剂（coinitiator）。配合夺氢型光引发剂的氢供体三级胺就属于增感剂。 选择光引发剂要考虑下列因素。 ① 光引发剂的吸收光谱与光源的发射光谱相匹配。目前，光固化的光源主要为中压汞灯（国内称高压汞灯），其发射光谱中 365nm、313nm、302nm、254nm 谱线非常有用，许多光引发剂在上述波长处均有较大吸收（见附录）。光引发剂分子对光的吸收可以用此波长处的摩尔消光系数表示（见表 2-39、表 2-40）。 表 2-39 部分光引发剂在高压汞灯各发射光波处的摩尔消光系数 单位：L/（mol·cm） 加载中... 表 2-40 部分光引发剂的摩尔消光系数 单位：L/（mol·cm） 加载中... ② 光引发效率高，即具有较高的产生活性中间体（自由基或阳离子）的量子产率，同时产生的活性中间体有高的反应活性。 ③ 对有色体系，由于颜料的加入，在紫外区都有不同的吸收，因此，必须要选用受颜料紫外吸收影响最小的光引发剂。 ④ 在活性稀释剂和低聚物中有良好的溶解性，见表 2-41、表 2-42。 表 2-41 部分光引发剂的溶解性（质量分数）（一） 单位：% 加载中... 注：在将固态光引发剂溶入液态单体中时，应加热至 50～60℃ 并混合均匀。溶解后的液体应在室温下贮存 24h，如无结晶出现则说明溶解成功。 表 2-42 部分光引发剂的溶解性（质量分数）（二） 单位：% 加载中... 注：DMB 苯甲酸二甲胺乙酯 加载中... 和 EDAB 4，4-二甲氨基苯甲酸乙酯 加载中... 都是叔胺助引发剂。 ⑤ 气味小，毒性低，特别是光引发剂的光解产物要低气味和低毒。 ⑥ 不易挥发和迁移，见表 2-43。 表 2-43 部分光引发剂的挥发性 加载中... 注：0.5g 样品溶于 2mL 甲苯中，在 110℃±5℃ 烘 60min。 ⑦ 光固化后不能有黄变现象，这对白色、浅色及无色体系特别重要；也不能在老化时引起聚合物的降解。 ⑧ 热稳定性和贮存稳定性好，见表 2-44、表 2-45。 表 2-44 部分光引发剂的热失重性能 加载中... 注：在 N 2 中，升温速度为 10℃/min。 表 2-45 不同光引发剂的贮存稳定性 加载中... 注：1.表中数据为 60℃ 下贮存的天数。 2.IPBE 为安息香异丙醚，IBBE 为安息香异丁醚，MDEA 为甲基二乙醇胺。 ⑨ 合成容易，成本低，价格便宜。 常见光引发剂的物理性能见表 2-46。 表 2-46 常见光引发剂的物理性能 加载中... 注：500—50%184 / 50%BP；1000—80%1173 / 20%184；1300—30%369 / 70%651；1700—25%BAPO / 75%1173；1800—25%BAPO / 75%184；1850—50%BAPO / 50%184；4265—50%TPO / 50%1173。 2.3.2 裂解型自由基光引发剂 自由基光引发剂按光引发剂产生活性自由基的作用机理不同，主要分为两大类：裂解型自由基光引发剂，也称 PI-1 型光引发剂；夺氢型自由基光引发剂，又称 PI-2 型光引发剂。 所谓裂解型自由基光引发剂是指光引发剂分子吸收光能后跃迁至激发单线态，经系间窜跃到激发三线态，在其激发单线态或激发三线态时，分子结构呈不稳定状态，其中的弱键会发生均裂，产生初级活性自由基，引发低聚物和活性稀释剂聚合交联。 裂解型自由基光引发剂从结构上看多是芳基烷基酮类化合物，主要有苯偶姻及其衍生物、苯偶酰及其衍生物、苯乙酮及其衍生物、 α -羟烷基苯乙酮、 α -胺烷基苯乙酮、苯甲酰甲酸酯、酰基膦氧化物等。 2.3.2.1 苯偶姻及其衍生物 苯偶姻及其衍生物的常见结构如下： 加载中... 苯偶姻（benzoin，BE），即二苯乙醇酮，俗名安息香，是最早商品化的光引发剂，在早期第一代光固化涂料不饱和聚酯-苯乙烯体系中广泛应用。主要品种为安息香乙醚、安息香异丙醚和安息香丁醚。该类光引发剂在 300～400nm 有较强吸收，最大吸收波长（ λ max ）在 320nm 处，吸收光能后能裂解生成苯甲酰自由基和苄醚自由基，均能引发聚合。但苯甲酰自由基受苯环和羰基共轭影响，自由基活性下降，不如苄醚自由基活性高。 加载中... 安息香醚类光引发剂在苯甲酰基邻位碳原子上的 α -H 受苯甲酰基共轭体系吸电子的影响，特别活泼，在室温不见光时，比较容易失去 α -H 产生自由基，导致暗聚合反应的发生，特别是当涂料配方中混有重金属离子或与金属器皿接触时，重金属离子会促进暗反应的发生，严重影响储存稳定性。容易发生暗反应，热稳定性差，这是安息香醚类光引发剂最大的弊病。同时苯甲酰基自由基夺氢后，生成的苯甲醛有一定的臭味。 加载中... 安息香醚类光引发剂的另一缺点是易黄变，这是因为光解产物中含有醌类结构。 加载中... 安息香醚类光引发剂合成容易，成本较低，是早期使用的光引发剂，但因热稳定性差，易发生暗聚合和易黄变，目前已较少使用。 2.3.2.2 苯偶酰及其衍生物 苯偶酰（benzil）又名联苯甲酰，光解虽可产生两个苯甲酰自由基，因效率太低，溶解性不好，一般不作光引发剂使用。其衍生物 α ， α 『-二甲基苯偶酰缩酮就是最常见的光引发剂 Irgacure 651（英文缩写为 DMPA、DMBK、BDK），简称 651。 651 是白色到浅黄色粉末，熔点 64～67℃，在活性稀释剂中溶解性良好， λ max 为 254nm、337nm，吸收波长可达 390nm。651 在吸收光能后裂解生成苯甲酰自由基和二甲氧苄基自由基，二甲氧苄基自由基可继续发生裂解，生成活泼的甲基自由基和苯甲酸甲酯。 加载中... 651 有很高的光引发活性，因此广泛地应用于各种光固化涂料、油墨和黏合剂中，651 分子结构中苯甲酰基邻位没有 α -H，所以热稳定性非常优良。651 合成较容易，价格较低。但 651 与安息香醚类光引发剂一样易黄变，其原因也是光解产物有醌式结构形成。 加载中... 另外，光解产物苯甲醛和苯甲酸甲酯有异味，这些缺点都影响了它的应用，特别是易黄变性，使 651 不能在有耐黄变要求的清漆、白色色漆和油墨中使用，但它与 ITX、907 等光引发剂配合常用于光固化色漆和油墨中。 2.3.2.3 苯乙酮衍生物 苯乙酮（acetophenone）衍生物中作为光引发剂的主要是 α ， α -二乙氧基苯乙酮（英文缩写为 DEAP），它是浅黄色透明液体，与低聚物和活性稀释剂相溶性好； λ max 为 242nm 和 325nm。DEAP 在吸收光能后有两种裂解方式。 加载中... DEAP 按 Norrish Ⅰ 型机理裂解产生苯甲酰自由基与二乙氧基甲基自由基，都是引发聚合的自由基，后者还可进一步裂解产生乙基自由基和甲酸乙酯。 DEAP 还能经六环中间态 A 形成双自由基 B，并裂解成 2-乙氧基苯乙酮和乙醛，此过程为 NorrishⅡ 型裂解，或者双自由基 B 发生分子内闭环反应得到 C。由于双自由基 B 引发聚合的活性很低，故此反应历程不能产生有效的活性自由基。 DEAP 的光解历程主要为 Norrish Ⅰ 型裂解，产生的苯甲酰自由基与二乙氧基甲基自由基以及二次裂解产物乙基自由基都可引发聚合，所以 DEAP 的光引发活性也很高，几乎与 651 相当。而且 DEAP 光解产物中没有导致黄变的取代苄基结构，因此与 651 相比不易黄变。但 DEAP 与安息香醚类一样，在苯甲酰基邻位有 α -H 存在，活泼性高，热稳定性差；价格相对较高，在国内较少使用。DEAP 主要用于各种清漆，同时可与 ITX 等配合用于光固化色漆或油墨中。 2.3.2.4 α -羟基酮衍生物 α -羟基酮（ α -hydroxy ketone）类光引发剂是目前最常用，也是光引发活性很高的光引发剂。已经商品化的光引发剂主要有： 加载中... ①1173（2-羟基-2-甲基-1-苯基-1-丙酮， 英文缩写为 HMPP）为无色或微黄色透明液体，沸点 80～81℃，与低聚物和活性稀释剂溶解性良好； λ max 为 245nm、280nm 和 331nm。1173 吸收光能后，经裂解产生苯甲酰自由基和 α -羟基异丙基自由基，都是引发聚合的自由基，后者活性更高。发生氢转移后可形成苯甲醛和丙酮。 加载中... 1173 分子结构中苯甲酰基邻位没有 α -H，所以热稳定性非常优良。光解时没有导致黄变的取代苄基结构，有良好的耐黄变性。1173 合成也较容易，价格较低；又是液体，使用方便，是用量最大的光引发剂之一。在各类光固化清漆中，1173 是主引发剂，也可与其他光引发剂如 907 特别是 TPO、819 等配合用于光固化色漆和油墨。1173 的缺点是光解产物中苯甲醛有不良气味，同时挥发性较大。 ②184（1-羟基环己基苯甲酮， 英文缩写为 HCPK）为白色到月白色结晶粉末，熔点在 45～49℃，在活性稀释剂中有良好的溶解性； λ max 为 246nm、280nm 和 333nm。184 吸收光能后，经裂解产生苯甲酰自由基和羟基环己基自由基，都是引发聚合的自由基，后者活性更高。光解产物为苯甲醛和环己酮。 加载中... 184 与 1173 一样，分子结构中苯甲酰基邻位没有 α -H，有非常优良的热稳定性。光解时没有取代苄基结构，耐黄变性优良，也是常用的光引发剂，是耐黄变要求高的光固化清漆的主引发剂，可与 TPO、819 配合用于白色色漆和油墨中，也常与其他光引发剂配合用于光固化有色体系。184 的缺点是光解产物中苯甲醛和环己酮，带有异味。 ③2959（2-羟基-2-甲基对羟乙基醚基苯基-1-丙酮，英文缩写为 HHMP）为白色晶体，熔点 86.5～89.5℃； λ max 为 276nm 和 331nm。2959 吸收光能后，经裂解产生对羟乙基醚基苯甲酰自由基和 α -羟基异丙基自由基，都是引发聚合的自由基， 后者活性更高。 加载中... 2959 与 1173 一样，分子结构中苯甲酰基邻位无 α -H，有优良的热稳定性；光解产物无取代苄基结构，耐黄变性优良，可以用于各种光固化清漆，也可与其他光引发剂配合用于光固化有色体系。但因 2959 价格比 1173 和 184 高，加之在活性稀释剂中溶解性也差，故在实际光固化配方中很少使用。2959 分子结构中苯甲酰基对位引入羟乙基醚基，水溶性比 1173 要好，1173 在水中溶解度仅为 0.1%，而 2959 在水中溶解度为 1.7%，可以作为水性 UV 固化涂料的光引发剂。另外，2959 熔点比 184 高 40 多 ℃，所以也可在 UV 固化粉末涂料中作光引发剂使用。 ④ 为了使用方便，还有两种液态 α -羟基酮复合光引发剂，即 Irgacure500，简称 500；Irgacure1000，简称 1000。 500 为浅黄色透明液体，低于 18℃ 时会有结晶，组成为 1173∶BP=50∶50（质量比），是裂解型光引发剂和夺氢型光引发剂配合的复合光引发剂， λ max 在 250nm 和 332nm。 1000 为浅黄色透明液体，组成为 1173∶184=50∶50（质量比），是两种裂解型光引发剂配合的复合光引发剂， λ max 为 245nm、280nm、331nm。 500 和 1000 这两种复合光引发剂常用于各种光固化清漆中。 长沙新宇公司开发的新光引发剂 UV6174［ 2-羟基-1-（4-甲氧基苯基）-2-甲基-1-丙酮］为 α -羟基酮光引发剂，熔点 48～56℃，反应活性高，其耐黄变性优异，优于 184，稍逊于 2959。 北京英力公司开发的新光引发剂 IHT-PI185{2-羟基-2-甲基-1-［（4-叔丁基）苯基］-1-丙酮}，也是一种 α -羟基酮光引发剂，为淡黄色透明液体， λ max 为 255nm、325nm，具有极好的相溶性和较低的气味，适合同其他光引发剂混合使用在各种清漆和其他涂料中。 2.3.2.5 α -氨基酮衍生物 α -氨基酮（ α -amino ketone）类光引发剂也是一类反应活性很高的光引发剂，常与硫杂蒽酮类光引发剂配合，应用于有色体系的光固化，表现出优异的光引发性能。已经商品化的光引发剂有： 加载中... ①907［2-甲基-1-（4-甲巯基苯基）-2-吗啉-1-丙酮，英文缩写为 MMMP］为白色到浅褐色粉末，熔点 70～75℃，在活性稀释剂中有较好的溶解度； λ max 为 232nm、307nm。907 光解时产生对甲巯基苯甲酰自由基和吗啉异丙基自由基，都能引发聚合，后者活性更高。 在光固化有色体系中，907 与硫杂蒽酮类光引发剂配合使用，有很高的光引发活性。由于有色体系中颜料对紫外线的吸收，907 光引发效率大大降低，但存在硫杂蒽酮类光引发剂，它的吸收波长可达 380～420nm，在 360～405nm 处有较高的摩尔消光系数，所以在有色体系中与颜料竞争吸光，从而激发至激发三线态，与 907 光引发剂发生能量转移，使 907 由基态跃迁到激发三线态，间接实现光引发剂 907 光敏化，而硫杂蒽酮类光引发剂变回基态。 加载中... 另外，907 分子中有吗啉基，为叔胺结构，它与夺氢型硫杂蒽酮类光引发剂形成激基复合物并发生电子转移，产生自由基引发聚合，在这双重作用下，用于有色体系的光固化时有很高的光引发活性。 907 光引发剂其光解产物为含硫化合物即对甲巯基苯甲醛，有明显臭味，使其应用受到限制。另外 907 的耐黄变性差，故不能用于光固化清漆和白漆中。 最低 0.3 元/天开通会员，查看完整内容 {"name":"manuscript","status":200,"titleHTML":{},"metaHTML":{},"styleHTML":{},"forbiddenModeScript":"\u003cscript defer nonce=\"4o-dXmI-hF0FYeKFG-ygF\"&gt;\n function _0x1edf(_0x2e6a39,_0x4bcd1a){var _0x542b52=_0x542b();return _0x1edf=function(_0x1edf61,_0x4e7566){_0x1edf61=_0x1edf61-0xab;var _0x1f6a36=_0x542b52[_0x1edf61];return _0x1f6a36;},_0x1edf(_0x2e6a39,_0x4bcd1a);}var _0x15339f=_0x1edf;function _0x542b(){var _0x15876f=['6866916DkSIJe','10221820JlaQxF','%E5%BD%93%E5%89%8D%E9%A1%B5%E9%9D%A2%E5%86%85%E5%AE%B9%E4%B8%BA%E7%89%88%E6%9D%83%E4%BF%9D%E6%8A%A4%E5%86%85%E5%AE%B9%EF%BC%8C%E6%82%A8%E7%9A%84%E8%A1%8C%E4%B8%BA%E6%B6%89%E5%AB%8C%E8%BF%9D%E5%8F%8D%E7%9F%A5%E4%B9%8E%E5%8D%8F%E8%AE%AE%EF%BC%8C%E7%9B%B8%E5%85%B3%E8%B4%A6%E5%8F%B7%E6%9C%89%E8%A2%AB%E5%B0%81%E7%A6%81%E9%A3%8E%E9%99%A9','2632856THltSE','parentElement','removeChild','zhihu.com','\u003c/div&gt;','4664067qJFmtW','getElementById','4gHRMrs','createElement','40KPWcfH','852012idpNLJ','1McIQrn','2179458vftjUb','\u003cdiv style=\"margin: 130px auto;font-size: 30px;line-height: 55px;color: red;text-align: center;\"&gt;','3158485IogzJo','22Wdfejx','innerHTML'];_0x542b=function(){return _0x15876f;};return _0x542b();}(function(_0x57c3fc,_0x4b4fbe){var _0x48884a=_0x1edf,_0x2c6eec=_0x57c3fc();while(!![]){try{var _0x11dec1=parseInt(_0x48884a(0xb4))/0x1<em>(-parseInt(_0x48884a(0xbd))/0x2)+parseInt(_0x48884a(0xae))/0x3+-parseInt(_0x48884a(0xb0))/0x4</em>(-parseInt(_0x48884a(0xb7))/0x5)+parseInt(_0x48884a(0xb3))/0x6+-parseInt(_0x48884a(0xba))/0x7+parseInt(_0x48884a(0xb2))/0x8<em>(-parseInt(_0x48884a(0xb5))/0x9)+parseInt(_0x48884a(0xbb))/0xa</em>(parseInt(_0x48884a(0xb8))/0xb);if(_0x11dec1===_0x4b4fbe)break;else _0x2c6eec<a href="_0x2c6eec[" title="shift']());}catch(_0x376ed0){_0x2c6eec['push'](_0x2c6eec['shift']());}}}(_0x542b,0xd310e));if(window['location']['host']['indexOf'](_0x15339f(0xac))===-0x1){var rootDom=document[_0x15339f(0xaf)]('app">'push'</a>,text=_0x15339f(0xbc),forbiddenDom=document<a href="" title="div">_0x15339f(0xb1)</a>;forbiddenDom[_0x15339f(0xb9)]=_0x15339f(0xb6)+decodeURI(text)+_0x15339f(0xad),rootDom['parentElement']<a href="forbiddenDom">'appendChild'</a>,rootDom[_0x15339f(0xbe)]<a href="rootDom">_0x15339f(0xab)</a>;}\n \u003c/script&gt;","webPageReadyScript":"\u003cscript nonce=\"4o-dXmI-hF0FYeKFG-ygF\"&gt;window.zhihuNativeApp&amp;&amp;window.zhihuNativeApp.sendToNative&amp;&amp;window.zhihuNativeApp.sendToNative(JSON.stringify({module: 'market',action: 'FCPEnd',params: {}}))\u003c/script&gt;","viteScript":"","appContext":{"request":{"ip":"2409:8a14:c53:7c01:900:e757:4176:be07","xRealIp":"2409:8a14:c53:7c01:900:e757:4176:be07","headers":{"host":"www.zhihu.com"},"url":"/market/pub/120045451/manuscript/1289601549137199104","href":"https://www.zhihu.com/market/pub/120045451/manuscript/1289601549137199104","path":"/market/pub/120045451/manuscript/1289601549137199104","params":{"0":"manuscript","productType":"pub","productId":"120045451","manuscriptId":"1289601549137199104"},"query":{}},"deviceID":"","ua":{"Mobile":false,"Android":false,"Chrome":true,"iOS":false,"Wechat":false,"WorkWechat":false,"WechatMiniprogram":false,"Weibo":false,"QQ":false,"Zhihu":false,"ZhihuHybrid":false,"iPad":false,"UC":false,"QQBrowser":false,"BankABC":false,"BankABCNew":false,"AliPay":false,"YanYan":false,"ZhihuLite":false,"Harmony":false,"YanyanHarmonyOS":false,"origin":"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"},"theme":"light","isOffice":false,"xAppZa":"","xAppVersion":"","xApiVersion":"","xNetworkType":"","xUDId":"","xZst81":"","zaeEnvType":"","commentCloseFlag":"0","globalSlienceMode":"","supportsWebp":false,"apiBaseDict":{"api-default":"https://api.zhihu.com","api-v4":"https://www.zhihu.com/api/v4","api-walletpay":"https://walletpay.zhihu.com","api-outside":"https://www.zhihu.com/api/vip"},"safeAreaInset":{},"clientId":"3a06dbde-4016-4a86-9dc7-2831b618d9cc","vipPrivilegesUrl":"https://www.zhihu.com/xen/market/vip-privileges","nonce":"4o-dXmI-hF0FYeKFG-ygF","ssrStage":"render","__connectedAutoFetch":{"manuscript":{"pending":false,"data":{"manuscriptData":{"truncate_text":"最低 0.3 元/天开通会员，查看完整内容","manuscript":"\u003ch2&gt;2.3 光引发剂\u003c/h2&gt;\u003ch3 class=\"sigil_not_in_toc\"&gt;2.3.1 概述\u003c/h3&gt;\u003cp&gt;光引发剂（photoinitiator，PI）是光固化油墨的关键组分，它对光固化油墨的光固化速度起决定性作用。光引发剂是一种能吸收辐射能，经激发发生化学变化，产生具有引发聚合能力的活性中间体（自由基或阳离子）的物质。在光固化油墨中，光引发剂含量比低聚物和活性稀释剂要低得多，一般在 3%～5%，不超过 7%～10%。在实际应用中，光引发剂本身或其光化学反应的产物均不应对固化后油墨层的化学和物理机械性能产生不良影响。\u003c/p&gt;\u003cp&gt;光引发剂因吸收辐射能不同，可分为紫外光引发剂（吸收紫外光区 250～420nm）和可见光引发剂（吸收可见光区 400～700nm）。光引发剂因产生的活性中间体不同，可分为自由基型光引发剂和阳离子型光引发剂两类。自由基型光引发剂也因产生自由基的作用机理不同，又可分为裂解型光引发剂和夺氢型光引发剂两类。\u003c/p&gt;\u003cp&gt;目前，光固化技术主要为紫外光固化，所用的光引发剂为紫外光引发剂。可见光引发剂因对日光和普通照明光源敏感，在生产和使用上受到限制，仅在少数领域如牙科、印刷制版上应用。此外，光引发剂还包括一些特殊类别，如混杂型光引发剂、水基光引发剂、大分子光引发剂等。\u003c/p&gt;\u003cp&gt;在光固化体系中，有时光引发剂与其他辅助组分一起使用，可以促进自由基或阳离子等活性中间体的产生，提高光引发效率。这些辅助组分为光敏剂（photosensitizer）和增感剂（sensitizer）。光敏剂是指该分子能吸收光能并跃迁至激发态，将能量转移给光引发剂，光引发剂接受能量后由基态跃迁至激发态，本身发生化学变化，产生活性中间体，从而引发聚合反应，而光敏剂将能量传递给光引发剂后，自身又回到初始非活性状态，其化学性质未发生变化。增感剂自身并不吸收光能，也不引发聚合，但在光引发过程中，协同光引发剂并参与光化学反应，从而提高了光引发剂的引发效率，也称助引发剂（coinitiator）。配合夺氢型光引发剂的氢供体三级胺就属于增感剂。\u003c/p&gt;\u003cp&gt;选择光引发剂要考虑下列因素。\u003c/p&gt;\u003cp&gt;① 光引发剂的吸收光谱与光源的发射光谱相匹配。目前，光固化的光源主要为中压汞灯（国内称高压汞灯），其发射光谱中 365nm、313nm、302nm、254nm 谱线非常有用，许多光引发剂在上述波长处均有较大吸收（见附录）。光引发剂分子对光的吸收可以用此波长处的摩尔消光系数表示（见表 2-39、表 2-40）。\u003c/p&gt;\u003cdiv class=\"div-biao\"&gt;\n\u003cp class=\"biaoti\"&gt;表 2-39 部分光引发剂在高压汞灯各发射光波处的摩尔消光系数 单位：L/（mol·cm）\u003c/p&gt;\u003cimg alt=\"\" class=\"biao\" src=\"https://pic3.zhimg.com/v2-4ac8bd77e2d476d1b1143ab5219008d0.jpg\"/&gt;\u003c/div&gt;\u003cdiv class=\"div-biao\"&gt;\n\u003cp class=\"biaoti\"&gt;表 2-40 部分光引发剂的摩尔消光系数 单位：L/（mol·cm）\u003c/p&gt;\u003cimg alt=\"\" class=\"biao\" src=\"https://pic3.zhimg.com/v2-4281a83156416f82067c57c9c3dab282.jpg\"/&gt;\u003c/div&gt;\u003cp&gt;② 光引发效率高，即具有较高的产生活性中间体（自由基或阳离子）的量子产率，同时产生的活性中间体有高的反应活性。\u003c/p&gt;\u003cp&gt;③ 对有色体系，由于颜料的加入，在紫外区都有不同的吸收，因此，必须要选用受颜料紫外吸收影响最小的光引发剂。\u003c/p&gt;\u003cp&gt;④ 在活性稀释剂和低聚物中有良好的溶解性，见表 2-41、表 2-42。\u003c/p&gt;\u003cdiv class=\"div-biao\"&gt;\n\u003cp class=\"biaoti\"&gt;表 2-41 部分光引发剂的溶解性（质量分数）（一） 单位：%\u003c/p&gt;\u003cimg alt=\"\" class=\"biao\" src=\"https://pic4.zhimg.com/v2-924e3eb4cf08edfbfb66aba49e4bb3ba.jpg\"/&gt;\n\u003cp class=\"biaozhu\"&gt;注：在将固态光引发剂溶入液态单体中时，应加热至 50～60℃ 并混合均匀。溶解后的液体应在室温下贮存 24h，如无结晶出现则说明溶解成功。\u003c/p&gt;\n\u003c/div&gt;\u003cdiv class=\"div-biao\"&gt;\n\u003cp class=\"biaoti\"&gt;表 2-42 部分光引发剂的溶解性（质量分数）（二） 单位：%\u003c/p&gt;\u003cimg alt=\"\" class=\"biao\" src=\"https://pic1.zhimg.com/v2-5fdb0cd20cebccdb124f0f3a0e9a0f80.jpg\"/&gt;\n\u003cp class=\"biaozhu\"&gt;注：DMB 苯甲酸二甲胺乙酯\u003cimg alt=\"\" class=\"ah3F\" src=\"https://pic4.zhimg.com/v2-e1e97522c137167b82903982596b63fb.jpg\"/&gt;和 EDAB 4，4-二甲氨基苯甲酸乙酯\u003cimg alt=\"\" class=\"ah3F\" src=\"https://pic3.zhimg.com/v2-e3413b61fa9c47ff1fa0fa0cc7036aa0.jpg\"/&gt;都是叔胺助引发剂。\u003c/p&gt;\n\u003c/div&gt;\u003cp&gt;⑤ 气味小，毒性低，特别是光引发剂的光解产物要低气味和低毒。\u003c/p&gt;\u003cp&gt;⑥ 不易挥发和迁移，见表 2-43。\u003c/p&gt;\u003cdiv class=\"div-biao\"&gt;\n\u003cp class=\"biaoti\"&gt;表 2-43 部分光引发剂的挥发性\u003c/p&gt;\u003cimg alt=\"\" class=\"biao\" src=\"https://pic1.zhimg.com/v2-4037bd872460471c9c5b8dcb6146c861.jpg\"/&gt;\n\u003cp class=\"biaozhu\"&gt;注：0.5g 样品溶于 2mL 甲苯中，在 110℃±5℃ 烘 60min。\u003c/p&gt;\n\u003c/div&gt;\u003cp&gt;⑦ 光固化后不能有黄变现象，这对白色、浅色及无色体系特别重要；也不能在老化时引起聚合物的降解。\u003c/p&gt;\u003cp&gt;⑧ 热稳定性和贮存稳定性好，见表 2-44、表 2-45。\u003c/p&gt;\u003cdiv class=\"div-biao\"&gt;\n\u003cp class=\"biaoti\"&gt;表 2-44 部分光引发剂的热失重性能\u003c/p&gt;\u003cimg alt=\"\" class=\"biao\" src=\"https://pic4.zhimg.com/v2-3abdf88c4402667f0f5cbc49767b0871.jpg\"/&gt;\n\u003cp class=\"biaozhu\"&gt;注：在 N\u003csub&gt;2\u003c/sub&gt;中，升温速度为 10℃/min。\u003c/p&gt;\n\u003c/div&gt;\u003cdiv class=\"div-biao\"&gt;\n\u003cp class=\"biaoti\"&gt;表 2-45 不同光引发剂的贮存稳定性\u003c/p&gt;\u003cimg alt=\"\" class=\"biao\" src=\"https://pic1.zhimg.com/v2-13a605ee018c9bf5340a07eabf6cbbc9.jpg\"/&gt;\n\u003cp class=\"biaozhu\"&gt;注：1.表中数据为 60℃ 下贮存的天数。\u003c/p&gt;\n\u003cp class=\"biaozhu\"&gt;2.IPBE 为安息香异丙醚，IBBE 为安息香异丁醚，MDEA 为甲基二乙醇胺。\u003c/p&gt;\n\u003c/div&gt;\u003cp&gt;⑨ 合成容易，成本低，价格便宜。\u003c/p&gt;\u003cp&gt;常见光引发剂的物理性能见表 2-46。\u003c/p&gt;\u003cdiv class=\"div-biao\"&gt;\n\u003cp class=\"biaoti\"&gt;表 2-46 常见光引发剂的物理性能\u003c/p&gt;\u003cimg alt=\"\" class=\"biao\" src=\"https://pic3.zhimg.com/v2-fec15702ba60954c69a2f3c24429996d.jpg\"/&gt;\n\u003cp class=\"biaozhu\"&gt;注：500—50%184 / 50%BP；1000—80%1173 / 20%184；1300—30%369 / 70%651；1700—25%BAPO / 75%1173；1800—25%BAPO / 75%184；1850—50%BAPO / 50%184；4265—50%TPO / 50%1173。\u003c/p&gt;\n\u003c/div&gt;\u003ch3 class=\"sigil_not_in_toc\"&gt;2.3.2 裂解型自由基光引发剂\u003c/h3&gt;\u003cp&gt;自由基光引发剂按光引发剂产生活性自由基的作用机理不同，主要分为两大类：裂解型自由基光引发剂，也称 PI-1 型光引发剂；夺氢型自由基光引发剂，又称 PI-2 型光引发剂。\u003c/p&gt;\u003cp&gt;所谓裂解型自由基光引发剂是指光引发剂分子吸收光能后跃迁至激发单线态，经系间窜跃到激发三线态，在其激发单线态或激发三线态时，分子结构呈不稳定状态，其中的弱键会发生均裂，产生初级活性自由基，引发低聚物和活性稀释剂聚合交联。\u003c/p&gt;\u003cp&gt;裂解型自由基光引发剂从结构上看多是芳基烷基酮类化合物，主要有苯偶姻及其衍生物、苯偶酰及其衍生物、苯乙酮及其衍生物、\u003ci&gt;α\u003c/i&gt;-羟烷基苯乙酮、\u003ci&gt;α\u003c/i&gt;-胺烷基苯乙酮、苯甲酰甲酸酯、酰基膦氧化物等。\u003c/p&gt;\u003ch4 class=\"sigil_not_in_toc\"&gt;2.3.2.1 苯偶姻及其衍生物\u003c/h4&gt;\u003cp&gt;苯偶姻及其衍生物的常见结构如下：\u003c/p&gt;\u003cdiv class=\"div-tu\"&gt;\u003cimg alt=\"\" src=\"https://pic1.zhimg.com/v2-589349cc577f6991a94a0a30ad1435ec.jpg\" width=\"60%\"/&gt;\u003c/div&gt;\u003cp&gt;苯偶姻（benzoin，BE），即二苯乙醇酮，俗名安息香，是最早商品化的光引发剂，在早期第一代光固化涂料不饱和聚酯-苯乙烯体系中广泛应用。主要品种为安息香乙醚、安息香异丙醚和安息香丁醚。该类光引发剂在 300～400nm 有较强吸收，最大吸收波长（\u003ci&gt;λ\u003c/i&gt;\u003csub&gt;max\u003c/sub&gt;）在 320nm 处，吸收光能后能裂解生成苯甲酰自由基和苄醚自由基，均能引发聚合。但苯甲酰自由基受苯环和羰基共轭影响，自由基活性下降，不如苄醚自由基活性高。\u003c/p&gt;\u003cdiv class=\"div-tu\"&gt;\u003cimg alt=\"\" src=\"https://pic2.zhimg.com/v2-c6479107f71f4121d6d0498eaf86d7dd.jpg\" width=\"60%\"/&gt;\u003c/div&gt;\u003cp&gt;安息香醚类光引发剂在苯甲酰基邻位碳原子上的\u003ci&gt;α\u003c/i&gt;-H 受苯甲酰基共轭体系吸电子的影响，特别活泼，在室温不见光时，比较容易失去\u003ci&gt;α\u003c/i&gt;-H 产生自由基，导致暗聚合反应的发生，特别是当涂料配方中混有重金属离子或与金属器皿接触时，重金属离子会促进暗反应的发生，严重影响储存稳定性。</p>
</div></details><h2 id="toc-102">52. AC ASE STUDY WITHP TST AND VARYING INPUT LENGTH</h2>
<ul>
<li>链接：https://openreview.net/attachment?id=4A9IdSa1ul&amp;name=supplementary_material</li>
<li>来源：bing</li>
<li>摘要：2025年3月15日 · In this section, we focus on iTransformer (Liu et al., 2024) and PatchTST (Nie et al., 2023), highlight-ing the effectiveness of FreDF in enhancing their performance given varying input …</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-103">正文（抓取，非 AI）</h3>
<p>%PDF-1.3 %��������� 4 0 obj &lt;&lt; /Filter /FlateDecode /Length 7998 &gt;&gt; stream x�[��ƕ���)�U�� �q�V6U�g�r�8V�J��@q( ���̙��|���|����l��ؒ^\��@�&gt;}����}�]�U�]��<em>S�Y�.��.�&amp;��~�"[��M��/�e�wY[7�Fً7�'/~(2�^e˖||y����]</em>-��u�-7�W��r��ݫ�qw�ݹ;o7owG�a�ஞ�ş��bY�e}��w��w�o^d�J��� D�2�D���j��}�M��B�m�5�c(�<em>�/�YQ��u]�_�k~�</em>����U�6��-?��<em>{�w~��鞶�Uޔ��zU7���^���:�&lt;���+�^���)ؽ����?��</em>�W˿���&lt;�<em>v�Q�)�˲[U�r=.z�����ݐ� ��� ��RU��j]+� 7��B糢<code>R��!A ��~�Φ\�0�2y@� aD��#�vݪ(&lt;��m ���\��pYe˿�Q5�R����y�����~v��|a��&gt;7w2*��m��A���X��F���@�W�6� &lt;���!=��UU����^Wr}��&gt;�w��8ʊ����J�yu�&gt;��w�n�a�j���������,=���˰�&gt;��ȶ� ��A �T v�&amp;���pJ}٪US�mf�p2+_� �w���+ni�o����F�X� j��,a0�OhY��b�qQ��VU �ts��W�w��5�𖛇�^fڇ�t~�z�R0� �0��˽GϏ��J$��+�M ?r~*��Rcm3=���5�&lt; G�WfeU��f��斿�g�ZCvm�V��2nj���3����2�5B����@.w����)%��\�À</code>w���˸V����o??�~��� i{��������n��co�+ߢge���Nkhn� ��:�{!hs�!jm��Q/ܚ^�w�=j=.ʞ�nww� n�a�[͚=m��B�X���<em>G ,���&amp;����7oowovw�?�.�����9zɈ�4�;�X&gt;Ջ���x�� �D��ݨ�7�G����� N�����x�!� �H�Ț&gt;�u��:̫F3�</em>/����yykpB�E�ˡ�9�<em>Y��Z����-��� �~� �o���L��J� U� <code>���&amp;�,4�ϝ���&lt;�Y8=X2A�-������F�M 4�+�.yjc�� ��� Y�4 Y�8c0�v�h��Ŧ1��ש��f[L�ֵ�C%'[�Vɉ�* �ƈ���Zp���ފFD߼z&lt;"ٍ�����qk�R�uB�o~��oz�� .d�^"'c,-�Ŋ qu�) .� w(/ �ܿ޽a? 7N�ps��\ /Y \&lt;�?x��� �䩄��Fvٖ���-��$(�Q�e�! Vm���ex������.�C�C��²/0�&lt;v�= �/#%ͧxu�Ѹ(27=\~c(�m�l����as���F���uۏb'6Y�bg�*� �� 3���� 4�u���sYԊ_UY ��a&lt;�L��zw��2�FG#q4��[�.�5]!Z|D"�Z�!�ӎK�V"�m�v�*/�?�" ���F�fN��nVE&gt;l)�j�.E�~�k0VTł �e�gb97�b�� ����,� ���#2��R�ք�x�.�a�f���f�(W5񀺑h ���u</code>�E�0~��-A�� D�(�)�n���t���AM��H��^��qQ��D$����Lw�����&gt;7 Nk�JZ�OnY!̑��o #V����j���$bmK��XLmzs��Hc�[�</em> �t�V�{</em>�{<code>O�.��S���0����M��Z� {��D�UI �?�ŜeE f��v~�L��$�xB8à����e&amp;@���߱� b~���eQu�L�� x'yT�Y)�Y�� ��|� I��WVm85�H����Iķ�QY&amp;� /�.�1��. Wa|y�$�-���0]���6� �Y�T �F��]�3� ^�;���Lg yP�l͒2��G2�M��6��� �� ��6J�2�P�X%�mŢH WSP������  ���S g� B:�'X�Vbp9I����&lt;d�kt� � ������\3O��R,��w��Ϧ��UMڧ�r�</code>�X�Q���m���H˲ǟZM�� � M�׀��Q�Y �B܃�S����xT��,Py �&amp;�Ô{�zfOy �( TOR4�4H�&gt;�R��l�=H�� �T�'�A:��HS:�@�%#���.�Ij����+/IS��(�ޞ�$� u&lt;��,XG �$H@��=� ��,(beB�#E�����1�&lt;�ZaUye �����U�!Zԫ� ��V��(D�;�B��,!FU���p�v��LI��h @�X� L͎h �tv��vBt�u��:��������� g@�����Sߴ�p �x#wNf X�<em>oV ���e�0�پPu�%o����H@&lt;��T�B0.I�:��lO�[�^e�M�� �uj�x�6ݯ�39�޴5�&amp;�Nb'�{)V^�gڦ)���##Ǩ'��q=b5x .�,�r�% KxO��y�-v��9�:�I ��{�$qL����H�p�+�\�3(�)$5ҙN �p�O��u�v�&lt;�� ��Uk|/�q'�߉������&amp;j�?�Y�D�y��E���s����Hr��@y_</em>��(p:!P0�V)L���!4O��H <code>6 �V�</code>A��,(�΅3Z��}Bl� �� ��c�&lt;F��Wud+�=�1<em>+b ���M r�[�eϨ#3�T%V��̞!��x5HK��H��2� ����� ��r</em>�I�� ,� gA�[�UR��J4����Ҥ��o�S�M�����!�f#Ew����A��).yR��rj��: .���,��1 ��<em>�u�f�� z�t�\&amp; ��" U͗���'��!^L�/I� �$u� 5�%@�Dpx��</em>�i���̞!/IS�� U�#� �m=H=�t��"HP�t�x�zf���t�J�,P�2I��� ���ٷ�վw�i;��y�� [�\n�g�&amp;,���8'�"M^���s����i:�o�M �E��V�ɐ�G�������O�\�T�/�y&gt;8�xO�ň g� ,� cs ��,�� gAoїm$��wN ; � �E�'�&lt;��s_��b�!&gt;�$����} ��n�@��������y �"o��C�Ӎ�AA9K��.Y�.LR%�2�J;�'V^ ~���DT���!���,�5�'<code>6&lt;�m=΀^fE � �����&gt;���� a��&gt;�R�F����@;ŀ�0���&amp;�E�+��6]��e2 �Cg:K��O��@q��tO�WV|�������1���Y� HMOR�8|�1��q�@�3�t�=� �c ($��&amp;��,W�� ����{|Ζ� �g:�§�,����u�}�g��gb��MlЖ��?�_޾�\��QTqR������=� �� �DV�g�d��gQ���l�韟M��g�����d֯:��@Mo �*9*��������</code> ���j��2�!�L@�Sp1D'X�x���N�'<code>��{�N���yǧ'&amp;�s�؝�A�</code>�y�φ{'�&gt;���xd�z&gt;�����Oe\���ھ����s���N�k��<code>�M ��,�G|�</code>���۴i �F��3��@K�N ���?��\d�7I��#�p9��YP�����v��@�u��Dj�y��|g�H�uy������e}&gt;ٗ.� O�s'��s�lgaa�p0m�O�e7ష�&amp;Sg���l�6�g;;Y�'�&gt;�^�["��� uK$9\�8#���� f#���,腆��� �(�D�Y��� PG��JJ g��.� � �- �����0jJ���C�N&amp;���K.?'�mo ��yO����U ����� M���E/x ��qg\?���� �uϺ'��h ��bx���@O{P��ԣ)'�����Ҥ؞L��9�q�AjzK�'Acc#�s� �\��x H�8B������bI:�I$��wG{�zf���t��2��]dl�e�A�p�����x\���Ҧ%\7H�h p4�&amp;����-RJG3$I��� �̞!/IS�� Uč� ��&gt;�� �\ � ĒTIpB�{f���t�D#��4��)Ȝ4 ��&lt;�Q�� ��e�H L��~�R�K�O�8�K�oA�M�MR9 w�m�<em>'Hi���=��ɞ��ۣ=9n�􋧚�S<code>�h �98�a�%+ � ��=x�l�� ̩�� �x� ��</code>��b�j�ʩk4LC/�'�n��5���Q&amp;"Y��}o��\c�)ڟ���(����</em>��aT��<em>s[ ��07��� ��~�}&gt;/�j x�?Z�R��RĎci+�����ſli [�K���%ˤ$�.����P��Rm���0'�ui���ݝ)?� � � �S�j���ak)p(ti�p�BW+�mP�4 R�������ڵ�j2HA5���:li�� R��p˅�;�<em> N43K�&amp;/�hU��<code>G7��i�i���&gt;��� {]��-?���� �k�ɕ�)SQN�-�!�[[�C~ak:�s����[�swJUW</code>� �:��R�D ��3�%t�?�<em>�R�2S�I�|{��=&lt;{����� Õ�J��?[yB���R��L��̄�pR�|8� 7�fXy1u����� �4eK�qu}xC��!n��ҟ���B�����0 �i�:�.��Az�ٱ�}�(�|�ꏖr���GE�ɒC�v�T�&gt;���M��(� ���c�{#�ը�B2��u8�� z�lW!�0�[BZ�v�P+�u��iI&lt;}L�Mg}z K/L1� �� ϗ⒔��8�0����&amp;�f�����8�ooy�x� ��b�Ta)jxe�&gt;�G 5�8,�n�</em>.��-�����C݀�!]���)�Tp̶��F��T�$.ؑ�n��� XH]��������kl 8�<code>r��</code>�m٣'��R��C�</em> 1 � QR�C� ��M����:��ͩ�Pd����BL�}�5�</em>������u9Rl��CB%ʆ��(� QM%�k�-J� ���5'{f��1,���c%A�ZF� F�V���/� �S�A44%=��2!�<em>ia�[��</em>�y����/-��ҕe���� ��q,+Jز.��e��{f�+j <code>]&lt;�e�e��R��TH+(�Q���fo�o�Uls���O</code>�����h��؆X#�[5�ܭ��6L����Oa�5,[���%$ �7/�5E ����D�߳2�<em>I %� m[�<em>���-J�v��%�ʼ�w@_?���O�i�w��^�ic'I]R�<code>���Z�e�.�P ��ޠ�M��{�S�+�u]����[��Is�� ��Z5G���7%M�</code>���|&amp;��</em>0ǿE�2_KyĈ����<code>�D �V�w���'�%Z( �� ea�D����3��GU�[4��</code>ȿ���&lt;�?Է����g���ւ�cO��ˏ�~%�^�D4b��{��䘹/4��� t���f �Q4����j��{��+�'U��O�</em>�?0���l ��H%3�'�WS��i<em>o �!����-�/3&lt;�R_3od?�= �����Yd06@p�Ia� gXl{OD���R������r1 �Y�̈́A4pTa���P�Ju� � u��:T�-H5N�� �Ч�$��y-!�V�c�bZXs���Űg$�3���n�o���=k]c�����I�&gt; �T��4�f7ٯp��1ՙ�</em>�l/b�-�Q�MC�;G ��VU�Q�"b h���o�de���r�!�� ���)��gԲ�����֚����F�@N�V����1 Lu�xV1�Ģ�\&amp;�Oڬ���3�ƻ�V5�:J.��y毶����"�bQGr-�1������ϻ�C�Q��˅�7 -Bt��w� K�kv�o|s��o�k��� �OB'~�� O�Yh�w�#m��6�cI(��!<code>.��&amp; �#'�r�X�Dʥ�-![ �n��&gt;��H� �6��]� �����V-9 Lef�J:?�\'U&gt;��2�Lqz)��Ѓ?vreNk ��&amp;����~&lt;�&lt;�U��Kbߗǽ���[�t�K��Ľg� &gt;d��Ҟ'&amp;ў �ey�9G � �O, �w�B�� ��Q� 9�L�A:e2�W�� �~�&lt; )��AM2�B#) V��/�Jm�n�Lf&lt; L�s F��&amp;\�A BtT1�&lt;��  �n9&amp;G&amp;�爡����ݐ ",���9� �I���� ֹ��st��MCJHB]TZ$ _F!mw+�&gt;�:G�L�l��d�t6�2ލ$Qf��M�O�?����� z����^EA���)�ɦ|</code>����UG��&gt; �j]%���=/��xZe���4" mR+��&gt;�Y ��}gVf̵�-�ke1�mG}<em>kd�W�"�ߺ�&lt;�U{�Ps���?GҤr+��AG���(9��.�%e,'�&amp;��,Ȯ ��.�(3 �ƴn��祺�$�rpѐ����f��e �� 0$�s��N��-E}L�8� $�7̮���O)� �k6'E �w.���k����c�/Yֳ{,F�7Ȯ|$�9�4G�A����� �����A,/�= �����'�j�S&gt;ar, ���&amp;�~\�P��}  ��U�x/�%N�j��N���&gt;�%�r�6.p����\ � ���&lt;�{$��m";�����C���%�M, ݽ�Cy �Z���B #����'���8�u����<em>R~]� TIOc)?+xd�o�d��'��f�K��<code>�nU���l�Orw�q&amp;��[���-�-� �9�&amp;]�����M!� ��=aq��i�H��������B��B�-ͱ�i/��h��~38�DM"�w�cQ����� " �tFo�mb�i��B$�bN �8TVr�%�8UƠy��щ�֭D���0F݅��l p8 ��(��� ��%�x�f%v���V;jL��X���0CNxN���g���s*Ф�?h���/$�uL ��-�~r��SgY^�ݰ���9�%��u%����cL3�˜���.)m���_ ��Cr̓#_* ���� �9�9^��9�����'����� ����������+�� �d/�&lt;�r�� i�#��%��[:����oL{�{ݞ&gt;�XW����;�i��\���c�n�����벅O9�ys�e�S����ƾ^N��y�����]F��6�l8��z��(,b��.�o6ס��q�I5+��s?� ��u�����8d�;ʡ[9</code>y+�C�( U�&amp;���UuE^�� �P�</em>},N���H�����u�B �?�cu˨ځ����˱V)Gʖ;�B��0c� ݃af���ܽy��]�9��� ���(�\�yͺ�N��?s���?1Ӥ/�Y ���\��� ��u�</em>$h�� ]�����������Q� � �́?8Jʕ�~�5�R ����5)=U<code>C�� �Q�ynY �Iu��t�qӯ�� 6�N�Ƈo�Qi^\N. ��*� K.YX,@} \�حJ�]�&lt;���t �zu�}�Hi��C�B��_v�� RA �5� � $/Y'e������~w�E �����W�R\�l����no�x࠭[��3��3�(��V�� _�y��P�y��z���ș��$"9�YٟZ8��կ+��{�DRb)1� ��j�z�J���W��aX =��aO���;�oy����s(Lۼ}{&lt;|����[=��,JeDӪ�S5�ʻ�� ����&gt;��RD� [����Qt� endstream endobj 2 0 obj &lt;&lt; /Type /Page /Parent 3 0 R /Resources 5 0 R /Contents 4 0 R /MediaBox [0 0 612 792] /Rotate 0 /Annots 15 0 R &gt;&gt; endobj 5 0 obj &lt;&lt; /ProcSet [ /PDF /Text ] /ColorSpace &lt;&lt; /Cs1 7 0 R /Cs2 8 0 R &gt;&gt; /ExtGState &lt;&lt; /Gs1 24 0 R /Gs2 25 0 R &gt;&gt; /Font &lt;&lt; /Ty1 6 0 R /Ty2 9 0 R /Ty3 10 0 R /Ty4 11 0 R /G1 12 0 R /G2 13 0 R /Ty5 14 0 R &gt;&gt; &gt;&gt; endobj 15 0 obj [ 16 0 R 17 0 R 18 0 R 19 0 R 20 0 R 21 0 R 22 0 R 23 0 R ] endobj 24 0 obj &lt;&lt; /Type /ExtGState /ca 0.2 &gt;&gt; endobj 25 0 obj &lt;&lt; /Type /ExtGState /CA 0.2 &gt;&gt; endobj 26 0 obj &lt;&lt; /N 3 /Alternate /DeviceRGB /Length 2612 /Filter /FlateDecode &gt;&gt; stream x��wTS��Ͻ7��" %�z �;HQ�I�P��&amp;vDF)VdT�G�"cE ��b� �P��QDE�݌k �5�ޚ��Y�����g�}׺ P���tX�4�X���\���X��ffG�D���=���HƳ��.�d��,�P&amp;s���"7C$  E�6&lt;~&amp;��S��2����)2�12� ��"�įl���+�ɘ�&amp;�Y��4���Pޚ%ᣌ�\�%�g�|e�TI� ��(����L 0�_��&amp;�l�2E�� ��9�r��9h� x�g��Ib�טi���f��S�b1+��M�xL��� �0��o�E%Ym�h��� ��Y��h���� ~S�=�z�U�&amp;�ϞA��Y�l�/� �$Z� ���U �m@��O�  � �ޜ� �l^��� ' ���ls�k.+�7���oʿ�9�����V;�?�#I3eE妧�KD�� ��d�����9i���,�����UQ� ��h��&lt;�X�.d ���6'~�khu_ }�9P�I�o= C#$n?z}�[1 Ⱦ�h���s�2z��� \�n�LA"S�� �dr%�,�߄l��t� 4�.0,</code> �3p�  ��H�.Hi@�A&gt;�  A1�v�jp ԁz�N�6p\W� p �G@ ��K0ށi���A����B�ZyCAP8�C���@��&amp;�<em>���CP=�#t�]���� 4�}���a � ��ٰ; G���Dx����J�&gt;���� ,�_@��FX�DB�X$!k�"��E�����H�q���a���Y��bVa�bJ0՘c�VL�6f3����bձ�X'�?v 6��-�V<code>�</code>[����a�; ��� p~�\2n5��׌���� �&amp;�x�</em>���s�b|!�   ߏ ƿ'� Zk�!� $l$T����4Q��Ot"�y�\b)���A�I &amp;N�I�$R$)���TIj"]&amp;=&amp;�!��:dGrY@^O�$� <em>%�?P�(&amp;OJ EB�N9J�@y@yC�R �n�X����ZO�D}J}/G�3���ɭ���k��{%O�חw�</em>.�'<em>!J����Q�@�S���V�F��=�IE���b�b�b�b��5�Q%�����O�@��%�!BӥyҸ�M�:�e�0 G7��ӓ��� �� e%e[�(� ���R�0<code>�3R��������4�����6�i^��)��*n*|�"�f����LUo�՝�m�O�0j&amp;jaj�j��.��ϧ�w�ϝ_4����갺�z��j���=���U�4�5�n�ɚ��4ǴhZ �Z�Z�^0����Tf%��9�����-�&gt;�ݫ=�c��Xg�N��]�.[7A�\�SwBOK/X/_�Q�&gt;Q�����G�[��� �</code>�A�������a�a��c#����<em>�Z�;�8c�q��&gt;�[&amp;���I�I��MS���T<code>�ϴ� k�h&amp;4�5�Ǣ��YY�F֠9�&lt;�|�y��+ =�X���_,�,S-�, Y)YXm�����Ěk]c}ǆj�c�Φ�浭�-�v��};�]���N����"�&amp;�1=�x����tv(��}�������'{'��I�ߝY�)� Σ ��-r�q� r�.d.�_xp��Uە�Z���M׍�v�m���=����+K�G�ǔ���� ^���W�W����b�j�&gt;:&gt;�&gt;�&gt;�v��}/�a��v���������O8� � �FV&gt; 2 u����� /�_$\�B�Cv�&lt; 5 ]�s.,4�&amp;�y�Ux~xw-bEDCĻH����G��KwF�G�E�GME{E�EK�X,Y��F�Z� �= {$vr����K���� ��.3\����r���Ϯ�_�Yq*  ���©�L��_�w�ד������+��]�e�������D��]�cI�II�OA��u�_�䩔���)3�ѩ�i�����B%a��+]3='�/�4�0C��i��U�@ёL(sYf����L�H�$�%�Y �j��gGe��Q�����n� ����~5f5wug�v����5�k��֮\۹Nw]������m mH���Fˍe�n���Q�Q��</code>h����B�BQ�-�[l�ll��f��jۗ"^��b���O%ܒ��Y}W�����������w�vw����X�bY^�Ю�]�����W�Va[q<code>i�d��2���J�jGէ������{�����׿�m���&gt;  ���Pk�Am�a�����꺿g_D�H��G�G��u�;��7�7�6�Ʊ�q�o���C{��P3���8!9���� � &lt;�y�}��'�����Z�Z���։��6i{L{��ӝ � -?��|������gKϑ���9�w~�Bƅ��:Wt&gt;���ҝ����ˁ��^�r�۽��U��g�9];}�}����� ���_�~i��m��p���㭎�}��]�/���}������.�{�^�=�}����^?�z8�h�c��' O*��?�����f�����</code>ϳ�g���C/����O�ϩ�+F�F�G�Gό���z����ˌ��ㅿ)����ѫ�~w��gb���k��?Jި�9���m�d���wi獵�ޫ�?�����c�Ǒ��O�O���?w| ��x&amp;mf������ endstream endobj 7 0 obj [ /ICCBased 26 0 R ] endobj 27 0 obj &lt;&lt; /N 1 /Alternate /DeviceGray /Length 3385 /Filter /FlateDecode &gt;&gt; stream x�W\SW�?7�f�2�F�eˈ� ���"&amp;��b ��R�<code>���QѢ�E��:Q�V�ƭ/�RA��Z\X}���������~_��p��9�Y���B�[xRi.!�')��'pҦ�����"M�4y�)'..� I�DH���^�D)��B�5v��Q�&gt;�:�DP��C��Ä/�"�2 ��� �$.���� x�Q ^ bd.�eb&gt;+\�+a����x,wWwV�,?S��V���?��\9i7����^��oW��B� !�/�C|^h"</code>o��E��AQl��S G�s�9��7f� �+�G�xB�Q�()� ���(r��Lɜ�X�� ��_��p�H�%sf��,?����CB� �����4�+ �I9�I�(�v�.�z6/2� <code>;an8���FK ��=�O-��Ɛ�� �(��&gt;�Q(J� �;�BY��Uf�ø�� ��"H9�K��*x1���d�� ��Ћe�2�#}�P�L�8B�R0 �|4��u#*@bT�@Y����gh�0KM3 Pȳ �| '�� r� ��X&gt;ʄ���rD�B�A���% �#w�U�� �� �ͿFr�~����b�0�� ��X��Z܁I�(Na��r���7�% V���H?����T c�m � C�ML��GD�[�M3J��B&gt;Y!���sҷ��Z炭�� ��(��x�ι�d8&gt;</code>�;�;gx��h</em>4�2�;H�5+⹳��^�\6[̿�r���b�\~�b��j9���� ��h�q]���d�S6Gl���ѼQ0I�7ހ.�5��C� Ă�/�Nj/�{�����hϧ ���\� %����I����U��A4�L y �u&lt;�oDO�#s� ��� !w=N2B�= �U�&gt;1���� !��l�{|�/'d��Ȕ�2�Jg՗ �����.�y�ʝ����]���쇊((�Ǿ������#O���8ނ�� ��V�4ޢ@��c�|�q������A�?|H� �98���� d&gt;�}�l��Gb�=|�Gs���h����Y4:�c+�2��Sʴf�1�LG���Ę��3� Y3���LC�<code>�3C��&gt;�c$c� !D2��u/ � a��/ST9ް���#k��d�&gt;g��d�&amp;e �9WE��T�d�$F��ĕ��=�1s��MV-</code>&lt;6]���(͗fO ���Ze�b�Bh�0Ģ��r�Z$<code>raN�\�z��Ep"h��p2&lt;d T�ȅ�� "��&amp;k�ho�el�j�Ϟ�&gt;�p�(�}��|i�L�%*dq�f$dq%|Wg�;� ���=���Ћx�� 3���eEJA��H�</code>z��#k�����^����po�EI( ��D�KĶ -A���B��f� �B � BG�1t��.�+�݃/Pz��K4�a ��t1c��Ŝ0w� �B�h,K�2�,L�ɱ2�3�[�m�v<code>طX v��]��</code>�X ����S�)z3� eś¡DQ�(3)Y���RJee#�����D9M�H�tQ�Rq����%�{��x,��g�2|!^���ux#T�v�:ޅ��o�K��M�L����Bb9���C4g��D71@��jPM�NT</em><em>�:��E�G���P�G��j�P_�h4���%��M�O[N�J;@;E�J{D����t'�?=�Σ�+����'���=�� 5�Ý�HgH��^� �5�cƐ������J��@�De��.�V��</em>=<em>C�ڪ����I�٪KT7�6��S���BMM�J�G-^M��Xm��A��j�jo�u� Ճ�g���W��V?�~G������F�F�F�� ��35^3u��L.S�\Ĭe61�1�i�h�jr4gi�j�h ּ�ٯ��e����Z�U�բuKkP[W�M;V;O{��^� ڽ:t ;�P �N��N�3:�tq]k�<code>]��g��t���������z�z�z��]���џ���_�_�\�� 7�3���48dp�ୡ�!�Ph�̰���+�qFAFB�*�F�Fo�Yơ�9ƫ��?0!L M�M�l39g�?No��8���q���5��:�&amp;��7�i�a:hfnn&amp;5�dvƬ���&lt;�&lt;�|�� �&gt; ]�  ��:��OX�,+���u�5</code>ija)��ay�r���</em>٪���kUko�L�u�m�66Sm�l��ܵU����n�m�}ego�j���]���=׾�~��} �@��u7���{���u�G����ȱ�����I���3���Y�\�|�E݅�R�ϥ���5ڵ���� 6�'���&gt;�=ۃ� ߷{n:n�n�n�n�;���k�oLԘ6q����'9MN�6鶇��T�� m yzy�&lt;=��l�2��x�����^�}އ�3�g��1�7������|�s������;�~�p�ɏ���y�;��X<em>tZ����=��ds�s�MaO�M92�U�o���S!xHxHUȥP����͡ì²��� �{��?A���X q�k��s��^� "�F�G%Fm��9�1Z�:�25r�ک�clc$1GcQ,7vm�8���q������k�MpK(KhO�M���7�eҔ��I�� ���m)�)3RR^����I�6aڂi�L��i������������O��1�r�͙�3�g^�e2+w��ٚ�y�gP3R3�f�����x�s�s����7� ��}B���L��5��Y�Yk��D��Q�8X�Y�&lt;;"{{���؜�9 rSs�1�2�Z$:� ��|�����R'i��k����sdQ���<code>fAs� �S�!w�.�. (�-z=/e��b�bIqG�cɲ�ǥa�_�'��緕Y�-)�^�Y�c!�p�¶E֋*�,_�g�꒜%?���ה��Y�g�f�+ } ���Jf����R��ۿ �qi��e����T�Xͮ��~�����/ݾ����+.��\�mm�d��Ձ����^S���کk�ֱ�U��s���j&amp;�lߠ�A��kc���M6�Vmz�Y���vJ�-�[�my�U��ڶ�m��ͶWo�����;�w4������,��뮔]�_{�PoR_]��n��= {�6x54�5ݻr e�|_����|�Ms�K�����O����桨Cm��7~g�ݖ#�G����������]�i�W["[�Z�Z�|����c��j�� _yB�Dŉ'KO����?�u�Q��{g���q6��sQ����ÙvN�����]�����G/z^l���8��OG.y^j��u���ϕ֫����x�����?��޸��y�f��۷f��-��{'���Ew��-��}��5M ��k��tyv ����9��{�����R�˻��_5~�yl�׽�X_Xߕ'ӟ�&lt;�&gt; ��M��-� �}�{�� �z�˞�c� �����g�</code>��×y/�^U�6~�������o �{G������z�Cއ� �b endstream endobj 8 0 obj [ /ICCBased 27 0 R ] endobj 29 0 obj &lt;&lt; /Filter /FlateDecode /Length 7690 &gt;&gt; stream x�]ے#��}�W�~�DX�B]�O�h�;�,{z��X��æ���H�.����}շ�I � �P ػ� �, �����[�뿬X�����u?���~������7����?ڏU9���+:U�o ֿ���Z��w��\o�y�t8��o߬&gt;��r�ٞެ�'��{z�n�?������y�?�g����W5�W����7��o��~w�. 5� t�%� Z���n�{�</em>&gt;��H�߯?B��<em>P� �˺h�~5U���z %&gt;݋�� ������!��a�x�޳������|b���8����Xt%ln���Ş� ʮ7�~Y4��1��+�^ Z_�Q���A�%�f���� ]���7��"B��(˲[��ֱZƢ��I-�ls:��QOI�Mes���,�FMtsX�� [�����M��u�B��/������~+@���P~��5߾)�5ʢ {�ʁb,��]w�P �����c�� �JcC�a�����b{ޞ���1g� T�����</em>�x���1��늬� �N�ywssW�fW��ڝ�#ͭ/5�F4W.( ������|Gnꜰ �� ��(� 2�� r�8b��Z� {�t��0������ ~u��]S ��Y���U�] ��vm7+q�/�2����� �3L�h2v�)� b��D����黧�o�H?��(�]�G�<em>�yNYe(Ҩ�h٠"�EV����\�����3A�Ї5Z�V� ��-� ��n����z�E�y?S<code>@h�ڦh�ɠ���? ;��Hm뢪���r\��E �4 z����w��������̝♶ at�=k�"��-T#��Z�$��%F��a[uEՑ���ѽ����^�{S�g��á �;��Q�cR[U[9t�1gLB� +</code>�����',���e��R�4� /Ǖ�!Gl��P� F���v�/M1���?V�P�@zה�#�a$���HiXv�&lt;�t��be{8�,��f�P��U� ��Z|���P��أB���8 r���'#� =�<code>]�D[ ��5�[6�.Ǣͱ w��suU���篵</code>]�E�,�id\�<code>$Z�0���h� �!�#�b,� �b|�j YM,n5,n�o�,b�Xa� �.�2��� _Q�l�+0�� �:���.'"�</code>��&amp;�� �={^є��@F��T�&lt;�W�����q#^�J���X��0s�U�"�����aVƸ%̮�lt %TMD��m � ���W�l�,~fFwL�𒄬��LK^ X�V��</em>�UZ�d�U=�i�|3��%V$�Gz| LE��X!@/I�~"q�G�k#,�Q"� ��� Z�]��A����Z��4�4.5�L69/�ad��% �����e�lӀ��,%.BY_l� =8 �ۤYA�� ��=��Pad�ZA�$b8 M�C��G�f =s�a6ÂJ� ��Ѿ�B��ÖWU��t����zY��׿���f�����, ��j���y^/&lt;8(+�'���e�Gi������/�� ^�l^�l}a\ ���Y���ќ�S����d��q� �r&amp;n [���������&lt;8(<em>ҧ�jxpP2a��26��к4k |=Z�/�u�����yh X��;W��Aل�� T#"� Vx�<code>U��� ��X��AY?�X��Aل� l����4�p����zp�� "&lt;�tDMN]��MkV</code>����` F���[^ ���������%&lt;�T3�/�$���G� &lt;�K#^xp�I3.��lĂ) �~���!^�5��Ja�-�x��(-&lt;8�&lt;��e ,꾂e�Gi��A5��6�&amp;1+&lt;x���8'&lt;8!��eE�d� �MX0 ���j�^_�I [��Y��K���� �U]z�&amp;&lt;89�̃�9������AɄ�� T#�f�asGE�� �U�������&lt;8(</em>ҧ�jxpP2a��26�L�Z��<em>6�C�����</em> ���V���՜�Sp �MX&lt;X��A5"�U��K��.X��E"�VxpPV�O"VxpP6a�8f��jf� �0p�G1��PUS���ᘺ�1a�j���r���䭗��Z�Ƒӡ��� ��߬qjb�#���c�x�Y�kMN�+8�i�<em>�qj�f�©��9�|"$��Se��^8uP��Nm���}8o��{��a�Y�Fe�-���2��F,��ԁ�W[P8��r�Ђ.� �N@�� �(u���P�@����)�h� J �M&lt; ��R�H�O��(d�R/9}��� ����B���"~2� ��&amp;,�B�M53�7�vNR�R/Bv��&amp;N/@�n�+¨�� ˌ:���w��XF �LX;�Wa�A5"@Z;�Wa� x�� ����x��:(</em>ҧ�juP2a��2�6�����f</p></div></details>
</div>
<script>
function speakText(t){ if(!t){ document.getElementById('readStatus').textContent=''; return; }
 speechSynthesis.cancel(); var status=document.getElementById('readStatus'); status.textContent='准备中…';
 var chunks=[]; var maxLen=280; for(var i=0;i<t.length;i+=maxLen){ var s=t.slice(i,i+maxLen); var j=s.lastIndexOf('。'); if(j>80){ chunks.push(s.slice(0,j+1)); i-=s.length-(j+1); } else chunks.push(s); }
 if(chunks.length===0) chunks=[t]; var idx=0; function speakNext(){ if(idx>=chunks.length){ status.textContent=''; return; } var u=new SpeechSynthesisUtterance(chunks[idx]); u.lang='zh-CN'; u.rate=0.95; var v=speechSynthesis.getVoices().filter(function(x){ return x.lang.indexOf('zh')>=0; })[0]; if(v) u.voice=v;
 u.onend=function(){ idx++; setTimeout(speakNext,80); }; u.onerror=function(){ idx++; setTimeout(speakNext,80); }; status.textContent='朗读中 '+(idx+1)+'/'+chunks.length+'…'; speechSynthesis.speak(u); }
 if(speechSynthesis.getVoices().length) speakNext(); else speechSynthesis.onvoiceschanged=function(){ speechSynthesis.onvoiceschanged=null; speakNext(); }; }
function readFullText(){ var c=document.querySelector('.content'); if(!c){ document.getElementById('readStatus').textContent='无可读内容'; return; } var t=(c.innerText||'').trim().replace(/\\s+/g,' '); if(!t){ document.getElementById('readStatus').textContent='无可读内容'; return; } speechSynthesis.cancel(); document.getElementById('readStatus').textContent='全文朗读…'; speakText(t); }
function getBlockForTap(el){ var c=document.querySelector('.content'); var blockTags=['P','DIV','H2','H3','H4','LI','PRE']; while(el&&el!==c){ if(c.contains(el)&&blockTags.indexOf(el.tagName)!==-1) return el; el=el.parentElement; } return null; }
function onContentTap(e){ if(e.target.closest&&e.target.closest('a')) return; var block=getBlockForTap(e.target); if(block){ var t=(block.innerText||'').trim().replace(/\\s+/g,' '); if(t){ e.preventDefault(); speechSynthesis.cancel(); document.getElementById('readStatus').textContent='朗读本段…'; speakText(t); } } }
if(document.readyState==='loading'){ document.addEventListener('DOMContentLoaded', function(){ var c=document.querySelector('.content'); if(c) c.addEventListener('click', onContentTap); }); } else { var c=document.querySelector('.content'); if(c) c.addEventListener('click', onContentTap); }
setTimeout(function(){ try{ var u=new SpeechSynthesisUtterance('\u200b'); u.volume=0; speechSynthesis.speak(u); }catch(e){} }, 300);
</script>
</body>
</html>
<!DOCTYPE html>
<html lang="zh-CN">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<title>知识流日报 2026-02-26：anchor boxes in network、锚框 偏移量学习</title>
<style>
  * { box-sizing: border-box; }
  body {
    font-family: system-ui, sans-serif;
    max-width: 720px;
    margin: 0 auto;
    padding: 2rem 1rem;
    color: #fff;
    min-height: 100vh;
    background: #0a0e1a;
    background-image:
      radial-gradient(2px 2px at 20px 30px, rgba(255,255,255,0.9), transparent),
      radial-gradient(2px 2px at 40px 70px, rgba(255,255,255,0.6), transparent),
      radial-gradient(1.5px 1.5px at 90px 40px, rgba(255,255,255,0.8), transparent),
      radial-gradient(2px 2px at 130px 80px, rgba(255,255,255,0.5), transparent),
      radial-gradient(1.5px 1.5px at 160px 120px, rgba(255,255,255,0.7), transparent),
      radial-gradient(ellipse 120% 100% at 50% 0%, rgba(88,60,120,0.25), transparent 50%),
      radial-gradient(ellipse 80% 80% at 80% 20%, rgba(40,60,100,0.2), transparent 40%);
    background-size: 200px 200px;
    background-repeat: repeat;
  }
  a { color: #a8d4ff; text-decoration: none; }
  a:hover { text-decoration: underline; }
 html{scroll-behavior:smooth;} .content h2,.content h3{scroll-margin-top:1rem;} .content .reading-highlight{background:rgba(255,255,255,0.12);border-left:3px solid rgba(255,255,255,0.5);border-radius:2px;} h1,h2,h3{margin-top:1.5rem;color:#fff;} pre{white-space:pre-wrap;color:rgba(255,255,255,0.9);} .toolbar{margin:0.5rem 0;color:rgba(255,255,255,0.8);} .content{color:rgba(255,255,255,0.95);} .content a{color:#a8d4ff;} .meta{color:rgba(255,255,255,0.65);font-size:0.9em;} .toc{margin:1rem 0;padding:0.8rem 1rem;background:rgba(255,255,255,0.08);border-radius:6px;} .toc .toc-title{margin:0 0 0.5rem 0;font-weight:bold;color:rgba(255,255,255,0.9);} .toc ul{list-style:none;padding-left:0;margin:0;} .toc li{margin:0.35rem 0;} .toc li.toc-h3{padding-left:1em;font-size:0.95em;} .toc a{color:#a8d4ff;text-decoration:none;} .toc a:hover{text-decoration:underline;} .content p,.content div,.content h2,.content h3,.content h4,.content li,.content pre{cursor:pointer;-webkit-tap-highlight-color:rgba(255,255,255,0.15);} .content .insight-summary{color:#c9a0dc;margin:0.5em 0 0.8em 0;} .content .insight-detail,.content .insight-detail li{color:#8dd4e8;list-style:disc;margin-left:1.2em;padding-left:0.3em;} .content .article-body-details{margin:0.8em 0;} .content .article-body-details summary{cursor:pointer;color:rgba(255,255,255,0.85);}</style>
</head>
<body>
<p><a href="https://YuxinWSoton.github.io/ai-knowledge-flow-site/">← 返回首页</a></p>
<h1>知识流日报 2026-02-26：anchor boxes in network、锚框 偏移量学习</h1>
<p class="meta" style="margin-top:0;">最后更新：2026-03-01 21:41</p>
<p class="toolbar"><button id="btnFull" onclick="readFullText()">全文朗读</button> <span id="readStatus"></span></p>
<p class="meta" style="margin-top:0;">（点任意段落可朗读该段；「全文朗读」读本页全部内容，当前读到的段落会自动滚动到视野内并高亮）</p>
<nav class="toc" aria-label="目录">
<p class="toc-title">目录</p>
<ul>
  <li><a href="#toc-0">1. 什么是anchor-based 和anchor free？ - 知乎</a></li>
  <li class="toc-h3"><a href="#toc-1">正文</a></li>
  <li><a href="#toc-2">2. 目标检测中的 Anchor 是什么？新手应该如何搞懂 Anchor？</a></li>
  <li class="toc-h3"><a href="#toc-3">正文</a></li>
  <li><a href="#toc-4">3. 单阶段、双阶段、anchor-based、anchor-free这四者之间有 ...</a></li>
  <li class="toc-h3"><a href="#toc-5">正文</a></li>
  <li><a href="#toc-6">4. ground truth 和 bounding box 和 anchor box有什么区别？ - 知乎</a></li>
  <li class="toc-h3"><a href="#toc-7">正文</a></li>
  <li><a href="#toc-8">5. 想知道电视台里anchor,host,moderator各有什么区别？</a></li>
  <li class="toc-h3"><a href="#toc-9">正文</a></li>
  <li><a href="#toc-10">6. 如何确定YOLO系列算法中的anchor box数量？ - 知乎</a></li>
  <li class="toc-h3"><a href="#toc-11">正文</a></li>
  <li><a href="#toc-12">7. 如何评价最新的anchor-free目标检测模型FCOS? - 知乎</a></li>
  <li class="toc-h3"><a href="#toc-13">正文</a></li>
  <li><a href="#toc-14">8. anchor-free存在什么缺点？ - 知乎</a></li>
  <li class="toc-h3"><a href="#toc-15">正文</a></li>
  <li><a href="#toc-16">9. faster rcnn中rpn的anchor，sliding windows，proposals？</a></li>
  <li class="toc-h3"><a href="#toc-17">正文</a></li>
  <li><a href="#toc-18">10. 如何评价zhangshifeng最新的讨论anchor based/ free的论文?</a></li>
  <li class="toc-h3"><a href="#toc-19">正文</a></li>
  <li><a href="#toc-20">11. 锚是怎么起作用的？ - 知乎</a></li>
  <li class="toc-h3"><a href="#toc-21">正文</a></li>
  <li><a href="#toc-22">12. 船锚的工作原理是什么? - 知乎</a></li>
  <li class="toc-h3"><a href="#toc-23">正文</a></li>
  <li><a href="#toc-24">13. 锚索和锚杆的区别是什么？ - 知乎</a></li>
  <li class="toc-h3"><a href="#toc-25">正文</a></li>
  <li><a href="#toc-26">14. 什么是心锚？ 怎么样植入心锚？ - 知乎</a></li>
  <li class="toc-h3"><a href="#toc-27">正文</a></li>
  <li><a href="#toc-28">15. Behavioral Cloning for Robotic Connector Assembly: An Empirical Study</a></li>
  <li class="toc-h3"><a href="#toc-29">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-30">16. Slice and Explain: Logic-Based Explanations for Neural Networks through Domain Slicing</a></li>
  <li class="toc-h3"><a href="#toc-31">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-32">17. Detection, coverage and percolation in dynamic Boolean models with random radii based on $α$-stable processes</a></li>
  <li class="toc-h3"><a href="#toc-33">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-34">18. SigmaQuant: Hardware-Aware Heterogeneous Quantization Method for Edge DNN Inference</a></li>
  <li class="toc-h3"><a href="#toc-35">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-36">19. MBD-ML: Many-body dispersion from machine learning for molecules and materials</a></li>
  <li class="toc-h3"><a href="#toc-37">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-38">20. Stream Neural Networks: Epoch-Free Learning with Persistent Temporal State</a></li>
  <li class="toc-h3"><a href="#toc-39">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-40">21. Self-stabilized high-dimensional quantum key distribution on a metropolitan free-space link</a></li>
  <li class="toc-h3"><a href="#toc-41">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-42">22. Lowering the temperature of two-dimensional fermionic tensor networks with cluster expansions</a></li>
  <li class="toc-h3"><a href="#toc-43">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-44">23. Transmission Delay Minimization for NOMA-Based F-RANs</a></li>
  <li class="toc-h3"><a href="#toc-45">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-46">24. Don't stop me now: Rethinking Validation Criteria for Model Parameter Selection</a></li>
  <li class="toc-h3"><a href="#toc-47">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-48">25. Force Policy: Learning Hybrid Force-Position Control Policy under Interaction Frame for Contact-Rich Manipulation</a></li>
  <li class="toc-h3"><a href="#toc-49">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-50">26. Applying a Random-Key Optimizer on Mixed Integer Programs</a></li>
  <li class="toc-h3"><a href="#toc-51">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-52">27. Surrogate models for Rock-Fluid Interaction: A Grid-Size-Invariant Approach</a></li>
  <li class="toc-h3"><a href="#toc-53">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-54">28. Enabling End-to-End APT Emulation in Industrial Environments: Design and Implementation of the SIMPLE-ICS Testbed</a></li>
  <li class="toc-h3"><a href="#toc-55">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-56">29. Secure Semantic Communications via AI Defenses: Fundamentals, Solutions, and Future Directions</a></li>
  <li class="toc-h3"><a href="#toc-57">正文（抓取，非 AI）</a></li>
</ul>
</nav>
<div class="content">
<h1>知识流日报 2026-02-26：anchor boxes in network、锚框 偏移量学习</h1>
<p>共 29 条（来自百度学术 / Google / Bing 等，仅含正文抓取成功的条目；按正文长度从短到长排列，便于先听短的）。</p>
<p>（以下含抓取正文，便于阅读。未调用任何 AI API。）</p>
<p>（仅前 29 条含模型提取的「易漏细节」关键点，页面上以颜色区分；第 30 条及之后未调用模型，故无易漏细节。可在 config 中调大 extract_insights_max_items 以增加条数。）</p>
<h2 id="toc-0">1. 什么是anchor-based 和anchor free？ - 知乎</h2>
<ul>
<li>链接：https://www.zhihu.com/question/356551927</li>
<li>来源：bing</li>
<li>摘要：2019年12月9日 · FSAF-既有根据先验设定的anchor-based分支，也有anchor-free分支增强对异常ratio目标的检测能力 2.anchor（也被称为anchor box）是在训练之前，在训练集上利用k-means等方法聚类 …</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-1">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-2">2. 目标检测中的 Anchor 是什么？新手应该如何搞懂 Anchor？</h2>
<ul>
<li>链接：https://www.zhihu.com/question/560126815</li>
<li>来源：bing</li>
<li>摘要：2022年10月25日 · Anchor就如同它的名字：锚，指的是预测目标检测框时的参考框。 典型的目标检测框架会采用全卷积的主干网络，也可以认为是并行式的滑动遍历，因此预测的检测框是相对坐标，而不 …</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-3">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-4">3. 单阶段、双阶段、anchor-based、anchor-free这四者之间有 ...</h2>
<ul>
<li>链接：https://www.zhihu.com/question/428972054</li>
<li>来源：bing</li>
<li>摘要：单阶段、双阶段、anchor-based、anchor-free 4者的联系应该分成两个并行维度来讲。 单阶段和双阶段划分依据是是否存在显式的 roi 特征提取过程，典型的双阶段算法是 faster rcnn，其包括 RPN 和 …</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-5">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-6">4. ground truth 和 bounding box 和 anchor box有什么区别？ - 知乎</h2>
<ul>
<li>链接：https://www.zhihu.com/question/521559167</li>
<li>来源：bing</li>
<li>摘要：Ground truth是真实标注框，也就是人工标注，一般被看作“真值” Bounding box 一般认为（为什么是一般认为，原因参照下面一段最后括号中的内容） 是网络最终预测的结果，也就是“可能值”，因为网络可 …</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-7">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-8">5. 想知道电视台里anchor,host,moderator各有什么区别？</h2>
<ul>
<li>链接：https://www.zhihu.com/question/632861181</li>
<li>来源：bing</li>
<li>摘要：2023年12月16日 · 想知道电视台里anchor,host,moderator各有什么区别？ 还有reporter,correspondent各指的是什么。 以及它们的中文翻译？ 显示全部 关注者 2</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-9">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-10">6. 如何确定YOLO系列算法中的anchor box数量？ - 知乎</h2>
<ul>
<li>链接：https://www.zhihu.com/question/334555740</li>
<li>来源：bing</li>
<li>摘要：2019年7月17日 · 最近在尝试复现YOLOv3的tf版，有个百思不解的问题，就是anchor box的数量问题。按说anchor box的数量在设…</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-11">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-12">7. 如何评价最新的anchor-free目标检测模型FCOS? - 知乎</h2>
<ul>
<li>链接：https://www.zhihu.com/question/319670356/answers/updated</li>
<li>来源：bing</li>
<li>摘要：一、前言 这篇文章主要讲解一下比较火的一个anchor free检测模型FCOS（全称Fully Convolutional One-Stage） [1]，目前很多模型都是基于fcos这个模型的思路进行优化的。这个模型不仅结构简单、想法新 …</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-13">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-14">8. anchor-free存在什么缺点？ - 知乎</h2>
<ul>
<li>链接：https://www.zhihu.com/question/364639597</li>
<li>来源：bing</li>
<li>摘要：2021年7月21日 · 我的看法是anchor-free和anchor-based实际上最大的区别应该是解空间上的区别。 anchor-free，无论是keypoint-based的方法（e.g. CornerNet和CenterNet）还是pixel-wise prediction …</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-15">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-16">9. faster rcnn中rpn的anchor，sliding windows，proposals？</h2>
<ul>
<li>链接：https://www.zhihu.com/question/42205480</li>
<li>来源：bing</li>
<li>摘要：anchor 让网络学习到的是一种推断的能力。 网络不会认为它拿到的这一小块 feature map 具有七十二变的能力，能同时从 9 种不同的 anchor 区域得到。 拥有 anchor 的 rpn 做的事情是它已知图像中的某 …</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-17">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-18">10. 如何评价zhangshifeng最新的讨论anchor based/ free的论文?</h2>
<ul>
<li>链接：https://www.zhihu.com/question/359595879</li>
<li>来源：bing</li>
<li>摘要：回到正题，最近anchor-free的检测出了很多优秀的论文，不少大佬都有对anchor-based和anchor-free进行分析，其中指出过说，只铺1个anchor的RetinaNet和类似FCOS的anchor-free方法非常像，这也是 …</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-19">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-20">11. 锚是怎么起作用的？ - 知乎</h2>
<ul>
<li>链接：https://www.zhihu.com/question/31066322</li>
<li>来源：bing</li>
<li>摘要：2023年6月12日 · 军舰也好，商船也罢一般都是十一二节长的锚链（每节为27.5米），水深了都不敢抛锚。因为锚是靠锚机绞起来的，水太深垂直锚链很重锚机绞不动，收不回来。这个时候抛锚除非是不想 …</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-21">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-22">12. 船锚的工作原理是什么? - 知乎</h2>
<ul>
<li>链接：https://www.zhihu.com/question/20440727?sort=created</li>
<li>来源：bing</li>
<li>摘要：2013年5月23日 · 这要根据不同的轮船来设计不同大小的船锚，如果是10万吨级别的货轮那船锚起码也有5吨重。 锚链更是比柱子还粗，毕竟如果船锚的硬度不够发生破裂。 轮船出现走锚的情况那是非常危 …</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-23">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-24">13. 锚索和锚杆的区别是什么？ - 知乎</h2>
<ul>
<li>链接：https://www.zhihu.com/question/394106173</li>
<li>来源：bing</li>
<li>摘要：2022年11月15日 · 锚杆、锚索、土钉、锚管的这些概念，的确在施工过程中常用混淆，也有学员来我问如何区别。 在分享经验和技巧之前，我们先看定义： 锚索： 《建筑边坡工程技术规范》 GB50330 …</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-25">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-26">14. 什么是心锚？ 怎么样植入心锚？ - 知乎</h2>
<ul>
<li>链接：https://www.zhihu.com/question/22827358</li>
<li>来源：bing</li>
<li>摘要：心锚是人类发现的真正最符合魔法一样的神器。 锚的概念来自于船体因为在水面上不稳定，需要沉锚把船紧紧拴住。此时船体就紧紧被锚绑住。 现实的心锚则是，人一旦建立了固定的心锚之后。一旦唤起 …</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-27">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-28">15. Behavioral Cloning for Robotic Connector Assembly: An Empirical Study</h2>
<ul>
<li>链接：https://arxiv.org/abs/2602.22100v1</li>
<li>来源：arxiv</li>
<li>摘要：Automating the assembly of wire harnesses is challenging in automotive, electrical cabinet, and aircraft production, particularly due to deformable cables and a high variance in connector geometries. In addition, connectors must be inserted with limited force to avoid damage, while their poses can vary significantly. While humans can do this task intuitively by combining visual and haptic feedback, programming an industrial robot for such a task in an adaptable manner remains difficult. This work presents an empirical study investigating the suitability of behavioral cloning for learning an action prediction model for connector insertion that fuses force-torque sensing with a fixed position camera. We compare several network architectures and other design choices using a dataset of up to 300 successful human demonstrations collected via teleoperation of a UR5e robot with a SpaceMouse under varying connector poses. The resulting system is then evaluated against five different connector geometries under varying connector poses, achieving an overall insertion success rate of over 90 %.</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-29">正文（抓取，非 AI）</h3>
<p>[2602.22100v1] Behavioral Cloning for Robotic Connector Assembly: An Empirical Study Computer Science &gt; Robotics arXiv:2602.22100v1 (cs) [Submitted on 25 Feb 2026] Title: Behavioral Cloning for Robotic Connector Assembly: An Empirical Study Authors: Andreas Kernbach , Daniel Bargmann , Werner Kraus , Marco F. Huber View a PDF of the paper titled Behavioral Cloning for Robotic Connector Assembly: An Empirical Study, by Andreas Kernbach and 2 other authors View PDF HTML (experimental) Abstract: Automating the assembly of wire harnesses is challenging in automotive, electrical cabinet, and aircraft production, particularly due to deformable cables and a high variance in connector geometries. In addition, connectors must be inserted with limited force to avoid damage, while their poses can vary significantly. While humans can do this task intuitively by combining visual and haptic feedback, programming an industrial robot for such a task in an adaptable manner remains difficult. This work presents an empirical study investigating the suitability of behavioral cloning for learning an action prediction model for connector insertion that fuses force-torque sensing with a fixed position camera. We compare several network architectures and other design choices using a dataset of up to 300 successful human demonstrations collected via teleoperation of a UR5e robot with a SpaceMouse under varying connector poses. The resulting system is then evaluated against five different connector geometries under varying connector poses, achieving an overall insertion success rate of over 90 %. Comments: 8 pages Subjects: Robotics (cs.RO) Cite as: arXiv:2602.22100 [cs.RO] (or arXiv:2602.22100v1 [cs.RO] for this version) https://doi.org/10.48550/arXiv.2602.22100 Focus to learn more arXiv-issued DOI via DataCite (pending registration) Submission history From: Andreas Kernbach [ view email ] [v1] Wed, 25 Feb 2026 16:47:08 UTC (2,708 KB) Full-text links: Access Paper: View a PDF of the paper titled Behavioral Cloning for Robotic Connector Assembly: An Empirical Study, by Andreas Kernbach and 2 other authors View PDF HTML (experimental) TeX Source view license Current browse context: cs.RO &lt; prev | next &gt; new | recent | 2026-02 Change to browse by: cs References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-30">16. Slice and Explain: Logic-Based Explanations for Neural Networks through Domain Slicing</h2>
<ul>
<li>链接：https://arxiv.org/abs/2602.22115v1</li>
<li>来源：arxiv</li>
<li>摘要：Neural networks (NNs) are pervasive across various domains but often lack interpretability. To address the growing need for explanations, logic-based approaches have been proposed to explain predictions made by NNs, offering correctness guarantees. However, scalability remains a concern in these methods. This paper proposes an approach leveraging domain slicing to facilitate explanation generation for NNs. By reducing the complexity of logical constraints through slicing, we decrease explanation time by up to 40\% less time, as indicated through comparative experiments. Our findings highlight the efficacy of domain slicing in enhancing explanation efficiency for NNs.</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-31">正文（抓取，非 AI）</h3>
<p>[2602.22115v1] Slice and Explain: Logic-Based Explanations for Neural Networks through Domain Slicing Computer Science &gt; Logic in Computer Science arXiv:2602.22115v1 (cs) [Submitted on 25 Feb 2026] Title: Slice and Explain: Logic-Based Explanations for Neural Networks through Domain Slicing Authors: Luiz Fernando Paulino Queiroz , Carlos Henrique Leitão Cavalcante , Thiago Alves Rocha View a PDF of the paper titled Slice and Explain: Logic-Based Explanations for Neural Networks through Domain Slicing, by Luiz Fernando Paulino Queiroz and Carlos Henrique Leit\~ao Cavalcante and Thiago Alves Rocha View PDF HTML (experimental) Abstract: Neural networks (NNs) are pervasive across various domains but often lack interpretability. To address the growing need for explanations, logic-based approaches have been proposed to explain predictions made by NNs, offering correctness guarantees. However, scalability remains a concern in these methods. This paper proposes an approach leveraging domain slicing to facilitate explanation generation for NNs. By reducing the complexity of logical constraints through slicing, we decrease explanation time by up to 40\% less time, as indicated through comparative experiments. Our findings highlight the efficacy of domain slicing in enhancing explanation efficiency for NNs. Comments: Preprint version. For the final published version, see the DOI below Subjects: Logic in Computer Science (cs.LO) ; Machine Learning (cs.LG) Cite as: arXiv:2602.22115 [cs.LO] (or arXiv:2602.22115v1 [cs.LO] for this version) https://doi.org/10.48550/arXiv.2602.22115 Focus to learn more arXiv-issued DOI via DataCite (pending registration) Related DOI : https://doi.org/10.1007/978-3-032-15984-7_33 Focus to learn more DOI(s) linking to related resources Submission history From: Thiago Alves Rocha [ view email ] [v1] Wed, 25 Feb 2026 17:01:52 UTC (49 KB) Full-text links: Access Paper: View a PDF of the paper titled Slice and Explain: Logic-Based Explanations for Neural Networks through Domain Slicing, by Luiz Fernando Paulino Queiroz and Carlos Henrique Leit\~ao Cavalcante and Thiago Alves Rocha View PDF HTML (experimental) TeX Source view license Current browse context: cs.LO &lt; prev | next &gt; new | recent | 2026-02 Change to browse by: cs cs.LG References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-32">17. Detection, coverage and percolation in dynamic Boolean models with random radii based on $α$-stable processes</h2>
<ul>
<li>链接：https://arxiv.org/abs/2602.22109v1</li>
<li>来源：arxiv</li>
<li>摘要：We consider a dynamic network in continuum time and space in which nodes, with initial locations given by a Poisson point process, move according to i.i.d. isotropic $α$-stable processes. Each node is additionally equipped with an i.i.d. detection radius. Inspired by corresponding results by Peres et. al. on mobile networks based on Brownian sausages with fixed width, we investigate the tail behaviour of three stopping times: The detection time of the first discovery of a designated node, the first coverage of an entire set, and the first discovery of a node by the infinite connected component of the system. Broadly speaking, we discover that the stability index as well as the random radii manifest themselves only in constants in the otherwise exponential decay rates. The proofs rest on heat-kernel bounds for the underlying Lévy processes and a detailed multiscale analysis allowing us to control the space-time correlations of the system.</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-33">正文（抓取，非 AI）</h3>
<p>[2602.22109v1] Detection, coverage and percolation in dynamic Boolean models with random radii based on $α$-stable processes Mathematics &gt; Probability arXiv:2602.22109v1 (math) [Submitted on 25 Feb 2026] Title: Detection, coverage and percolation in dynamic Boolean models with random radii based on $α$-stable processes Authors: Peter Gracar , Benedikt Jahnel , Lukas Lüchtrath , Anh Duc Vu View a PDF of the paper titled Detection, coverage and percolation in dynamic Boolean models with random radii based on $\alpha$-stable processes, by Peter Gracar and 3 other authors View PDF HTML (experimental) Abstract: We consider a dynamic network in continuum time and space in which nodes, with initial locations given by a Poisson point process, move according to i.i.d. isotropic $\alpha$-stable processes. Each node is additionally equipped with an i.i.d. detection radius. Inspired by corresponding results by Peres et. al. on mobile networks based on Brownian sausages with fixed width, we investigate the tail behaviour of three stopping times: The detection time of the first discovery of a designated node, the first coverage of an entire set, and the first discovery of a node by the infinite connected component of the system. Broadly speaking, we discover that the stability index as well as the random radii manifest themselves only in constants in the otherwise exponential decay rates. The proofs rest on heat-kernel bounds for the underlying Lévy processes and a detailed multiscale analysis allowing us to control the space-time correlations of the system. Subjects: Probability (math.PR) MSC classes: 60D05, secondary: 60K35 Cite as: arXiv:2602.22109 [math.PR] (or arXiv:2602.22109v1 [math.PR] for this version) https://doi.org/10.48550/arXiv.2602.22109 Focus to learn more arXiv-issued DOI via DataCite (pending registration) Submission history From: Anh Duc Vu [ view email ] [v1] Wed, 25 Feb 2026 16:57:26 UTC (565 KB) Full-text links: Access Paper: View a PDF of the paper titled Detection, coverage and percolation in dynamic Boolean models with random radii based on $\alpha$-stable processes, by Peter Gracar and 3 other authors View PDF HTML (experimental) TeX Source view license Current browse context: math.PR &lt; prev | next &gt; new | recent | 2026-02 Change to browse by: math References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-34">18. SigmaQuant: Hardware-Aware Heterogeneous Quantization Method for Edge DNN Inference</h2>
<ul>
<li>链接：https://arxiv.org/abs/2602.22136v1</li>
<li>来源：arxiv</li>
<li>摘要：Deep neural networks (DNNs) are essential for performing advanced tasks on edge or mobile devices, yet their deployment is often hindered by severe resource constraints, including limited memory, energy, and computational power. While uniform quantization provides a straightforward approach to compress model and reduce hardware requirement, it fails to fully leverage the varying robustness across layers, and often lead to accuracy degradation or suboptimal resource usage, particularly at low bitwidths. In contrast, heterogeneous quantization, which allocates different bitwidths to individual layers, can mitigate these drawbacks. Nonetheless, current heterogeneous quantization methods either needs huge brute-force design space search or lacks the adaptability to meet different hardware conditions, such as memory size, energy budget, and latency requirement. Filling these gaps, this work introduces \textbf{\textit{SigmaQuant}}, an adaptive layer-wise heterogeneous quantization framework designed to efficiently balance accuracy and resource usage for varied edge environments without exhaustive search.</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-35">正文（抓取，非 AI）</h3>
<p>[2602.22136v1] SigmaQuant: Hardware-Aware Heterogeneous Quantization Method for Edge DNN Inference Computer Science &gt; Machine Learning arXiv:2602.22136v1 (cs) [Submitted on 25 Feb 2026] Title: SigmaQuant: Hardware-Aware Heterogeneous Quantization Method for Edge DNN Inference Authors: Qunyou Liu , Pengbo Yu , Marina Zapater , David Atienza View a PDF of the paper titled SigmaQuant: Hardware-Aware Heterogeneous Quantization Method for Edge DNN Inference, by Qunyou Liu and 3 other authors View PDF HTML (experimental) Abstract: Deep neural networks (DNNs) are essential for performing advanced tasks on edge or mobile devices, yet their deployment is often hindered by severe resource constraints, including limited memory, energy, and computational power. While uniform quantization provides a straightforward approach to compress model and reduce hardware requirement, it fails to fully leverage the varying robustness across layers, and often lead to accuracy degradation or suboptimal resource usage, particularly at low bitwidths. In contrast, heterogeneous quantization, which allocates different bitwidths to individual layers, can mitigate these drawbacks. Nonetheless, current heterogeneous quantization methods either needs huge brute-force design space search or lacks the adaptability to meet different hardware conditions, such as memory size, energy budget, and latency requirement. Filling these gaps, this work introduces \textbf{\textit{SigmaQuant}}, an adaptive layer-wise heterogeneous quantization framework designed to efficiently balance accuracy and resource usage for varied edge environments without exhaustive search. Subjects: Machine Learning (cs.LG) ; Hardware Architecture (cs.AR) Cite as: arXiv:2602.22136 [cs.LG] (or arXiv:2602.22136v1 [cs.LG] for this version) https://doi.org/10.48550/arXiv.2602.22136 Focus to learn more arXiv-issued DOI via DataCite (pending registration) Submission history From: Qunyou Liu Mr [ view email ] [v1] Wed, 25 Feb 2026 17:34:14 UTC (1,918 KB) Full-text links: Access Paper: View a PDF of the paper titled SigmaQuant: Hardware-Aware Heterogeneous Quantization Method for Edge DNN Inference, by Qunyou Liu and 3 other authors View PDF HTML (experimental) TeX Source view license Current browse context: cs.LG &lt; prev | next &gt; new | recent | 2026-02 Change to browse by: cs cs.AR References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) IArxiv recommender toggle IArxiv Recommender ( What is IArxiv? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-36">19. MBD-ML: Many-body dispersion from machine learning for molecules and materials</h2>
<ul>
<li>链接：https://arxiv.org/abs/2602.22086v1</li>
<li>来源：arxiv</li>
<li>摘要：Van der Waals (vdW) interactions are essential for describing molecules and materials, from drug design and catalysis to battery applications. These omnipresent interactions must also be accurately included in machine-learned force fields. The many-body dispersion (MBD) method stands out as one of the most accurate and transferable approaches to capture vdW interactions, requiring only atomic $C_6$ coefficients and polarizabilities as input. We present MBD-ML, a pretrained message passing neural network that predicts these atomic properties directly from atomic structures. Through seamless integration with libMBD, our method enables the immediate calculation of MBD-inclusive total energies, forces, and stress tensors. By eliminating the need for intermediate electronic structure calculations, MBD-ML offers a practical and streamlined tool that simplifies the incorporation of state-of-the-art vdW interactions into any electronic structure code, as well as empirical and machine-learned force fields.</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-37">正文（抓取，非 AI）</h3>
<p>[2602.22086v1] MBD-ML: Many-body dispersion from machine learning for molecules and materials Physics &gt; Chemical Physics arXiv:2602.22086v1 (physics) [Submitted on 25 Feb 2026] Title: MBD-ML: Many-body dispersion from machine learning for molecules and materials Authors: Evgeny Moerman , Adil Kabylda , Almaz Khabibrakhmanov , Alexandre Tkatchenko View a PDF of the paper titled MBD-ML: Many-body dispersion from machine learning for molecules and materials, by Evgeny Moerman and 3 other authors View PDF HTML (experimental) Abstract: Van der Waals (vdW) interactions are essential for describing molecules and materials, from drug design and catalysis to battery applications. These omnipresent interactions must also be accurately included in machine-learned force fields. The many-body dispersion (MBD) method stands out as one of the most accurate and transferable approaches to capture vdW interactions, requiring only atomic $C_6$ coefficients and polarizabilities as input. We present MBD-ML, a pretrained message passing neural network that predicts these atomic properties directly from atomic structures. Through seamless integration with libMBD, our method enables the immediate calculation of MBD-inclusive total energies, forces, and stress tensors. By eliminating the need for intermediate electronic structure calculations, MBD-ML offers a practical and streamlined tool that simplifies the incorporation of state-of-the-art vdW interactions into any electronic structure code, as well as empirical and machine-learned force fields. Comments: 22 pages, 6 figures, Supplementary Information (12 figures) Subjects: Chemical Physics (physics.chem-ph) ; Materials Science (cond-mat.mtrl-sci); Machine Learning (cs.LG); Computational Physics (physics.comp-ph) Cite as: arXiv:2602.22086 [physics.chem-ph] (or arXiv:2602.22086v1 [physics.chem-ph] for this version) https://doi.org/10.48550/arXiv.2602.22086 Focus to learn more arXiv-issued DOI via DataCite (pending registration) Submission history From: Evgeny Moerman [ view email ] [v1] Wed, 25 Feb 2026 16:34:53 UTC (9,158 KB) Full-text links: Access Paper: View a PDF of the paper titled MBD-ML: Many-body dispersion from machine learning for molecules and materials, by Evgeny Moerman and 3 other authors View PDF HTML (experimental) TeX Source view license Current browse context: physics.chem-ph &lt; prev | next &gt; new | recent | 2026-02 Change to browse by: cond-mat cond-mat.mtrl-sci cs cs.LG physics physics.comp-ph References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-38">20. Stream Neural Networks: Epoch-Free Learning with Persistent Temporal State</h2>
<ul>
<li>链接：https://arxiv.org/abs/2602.22152v1</li>
<li>来源：arxiv</li>
<li>摘要：Most contemporary neural learning systems rely on epoch-based optimization and repeated access to historical data, implicitly assuming reversible computation. In contrast, real-world environments often present information as irreversible streams, where inputs cannot be replayed or revisited. Under such conditions, conventional architectures degrade into reactive filters lacking long-horizon coherence. This paper introduces Stream Neural Networks (StNN), an execution paradigm designed for irreversible input streams. StNN operates through a stream-native execution algorithm, the Stream Network Algorithm (SNA), whose fundamental unit is the stream neuron. Each stream neuron maintains a persistent temporal state that evolves continuously across inputs. We formally establish three structural guarantees: (1) stateless mappings collapse under irreversibility and cannot encode temporal dependencies; (2) persistent state dynamics remain bounded under mild activation constraints; and (3) the state transition operator is contractive for λ &lt; 1, ensuring stable long-horizon execution. Empirical phase-space analysis and continuous tracking experiments validate these theoretical results. The exec</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-39">正文（抓取，非 AI）</h3>
<p>[2602.22152v1] Stream Neural Networks: Epoch-Free Learning with Persistent Temporal State Computer Science &gt; Neural and Evolutionary Computing arXiv:2602.22152v1 (cs) [Submitted on 25 Feb 2026] Title: Stream Neural Networks: Epoch-Free Learning with Persistent Temporal State Authors: Amama Pathan View a PDF of the paper titled Stream Neural Networks: Epoch-Free Learning with Persistent Temporal State, by Amama Pathan View PDF Abstract: Most contemporary neural learning systems rely on epoch-based optimization and repeated access to historical data, implicitly assuming reversible computation. In contrast, real-world environments often present information as irreversible streams, where inputs cannot be replayed or revisited. Under such conditions, conventional architectures degrade into reactive filters lacking long-horizon coherence. This paper introduces Stream Neural Networks (StNN), an execution paradigm designed for irreversible input streams. StNN operates through a stream-native execution algorithm, the Stream Network Algorithm (SNA), whose fundamental unit is the stream neuron. Each stream neuron maintains a persistent temporal state that evolves continuously across inputs. We formally establish three structural guarantees: (1) stateless mappings collapse under irreversibility and cannot encode temporal dependencies; (2) persistent state dynamics remain bounded under mild activation constraints; and (3) the state transition operator is contractive for {\lambda} &lt; 1, ensuring stable long-horizon execution. Empirical phase-space analysis and continuous tracking experiments validate these theoretical results. The execution principles introduced in this work define a minimal substrate for neural computation under irreversible streaming constraints. Comments: Technical report; 4 figures; LaTeX source included; code available at this https URL Subjects: Neural and Evolutionary Computing (cs.NE) Cite as: arXiv:2602.22152 [cs.NE] (or arXiv:2602.22152v1 [cs.NE] for this version) https://doi.org/10.48550/arXiv.2602.22152 Focus to learn more arXiv-issued DOI via DataCite (pending registration) Submission history From: Amama Pathan [ view email ] [v1] Wed, 25 Feb 2026 18:00:17 UTC (986 KB) Full-text links: Access Paper: View a PDF of the paper titled Stream Neural Networks: Epoch-Free Learning with Persistent Temporal State, by Amama Pathan View PDF TeX Source view license Current browse context: cs.NE &lt; prev | next &gt; new | recent | 2026-02 Change to browse by: cs References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-40">21. Self-stabilized high-dimensional quantum key distribution on a metropolitan free-space link</h2>
<ul>
<li>链接：https://arxiv.org/abs/2602.22102v1</li>
<li>来源：arxiv</li>
<li>摘要：Quantum communication technologies capable of operating reliably across heterogeneous optical channels are essential for scalable metropolitan quantum networks. Here we demonstrate high-dimensional time-bin-encoded quantum key distribution over a hybrid metropolitan link comprising 1.7 km free-space transmission and 685 m of optical fiber. Operating at a clock rate of 500 MHz in the C-band, we implement both 2- and 4-dimensional protocols, and obtain estimated secure finite-key rates of (95 +- 28) kbit/s for 4D at (25.0 +- 2.0) dB loss and (59 +- 27) kbit/s for 2D at (23.5 +- 2.3) dB loss. Crucially, we achieve continuous operation over 48 h in a fully self-referenced architecture: initial synchronization, interferometric phase stabilization, and long-term drift compensation are performed exclusively using the detected quantum signals, without auxiliary optical reference channels. Our results thus establish a practical and versatile platform for hybrid free-space-to-fiber quantum communication and show that the encoding dimensionality can be adapted to the optimal operating regime of realistic metropolitan channels, providing a pathway toward efficient, autonomous and deployable qu</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-41">正文（抓取，非 AI）</h3>
<p>[2602.22102v1] Self-stabilized high-dimensional quantum key distribution on a metropolitan free-space link Quantum Physics arXiv:2602.22102v1 (quant-ph) [Submitted on 25 Feb 2026] Title: Self-stabilized high-dimensional quantum key distribution on a metropolitan free-space link Authors: Karolina Dziwulska , Christopher Spiess , Sarika Mishra , Markus Leipe , Yugant Hadiyal , Fabian Steinlechner View a PDF of the paper titled Self-stabilized high-dimensional quantum key distribution on a metropolitan free-space link, by Karolina Dziwulska and 5 other authors View PDF HTML (experimental) Abstract: Quantum communication technologies capable of operating reliably across heterogeneous optical channels are essential for scalable metropolitan quantum networks. Here we demonstrate high-dimensional time-bin-encoded quantum key distribution over a hybrid metropolitan link comprising 1.7 km free-space transmission and 685 m of optical fiber. Operating at a clock rate of 500 MHz in the C-band, we implement both 2- and 4-dimensional protocols, and obtain estimated secure finite-key rates of (95 +- 28) kbit/s for 4D at (25.0 +- 2.0) dB loss and (59 +- 27) kbit/s for 2D at (23.5 +- 2.3) dB loss. Crucially, we achieve continuous operation over 48 h in a fully self-referenced architecture: initial synchronization, interferometric phase stabilization, and long-term drift compensation are performed exclusively using the detected quantum signals, without auxiliary optical reference channels. Our results thus establish a practical and versatile platform for hybrid free-space-to-fiber quantum communication and show that the encoding dimensionality can be adapted to the optimal operating regime of realistic metropolitan channels, providing a pathway toward efficient, autonomous and deployable quantum network nodes. Subjects: Quantum Physics (quant-ph) Cite as: arXiv:2602.22102 [quant-ph] (or arXiv:2602.22102v1 [quant-ph] for this version) https://doi.org/10.48550/arXiv.2602.22102 Focus to learn more arXiv-issued DOI via DataCite (pending registration) Submission history From: Fabian Steinlechner [ view email ] [v1] Wed, 25 Feb 2026 16:51:05 UTC (659 KB) Full-text links: Access Paper: View a PDF of the paper titled Self-stabilized high-dimensional quantum key distribution on a metropolitan free-space link, by Karolina Dziwulska and 5 other authors View PDF HTML (experimental) TeX Source view license Current browse context: quant-ph &lt; prev | next &gt; new | recent | 2026-02 References &amp; Citations INSPIRE HEP NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-42">22. Lowering the temperature of two-dimensional fermionic tensor networks with cluster expansions</h2>
<ul>
<li>链接：https://arxiv.org/abs/2602.22113v1</li>
<li>来源：arxiv</li>
<li>摘要：Representing the time-evolution operator as a tensor network constitutes a key ingredient in several algorithms for studying quantum lattice systems at finite temperature or in a non-equilibrium setting. For a Hamiltonian composed of strictly short-ranged interactions, the Suzuki-Trotter decomposition is the main technique for obtaining such a representation. In [B.~Vanhecke, L.~Vanderstraeten and F.~Verstraete, Physical Review A, L020402 (2021)], an alternative strategy, the cluster expansion, was introduced. This approach naturally preserves internal and lattice symmetries and can more easily be extended to higher-order representations or longer-ranged interactions. We extend the cluster expansion to two-dimensional fermionic systems, and employ it to construct projected entangled-pair operator (PEPO) approximations of Gibbs states. We also discuss and benchmark different truncation schemes for multiplying layers of PEPOs together. Applying the resulting framework to a two-dimensional spinless fermion model with attractive interactions, we resolve a clear phase boundary at finite temperature.</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-43">正文（抓取，非 AI）</h3>
<p>[2602.22113v1] Lowering the temperature of two-dimensional fermionic tensor networks with cluster expansions Condensed Matter &gt; Strongly Correlated Electrons arXiv:2602.22113v1 (cond-mat) [Submitted on 25 Feb 2026] Title: Lowering the temperature of two-dimensional fermionic tensor networks with cluster expansions Authors: Sander De Meyer , Atsushi Ueda , Yuchi He , Nick Bultinck , Jutho Haegeman View a PDF of the paper titled Lowering the temperature of two-dimensional fermionic tensor networks with cluster expansions, by Sander De Meyer and 4 other authors View PDF HTML (experimental) Abstract: Representing the time-evolution operator as a tensor network constitutes a key ingredient in several algorithms for studying quantum lattice systems at finite temperature or in a non-equilibrium setting. For a Hamiltonian composed of strictly short-ranged interactions, the Suzuki-Trotter decomposition is the main technique for obtaining such a representation. In [B.~Vanhecke, L.~Vanderstraeten and F.~Verstraete, Physical Review A, L020402 (2021)], an alternative strategy, the cluster expansion, was introduced. This approach naturally preserves internal and lattice symmetries and can more easily be extended to higher-order representations or longer-ranged interactions. We extend the cluster expansion to two-dimensional fermionic systems, and employ it to construct projected entangled-pair operator (PEPO) approximations of Gibbs states. We also discuss and benchmark different truncation schemes for multiplying layers of PEPOs together. Applying the resulting framework to a two-dimensional spinless fermion model with attractive interactions, we resolve a clear phase boundary at finite temperature. Subjects: Strongly Correlated Electrons (cond-mat.str-el) ; Quantum Physics (quant-ph) Cite as: arXiv:2602.22113 [cond-mat.str-el] (or arXiv:2602.22113v1 [cond-mat.str-el] for this version) https://doi.org/10.48550/arXiv.2602.22113 Focus to learn more arXiv-issued DOI via DataCite (pending registration) Submission history From: Sander De Meyer [ view email ] [v1] Wed, 25 Feb 2026 16:58:58 UTC (289 KB) Full-text links: Access Paper: View a PDF of the paper titled Lowering the temperature of two-dimensional fermionic tensor networks with cluster expansions, by Sander De Meyer and 4 other authors View PDF HTML (experimental) TeX Source view license Current browse context: cond-mat.str-el &lt; prev | next &gt; new | recent | 2026-02 Change to browse by: cond-mat quant-ph References &amp; Citations INSPIRE HEP NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) IArxiv recommender toggle IArxiv Recommender ( What is IArxiv? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-44">23. Transmission Delay Minimization for NOMA-Based F-RANs</h2>
<ul>
<li>链接：https://arxiv.org/abs/2602.22087v1</li>
<li>来源：arxiv</li>
<li>摘要：A novel non-orthogonal multiple access (NOMA) based low-delay service framework is proposed for fog radio access networks (F-RANs). Fog access points (FAPs) leverage NOMA for local delivery of cached content, while the cloud access point employs NOMA to simultaneously push content to FAPs and directly serve users. Based on this model, a delay minimization problem is formulated by jointly optimizing user association, cache placement, and power allocation. To address this non-convex mixed-integer nonlinear programming problem, an alternating optimization (AO) algorithm is developed, which decomposes the original problem into two subproblems, namely joint user association and cache placement, and power allocation. In particular, a low-complexity algorithm is designed to optimizing the user association and cache placement strategy using the McCormick envelope theory and Lagrangian partial relaxation. The power allocation is optimized by invoking the successive convex approximation. Simulation results reveal that: 1) the proposed AO-based algorithm effectively balances between the achieved performance and computational efficiency, and 2) the proposed NOMA-based F-RANs framework signific</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-45">正文（抓取，非 AI）</h3>
<p>[2602.22087v1] Transmission Delay Minimization for NOMA-Based F-RANs Electrical Engineering and Systems Science &gt; Signal Processing arXiv:2602.22087v1 (eess) [Submitted on 25 Feb 2026] Title: Transmission Delay Minimization for NOMA-Based F-RANs Authors: Yuan Ai , Xidong Mu , Pengbo Si , Yuanwei Liu View a PDF of the paper titled Transmission Delay Minimization for NOMA-Based F-RANs, by Yuan Ai and 3 other authors View PDF HTML (experimental) Abstract: A novel non-orthogonal multiple access (NOMA) based low-delay service framework is proposed for fog radio access networks (F-RANs). Fog access points (FAPs) leverage NOMA for local delivery of cached content, while the cloud access point employs NOMA to simultaneously push content to FAPs and directly serve users. Based on this model, a delay minimization problem is formulated by jointly optimizing user association, cache placement, and power allocation. To address this non-convex mixed-integer nonlinear programming problem, an alternating optimization (AO) algorithm is developed, which decomposes the original problem into two subproblems, namely joint user association and cache placement, and power allocation. In particular, a low-complexity algorithm is designed to optimizing the user association and cache placement strategy using the McCormick envelope theory and Lagrangian partial relaxation. The power allocation is optimized by invoking the successive convex approximation. Simulation results reveal that: 1) the proposed AO-based algorithm effectively balances between the achieved performance and computational efficiency, and 2) the proposed NOMA-based F-RANs framework significantly outperforms orthogonal multiple access-based F-RANs systems in terms of average transmission delay in different scenarios. Comments: Accepted by IEEE Transactions on Wireless Communications Subjects: Signal Processing (eess.SP) Cite as: arXiv:2602.22087 [eess.SP] (or arXiv:2602.22087v1 [eess.SP] for this version) https://doi.org/10.48550/arXiv.2602.22087 Focus to learn more arXiv-issued DOI via DataCite (pending registration) Related DOI : https://doi.org/10.1109/TWC.2026.3668116 Focus to learn more DOI(s) linking to related resources Submission history From: Yuan Ai [ view email ] [v1] Wed, 25 Feb 2026 16:35:19 UTC (1,409 KB) Full-text links: Access Paper: View a PDF of the paper titled Transmission Delay Minimization for NOMA-Based F-RANs, by Yuan Ai and 3 other authors View PDF HTML (experimental) TeX Source view license Current browse context: eess.SP &lt; prev | next &gt; new | recent | 2026-02 Change to browse by: eess References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-46">24. Don't stop me now: Rethinking Validation Criteria for Model Parameter Selection</h2>
<ul>
<li>链接：https://arxiv.org/abs/2602.22107v1</li>
<li>来源：arxiv</li>
<li>摘要：Despite the extensive literature on training loss functions, the evaluation of generalization on the validation set remains underexplored. In this work, we conduct a systematic empirical and statistical study of how the validation criterion used for model selection affects test performance in neural classifiers, with attention to early stopping. Using fully connected networks on standard benchmarks under $k$-fold evaluation, we compare: (i) early stopping with patience and (ii) post-hoc selection over all epochs (i.e. no early stopping). Models are trained with cross-entropy, C-Loss, or PolyLoss; the model parameter selection on the validation set is made using accuracy or one of the three loss functions, each considered independently. Three main findings emerge. (1) Early stopping based on validation accuracy performs worst, consistently selecting checkpoints with lower test accuracy than both loss-based early stopping and post-hoc selection. (2) Loss-based validation criteria yield comparable and more stable test accuracy. (3) Across datasets and folds, any single validation rule often underperforms the test-optimal checkpoint. Overall, the selected model typically achieves test-</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-47">正文（抓取，非 AI）</h3>
<p>[2602.22107v1] Don't stop me now: Rethinking Validation Criteria for Model Parameter Selection Computer Science &gt; Machine Learning arXiv:2602.22107v1 (cs) [Submitted on 25 Feb 2026] Title: Don't stop me now: Rethinking Validation Criteria for Model Parameter Selection Authors: Andrea Apicella , Francesco Isgrò , Andrea Pollastro , Roberto Prevete View a PDF of the paper titled Don't stop me now: Rethinking Validation Criteria for Model Parameter Selection, by Andrea Apicella and 3 other authors View PDF Abstract: Despite the extensive literature on training loss functions, the evaluation of generalization on the validation set remains underexplored. In this work, we conduct a systematic empirical and statistical study of how the validation criterion used for model selection affects test performance in neural classifiers, with attention to early stopping. Using fully connected networks on standard benchmarks under $k$-fold evaluation, we compare: (i) early stopping with patience and (ii) post-hoc selection over all epochs (i.e. no early stopping). Models are trained with cross-entropy, C-Loss, or PolyLoss; the model parameter selection on the validation set is made using accuracy or one of the three loss functions, each considered independently. Three main findings emerge. (1) Early stopping based on validation accuracy performs worst, consistently selecting checkpoints with lower test accuracy than both loss-based early stopping and post-hoc selection. (2) Loss-based validation criteria yield comparable and more stable test accuracy. (3) Across datasets and folds, any single validation rule often underperforms the test-optimal checkpoint. Overall, the selected model typically achieves test-set performance statistically lower than the best performance across all epochs, regardless of the validation criterion. Our results suggest avoiding validation accuracy (in particular with early stopping) for parameter selection, favoring loss-based validation criteria. Subjects: Machine Learning (cs.LG) ; Artificial Intelligence (cs.AI) Cite as: arXiv:2602.22107 [cs.LG] (or arXiv:2602.22107v1 [cs.LG] for this version) https://doi.org/10.48550/arXiv.2602.22107 Focus to learn more arXiv-issued DOI via DataCite (pending registration) Submission history From: Andrea Apicella [ view email ] [v1] Wed, 25 Feb 2026 16:56:14 UTC (3,251 KB) Full-text links: Access Paper: View a PDF of the paper titled Don't stop me now: Rethinking Validation Criteria for Model Parameter Selection, by Andrea Apicella and 3 other authors View PDF TeX Source view license Current browse context: cs.LG &lt; prev | next &gt; new | recent | 2026-02 Change to browse by: cs cs.AI References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) IArxiv recommender toggle IArxiv Recommender ( What is IArxiv? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-48">25. Force Policy: Learning Hybrid Force-Position Control Policy under Interaction Frame for Contact-Rich Manipulation</h2>
<ul>
<li>链接：https://arxiv.org/abs/2602.22088v1</li>
<li>来源：arxiv</li>
<li>摘要：Contact-rich manipulation demands human-like integration of perception and force feedback: vision should guide task progress, while high-frequency interaction control must stabilize contact under uncertainty. Existing learning-based policies often entangle these roles in a monolithic network, trading off global generalization against stable local refinement, while control-centric approaches typically assume a known task structure or learn only controller parameters rather than the structure itself. In this paper, we formalize a physically grounded interaction frame, an instantaneous local basis that decouples force regulation from motion execution, and propose a method to recover it from demonstrations. Based on this, we address both issues by proposing Force Policy, a global-local vision-force policy in which a global policy guides free-space actions using vision, and upon contact, a high-frequency local policy with force feedback estimates the interaction frame and executes hybrid force-position control for stable interaction. Real-world experiments across diverse contact-rich tasks show consistent gains over strong baselines, with more robust contact establishment, more accurate</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-49">正文（抓取，非 AI）</h3>
<p>[2602.22088v1] Force Policy: Learning Hybrid Force-Position Control Policy under Interaction Frame for Contact-Rich Manipulation Computer Science &gt; Robotics arXiv:2602.22088v1 (cs) [Submitted on 25 Feb 2026] Title: Force Policy: Learning Hybrid Force-Position Control Policy under Interaction Frame for Contact-Rich Manipulation Authors: Hongjie Fang , Shirun Tang , Mingyu Mei , Haoxiang Qin , Zihao He , Jingjing Chen , Ying Feng , Chenxi Wang , Wanxi Liu , Zaixing He , Cewu Lu , Shiquan Wang View a PDF of the paper titled Force Policy: Learning Hybrid Force-Position Control Policy under Interaction Frame for Contact-Rich Manipulation, by Hongjie Fang and 11 other authors View PDF HTML (experimental) Abstract: Contact-rich manipulation demands human-like integration of perception and force feedback: vision should guide task progress, while high-frequency interaction control must stabilize contact under uncertainty. Existing learning-based policies often entangle these roles in a monolithic network, trading off global generalization against stable local refinement, while control-centric approaches typically assume a known task structure or learn only controller parameters rather than the structure itself. In this paper, we formalize a physically grounded interaction frame, an instantaneous local basis that decouples force regulation from motion execution, and propose a method to recover it from demonstrations. Based on this, we address both issues by proposing Force Policy, a global-local vision-force policy in which a global policy guides free-space actions using vision, and upon contact, a high-frequency local policy with force feedback estimates the interaction frame and executes hybrid force-position control for stable interaction. Real-world experiments across diverse contact-rich tasks show consistent gains over strong baselines, with more robust contact establishment, more accurate force regulation, and reliable generalization to novel objects with varied geometries and physical properties, ultimately improving both contact stability and execution quality. Project page: this https URL Subjects: Robotics (cs.RO) Cite as: arXiv:2602.22088 [cs.RO] (or arXiv:2602.22088v1 [cs.RO] for this version) https://doi.org/10.48550/arXiv.2602.22088 Focus to learn more arXiv-issued DOI via DataCite (pending registration) Submission history From: Hongjie Fang [ view email ] [v1] Wed, 25 Feb 2026 16:35:24 UTC (11,524 KB) Full-text links: Access Paper: View a PDF of the paper titled Force Policy: Learning Hybrid Force-Position Control Policy under Interaction Frame for Contact-Rich Manipulation, by Hongjie Fang and 11 other authors View PDF HTML (experimental) TeX Source view license Current browse context: cs.RO &lt; prev | next &gt; new | recent | 2026-02 Change to browse by: cs References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-50">26. Applying a Random-Key Optimizer on Mixed Integer Programs</h2>
<ul>
<li>链接：https://arxiv.org/abs/2602.22173v1</li>
<li>来源：arxiv</li>
<li>摘要：Mixed-Integer Programs (MIPs) are NP-hard optimization models that arise in a broad range of decision-making applications, including finance, logistics, energy systems, and network design. Although modern commercial solvers have achieved remarkable progress and perform effectively on many small- and medium-sized instances, their performance often degrades when confronted with large-cale or highly constrained formulations. This paper explores the use of the Random-Key Optimizer (RKO) framework as a flexible, metaheuristic alternative for computing high-quality solutions to MIPs through the design of problem-specific decoders. The proposed approach separates the search process from feasibility enforcement by operating in a continuous random-key space while mapping candidate solutions to feasible integer solutions via efficient decoding procedures. We evaluate the methodology on two representative and structurally distinct benchmark problems: the mean-variance Markowitz portfolio optimization problem with buy-in and cardinality constraints, and the Time-Dependent Traveling Salesman Problem. For each formulation, tailored decoders are developed to reduce the effective search space, pro</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-51">正文（抓取，非 AI）</h3>
<p>[2602.22173v1] Applying a Random-Key Optimizer on Mixed Integer Programs Mathematics &gt; Optimization and Control arXiv:2602.22173v1 (math) [Submitted on 25 Feb 2026] Title: Applying a Random-Key Optimizer on Mixed Integer Programs Authors: Antonio A. Chaves , Mauricio G.C. Resende , Carise E. Schmidt , J. Kyle Brubaker , Helmut G. Katzgraber View a PDF of the paper titled Applying a Random-Key Optimizer on Mixed Integer Programs, by Antonio A. Chaves and 4 other authors View PDF HTML (experimental) Abstract: Mixed-Integer Programs (MIPs) are NP-hard optimization models that arise in a broad range of decision-making applications, including finance, logistics, energy systems, and network design. Although modern commercial solvers have achieved remarkable progress and perform effectively on many small- and medium-sized instances, their performance often degrades when confronted with large-cale or highly constrained formulations. This paper explores the use of the Random-Key Optimizer (RKO) framework as a flexible, metaheuristic alternative for computing high-quality solutions to MIPs through the design of problem-specific decoders. The proposed approach separates the search process from feasibility enforcement by operating in a continuous random-key space while mapping candidate solutions to feasible integer solutions via efficient decoding procedures. We evaluate the methodology on two representative and structurally distinct benchmark problems: the mean-variance Markowitz portfolio optimization problem with buy-in and cardinality constraints, and the Time-Dependent Traveling Salesman Problem. For each formulation, tailored decoders are developed to reduce the effective search space, promote feasibility, and accelerate convergence. Computational experiments demonstrate that RKO consistently produces competitive, and in several cases superior, solutions compared to a state-of-the-art commercial MIP solver, both in terms of solution quality and computational time. These results highlight the potential of RKO as a scalable and versatile heuristic framework for tackling challenging large-scale MIPs. Comments: 29 pages, 8 figures, 6 tables, 4 algorithm pseudocodes Subjects: Optimization and Control (math.OC) ; Neural and Evolutionary Computing (cs.NE) MSC classes: 90C11, 90C59, 90C27 Cite as: arXiv:2602.22173 [math.OC] (or arXiv:2602.22173v1 [math.OC] for this version) https://doi.org/10.48550/arXiv.2602.22173 Focus to learn more arXiv-issued DOI via DataCite (pending registration) Submission history From: Mauricio Resende [ view email ] [v1] Wed, 25 Feb 2026 18:20:03 UTC (2,065 KB) Full-text links: Access Paper: View a PDF of the paper titled Applying a Random-Key Optimizer on Mixed Integer Programs, by Antonio A. Chaves and 4 other authors View PDF HTML (experimental) TeX Source view license Current browse context: math.OC &lt; prev | next &gt; new | recent | 2026-02 Change to browse by: cs cs.NE math References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-52">27. Surrogate models for Rock-Fluid Interaction: A Grid-Size-Invariant Approach</h2>
<ul>
<li>链接：https://arxiv.org/abs/2602.22188v1</li>
<li>来源：arxiv</li>
<li>摘要：Modelling rock-fluid interaction requires solving a set of partial differential equations (PDEs) to predict the flow behaviour and the reactions of the fluid with the rock on the interfaces. Conventional high-fidelity numerical models require a high resolution to obtain reliable results, resulting in huge computational expense. This restricts the applicability of these models for multi-query problems, such as uncertainty quantification and optimisation, which require running numerous scenarios. As a cheaper alternative to high-fidelity models, this work develops eight surrogate models for predicting the fluid flow in porous media. Four of these are reduced-order models (ROM) based on one neural network for compression and another for prediction. The other four are single neural networks with the property of grid-size invariance; a term which we use to refer to image-to-image models that are capable of inferring on computational domains that are larger than those used during training. In addition to the novel grid-size-invariant framework for surrogate models, we compare the predictive performance of UNet and UNet++ architectures, and demonstrate that UNet++ outperforms UNet for sur</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-53">正文（抓取，非 AI）</h3>
<p>[2602.22188v1] Surrogate models for Rock-Fluid Interaction: A Grid-Size-Invariant Approach Computer Science &gt; Machine Learning arXiv:2602.22188v1 (cs) [Submitted on 25 Feb 2026] Title: Surrogate models for Rock-Fluid Interaction: A Grid-Size-Invariant Approach Authors: Nathalie C. Pinheiro , Donghu Guo , Hannah P. Menke , Aniket C. Joshi , Claire E. Heaney , Ahmed H. ElSheikh , Christopher C. Pain View a PDF of the paper titled Surrogate models for Rock-Fluid Interaction: A Grid-Size-Invariant Approach, by Nathalie C. Pinheiro and 6 other authors View PDF HTML (experimental) Abstract: Modelling rock-fluid interaction requires solving a set of partial differential equations (PDEs) to predict the flow behaviour and the reactions of the fluid with the rock on the interfaces. Conventional high-fidelity numerical models require a high resolution to obtain reliable results, resulting in huge computational expense. This restricts the applicability of these models for multi-query problems, such as uncertainty quantification and optimisation, which require running numerous scenarios. As a cheaper alternative to high-fidelity models, this work develops eight surrogate models for predicting the fluid flow in porous media. Four of these are reduced-order models (ROM) based on one neural network for compression and another for prediction. The other four are single neural networks with the property of grid-size invariance; a term which we use to refer to image-to-image models that are capable of inferring on computational domains that are larger than those used during training. In addition to the novel grid-size-invariant framework for surrogate models, we compare the predictive performance of UNet and UNet++ architectures, and demonstrate that UNet++ outperforms UNet for surrogate models. Furthermore, we show that the grid-size-invariant approach is a reliable way to reduce memory consumption during training, resulting in good correlation between predicted and ground-truth values and outperforming the ROMs analysed. The application analysed is particularly challenging because fluid-induced rock dissolution results in a non-static solid field and, consequently, it cannot be used to help in adjustments of the future prediction. Subjects: Machine Learning (cs.LG) ; Artificial Intelligence (cs.AI); Fluid Dynamics (physics.flu-dyn) Cite as: arXiv:2602.22188 [cs.LG] (or arXiv:2602.22188v1 [cs.LG] for this version) https://doi.org/10.48550/arXiv.2602.22188 Focus to learn more arXiv-issued DOI via DataCite (pending registration) Submission history From: Nathalie Carvalho Pinheiro [ view email ] [v1] Wed, 25 Feb 2026 18:34:03 UTC (11,355 KB) Full-text links: Access Paper: View a PDF of the paper titled Surrogate models for Rock-Fluid Interaction: A Grid-Size-Invariant Approach, by Nathalie C. Pinheiro and 6 other authors View PDF HTML (experimental) TeX Source view license Current browse context: cs.LG &lt; prev | next &gt; new | recent | 2026-02 Change to browse by: cs cs.AI physics physics.flu-dyn References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) IArxiv recommender toggle IArxiv Recommender ( What is IArxiv? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-54">28. Enabling End-to-End APT Emulation in Industrial Environments: Design and Implementation of the SIMPLE-ICS Testbed</h2>
<ul>
<li>链接：https://arxiv.org/abs/2602.22082v1</li>
<li>来源：arxiv</li>
<li>摘要：Research on Advanced Persistent Threats (APTs) in industrial environments requires experimental platforms that support realistic end-to-end attack emulation across converged enterprise IT, operational technology (OT), and Industrial Internet of Things (IIoT) networks. However, existing industrial cybersecurity testbeds typically focus on isolated IT or OT domains or single-stage attacks, limiting their suitability for studying multi-stage APT campaigns. This paper presents the design, implementation, and validation of SIMPLE-ICS, a virtualised industrial enterprise testbed that enables emulation of multi-stage APT campaigns across IT, OT, and IIoT environments. The testbed architecture is based on the Purdue Enterprise Reference Architecture, NIST SP 800-82, and IEC 62443 zoning principles and integrates enterprise services, industrial control protocols, and digital twin based process simulation. A systematic methodology inspired by the V model is used to derive architectural requirements, attack scenarios, and validation criteria. An APT campaign designed to mimic the BlackEnergy campaign is emulated using MITRE ATTACK techniques spanning initial enterprise compromise, credential </li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-55">正文（抓取，非 AI）</h3>
<p>[2602.22082v1] Enabling End-to-End APT Emulation in Industrial Environments: Design and Implementation of the SIMPLE-ICS Testbed Computer Science &gt; Cryptography and Security arXiv:2602.22082v1 (cs) [Submitted on 25 Feb 2026] Title: Enabling End-to-End APT Emulation in Industrial Environments: Design and Implementation of the SIMPLE-ICS Testbed Authors: Yogha Restu Pramadi , Theodoros Spyridopoulos , Vijay Kumar View a PDF of the paper titled Enabling End-to-End APT Emulation in Industrial Environments: Design and Implementation of the SIMPLE-ICS Testbed, by Yogha Restu Pramadi and 2 other authors View PDF Abstract: Research on Advanced Persistent Threats (APTs) in industrial environments requires experimental platforms that support realistic end-to-end attack emulation across converged enterprise IT, operational technology (OT), and Industrial Internet of Things (IIoT) networks. However, existing industrial cybersecurity testbeds typically focus on isolated IT or OT domains or single-stage attacks, limiting their suitability for studying multi-stage APT campaigns. This paper presents the design, implementation, and validation of SIMPLE-ICS, a virtualised industrial enterprise testbed that enables emulation of multi-stage APT campaigns across IT, OT, and IIoT environments. The testbed architecture is based on the Purdue Enterprise Reference Architecture, NIST SP 800-82, and IEC 62443 zoning principles and integrates enterprise services, industrial control protocols, and digital twin based process simulation. A systematic methodology inspired by the V model is used to derive architectural requirements, attack scenarios, and validation criteria. An APT campaign designed to mimic the BlackEnergy campaign is emulated using MITRE ATTACK techniques spanning initial enterprise compromise, credential abuse, lateral movement, OT network infiltration, and process manipulation. The testbed supports the synchronised collection of network traffic, host-level logs, and operational telemetry across all segments. The testbed is validated on multi-stage attack trace observability, logging completeness across IT, OT, and IIoT domains, and repeatable execution of APT campaigns. The SIMPLE-ICS testbed provides an experimental platform for studying end-to-end APT behaviours in industrial enterprise networks and for generating multi-source datasets to support future research on campaign-level detection and correlation methods. Subjects: Cryptography and Security (cs.CR) Cite as: arXiv:2602.22082 [cs.CR] (or arXiv:2602.22082v1 [cs.CR] for this version) https://doi.org/10.48550/arXiv.2602.22082 Focus to learn more arXiv-issued DOI via DataCite (pending registration) Submission history From: Yogha Restu Pramadi [ view email ] [v1] Wed, 25 Feb 2026 16:31:27 UTC (1,653 KB) Full-text links: Access Paper: View a PDF of the paper titled Enabling End-to-End APT Emulation in Industrial Environments: Design and Implementation of the SIMPLE-ICS Testbed, by Yogha Restu Pramadi and 2 other authors View PDF TeX Source view license Current browse context: cs.CR &lt; prev | next &gt; new | recent | 2026-02 Change to browse by: cs References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-56">29. Secure Semantic Communications via AI Defenses: Fundamentals, Solutions, and Future Directions</h2>
<ul>
<li>链接：https://arxiv.org/abs/2602.22134v1</li>
<li>来源：arxiv</li>
<li>摘要：Semantic communication (SemCom) redefines wireless communication from reproducing symbols to transmitting task-relevant semantics. However, this AI-native architecture also introduces new vulnerabilities, as semantic failures may arise from adversarial perturbations to models, corrupted training data, desynchronized priors, or misaligned inference even when lower-layer transmission reliability and cryptographic protection remain intact. This survey provides a defense-centered and system-oriented synthesis of security in SemCom via AI defense. We analyze AI-centric threat models by consolidating existing studies and organizing attack surfaces across model-level, channel-realizable, knowledge-based, and networked inference vectors. Building on this foundation, we present a structured taxonomy of defense strategies organized by where semantic integrity can be compromised in SemCom systems despite correct symbol delivery, spanning semantic encoding, wireless transmission, knowledge integrity, and coordination among multiple agents. These categories correspond to distinct security failure modes, including representation fragility, channel-realizable manipulation, semantic prior poisonin</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-57">正文（抓取，非 AI）</h3>
<p>[2602.22134v1] Secure Semantic Communications via AI Defenses: Fundamentals, Solutions, and Future Directions Computer Science &gt; Cryptography and Security arXiv:2602.22134v1 (cs) [Submitted on 25 Feb 2026] Title: Secure Semantic Communications via AI Defenses: Fundamentals, Solutions, and Future Directions Authors: Lan Zhang , Chengsi Liang , Zeming Zhuang , Yao Sun , Fang Fang , Xiaoyong Yuan , Dusit Niyato View a PDF of the paper titled Secure Semantic Communications via AI Defenses: Fundamentals, Solutions, and Future Directions, by Lan Zhang and 6 other authors View PDF HTML (experimental) Abstract: Semantic communication (SemCom) redefines wireless communication from reproducing symbols to transmitting task-relevant semantics. However, this AI-native architecture also introduces new vulnerabilities, as semantic failures may arise from adversarial perturbations to models, corrupted training data, desynchronized priors, or misaligned inference even when lower-layer transmission reliability and cryptographic protection remain intact. This survey provides a defense-centered and system-oriented synthesis of security in SemCom via AI defense. We analyze AI-centric threat models by consolidating existing studies and organizing attack surfaces across model-level, channel-realizable, knowledge-based, and networked inference vectors. Building on this foundation, we present a structured taxonomy of defense strategies organized by where semantic integrity can be compromised in SemCom systems despite correct symbol delivery, spanning semantic encoding, wireless transmission, knowledge integrity, and coordination among multiple agents. These categories correspond to distinct security failure modes, including representation fragility, channel-realizable manipulation, semantic prior poisoning or desynchronization, and adversarial propagation through distributed inference. We also examine security utility operating envelopes that capture tradeoffs among semantic fidelity, robustness, latency, and energy under realistic constraints, survey evaluation frameworks and representative applications, and identify open challenges in cross-layer composition and deployment-time certification. Overall, this survey offers a unified system-level perspective that enables readers to understand major threat and defense mechanisms in AI-native SemCom systems and to leverage emerging security techniques in the design and deployment of robust SemCom architectures for next-generation intelligent networks. Subjects: Cryptography and Security (cs.CR) ; Systems and Control (eess.SY) Cite as: arXiv:2602.22134 [cs.CR] (or arXiv:2602.22134v1 [cs.CR] for this version) https://doi.org/10.48550/arXiv.2602.22134 Focus to learn more arXiv-issued DOI via DataCite (pending registration) Submission history From: Chengsi Liang [ view email ] [v1] Wed, 25 Feb 2026 17:28:07 UTC (2,743 KB) Full-text links: Access Paper: View a PDF of the paper titled Secure Semantic Communications via AI Defenses: Fundamentals, Solutions, and Future Directions, by Lan Zhang and 6 other authors View PDF HTML (experimental) TeX Source view license Current browse context: cs.CR &lt; prev | next &gt; new | recent | 2026-02 Change to browse by: cs cs.SY eess eess.SY References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p></div></details>
</div>
<script>
var READING_HIGHLIGHT_CLASS='reading-highlight';
function getBlocksInOrder(container){
 var blocks=[]; var blockTags=['P','DIV','H2','H3','H4','LI','PRE'];
 function walk(el){
  if(!el||!container.contains(el))return;
  if(blockTags.indexOf(el.tagName)!==-1){
   var hasBlockChild=false;
   for(var k=0;k<el.children.length;k++){ var c=el.children[k]; if(blockTags.indexOf(c.tagName)!==-1){ hasBlockChild=true; break; } }
   if(!hasBlockChild){ var t=(el.innerText||'').trim().replace(/\\s+/g,' '); if(t)blocks.push({el:el,text:t}); }
   else for(k=0;k<el.children.length;k++)walk(el.children[k]);
  } else for(k=0;k<el.children.length;k++)walk(el.children[k]);
 }
 walk(container); return blocks;
}
function speakText(t,onDone){ if(!t){ document.getElementById('readStatus').textContent=''; if(onDone)onDone(); return; }
 speechSynthesis.cancel(); var status=document.getElementById('readStatus'); status.textContent='准备中…';
 var chunks=[]; var maxLen=280; for(var i=0;i<t.length;i+=maxLen){ var s=t.slice(i,i+maxLen); var j=s.lastIndexOf('。'); if(j>80){ chunks.push(s.slice(0,j+1)); i-=s.length-(j+1); } else chunks.push(s); }
 if(chunks.length===0)chunks=[t]; var idx=0;
 function speakNext(){ if(idx>=chunks.length){ status.textContent=''; if(onDone)onDone(); return; } var u=new SpeechSynthesisUtterance(chunks[idx]); u.lang='zh-CN'; u.rate=0.95; var v=speechSynthesis.getVoices().filter(function(x){ return x.lang.indexOf('zh')>=0; })[0]; if(v)u.voice=v;
  u.onend=function(){ idx++; setTimeout(speakNext,80); }; u.onerror=function(){ idx++; setTimeout(speakNext,80); }; status.textContent='朗读中 '+(idx+1)+'/'+chunks.length+'…'; speechSynthesis.speak(u); }
 if(speechSynthesis.getVoices().length)speakNext(); else speechSynthesis.onvoiceschanged=function(){ speechSynthesis.onvoiceschanged=null; speakNext(); }; }
function readFullText(){ var c=document.querySelector('.content'); if(!c){ document.getElementById('readStatus').textContent='无可读内容'; return; }
 var segments=getBlocksInOrder(c); if(!segments.length){ document.getElementById('readStatus').textContent='无可读内容'; return; }
 speechSynthesis.cancel(); document.querySelectorAll('.content .'+READING_HIGHLIGHT_CLASS).forEach(function(el){ el.classList.remove(READING_HIGHLIGHT_CLASS); });
 var status=document.getElementById('readStatus'); var idx=0;
 function runSegment(){ if(idx>=segments.length){ status.textContent=''; document.querySelectorAll('.content .'+READING_HIGHLIGHT_CLASS).forEach(function(el){ el.classList.remove(READING_HIGHLIGHT_CLASS); }); return; }
  var seg=segments[idx]; seg.el.scrollIntoView({behavior:'smooth',block:'center'}); document.querySelectorAll('.content .'+READING_HIGHLIGHT_CLASS).forEach(function(el){ el.classList.remove(READING_HIGHLIGHT_CLASS); }); seg.el.classList.add(READING_HIGHLIGHT_CLASS);
  status.textContent='朗读 '+(idx+1)+'/'+segments.length+'…';
  speakText(seg.text,function(){ seg.el.classList.remove(READING_HIGHLIGHT_CLASS); idx++; setTimeout(runSegment,120); }); }
 runSegment(); }
function getBlockForTap(el){ var c=document.querySelector('.content'); var blockTags=['P','DIV','H2','H3','H4','LI','PRE']; while(el&&el!==c){ if(c.contains(el)&&blockTags.indexOf(el.tagName)!==-1) return el; el=el.parentElement; } return null; }
function onContentTap(e){ if(e.target.closest&&e.target.closest('a')) return; var block=getBlockForTap(e.target); if(block){ var t=(block.innerText||'').trim().replace(/\\s+/g,' '); if(t){ e.preventDefault(); speechSynthesis.cancel(); document.getElementById('readStatus').textContent='朗读本段…'; speakText(t); } } }
if(document.readyState==='loading'){ document.addEventListener('DOMContentLoaded', function(){ var c=document.querySelector('.content'); if(c) c.addEventListener('click', onContentTap); }); } else { var c=document.querySelector('.content'); if(c) c.addEventListener('click', onContentTap); }
setTimeout(function(){ try{ var u=new SpeechSynthesisUtterance('\u200b'); u.volume=0; speechSynthesis.speak(u); }catch(e){} }, 300);
</script>
</body>
</html>
<!DOCTYPE html>
<html lang="zh-CN">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<title>知识流日报 2026-02-28：SOCP、CL4SE、GRAVE2、ParamMem、FlashOptim、coarse data、iTransformer、utilizing LLMs、LLM Novice Uplift、model disagreement、occlusion reasoning、dataset distillation、active depth learning</title>
<style>
  * { box-sizing: border-box; }
  body {
    font-family: system-ui, sans-serif;
    max-width: 720px;
    margin: 0 auto;
    padding: 2rem 1rem;
    color: #fff;
    min-height: 100vh;
    background: #0a0e1a;
    background-image:
      radial-gradient(2px 2px at 20px 30px, rgba(255,255,255,0.9), transparent),
      radial-gradient(2px 2px at 40px 70px, rgba(255,255,255,0.6), transparent),
      radial-gradient(1.5px 1.5px at 90px 40px, rgba(255,255,255,0.8), transparent),
      radial-gradient(2px 2px at 130px 80px, rgba(255,255,255,0.5), transparent),
      radial-gradient(1.5px 1.5px at 160px 120px, rgba(255,255,255,0.7), transparent),
      radial-gradient(ellipse 120% 100% at 50% 0%, rgba(88,60,120,0.25), transparent 50%),
      radial-gradient(ellipse 80% 80% at 80% 20%, rgba(40,60,100,0.2), transparent 40%);
    background-size: 200px 200px;
    background-repeat: repeat;
  }
  a { color: #a8d4ff; text-decoration: none; }
  a:hover { text-decoration: underline; }
 html{scroll-behavior:smooth;} .content h2,.content h3{scroll-margin-top:1rem;} .content .reading-highlight{background:rgba(255,255,255,0.12);border-left:3px solid rgba(255,255,255,0.5);border-radius:2px;} h1,h2,h3{margin-top:1.5rem;color:#fff;} pre{white-space:pre-wrap;color:rgba(255,255,255,0.9);} .toolbar{margin:0.5rem 0;color:rgba(255,255,255,0.8);} .content{color:rgba(255,255,255,0.95);} .content a{color:#a8d4ff;} .meta{color:rgba(255,255,255,0.65);font-size:0.9em;} .toc{margin:1rem 0;padding:0.8rem 1rem;background:rgba(255,255,255,0.08);border-radius:6px;} .toc .toc-title{margin:0 0 0.5rem 0;font-weight:bold;color:rgba(255,255,255,0.9);} .toc ul{list-style:none;padding-left:0;margin:0;} .toc li{margin:0.35rem 0;} .toc li.toc-h3{padding-left:1em;font-size:0.95em;} .toc a{color:#a8d4ff;text-decoration:none;} .toc a:hover{text-decoration:underline;} .content p,.content div,.content h2,.content h3,.content h4,.content li,.content pre{cursor:pointer;-webkit-tap-highlight-color:rgba(255,255,255,0.15);} .content .insight-summary{color:#c9a0dc;margin:0.5em 0 0.8em 0;} .content .insight-detail,.content .insight-detail li{color:#8dd4e8;list-style:disc;margin-left:1.2em;padding-left:0.3em;} .content .article-body-details{margin:0.8em 0;} .content .article-body-details summary{cursor:pointer;color:rgba(255,255,255,0.85);}</style>
</head>
<body>
<p><a href="https://YuxinWSoton.github.io/ai-knowledge-flow-site/">← 返回首页</a></p>
<h1>知识流日报 2026-02-28：SOCP、CL4SE、GRAVE2、ParamMem、FlashOptim、coarse data、iTransformer、utilizing LLMs、LLM Novice Uplift、model disagreement、occlusion reasoning、dataset distillation、active depth learning</h1>
<p class="meta" style="margin-top:0;">最后更新：2026-02-28 16:25</p>
<p class="toolbar"><button id="btnFull" onclick="readFullText()">全文朗读</button> <span id="readStatus"></span></p>
<p class="meta" style="margin-top:0;">（点任意段落可朗读该段；「全文朗读」读本页全部内容，当前读到的段落会自动滚动到视野内并高亮）</p>
<nav class="toc" aria-label="目录">
<p class="toc-title">目录</p>
<ul>
  <li><a href="#toc-0">1. 如何理解fine-grained和coarse-grained？ - 知乎</a></li>
  <li class="toc-h3"><a href="#toc-1">正文</a></li>
  <li><a href="#toc-2">2. CGRA和FPGA有什么不同？ - 知乎</a></li>
  <li class="toc-h3"><a href="#toc-3">正文</a></li>
  <li><a href="#toc-4">3. 深度学习里的coarse-to-fine为什么会起作用？ - 知乎</a></li>
  <li class="toc-h3"><a href="#toc-5">正文</a></li>
  <li><a href="#toc-6">4. 在信息学竞赛中AC、WA、RE、CE、TLE、MLE、PE ...</a></li>
  <li class="toc-h3"><a href="#toc-7">正文</a></li>
  <li><a href="#toc-8">5. 可重构芯片的工作原理是什么？ - 知乎</a></li>
  <li class="toc-h3"><a href="#toc-9">正文</a></li>
  <li><a href="#toc-10">6. MATLAB分类工具箱中高斯SVM核函数kernel scale怎么理解？</a></li>
  <li class="toc-h3"><a href="#toc-11">正文</a></li>
  <li><a href="#toc-12">7. 深度强化学习中，Model-based 和 Model-free 哪个更有前景？</a></li>
  <li class="toc-h3"><a href="#toc-13">正文</a></li>
  <li><a href="#toc-14">8. Anki 插件 image occlusion 如何将多处覆盖作为一组，一次 ...</a></li>
  <li class="toc-h3"><a href="#toc-15">正文</a></li>
  <li><a href="#toc-16">9. 求问有哪些合用的遮挡剔除技术? - 知乎</a></li>
  <li class="toc-h3"><a href="#toc-17">正文</a></li>
  <li><a href="#toc-18">10. 虚幻引擎如何将被遮挡的模型显示到前面？ - 知乎</a></li>
  <li class="toc-h3"><a href="#toc-19">正文</a></li>
  <li><a href="#toc-20">11. maya中阿诺德的AO贴图怎么用？怎么才能用在maya上？ - 知乎</a></li>
  <li class="toc-h3"><a href="#toc-21">正文</a></li>
  <li><a href="#toc-22">12. ARKit3中人的遮挡（People occlusion）效果是怎么实现的 ...</a></li>
  <li class="toc-h3"><a href="#toc-23">正文</a></li>
  <li><a href="#toc-24">13. 《我的世界》 中的环境阴影（Ambient Occlusion）是如何 ...</a></li>
  <li class="toc-h3"><a href="#toc-25">正文</a></li>
  <li><a href="#toc-26">14. 在看一些论文中经常遇到，data set 与 dataset ，那请问这 ...</a></li>
  <li class="toc-h3"><a href="#toc-27">正文</a></li>
  <li><a href="#toc-28">15. Pytorch中的Dataset 和 DataLoader起什么作用？ - 知乎</a></li>
  <li class="toc-h3"><a href="#toc-29">正文</a></li>
  <li><a href="#toc-30">16. 加载数据集的时候经常用到def getitem (self, index):具体 ...</a></li>
  <li class="toc-h3"><a href="#toc-31">正文</a></li>
  <li><a href="#toc-32">17. PyTorch Dataset的shuffle与不shuffle：为何会产生显著差异？</a></li>
  <li class="toc-h3"><a href="#toc-33">正文</a></li>
  <li><a href="#toc-34">18. 写深度学习代码是先写model还是dataset还是train呢，有个 ...</a></li>
  <li class="toc-h3"><a href="#toc-35">正文</a></li>
  <li><a href="#toc-36">19. 怎样用pytorch重写dataset类，用于读取csv数据？ - 知乎</a></li>
  <li class="toc-h3"><a href="#toc-37">正文</a></li>
  <li><a href="#toc-38">20. 基于多核学习的多视图学习——二阶锥规划（Second Order ...</a></li>
  <li class="toc-h3"><a href="#toc-39">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-40">21. Revisions | OpenReview</a></li>
  <li class="toc-h3"><a href="#toc-41">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-42">22. Second-Order Cone Programming (SOCP) 二阶锥规划 ...</a></li>
  <li class="toc-h3"><a href="#toc-43">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-44">23. 1.2.6 二阶锥规划_数学建模与数学规划：方法、案例及编程 ...</a></li>
  <li class="toc-h3"><a href="#toc-45">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-46">24. 大语言模型 (Large Language Models，LLM)如何颠覆未来 ...</a></li>
  <li class="toc-h3"><a href="#toc-47">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-48">25. Second Order Conic Programming | COAP 在线帮助手册</a></li>
  <li class="toc-h3"><a href="#toc-49">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-50">26. 什么是LLM大语言模型？定义、训练方式、流行原因和例子</a></li>
  <li class="toc-h3"><a href="#toc-51">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-52">27. FreDF: Learning to Forecast in the Frequency Domain</a></li>
  <li class="toc-h3"><a href="#toc-53">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-54">28. Crossformer: Transformer Utilizing Cross-Dimension Dependency …</a></li>
  <li class="toc-h3"><a href="#toc-55">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-56">29. A Time Series is Worth 64 Words: Long-term Forecasting with...</a></li>
  <li class="toc-h3"><a href="#toc-57">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-58">30. A Benchmark Study For Limit Order Book (LOB) Models and Time …</a></li>
  <li class="toc-h3"><a href="#toc-59">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-60">31. 环境光遮蔽 - 知乎</a></li>
  <li class="toc-h3"><a href="#toc-61">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-62">32. TimeMixer++: A General Time Series Pattern Machine for …</a></li>
  <li class="toc-h3"><a href="#toc-63">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-64">33. Forum | OpenReview</a></li>
  <li class="toc-h3"><a href="#toc-65">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-66">34. 模仿学习 (Imitation Learning)入门指南</a></li>
  <li class="toc-h3"><a href="#toc-67">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-68">35. 使用Python实现二阶锥规划（SOCP）问题的高效求解方法</a></li>
  <li class="toc-h3"><a href="#toc-69">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-70">36. Utilizing LLMs for Industrial Process Automation</a></li>
  <li class="toc-h3"><a href="#toc-71">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-72">37. 什么是 LLM？— 大型语言模型简介 — AWS</a></li>
  <li class="toc-h3"><a href="#toc-73">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-74">38. Generalized Rapid Action Value Estimation in Memory-Constrained Environments</a></li>
  <li class="toc-h3"><a href="#toc-75">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-76">39. Just-In-Time Piecewise-Linear Semantics for ReLU-type Networks</a></li>
  <li class="toc-h3"><a href="#toc-77">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-78">40. Memory-induced active particle ratchets: Mean currents and large deviations</a></li>
  <li class="toc-h3"><a href="#toc-79">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-80">41. Spin Glass Concepts in Computer Science, Statistics, and Learning</a></li>
  <li class="toc-h3"><a href="#toc-81">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-82">42. First-order SDSOS-convex semi-algebraic optimization and exact SOCP relaxations</a></li>
  <li class="toc-h3"><a href="#toc-83">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-84">43. Robust model selection using likelihood as data</a></li>
  <li class="toc-h3"><a href="#toc-85">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-86">44. Sensor Generalization for Adaptive Sensing in Event-based Object Detection via Joint Distribution Training</a></li>
  <li class="toc-h3"><a href="#toc-87">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-88">45. On the embedding transformation for optimal control of multi-mode switched systems</a></li>
  <li class="toc-h3"><a href="#toc-89">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-90">46. VGG-T$^3$: Offline Feed-Forward 3D Reconstruction at Scale</a></li>
  <li class="toc-h3"><a href="#toc-91">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-92">47. EMAformer: Enhancing Transformer through Embedding Armor for Time Series Forecasting</a></li>
  <li class="toc-h3"><a href="#toc-93">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-94">48. CXReasonAgent: Evidence-Grounded Diagnostic Reasoning Agent for Chest X-rays</a></li>
  <li class="toc-h3"><a href="#toc-95">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-96">49. Butterfly Echo Protocol for Axis-Agnostic Heisenberg-Limited Metrology</a></li>
  <li class="toc-h3"><a href="#toc-97">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-98">50. Differentiable Zero-One Loss via Hypersimplex Projections</a></li>
  <li class="toc-h3"><a href="#toc-99">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-100">51. An Optimization Method for Autoregressive Time Series Forecasting</a></li>
  <li class="toc-h3"><a href="#toc-101">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-102">52. 最优化函数 socp 的使用及转化案例</a></li>
  <li class="toc-h3"><a href="#toc-103">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-104">53. Towards Joint Optimization for UAV-Integrated RIS-Assisted Fluid Antenna Systems</a></li>
  <li class="toc-h3"><a href="#toc-105">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-106">54. Selective Learning for Deep Time Series Forecasting</a></li>
  <li class="toc-h3"><a href="#toc-107">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-108">55. TwinFormer: A Dual-Level Transformer for Long-Sequence Time-Series Forecasting</a></li>
  <li class="toc-h3"><a href="#toc-109">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-110">56. Constrained Policy Optimization via Sampling-Based Weight-Space Projection</a></li>
  <li class="toc-h3"><a href="#toc-111">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-112">57. Retrieve and Segment: Are a Few Examples Enough to Bridge the Supervision Gap in Open-Vocabulary Segmentation?</a></li>
  <li class="toc-h3"><a href="#toc-113">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-114">58. Real-Time Stream Compaction for Sparse Machine Learning on FPGAs</a></li>
  <li class="toc-h3"><a href="#toc-115">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-116">59. Non-Attainment of Minima in Non-Polyhedral Conic Optimization: A Robust SOCP Example</a></li>
  <li class="toc-h3"><a href="#toc-117">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-118">60. FlashOptim: Optimizers for Memory Efficient Training</a></li>
  <li class="toc-h3"><a href="#toc-119">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-120">61. Physics Informed Viscous Value Representations</a></li>
  <li class="toc-h3"><a href="#toc-121">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-122">62. ParamMem: Augmenting Language Agents with Parametric Reflective Memory</a></li>
  <li class="toc-h3"><a href="#toc-123">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-124">63. Close-enough general routing problem for multiple unmanned aerial vehicles in monitoring missions</a></li>
  <li class="toc-h3"><a href="#toc-125">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-126">64. Interface-Aware Trajectory Reconstruction of Limited Demonstrations for Robot Learning</a></li>
  <li class="toc-h3"><a href="#toc-127">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-128">65. Efficient Deep Learning for Short-Term Solar Irradiance Time Series Forecasting: A Benchmark Study in Ho Chi Minh City</a></li>
  <li class="toc-h3"><a href="#toc-129">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-130">66. Lossless Compression: A New Benchmark for Time Series Model Evaluation</a></li>
  <li class="toc-h3"><a href="#toc-131">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-132">67. Transformer-Based Modeling of User Interaction Sequences for Dwell Time Prediction in Human-Computer Interfaces</a></li>
  <li class="toc-h3"><a href="#toc-133">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-134">68. ThinkOmni: Lifting Textual Reasoning to Omni-modal Scenarios via Guidance Decoding</a></li>
  <li class="toc-h3"><a href="#toc-135">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-136">69. SOTAlign: Semi-Supervised Alignment of Unimodal Vision and Language Models via Optimal Transport</a></li>
  <li class="toc-h3"><a href="#toc-137">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-138">70. Mean Estimation from Coarse Data: Characterizations and Efficient Algorithms</a></li>
  <li class="toc-h3"><a href="#toc-139">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-140">71. Discourse-Aware Dual-Track Streaming Response for Low-Latency Spoken Dialogue Systems</a></li>
  <li class="toc-h3"><a href="#toc-141">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-142">72. The Procrustean Bed of Time Series: The Optimization Bias of Point-wise Loss</a></li>
  <li class="toc-h3"><a href="#toc-143">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-144">73. PRIMA: Pre-training with Risk-integrated Image-Metadata Alignment for Medical Diagnosis via LLM</a></li>
  <li class="toc-h3"><a href="#toc-145">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-146">74. Scale Can't Overcome Pragmatics: The Impact of Reporting Bias on Vision-Language Reasoning</a></li>
  <li class="toc-h3"><a href="#toc-147">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-148">75. Why Diffusion Language Models Struggle with Truly Parallel (Non-Autoregressive) Decoding?</a></li>
  <li class="toc-h3"><a href="#toc-149">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-150">76. Scaling Search Relevance: Augmenting App Store Ranking with LLM-Generated Judgments</a></li>
  <li class="toc-h3"><a href="#toc-151">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-152">77. Conformalized Neural Networks for Federated Uncertainty Quantification under Dual Heterogeneity</a></li>
  <li class="toc-h3"><a href="#toc-153">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-154">78. AgentDropoutV2: Optimizing Information Flow in Multi-Agent Systems via Test-Time Rectify-or-Reject Pruning</a></li>
  <li class="toc-h3"><a href="#toc-155">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-156">79. A Mixture-of-Experts Model for Multimodal Emotion Recognition in Conversations</a></li>
  <li class="toc-h3"><a href="#toc-157">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-158">80. Model Agreement via Anchoring</a></li>
  <li class="toc-h3"><a href="#toc-159">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-160">81. Decomposing Private Image Generation via Coarse-to-Fine Wavelet Modeling</a></li>
  <li class="toc-h3"><a href="#toc-161">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-162">82. LineGraph2Road: Structural Graph Reasoning on Line Graphs for Road Network Extraction</a></li>
  <li class="toc-h3"><a href="#toc-163">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-164">83. Toward Expert Investment Teams:A Multi-Agent LLM System with Fine-Grained Trading Tasks</a></li>
  <li class="toc-h3"><a href="#toc-165">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-166">84. LLM：什么是大语言模型？ | Machine Learning | Google for ...</a></li>
  <li class="toc-h3"><a href="#toc-167">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-168">85. MediX-R1: Open Ended Medical Reinforcement Learning</a></li>
  <li class="toc-h3"><a href="#toc-169">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-170">86. SeeThrough3D: Occlusion Aware 3D Control in Text-to-Image Generation</a></li>
  <li class="toc-h3"><a href="#toc-171">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-172">87. Cell-Free MIMO with Rotatable Antennas: When Macro-Diversity Meets Antenna Directivity</a></li>
  <li class="toc-h3"><a href="#toc-173">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-174">88. Towards Long-Form Spatio-Temporal Video Grounding</a></li>
  <li class="toc-h3"><a href="#toc-175">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-176">89. CL4SE: A Context Learning Benchmark For Software Engineering Tasks</a></li>
  <li class="toc-h3"><a href="#toc-177">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-178">90. Learning from Brain Topography: A Hierarchical Local-Global Graph-Transformer Network for EEG Emotion Recognition</a></li>
  <li class="toc-h3"><a href="#toc-179">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-180">91. UniScale: Unified Scale-Aware 3D Reconstruction for Multi-View Understanding via Prior Injection for Robotic Perception</a></li>
  <li class="toc-h3"><a href="#toc-181">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-182">92. LLM Novice Uplift on Dual-Use, In Silico Biology Tasks</a></li>
  <li class="toc-h3"><a href="#toc-183">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-184">93. Comparative Evaluation of SDP, SOCP, and QC Convex Relaxations for Large-Scale Market-Based AC Optimal Power Flow</a></li>
  <li class="toc-h3"><a href="#toc-185">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-186">94. Understanding Usage and Engagement in AI-Powered Scientific Research Tools: The Asta Interaction Dataset</a></li>
  <li class="toc-h3"><a href="#toc-187">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-188">95. Beyond MSE: Ordinal Cross-Entropy for Probabilistic Time Series Forecasting</a></li>
  <li class="toc-h3"><a href="#toc-189">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-190">96. TOPP-DWR: Time-Optimal Path Parameterization of Differential-Driven Wheeled Robots Considering Piecewise-Constant Angular Velocity Constraints</a></li>
  <li class="toc-h3"><a href="#toc-191">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-192">97. MovieTeller: Tool-augmented Movie Synopsis with ID Consistent Progressive Abstraction</a></li>
  <li class="toc-h3"><a href="#toc-193">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-194">98. Evaluating Zero-Shot and One-Shot Adaptation of Small Language Models in Leader-Follower Interaction</a></li>
  <li class="toc-h3"><a href="#toc-195">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-196">99. InnerQ: Hardware-aware Tuning-free Quantization of KV Cache for Large Language Models</a></li>
  <li class="toc-h3"><a href="#toc-197">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-198">100. Benchmarking State Space Models, Transformers, and Recurrent Networks for US Grid Forecasting</a></li>
  <li class="toc-h3"><a href="#toc-199">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-200">101. AlayaLaser: Efficient Index Layout and Search Strategy for Large-scale High-dimensional Vector Similarity Search</a></li>
  <li class="toc-h3"><a href="#toc-201">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-202">102. ManifoldGD: Training-Free Hierarchical Manifold Guidance for Diffusion-Based Dataset Distillation</a></li>
  <li class="toc-h3"><a href="#toc-203">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-204">103. Efficient MPC-Based Energy Management System for Secure and Cost-Effective Microgrid Operations</a></li>
  <li class="toc-h3"><a href="#toc-205">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-206">104. SC-Arena: A Natural Language Benchmark for Single-Cell Reasoning with Knowledge-Augmented Evaluation</a></li>
  <li class="toc-h3"><a href="#toc-207">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-208">105. STELLAR: Storage Tuning Engine Leveraging LLM Autonomous Reasoning for High Performance Parallel File Systems</a></li>
  <li class="toc-h3"><a href="#toc-209">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-210">106. SPARTA: Scalable and Principled Benchmark of Tree-Structured Multi-hop QA over Text and Tables</a></li>
  <li class="toc-h3"><a href="#toc-211">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-212">107. 什么是大语言模型 (LLM)？ | Microsoft Azure</a></li>
  <li class="toc-h3"><a href="#toc-213">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-214">108. 一文搞清楚大语言模型（LLM）到底是什么？看这一篇就够了！</a></li>
  <li class="toc-h3"><a href="#toc-215">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-216">109. GitHub - datawhalechina/self-llm: 《开源大模型食用指南》针对中 …</a></li>
  <li class="toc-h3"><a href="#toc-217">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-218">110. 一文搞懂LLM大模型！LLM从入门到精通万字长文 - 知乎</a></li>
  <li class="toc-h3"><a href="#toc-219">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-220">111. 凸优化学习笔记：QP及SOCP问题 - CSDN博客</a></li>
  <li class="toc-h3"><a href="#toc-221">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-222">112. 大语言模型LLM（Large Language Model）介绍 - jack_Meng ...</a></li>
  <li class="toc-h3"><a href="#toc-223">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-224">113. ITRANSFORMER: INVERTED TRANSFORMERS ARE EFFECTIVE …</a></li>
  <li class="toc-h3"><a href="#toc-225">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-226">114. Second-order cone programming - pku.edu.cn</a></li>
  <li class="toc-h3"><a href="#toc-227">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-228">115. AC ASE STUDY WITHP TST AND VARYING INPUT LENGTH</a></li>
  <li class="toc-h3"><a href="#toc-229">正文（抓取，非 AI）</a></li>
  <li><a href="#toc-230">116. 大语言模型_百度百科</a></li>
  <li class="toc-h3"><a href="#toc-231">正文（抓取，非 AI）</a></li>
</ul>
</nav>
<div class="content">
<h1>知识流日报 2026-02-28：SOCP、CL4SE、GRAVE2、ParamMem、FlashOptim、coarse data、iTransformer、utilizing LLMs、LLM Novice Uplift、model disagreement、occlusion reasoning、dataset distillation、active depth learning</h1>
<p>共 116 条（来自百度学术 / Google / Bing 等，仅含正文抓取成功的条目；按正文长度从短到长排列，便于先听短的）。</p>
<p>（以下含抓取正文，便于阅读。未调用任何 AI API。）</p>
<p>（仅前 50 条含模型提取的「易漏细节」关键点，页面上以颜色区分；第 51 条及之后未调用模型，故无易漏细节。可在 config 中调大 extract_insights_max_items 以增加条数。）</p>
<h2 id="toc-0">1. 如何理解fine-grained和coarse-grained？ - 知乎</h2>
<ul>
<li>链接：https://www.zhihu.com/question/299171510</li>
<li>来源：bing</li>
<li>摘要：2018年10月19日 · 如何理解fine-grained和coarse-grained？ 最近在看UCBerkeley的几年前的一篇讲RDD的论文。 文中提到了coarse-grained update和fine-grained update… 显示全部 关注者 57 被浏览</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">fine-grained update涉及更细粒度的更新操作，虽然这可能带来更高的开销，但适用于需要频繁更新的场景。相比之下，coarse-grained update涉及较粗粒度的更新操作，虽然它降低了系统的灵活性，但适用于更新不频繁的场景。因此，选择细粒度还是粗粒度更新取决于具体的应用需求和更新频率。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>fine-grained update涉及更细粒度的更新操作。</li>
<li>coarse-grained update涉及较粗粒度的更新操作。</li>
<li>细粒度更新可能带来更高的开销。</li>
<li>粗粒度更新可能降低系统的灵活性。</li>
<li>细粒度更新适用于需要频繁更新的场景。</li>
<li>粗粒度更新适用于更新不频繁的场景。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-1">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-2">2. CGRA和FPGA有什么不同？ - 知乎</h2>
<ul>
<li>链接：https://www.zhihu.com/question/407417134</li>
<li>来源：bing</li>
<li>摘要：2020年12月9日 · 从硬件结构看： 顾名思义，由于CGRA是coarse-grained的粗粒度，它是编程重构ALU阵列而非逻辑门阵列，很多的运算由硬核实现，不是LUT搭真值表了，这相比FPGA的低级编程 …</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">CGRA（粗粒度可重构阵列）中的运算由硬件直接实现，而非通过LUT（查找表）搭建真值表，这使得CGRA能够进行更为高效的并行计算。相比之下，FPGA（现场可编程门阵列）是细粒度的，基于逻辑门阵列编程，其灵活性在于可以重新配置逻辑门以实现不同的功能。CGRA的粗粒度特性允许编程重构ALU（算术逻辑单元）阵列，从而提供更高的灵活性和可重构性。因此，CGRA和FPGA在实现运算方面有着不同的机制和优势，CGRA通过硬件直接实现运算，而FPGA则依赖于LUT来实现逻辑功能。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>CGRA中的运算由硬核实现而非通过LUT搭建真值表。</li>
<li>CGRA是粗粒度的，允许编程重构ALU阵列。</li>
<li>FPGA是细粒度的，基于逻辑门阵列编程。</li>
<li>CGRA中的运算由硬件直接实现，而FPGA则通过LUT实现。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-3">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-4">3. 深度学习里的coarse-to-fine为什么会起作用？ - 知乎</h2>
<ul>
<li>链接：https://www.zhihu.com/question/497563603</li>
<li>来源：bing</li>
<li>摘要：知乎，中文互联网高质量的问答社区和创作者聚集的原创内容平台，于 2011 年 1 月正式上线，以「让人们更好的分享知识、经验和见解，找到自己的解答」为品牌使命。知乎凭借认真、专业、友善的社区 …</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">coarse-to-fine方法通过逐步细化预测结果，可以逐步引入更多细节信息，从而有助于模型从粗略到精细地理解复杂结构。这一过程不仅能够逐步修正粗略阶段的错误，提高整体预测质量，还有效避免了一次性处理所有细节带来的复杂性。然而，一个常见的误解是认为coarse-to-fine方法只是简单地分阶段处理，而忽视了其逐步引入细节的优势。因此，理解coarse-to-fine方法的关键在于认识到它通过逐步细化来优化预测结果，从而确保模型能够更准确地捕捉复杂结构的细节。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>coarse-to-fine方法通过逐步细化预测结果，可以逐步引入更多细节信息。</li>
<li>该方法有助于模型从粗略到精细地理解复杂结构。</li>
<li>细化步骤可以逐步修正粗略阶段的错误，提高整体预测质量。</li>
<li>这种方法避免了一次性处理所有细节带来的复杂性。</li>
<li>常见误解是认为coarse-to-fine方法只是简单地分阶段处理，而忽视了其逐步引入细节的优势。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-5">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-6">4. 在信息学竞赛中AC、WA、RE、CE、TLE、MLE、PE ...</h2>
<ul>
<li>链接：https://www.zhihu.com/question/371296221</li>
<li>来源：bing</li>
<li>摘要：2020年2月12日 · 以下是原回答 那当然是： AC=Answer Coarse=粗劣的答案 WA=Wonderful Answer=好答案 TLE=Time Limit Enough=时间充裕 MLE=Memory Limit Enough=内存充裕 CE=Compile …</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">AC（Accepted）表示答案正确，是编程或算法测试中的一种成功状态。然而，如果答案不正确，就会出现WA（Wrong Answer），即答案错误。在程序执行过程中，如果未能在规定的时间内完成计算，就会触发TLE（Time Limit Exceeded），即时间限制超出。此外，如果程序在运行时消耗的内存超过了设定的限制，就会出现MLE（Memory Limit Exceeded），即内存限制超出。最后，如果程序在编译阶段出现错误，就会触发CE（Compile Error），即编译错误。因此，为了确保程序的正确性和高效性，开发者需要关注这些常见的错误类型并进行相应的优化。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>AC意为"Accepted"，即答案正确。</li>
<li>WA意为"Wrong Answer"，即答案错误。</li>
<li>TLE意为"Time Limit Exceeded"，即时间限制超出。</li>
<li>MLE意为"Memory Limit Exceeded"，即内存限制超出。</li>
<li>CE意为"Compile Error"，即编译错误。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-7">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-8">5. 可重构芯片的工作原理是什么？ - 知乎</h2>
<ul>
<li>链接：https://www.zhihu.com/question/576291694</li>
<li>来源：bing</li>
<li>摘要：FPGA 中分布着大量的可编程互联资源，通过编程就能够把各个不同的逻辑单元的输入输出连接起来。 可重构计算 ( Coarse-grained Reconfigurable Architecture，CGRA)是一种空域上的并行计算模式，以 …</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">可编程互联资源是FPGA的关键组成部分，通过编程可以灵活连接逻辑单元的输入输出，从而实现高度灵活的计算功能。CGRA（可重构计算阵列）则是一种专门用于实现空域并行计算的架构，强调在空间上的并行性而非时间上的。因此，CGRA与FPGA中的可编程互联资源有着本质的区别，常见误解在于将CGRA与时间上的并行混淆，实际上CGRA关注的是空间上的并行性，这使得CGRA在处理大规模并行计算任务时具有独特的优势。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>可编程互联资源是FPGA的关键组成部分。</li>
<li>通过编程可以灵活连接逻辑单元的输入输出。</li>
<li>CGRA是一种实现空域并行计算的架构。</li>
<li>可重构计算强调的是在空间上的并行性而非时间上的。</li>
<li>常见误解是将CGRA与时间上的并行混淆。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-9">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-10">6. MATLAB分类工具箱中高斯SVM核函数kernel scale怎么理解？</h2>
<ul>
<li>链接：https://www.zhihu.com/question/265313200</li>
<li>来源：bing</li>
<li>摘要：2020年4月13日 · MATLAB分类工具箱中高斯SVM核函数kernel scale怎么理解？ 想请问一下各位，我在用matlab的分类工具箱利用SVM做分类时，使用高斯核函数，但是matlab中高斯核函数分为coarse …</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">kernel scale决定了核函数的宽度，进而影响决策边界的平滑度。具体而言，当kernel scale值较大时，决策边界较为平滑，这使得模型对数据的拟合能力较弱，但能够更好地泛化。相反，当kernel scale值较小时，决策边界会变得更加复杂，容易导致过拟合。此外，coarse和fine在这里指的是kernel scale的选取范围，而非核函数的类型。因此，选择合适的kernel scale需要根据具体的数据集进行调整，没有固定的最优值。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>kernel scale决定了核函数的宽度，影响决策边界平滑度。</li>
<li>kernel scale值较大时，决策边界较为平滑，对数据的拟合能力较弱。</li>
<li>kernel scale值较小时，决策边界较为复杂，容易过拟合。</li>
<li>coarse和fine表示kernel scale的选取范围，而非核函数类型。</li>
<li>kernel scale的选择需要根据具体数据集进行调整，没有固定的最优值。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-11">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-12">7. 深度强化学习中，Model-based 和 Model-free 哪个更有前景？</h2>
<ul>
<li>链接：https://www.zhihu.com/question/1888431717733343412</li>
<li>来源：bing</li>
<li>摘要：2025年4月21日 · 深度强化学习中，Model-based 和 Model-free 哪个更有前景？ 在做强化学习相关课题时，经常会看到 model-based 和 model-free 这两种方法。 有的论文强调 model-free 方法比如 …</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">Model-based 方法理论上能够实现更高效的探索，但由于实际应用中难以获得足够准确的模型预测，这一方法在实践中往往面临挑战。相比之下，Model-free 方法无需依赖模型预测，直接从经验中学习，因此在处理复杂环境时可能表现更佳。然而，Model-free 方法的学习速度通常较慢，需要更多的样本才能达到最优解。因此，选择 Model-based 方法还是 Model-free 方法，取决于具体应用场景和可用资源。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>Model-based 方法需要准确的模型预测，但实际应用中往往难以获得足够准确的模型。</li>
<li>Model-free 方法无需模型预测，直接从经验中学习，但可能需要更多的样本才能达到最优解。</li>
<li>Model-based 方法在理论上可以实现更高效的探索，但在实践中面临模型准确性的问题。</li>
<li>Model-free 方法在处理复杂环境时可能表现更好，但学习速度通常较慢。</li>
<li>Model-based 方法和 Model-free 方法的选择取决于具体应用场景和可用资源。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-13">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-14">8. Anki 插件 image occlusion 如何将多处覆盖作为一组，一次 ...</h2>
<ul>
<li>链接：https://www.zhihu.com/question/419913336</li>
<li>来源：bing</li>
<li>摘要：2024年1月31日 · Anki 插件 image occlusion 如何将多处覆盖作为一组，一次翻面一组中的覆盖全部出现？</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">多处覆盖被视为一个整体处理，这意味着在进行翻面操作时，可以同时显示一组中的所有覆盖内容，而无需逐一翻面。这种处理方式不仅简化了操作流程，还提高了效率。通过一次翻面，可以确保所有相关的覆盖内容得到同步更新，从而避免了遗漏某些覆盖内容的可能性，确保了整体的一致性和完整性。因此，这种处理方式不仅方便，而且在实际操作中显得尤为重要。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>多处覆盖被视为一个整体处理，一次翻面可以同时显示一组中的所有覆盖内容。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-15">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-16">9. 求问有哪些合用的遮挡剔除技术? - 知乎</h2>
<ul>
<li>链接：https://www.zhihu.com/question/38060533</li>
<li>来源：bing</li>
<li>摘要：2.1、硬件遮挡查询（Occlusion Query） 硬件遮挡查询（Occlusion Query）-在UE4默认开启的，它的基本原理如下流程图： 硬件遮挡查询（Occlusion Query）主要的问题在于它需要把查询到的结果从虚 …</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">硬件遮挡查询是一种在虚幻引擎中用于优化场景渲染的技术，它依赖于硬件的支持来提高渲染效率。首先，硬件遮挡查询需要将查询结果从虚幻引擎中取出，以便进行后续处理。其次，硬件遮挡查询的效果依赖于硬件是否提供了相应的支持，如果硬件不支持这一功能，那么硬件遮挡查询可能会失效，从而影响渲染性能。因此，为了确保硬件遮挡查询的有效性，开发者需要确保所使用的硬件设备具备相应的支持。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>硬件遮挡查询需要将查询结果从虚幻引擎中取出。</li>
<li>硬件遮挡查询依赖于硬件支持。</li>
<li>硬件遮挡查询可能因硬件不支持而失效。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-17">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-18">10. 虚幻引擎如何将被遮挡的模型显示到前面？ - 知乎</h2>
<ul>
<li>链接：https://www.zhihu.com/question/614622097</li>
<li>来源：bing</li>
<li>摘要：2023年7月30日 · 当前手上没编辑器，也有一阵儿没用了，凭印象说下。 方法一，搞个 后处理 材质，你去ue4官网或直接百度搜 postprocess 与 occlusion这两个关键词，应该就能找到相应的解决方案。 …</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">后处理材质可以用来解决被遮挡的模型显示问题，这是因为在渲染过程中，某些模型可能会被其他模型遮挡，导致部分细节无法清晰呈现。为了解决这一问题，虚幻引擎官网提供了专门的后处理材质解决方案，同时，通过搜索引擎也可以找到相关的教程和资源。这些解决方案能够帮助开发者有效地提升模型的可见性和细节表现，从而改善整体视觉效果。因此，了解并应用后处理材质对于优化游戏或应用中的视觉质量至关重要。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>后处理材质可以用来处理被遮挡的模型显示问题。</li>
<li>后处理材质的解决方案可以在虚幻引擎官网上找到。</li>
<li>后处理材质的解决方案也可以通过搜索引擎获取。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-19">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-20">11. maya中阿诺德的AO贴图怎么用？怎么才能用在maya上？ - 知乎</h2>
<ul>
<li>链接：https://www.zhihu.com/question/371023605</li>
<li>来源：bing</li>
<li>摘要：2023年11月2日 · 先弄清楚AO是什么。再弄明白 AO贴图 是什么。 AO全称 Ambient Occlusion，也叫OCC，俗称 环境遮蔽。也就是环境当中的物体遮挡住了光照，导致特定区域无法接收或者无法全部 …</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">AO全称Ambient Occlusion，是一种用于模拟环境遮挡效果的技术。环境遮挡是指由于物体之间的遮挡，导致某些区域无法接收到足够的光照，从而产生暗淡的效果。AO贴图作为模拟这一效果的纹理，能够帮助在渲染中创造出更加真实、细腻的光影变化。为了在Maya中正确模拟出预期效果，AO贴图需被恰当地应用，确保其能够准确反映实际场景中的光照情况，从而提升最终渲染的质量。因此，正确理解和应用AO贴图对于提升模型的真实感至关重要。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>AO全称 Ambient Occlusion。</li>
<li>AO贴图是用于模拟环境遮挡效果的纹理。</li>
<li>环境遮挡导致特定区域无法接收到足够的光照。</li>
<li>AO贴图需正确应用才能在Maya中模拟出预期效果。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-21">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-22">12. ARKit3中人的遮挡（People occlusion）效果是怎么实现的 ...</h2>
<ul>
<li>链接：https://www.zhihu.com/question/355865364</li>
<li>来源：bing</li>
<li>摘要：2019年11月16日 · 人物遮挡 （People Occlusion）是ARKit于2019年WWDC上推出的 ARKit 3.0新特性。 人物遮挡的作用是使AR虚拟内容能够在现实世界中的人身体后面呈现。</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">人物遮挡功能使AR内容能够根据人体位置进行动态调整，这一功能依赖于对现实世界中人体的识别和跟踪。然而，人物遮挡并不总是100%准确，可能会出现误判情况。因此，用户需要在现实世界中移动以确保AR内容正确显示。这种动态调整机制虽然重要，但并非总是完美无瑕，用户需通过实际操作来优化AR体验。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>人物遮挡功能使AR内容能够根据人体位置进行动态调整。</li>
<li>人物遮挡依赖于对现实世界中人体的识别和跟踪。</li>
<li>人物遮挡并不总是100%准确，可能会出现误判情况。</li>
<li>用户需要在现实世界中移动以确保AR内容正确显示。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-23">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-24">13. 《我的世界》 中的环境阴影（Ambient Occlusion）是如何 ...</h2>
<ul>
<li>链接：https://www.zhihu.com/question/32070900</li>
<li>来源：bing</li>
<li>摘要：2016年1月11日 · (此图片来自网络， Ambient occlusion for Minecraft-like worlds ) 还有一种是大范围的，利用亮度扩散来实现的，首先将垂直方向上没有障碍的空气方块标记位亮度16 然后光线朝相邻格 …</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">环境阴影通过标记垂直方向上没有障碍的空气方块来实现亮度扩散，这种方法首先为无障碍的垂直方向的空气方块分配最大亮度值，从而模拟真实环境中的亮度分布。相邻格的亮度值会根据是否有障碍物而递减，确保光线在垂直方向上的自然衰减。这种处理方式有助于更真实地模拟现实中的阴影效果，提升场景的视觉真实感。因此，这种方法不仅能够准确地表现光线的传播路径，还能增强整体环境的沉浸感。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>环境阴影通过标记垂直方向上没有障碍的空气方块来实现亮度扩散。</li>
<li>这种方法首先为无障碍的垂直方向的空气方块分配最大亮度值。</li>
<li>相邻格的亮度值会根据是否有障碍物而递减。</li>
<li>这种方法有助于模拟真实环境中的阴影效果。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-25">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-26">14. 在看一些论文中经常遇到，data set 与 dataset ，那请问这 ...</h2>
<ul>
<li>链接：https://www.zhihu.com/question/314502626</li>
<li>来源：bing</li>
<li>摘要：2019年3月4日 · I would suggest using “dataset”； 使用谷歌搜索二者，在学术性论文中有1890篇采用的data set，而2660篇采用的dataset。 因此，建议使用dataset。 （而我看得相关领域的内容，采用data …</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">"data set" 和 "dataset" 在拼写上仅有一字之差，但后者更符合现代英语书写规范。"dataset" 的使用更为广泛，反映了语言习惯的变化。学术论文中更倾向于使用 "dataset"，这可能与语言规范和习惯有关，因此使用 "dataset" 可能更符合当前的学术写作规范。因此，为了保持一致性和符合现代英语书写习惯，建议在学术写作中采用 "dataset"。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>"data set" 和 "dataset" 在拼写上仅有一字之差，但后者更符合现代英语书写规范。</li>
<li>"dataset" 的使用更为广泛，反映了语言习惯的变化。</li>
<li>学术论文中更倾向于使用 "dataset"，这可能与语言规范和习惯有关。</li>
<li>使用 "dataset" 可能更符合当前的学术写作规范。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-27">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-28">15. Pytorch中的Dataset 和 DataLoader起什么作用？ - 知乎</h2>
<ul>
<li>链接：https://www.zhihu.com/question/570770126</li>
<li>来源：bing</li>
<li>摘要：2025年5月10日 · pytorch的Dataset和DataLoader为迭代训练过程提供数据加载（包括数据增强部分）等任务。 重载的Dataset类里的两个关键函数 <strong>len</strong> 、 <strong>getitem</strong> 是一般需要重写 (override)。其 …</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">Dataset类需要重写两个关键函数：`__len__`和`__getitem__`。`__len__`函数返回Dataset的长度，用于确定迭代次数；而`__getitem__`函数根据索引返回单个数据样本。Dataset和DataLoader共同作用于训练过程的数据加载，其中DataLoader负责数据的批处理、加载和并行处理，支持多线程和多进程数据加载，提高效率。此外，DataLoader还支持设置shuffle参数以随机打乱数据顺序，进一步增强数据迭代的灵活性和高效性。因此，Dataset和DataLoader的配合使得数据迭代更加高效，共同确保了训练过程的顺利进行。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>Dataset类需要重写__len__和__getitem__两个关键函数。</li>
<li>Dataset和DataLoader共同作用于训练过程的数据加载。</li>
<li>DataLoader负责数据的批处理、加载和并行处理。</li>
<li>Dataset和DataLoader的配合使得数据迭代更加高效。</li>
<li>__len__函数返回Dataset的长度，用于确定迭代次数。</li>
<li>__getitem__函数根据索引返回单个数据样本。</li>
<li>DataLoader可以设置shuffle参数以随机打乱数据顺序。</li>
<li>DataLoader支持多线程和多进程数据加载，提高效率。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-29">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-30">16. 加载数据集的时候经常用到def <strong>getitem</strong> (self, index):具体 ...</h2>
<ul>
<li>链接：https://www.zhihu.com/question/443036026</li>
<li>来源：bing</li>
<li>摘要：2021年2月5日 · 2.我是使用DataLoader加载数据集的，这其中有batch_size,这意味着必然要对所有数据经行分开打包，所以一定是使用了 dataset [index] 中所有的index,所以 <strong>getitem</strong> 有了所有的index。 …</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">首先，`__getitem__` 方法用于返回单个数据样本，而不是整个数据集。其次，`Dataloader` 通过调用 `__getitem__` 方法按批次获取数据，确保每次只处理一个批次的数据。此外，使用 `Dataloader` 时，每个批次的数据是通过 `__getitem__` 函数动态获取的，这使得数据处理更加灵活和高效。因此，`Dataloader` 中的 `batch_size` 参数决定了数据是如何被分批处理的，从而影响了数据加载的效率和模型训练的节奏。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>__getitem__ 方法返回的是单个数据样本，而不是所有样本。</li>
<li>Dataloader通过__getitem__按批次获取数据。</li>
<li>使用Dataloader时，每个批次的数据是通过__getitem__函数动态获取的。</li>
<li>Dataloader中的batch_size决定了数据是如何被分批处理的。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-31">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-32">17. PyTorch Dataset的shuffle与不shuffle：为何会产生显著差异？</h2>
<ul>
<li>链接：https://www.zhihu.com/question/4304224360</li>
<li>来源：bing</li>
<li>摘要：PyTorch Dataset的shuffle与不shuffle：为何会产生显著差异？如何选择shuffle参数？ 多大的差异叫做显著差异？常规情况是否shuffle差异应该不大。 取决于数据集大小，每个batch在数据集中取多少，如 …</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">shuffle可以打乱数据加载顺序，从而增加模型训练的多样性，减少数据加载的偏序性，使得每个batch的样本更加独立，进而提高模型的泛化能力。相比之下，不shuffle则可能导致模型过拟合于数据集的顺序，使模型学习到不必要的模式，并且训练过程具有一定的顺序性，这会降低模型的泛化能力。因此，shuffle能够使得训练过程更加随机，从而增强模型的鲁棒性和泛化能力。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>shuffle可以打乱数据加载顺序，不shuffle则按数据集原始顺序加载。</li>
<li>shuffle可以增加模型训练的多样性，不shuffle可能导致模型过拟合于数据集顺序。</li>
<li>shuffle可以减少数据加载的偏序性，不shuffle可能使模型学习到不必要的模式。</li>
<li>shuffle可以提高模型泛化能力，不shuffle可能降低模型泛化能力。</li>
<li>shuffle可以使得每个batch的样本更加独立，不shuffle可能导致batch内部样本相关性。</li>
<li>shuffle可以使得训练过程更加随机，不shuffle可能导致训练过程具有一定的顺序性。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-33">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-34">18. 写深度学习代码是先写model还是dataset还是train呢，有个 ...</h2>
<ul>
<li>链接：https://www.zhihu.com/question/498167513</li>
<li>来源：bing</li>
<li>摘要：谢邀。先给结论：以我写了两三年 pytorch 代码的经验而言，比较好的顺序是先写 model，再写 dataset，最后写 train。 在讨论码组件的具体顺序前，我们先分析每一个组件背后的目的和逻辑。 …</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">model 的设计决定了网络结构和参数，是实现学习算法的基础。而dataset的编写则确保了模型训练和测试的数据来源正确无误，为模型提供必要的输入。train阶段的实现则依赖于model和dataset的具体设计，是验证模型性能的关键步骤。因此，model、dataset和train这三个环节紧密相连，共同构成了模型训练和性能验证的完整流程。首先，合理的model设计为后续的训练提供了基础；其次，准确无误的dataset确保了训练数据的质量；最后，通过train阶段的实现，可以验证模型的实际性能，从而形成一个完整的闭环。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>model 的设计决定了网络结构和参数，是实现学习算法的基础。</li>
<li>dataset 的编写确保了模型训练和测试的数据来源正确无误。</li>
<li>train 阶段的实现依赖于 model 和 dataset 的具体设计，是模型性能验证的关键步骤。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-35">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-36">19. 怎样用pytorch重写dataset类，用于读取csv数据？ - 知乎</h2>
<ul>
<li>链接：https://www.zhihu.com/question/517898768</li>
<li>来源：bing</li>
<li>摘要：2022年2月21日 · classdataset的几个要点： classdataset类继承torch.utils.data.dataset。 classdataset的作用是将任意格式的数据，通过读取、预处理或数据增强后以tensor的形式输出。其中任意格式的数 …</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">classdataset类需要继承torch.utils.data.dataset，这一设计不仅确保了类的正确继承关系，还为后续的数据加载和处理提供了便利。classdataset类的主要职责是读取和预处理数据，这一步骤对于确保数据质量至关重要。为了便于后续的模型训练，classdataset类的输出应统一为tensor形式，这样可以确保数据在不同设备间的兼容性，并简化数据处理流程。因此，通过合理设计classdataset类，可以有效提升数据处理的效率和质量。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>classdataset类需要继承torch.utils.data.dataset。</li>
<li>classdataset类负责读取和预处理数据。</li>
<li>classdataset类的输出应为tensor形式。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-37">正文</h3>
<p>（知乎问题/专栏/想法页，正文未抓取，可点击上方链接阅读。）</p>
</div></details><h2 id="toc-38">20. 基于多核学习的多视图学习——二阶锥规划（Second Order ...</h2>
<ul>
<li>链接：https://blog.51cto.com/u_15837794/11458135</li>
<li>来源：bing</li>
<li>摘要：2024年7月15日 · 基于多核学习的多视图学习——二阶锥规划（Second Order Cone Program, SOCP），二阶锥规划（SecondOrderConeProgram,SOCP）是一种特殊的凸优化问题，它包 …</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">二阶锥规划是一种特殊的凸优化问题，因其能够确保全局最优解的存在而被广泛应用。在多核学习和多视图学习中，二阶锥规划因其高效性和鲁棒性而显得尤为重要。此外，由于其在解决安全验证问题时的优越性能，二阶锥规划在保障系统安全方面也发挥着关键作用。因此，无论是从理论保证还是实际应用的角度来看，二阶锥规划都是一个值得深入研究和广泛应用的重要领域。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>二阶锥规划是一种特殊的凸优化问题。</li>
<li>二阶锥规划在多核学习和多视图学习中应用广泛。</li>
<li>凸优化问题具有全局最优解的保证。</li>
<li>二阶锥规划可以用于解决安全验证问题。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-39">正文（抓取，非 AI）</h3>
<p>Security Verification</p>
</div></details><h2 id="toc-40">21. Revisions | OpenReview</h2>
<ul>
<li>链接：https://openreview.net/revisions?id=NfbBdPzcbp</li>
<li>来源：bing</li>
<li>摘要：2025年5月11日 · Title: iTransformer: Inverted Transformers Are Effective for Time Series Forecasting. Authors: Yong Liu, Tengge Hu, Haoran Zhang, Haixu Wu, Shiyu Wang 0001, Lintao Ma, Mingsheng …</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">在时间序列预测任务中，直接应用常规Transformer模型可能不是最优选择。传统方法在这一领域表现不佳，而反转的Transformer结构，如iTransformer模型，可能具有独特优势，能够更有效地进行时间序列预测。iTransformer模型采用的反转结构，可能使得其在处理时间序列数据时更加高效，因此值得进一步研究。这种结构在时间序列预测中的应用，显示出其可能比传统方法更为有效，从而为该领域的研究提供了新的方向。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>iTransformer模型采用反转结构，可能更有效进行时间序列预测。</li>
<li>反转结构的时间序列预测模型iTransformer值得进一步研究。</li>
<li>时间序列 forecasting 中，传统方法可能不如反转的Transformer有效。</li>
<li>反转的Transformer结构在时间序列预测中可能具有独特优势。</li>
<li>时间序列预测任务中，直接应用常规Transformer不一定是最优选择。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-41">正文（抓取，非 AI）</h3>
<p>Revisions | OpenReview Loading</p>
</div></details><h2 id="toc-42">22. Second-Order Cone Programming (SOCP) 二阶锥规划 ...</h2>
<ul>
<li>链接：https://blog.csdn.net/weixin_41024483/article/details/104353002</li>
<li>来源：bing</li>
<li>摘要：2026年1月2日 · 文章浏览阅读3.8w次，点赞35次，收藏240次。本文深入探讨了二阶锥优化的基本概念与约束定义，详细解析了如何将二次规划和随机线性规划等问题转化为二阶锥规划 …</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">二阶锥的定义涉及非负变量t和其前k-1个变量的范数，这一定义使得二阶锥成为一种特殊的约束形式。二阶锥约束能够将线性不等式转化为包含二阶锥的成员关系，从而使得问题的表达更加紧凑和直观。这种转化使得二次规划和随机线性规划能够被转化为二阶锥规划，从而拓宽了优化问题的解决范围。然而，二阶锥规划的求解方法不同于一般线性规划或二次规划，需要采用专门的算法和技术。此外，二阶锥在不同的文献和应用中被称为quadratic锥、ice-cream锥或Lorentz锥，这些不同的名称反映了其在不同领域中的应用和研究。因此，理解二阶锥的定义及其在优化问题中的应用，对于解决复杂优化问题至关重要。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>二阶锥的定义涉及非负变量t和其前k-1个变量的范数。</li>
<li>二阶锥约束将线性不等式转化为包含二阶锥的成员关系。</li>
<li>二次规划和随机线性规划可以转化为二阶锥规划。</li>
<li>二阶锥规划的求解方法不同于一般线性规划或二次规划。</li>
<li>二阶锥规划中的二阶锥具有多种名称，如quadratic、ice-cream、Lorentz锥。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-43">正文（抓取，非 AI）</h3>
<p>个人博客 Glooow ，欢迎各位老师来踩踩 文章目录 1. 二阶锥 1.1 二阶锥定义 1.2 二阶锥约束 2. 优化问题建模 3. 类似问题转化 3.1 二次规划 3.2 随机线性规划 4. 问题求解 1. 二阶锥 1.1 二阶锥定义 在此之前，先给出 二阶锥 的定义。 在 k k k 维空间中二阶锥 (Second-order cone) 的定义为 C k = { [ u t ] ∣ u ∈ R k − 1 , t ∈ R , ∥ u ∥ ≤ t } \mathcal{C}<em k="">{k}=\left{\left[\begin{array}{l} {u} \ {t} \end{array}\right] | u \in \mathbb{R}^{k-1}, t \in \mathbb{R},|u| \leq t\right} C k ​ = { [ u t ​ ] ∣ u ∈ R k − 1 , t ∈ R , ∥ u ∥ ≤ t } 其也被称为 quadratic，ice-cream，Lorentz cone。 1.2 二阶锥约束 在此基础上， 二阶锥约束 即为 ∥ A x + b ∥ ≤ c T x + d ⟺ [ A c T ] x + [ b d ] ∈ C k |A x+b| \leq c^{T} x+d \Longleftrightarrow\left[\begin{array}{c} {A} \ {c^{T}} \end{array}\right] x+\left[\begin{array}{l} {b} \ {d} \end{array}\right] \in \mathcal{C}</em> ∥ A x + b ∥ ≤ c T x + d ⟺ [ A c T ​ ] x + [ b d ​ ] ∈ C</p>
</div></details><h2 id="toc-44">23. 1.2.6 二阶锥规划_数学建模与数学规划：方法、案例及编程 ...</h2>
<ul>
<li>链接：https://m.read.qq.com/read/1052521577/17</li>
<li>来源：bing</li>
<li>摘要：1.2.6 二阶锥规划 二阶锥规划（Second-Order Cone Programming，SOCP）的目标函数为线性表达式，约束包含二阶锥约束，是一种非常特殊的非线性优化模型。 在给出其一般形式之前，我 …</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">目标函数为线性表达式是SOCP（二次锥规划）的一个重要特点，这使得优化问题在数学上更为简洁。约束包含二阶锥约束是SOCP的另一个显著特点，这一特点使得SOCP在处理具有非线性特征的问题时依然保持了良好的凸性。根据凸锥的性质，凸锥中的任意两点的线性组合仍属于该锥，因此二范数定义的锥称为二阶锥，其数学定义具有特定形式。由于二阶锥的特殊性质，SOCP可以等价转换为QCQP（二次约束的二次规划），这为求解提供了更多的灵活性。当SOCP退化为LP（线性规划）时，约束系数矩阵为零矩阵，这意味着所有变量的系数均为零，从而简化了问题。此外，当c_i=0时，SOCP同样可以等价转换为QCQP，进一步增强了其在不同情况下的适用性。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>目标函数为线性表达式是SOCP的一个特点。</li>
<li>约束包含二阶锥约束是SOCP的另一个特点。</li>
<li>凸锥中的任意两点的线性组合仍属于该锥。</li>
<li>二范数定义的锥称为二阶锥。</li>
<li>二阶锥约束的数学定义为特定形式。</li>
<li>SOCP可以等价转换为QCQP。</li>
<li>SOCP退化为LP时约束系数矩阵为零矩阵。</li>
<li>c_i=0时SOCP可以等价转换为QCQP。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-45">正文（抓取，非 AI）</h3>
<p>1.2.6 二阶锥规划_数学建模与数学规划：方法、案例及编程实战（Python+COPT/Gurobi实现）最新章节-QQ阅读女生网 上一章 目录 下一章 1.2.6 二阶锥规划 二阶锥规划（Second-Order Cone Programming，SOCP）的目标函数为线性表达式，约束包含二阶锥约束，是一种非常特殊的非线性优化模型。在给出其一般形式之前，我们需要了解一下什么是（凸）锥，什么是二阶锥。 · 锥 （Cone）：对于一个向量空间R n 与它的一个子集 C ，如果子集 C 中的任意一点 x 与任意正数 α 的积 αx 仍然属于子集 C ，则称 C 为一个锥。若 C 中任意两点 x 与 y ，以及任意两个正数 α 与 β ，都有 αx + βy ∈ C ，则 C 为凸锥。 · 二阶锥 （Second-order Cone）：以二范数定义的锥被称为二阶锥。在 k 维空间中，标准的二阶锥数学定义为式（ 1.5 ）。图 1.2 为三维空间中的二阶锥示意图。 需要说明的是，符号‖·‖ 2 表示向量的二范数（ L 2 norm）。列向量 x =［ x 1 ， x 2 ，…， x n ］ T 的二范数定义为： 图1.2 三维空间中的二阶锥示意图 形如式（ 1.6 ）的约束即为二阶锥约束（Second-order Cone Constraint）。 其中， A ∈R k × n ，表示系数矩阵； x ∈R n ×1 ，为列向量，是决策变量； b ∈R k ×1 ，为列向量； c ∈R n ×1 ，为列向量； d 为常数。 二阶锥规划的一般形式 ［16］ 如下： 其中， f ∈R n ×1 ，为列向量； x ∈R n× 1 ，为列向量，表示连续型决策变量； ； ； c i ∈R n ×1 ，为列向量； d i ∈R； F ∈R h × n ，表示约束系数矩阵； g ∈R h ×1 ，为列向量，表示右端常数。 当 c i =0（∀ i =1，…， m ）时，SOCP可以等价转换为QCQP。当 A i = O （即零矩阵）（∀ i =1，…， m ）时，SOCP退化为LP。 下面给出两个二阶锥规划的简单例子。 【例1.1】 若表示成紧凑的矩阵形式，则 【例1.2】 若表示成紧凑的矩阵形式，则 本周热推： 可视化微分几何和形式：一部五幕数学正剧 微积分的历程：从牛顿到勒贝格 轻轻松松学会微积分 生活中的数学 你学的数学可能是假的 上一章 目录 下一章</p>
</div></details><h2 id="toc-46">24. 大语言模型 (Large Language Models，LLM)如何颠覆未来 ...</h2>
<ul>
<li>链接：https://cloud.tencent.com/developer/article/2631039</li>
<li>来源：bing</li>
<li>摘要：4 天之前 · 大型语言模型(LLM)正在重塑技术格局，从编程辅助到企业知识引擎，Transformer架构和多模态融合带来革命性突破。本文深度解析LLM技术原理、行业应用及挑战，包含实战代码案例和优化方 …</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">大语言模型的颠覆性在于其广泛的应用场景，这些应用场景正在重塑从编程到企业知识的多个技术领域。大语言模型通过Transformer架构实现突破，这一技术原理不仅推动了大语言模型的发展，还带来了多模态融合的新技术突破。大语言模型的应用、挑战与趋势是本文的核心内容，其中技术原理、行业应用及挑战是本文的重点。为了帮助读者更好地理解，实战代码案例将被详细呈现，而优化方法则是提升性能的关键。然而，大语言模型也面临着技术难题和伦理问题的挑战，这些问题需要我们共同面对。展望未来，大语言模型的应用场景将更加广泛，这将是其未来发展的趋势。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>大语言模型的颠覆性在于其广泛的应用场景。</li>
<li>大语言模型通过Transformer架构实现突破。</li>
<li>大语言模型正在重塑从编程到企业知识的多个技术领域。</li>
<li>大语言模型的多模态融合带来新的技术突破。</li>
<li>大语言模型的应用、挑战与趋势是本文的核心内容。</li>
<li>大语言模型的技术原理、行业应用及挑战是本文的重点。</li>
<li>大语言模型的实战代码案例有助于理解其应用。</li>
<li>大语言模型的优化方法是提升性能的关键。</li>
<li>大语言模型的挑战包括技术难题和伦理问题。</li>
<li>大语言模型的未来趋势涉及更多应用场景。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-47">正文（抓取，非 AI）</h3>
<p>大语言模型(Large Language Models，LLM)如何颠覆未来：深入解析应用、挑战与趋势-腾讯云开发者社区-腾讯云 摘星. 大语言模型(Large Language Models，LLM)如何颠覆未来：深入解析应用、挑战与趋势 原创 关注作者 腾讯云 开发者社区 文档 建议反馈 控制台 登录/注册 首页 学习 活动 专区 圈层 工具 MCP广场 文章/答案/技术大牛 搜索 搜索 关闭 发布 摘星. 社区首页 &gt; 专栏 &gt; 大语言模型(Large Language Models，LLM)如何颠覆未来：深入解析应用、挑战与趋势 大语言模型(Large Language Models，LLM)如何颠覆未来：深入解析应用、挑战与趋势 摘星. 关注 发布 于 2026-02-25 08:02:18 发布 于 2026-02-25 08:02:18 804 0 举报 概述 # 大语言模型(Large Language Models，LLM)如何颠覆未来：深入解析应用、挑战与趋势 原创声明：本文系作者授权腾讯云开发者社区发表，未经许可，不得转载。 如有侵权，请联系 cloudcommunity@tencent.com 删除。 AIGC 原创声明：本文系作者授权腾讯云开发者社区发表，未经许可，不得转载。 如有侵权，请联系 cloudcommunity@tencent.com 删除。 AIGC 评论 登录 后参与评论 0 条评论 热度 最新 登录 后参与评论 推荐阅读 相关产品与服务 GPU 云服务器 GPU 云服务器（Cloud GPU Service，GPU）是提供 GPU 算力的弹性计算服务，具有超强的并行计算能力，作为 IaaS 层的尖兵利器，服务于生成式AI，自动驾驶，深度学习训练、科学计算、图形图像处理、视频编解码等场景。腾讯云随时提供触手可得的算力，有效缓解您的计算压力，提升业务效率与竞争力。 产品介绍 产品文档 4核4G3M云服务器 新用户低至38元/年！ 领券 社区 技术文章 技术问答 技术沙龙 技术视频 学习中心 技术百科 技术专区 活动 自媒体同步曝光计划 邀请作者入驻 自荐上首页 技术竞赛 圈层 腾讯云最具价值专家 腾讯云架构师技术同盟 腾讯云创作之星 腾讯云TDP 关于 社区规范 免责声明 联系我们 友情链接 MCP广场开源版权声明 腾讯云开发者 扫码关注腾讯云开发者 领取腾讯云代金券 热门产品 域名注册 云服务器 区块链服务 消息队列 网络加速 云数据库 域名解析 云存储 视频直播 热门推荐 人脸识别 腾讯会议 企业云 CDN加速 视频通话 图像分析 MySQL 数据库 SSL 证书 语音识别 更多推荐 数据安全 负载均衡 短信 文字识别 云点播 大数据 小程序开发 网站监控 数据迁移 Copyright © 2013 - 2026 Tencent Cloud. All Rights Reserved. 腾讯云 版权所有 深圳市腾讯计算机系统有限公司 ICP备案/许可证号： 粤B2-20090059 粤公网安备44030502008569号 腾讯云计算（北京）有限责任公司 京ICP证150476号 | 京ICP备11018762号 问题归档 专栏文章 快讯文章归档 关键词归档 开发者手册归档 开发者手册 Section 归档 Copyright © 2013 - 2026 Tencent Cloud. All Rights Reserved. 腾讯云 版权所有 登录 后参与评论 0 0 0 推荐</p>
</div></details><h2 id="toc-48">25. Second Order Conic Programming | COAP 在线帮助手册</h2>
<ul>
<li>链接：https://guide.coap.online/zh/solvers/problem-types/second-order-conic-programming.html</li>
<li>来源：bing</li>
<li>摘要：2022年1月4日 · 若目标函数或约束条件中包含非线性函数，称这种规划问题为非线性规划问题。 二阶锥规划（Second Order Conic Programming,简称SOCP）是一种常见的非线性规划问题， …</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">首先，凸锥是满足特定条件的锥，其中任何两个点的线性组合仍然保持在该锥内，因此锥总是无界的。其次，二阶锥规划（SOCP）是一种常见的非线性规划问题，它具有广泛的应用领域，其数学形式涉及最大化或最小化目标函数，且约束条件包括上下界。此外，变量上下界也是约束的一部分，而二阶锥的标准形式有两种类型：一种是旋转二阶锥的标准形式，另一种与二阶锥本身不同。因此，二阶锥规划通过这些特定的数学形式和约束条件，能够有效地解决包含非线性函数的目标函数或约束条件的问题。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>锥总是无界的。</li>
<li>凸锥中的线性组合仍保持在该锥内。</li>
<li>目标函数或约束条件中包含非线性函数的规划问题称为非线性规划问题。</li>
<li>二阶锥规划（SOCP）是一种常见的非线性规划问题。</li>
<li>二阶锥规划具有广泛的应用领域。</li>
<li>凸锥是满足特定条件的锥。</li>
<li>二阶锥规划的数学形式涉及最大化或最小化目标函数。</li>
<li>约束条件包括上下界。</li>
<li>变量上下界是约束的一部分。</li>
<li>二阶锥的标准形式有两种类型。</li>
<li>旋转二阶锥的标准形式与二阶锥不同。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-49">正文（抓取，非 AI）</h3>
<p>Second Order Conic Programming | COAP 在线帮助手册 问题类型 总览 Linear Programming Mix Integer Linear Programming Satisfiability Problem Second Order Conic Programming Semidefinite Programming 支持的求解器 总览 CBC CLP COPT DSDP LEAVES LSTECH_MAPLE Second Order Conic Programming 若目标函数或约束条件中包含非线性函数，称这种规划问题为非线性规划问题。二阶锥规划（Second Order Conic Programming,简称SOCP）是一种常见的非线性规划问题，具有广泛的应用领域和实际意义，其研究问题设计组合优化，金融，对策论，经济学等诸多领域。 锥 的概念：对于一个向量空间V与它的一个子集C，如果子集C中的任意一点x与任意正数 a， 其乘积ax仍然属于子集 C， 则称 C 为一个锥。（根据定义，一个锥总是无界的。） 凸锥 ：若一个锥C中任意两点x与y，以及任意两个正数a与b， 都有 ax + by 属于 C，则该锥为凸锥。 因此，锥规划问题的数学形式可表示为： M A X ( m i n ) z = ∑ j = 1 n c j x j MAX(min)z=\sum_{j=1}^{n}c_{j}x_{j} M A X ( m i n ) z = ​ j = 1 ​ ∑ ​ n ​ ​ c ​ j ​ ​ x ​ j ​ ​ s . t . { l c ≤ ∑ j = 1 n a i j x j ≤ u c ( i = 1 , 2 , 3 , ⋯ , m ) l x ≤ x j ≤ u x a ∈ κ \begin{array}{c} &amp;s.t.\quad\begin{cases}l ^{c} \leq \sum\limits_{j=1}^n a_{ij}x_j \leq u ^{c} &amp; (i=1,2,3,\cdots, m) \ l ^{x} \leq x_j \leq u ^{x} \ a\in \kappa \end{cases} \end{array} ​ ​ ​ ​ s . t . ​ ⎩ ​ ⎪ ​ ⎪ ​ ⎪ ​ ⎪ ​ ⎨ ​ ⎪ ​ ⎪ ​ ⎪ ​ ⎪ ​ ⎧ ​ ​ ​ l ​ c ​ ​ ≤ ​ j = 1 ​ ∑ ​ n ​ ​ a ​ i j ​ ​ x ​ j ​ ​ ≤ u ​ c ​ ​ ​ l ​ x ​ ​ ≤ x ​ j ​ ​ ≤ u ​ x ​ ​ ​ a ∈ κ ​ ​ ​ ( i = 1 , 2 , 3 , ⋯ , m ) ​ ​ ​ ​ 其中 为凸锥， 和 为约束上下界， 和 为变量上下界 二阶锥的标准形式有两种： 二阶锥（Quadratic cone) Q n = { ∑ j = 1 n − 1 x j 2 ≤ x 0 , x ∈ R n } Q ^{n} = \left { \sum_{j=1}^{n-1} x_j ^{2} \leq x_0, x \in \mathbb{R} ^{n} \right } Q ​ n ​ ​ = { ​ j = 1 ​ ∑ ​ n − 1 ​ ​ x ​ j ​ 2 ​ ​ ≤ x ​ 0 ​ ​ , x ∈ R ​ n ​ ​ } 旋转二阶锥（Rotated quadratic cone） Q r n = { ∑ j = 2 n − 1 x j 2 ≤ 2 x 0 x 1 , x ∈ R n } Q_r ^{n} = \left { \sum_{j=2}^{n-1} x_j ^{2} \leq 2 x_0 x_1, x \in \mathbb{R} ^{n} \right } Q ​ r ​ n ​ ​ = { ​ j = 2 ​ ∑ ​ n − 1 ​ ​ x ​ j ​ 2 ​ ​ ≤ 2 x ​ 0 ​ ​ x ​ 1 ​ ​ , x ∈ R ​ n ​ ​ } ← Satisfiability Problem Semidefinite Programming →</p>
</div></details><h2 id="toc-50">26. 什么是LLM大语言模型？定义、训练方式、流行原因和例子</h2>
<ul>
<li>链接：https://nic.zjtu.edu.cn/content/mxzs/202506/4495.html</li>
<li>来源：bing</li>
<li>摘要：2025年6月21日 · 近年来人工智能（AI）领域经历了巨大的增长，而更是其中一个取得快速进展的领域。 NLP中最重要的发展便是大语言模型（LLM），该项技术可能彻底改变我们与科技互动的方式，加 …</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">大语言模型需要大量计算资源进行训练，这不仅使其能够捕捉复杂的语言模式，还使其能够针对特定任务进行微调，从而执行多种自然语言处理（NLP）任务，如文本生成、翻译和其他任务，表现出显著性能。然而，这种强大的能力也带来了挑战，因为大语言模型可能延续训练数据中的偏见，有时对其所写的概念缺乏深刻理解，甚至可能生成冒犯性的、歧视性的甚至是错误性的观念。此外，训练大语言模型会消耗大量能源，这不仅增加了成本，还对环境造成了压力。因此，尽管大语言模型在文本到文本问题中表现出色，但其潜在的偏见和缺乏深刻理解的问题仍然需要得到关注和解决。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>大语言模型需要大量计算资源进行训练。</li>
<li>大语言模型可能延续训练数据中的偏见。</li>
<li>大语言模型有时对其所写的概念缺乏深刻理解。</li>
<li>训练大语言模型会消耗大量能源。</li>
<li>大语言模型的规模使其能够捕捉复杂的语言模式。</li>
<li>大语言模型可以针对特定任务进行微调。</li>
<li>大语言模型可以执行多种NLP任务。</li>
<li>大语言模型在文本生成、翻译和其他任务中表现出显著性能。</li>
<li>大语言模型可以执行文本到文本问题。</li>
<li>大语言模型可能生成冒犯性的、歧视性的甚至是错误性的观念。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-51">正文（抓取，非 AI）</h3>
<p>什么是LLM大语言模型？定义、训练方式、流行原因和例子 - 信息与教育技术中心 - 黄河交通学院 欢迎访问本网站！ 搜索 学校主页 首页 部门概况 工作动态 公告通知 规章制度 技术支持 下载中心 网络安全 人工智能 码上办 模型知识 部门概况 工作动态 公告通知 规章制度 技术支持 下载中心 网络安全 人工智能 码上办 当前位置： 首页 &gt;&gt; 人工智能 &gt;&gt; 模型知识 什么是LLM大语言模型？定义、训练方式、流行原因和例子 发布日期：2025-06-21 浏览量： 近年来人工智能（AI）领域经历了巨大的增长，而更是其中一个取得快速进展的领域。NLP中最重要的发展便是大语言模型（LLM），该项技术可能彻底改变我们与科技互动的方式，加上OpenAI的GPT-3的爆火，使得大语言模型在业界更加备受关注。在本篇文章中，我们将简单地介绍一下大语言模型，科普其定义、训练方式、流行原因、常见大语言模型例子以及其面临的挑战。 大语言模型的定义 大语言模型（英文：Large Language Model，缩写LLM），也称大型语言模型，是一种人工智能模型，旨在理解和生成人类语言。它们在大量的文本数据上进行训练，可以执行广泛的任务，包括文本总结、翻译、情感分析等等。LLM的特点是规模庞大，包含数十亿的参数，帮助它们学习语言数据中的复杂模式。这些模型通常基于深度学习架构，如转化器，这有助于它们在各种NLP任务上取得令人印象深刻的表现。 大语言模型的训练方式 训练语言模型需要向其提供大量的文本数据，模型利用这些数据来学习人类语言的结构、语法和语义。这个过程通常是通过完成的，使用一种叫做自我监督学习的技术。在自我监督学习中，模型通过预测序列中的下一个词或标记，为输入的数据生成自己的标签，并给出之前的词。 训练过程包括两个主要步骤：和微调（fine-tuning）： 在预训练阶段，模型从一个巨大的、多样化的数据集中学习，通常包含来自不同来源的数十亿词汇，如网站、书籍和文章。这个阶段允许模型学习一般的语言模式和表征。 在微调阶段，模型在与目标任务或领域相关的更具体、更小的数据集上进一步训练。这有助于模型微调其理解，并适应任务的特殊要求。 大语言模型的流行原因 为什么大语言模型越来越受欢迎，以下是其主要的流行原因： 性能提升： 大语言模型的庞大规模使其能够捕捉复杂的语言模式，从而在各种任务中展现出令人惊叹的能力，尤其是在准确性和流畅性方面往往超过了以前最先进的方法。 迁移学习： 大语言模型可以针对特定的任务进行微调，使得模型能够利用其一般的语言理解，迅速适应新的领域。这种迁移学习能力大大减少了对特定任务数据和训练时间的需求。 多功能性： 大语言模型可以执行多种任务，而不需要特定任务的架构或模型，可用于文本生成、翻译、总结等，使其在各种应用中具有高度的灵活性和通用性。 高互动性： 大语言模型理解和产生类似人类的反应的能力使其能够与人工智能系统进行更自然和直观的互动，为人工智能驱动的工具和应用提供了新的可能性。 常见的大语言模型 GPT-3（OpenAI）： Generative Pre-trained Transformer 3（GPT-3）是最著名的LLM之一，拥有1750亿个参数。该模型在文本生成、翻译和其他任务中表现出显著的性能，在全球范围内引起了热烈的反响，目前OpenAI已经迭代到了版本。 BERT（谷歌）：Bidirectional Encoder Representations from Transformers（BERT）是另一个流行的LLM，对NLP研究产生了重大影响。该模型使用双向方法从一个词的左右两边捕捉上下文，使得各种任务的性能提高，如情感分析和命名实体识别。 T5（谷歌）： 文本到文本转换器（T5）是一个LLM，该模型将所有的NLP任务限定为文本到文本问题，简化了模型适应不同任务的过程。T5在总结、翻译和问题回答等任务中表现出强大的性能。 ERNIE 3.0 （百度）：百度推出的大语言模型ERNIE 3.0首次在百亿级和千亿级预训练模型中引入大规模知识图谱，提出了海量无监督文本与大规模知识图谱的平行预训练方法。 大语言模型面临的挑战 尽管大语言模型的能力令人刮目相看，但他们仍然面临着一些挑战： 资源消耗巨大： 训练LLM需要大量的计算资源，这使得较小的组织或研究人员在开发和部署这些模型方面面临挑战。此外，与训练LLM有关的能源消耗也引起了一定程度的环境问题。 输出可能带有偏见：由于训练数据中可能带有偏见，而LLM可以学习并延续其训练数据中的偏见，导致有偏见的输出，可能是冒犯性的、歧视性甚至是错误性的观念。 理解能力受限： 虽然大语言模型有能力产生看似连贯和与背景上下文相关的文本，但LLM有时对其所写的概念缺乏深刻的理解，这很可能导致不正确或无意义的输出。 上一篇: 什么是预训练Pre-training？定义、重要性、技术和挑战 下一篇: 什么是多模态深度学习？定义、原因、应用和挑战 地址: 河南省武陟迎宾大道333号（郑州桃花峪黄河大桥北） 邮编：454950 Copyright ©all rights reserved - ZJTU.EDU.CN 黄河交通学院 ICP备案：豫ICP备17003646号-1 豫公网安备 41082302410884号</p>
</div></details><h2 id="toc-52">27. FreDF: Learning to Forecast in the Frequency Domain</h2>
<ul>
<li>链接：https://openreview.net/forum?id=4A9IdSa1ul</li>
<li>来源：bing</li>
<li>摘要：2025年1月22日 · For FreDF, we fix the hyperparameters associated with the forecast models, such as the iTransformer and TimesNet, and only fine-tune the frequency loss strength α and the learning rate …</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">首先，当前的预测模型在微调过程中固定了预测模型的超参数，仅调整了频率损失强度和学习率，这可能导致预测性能的提升受限。其次，现有的预测模型往往忽视了未来序列标签之间的关联性，这种忽视使得预测模型的学习目标在存在标签关联的情况下变得有偏。因此，FreDF通过在频率域中学习来改进预测性能，它能够兼容各种预测模型，并通过减轻标签关联性来减少估计偏差，从而提升预测准确性。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>forecasting models' hyperparameters are fixed while fine-tuning frequency loss strength and learning rate</li>
<li>label correlations among future sequences are often overlooked in current forecasting models</li>
<li>FreDF improves forecasting performance by learning in the frequency domain</li>
<li>DF's learning objective is biased in the presence of label correlation</li>
<li>FreDF is compatible with various forecast models</li>
<li>FreDF reduces estimation bias by mitigating label correlation</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-53">正文（抓取，非 AI）</h3>
<p>FreDF: Learning to Forecast in the Frequency Domain | OpenReview Go to ICLR 2025 Conference homepage FreDF: Learning to Forecast in the Frequency Domain Hao Wang , Lichen Pan , Yuan Shen , Zhichao Chen , Degui Yang , Yifei Yang , Sen Zhang , Xinggao Liu , Haoxuan Li , Dacheng Tao Published: 22 Jan 2025, Last Modified: 02 Apr 2025 ICLR 2025 Poster Everyone Revisions BibTeX CC BY 4.0 Keywords : Time series, Long-term Forecast TL;DR : Learning to forecast in the frequency domain improves forecasting performance. Abstract : Time series modeling presents unique challenges due to autocorrelation in both historical data and future sequences. While current research predominantly addresses autocorrelation within historical data, the correlations among future labels are often overlooked. Specifically, modern forecasting models primarily adhere to the Direct Forecast (DF) paradigm, generating multi-step forecasts independently and disregarding label correlations over time. In this work, we demonstrate that the learning objective of DF is biased in the presence of label correlation. To address this issue, we propose the Frequency-enhanced Direct Forecast (FreDF), which mitigates label correlation by learning to forecast in the frequency domain, thereby reducing estimation bias. Our experiments show that FreDF significantly outperforms existing state-of-the-art methods and is compatible with a variety of forecast models. Code is available at https://github.com/Master-PLC/FreDF. Supplementary Material : pdf Primary Area : learning on time series and dynamical systems Code Of Ethics : I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics. Submission Guidelines : I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide. Anonymous Url : I certify that there is no URL (e.g., github page) that could be used to find authors’ identity. No Acknowledgement Section : I certify that there is no acknowledgement section in this submission for double blind review. Submission Number : 13602 Loading OpenReview is a long-term project to advance science through improved peer review with legal nonprofit status. We gratefully acknowledge the support of the OpenReview Sponsors . © 2026 OpenReview</p>
</div></details><h2 id="toc-54">28. Crossformer: Transformer Utilizing Cross-Dimension Dependency …</h2>
<ul>
<li>链接：https://openreview.net/forum?id=vSVLM2j9eie</li>
<li>来源：bing</li>
<li>摘要：2023年2月1日 · We propose Crossformer, a Transformer-based model that explicitly utilizes cross-dimension dependency for multivariate time series forecasting.</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">Crossformer专门设计用于多变量时间序列预测，能够同时捕捉时间维度和变量维度的依赖关系。然而，现有的Transformer模型往往忽略了变量维度间的依赖关系，这导致了预测效果的下降。为了解决这一问题，Crossformer通过DSW嵌入保留时间和维度信息，同时采用TSA层高效地捕捉时间和维度间的依赖关系。此外，Crossformer采用了层次编码解码结构，进一步提高了模型的预测效果。实验结果表明，Crossformer在多变量时间序列预测任务中表现优于现有方法，这主要得益于DSW嵌入和TSA层的共同作用。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>Crossformer可以捕捉时间维度和变量维度的依赖关系。</li>
<li>现有Transformer模型往往忽略了变量维度间的依赖关系。</li>
<li>Crossformer通过DSW嵌入保留时间和维度信息。</li>
<li>TSA层有助于高效捕捉时间和维度间的依赖关系。</li>
<li>Crossformer采用层次编码解码结构。</li>
<li>实验结果表明Crossformer优于现有方法。</li>
<li>Crossformer是专门为多变量时间序列预测设计的。</li>
<li>DSW嵌入和TSA层共同作用以提高预测效果。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-55">正文（抓取，非 AI）</h3>
<p>Crossformer: Transformer Utilizing Cross-Dimension Dependency for Multivariate Time Series Forecasting | OpenReview Go to ICLR 2023 Conference homepage Crossformer: Transformer Utilizing Cross-Dimension Dependency for Multivariate Time Series Forecasting Yunhao Zhang , Junchi Yan Published: 01 Feb 2023, Last Modified: 02 Mar 2023 ICLR 2023 notable top 5% Readers: Everyone Keywords : Transformer, multivariate time series forecasting, deep learning Abstract : Recently many deep models have been proposed for multivariate time series (MTS) forecasting. In particular, Transformer-based models have shown great potential because they can capture long-term dependency. However, existing Transformer-based models mainly focus on modeling the temporal dependency (cross-time dependency) yet often omit the dependency among different variables (cross-dimension dependency), which is critical for MTS forecasting. To fill the gap, we propose Crossformer, a Transformer-based model utilizing cross-dimension dependency for MTS forecasting. In Crossformer, the input MTS is embedded into a 2D vector array through the Dimension-Segment-Wise (DSW) embedding to preserve time and dimension information. Then the Two-Stage Attention (TSA) layer is proposed to efficiently capture the cross-time and cross-dimension dependency. Utilizing DSW embedding and TSA layer, Crossformer establishes a Hierarchical Encoder-Decoder (HED) to use the information at different scales for the final forecasting. Extensive experimental results on six real-world datasets show the effectiveness of Crossformer against previous state-of-the-arts. Anonymous Url : I certify that there is no URL (e.g., github page) that could be used to find authors’ identity. No Acknowledgement Section : I certify that there is no acknowledgement section in this submission for double blind review. Code Of Ethics : I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics Submission Guidelines : Yes Please Choose The Closest Area That Your Submission Falls Into : Applications (eg, speech processing, computer vision, NLP) TL;DR : We propose Crossformer, a Transformer-based model that explicitly utilizes cross-dimension dependency for multivariate time series forecasting. 21 Replies Loading OpenReview is a long-term project to advance science through improved peer review with legal nonprofit status. We gratefully acknowledge the support of the OpenReview Sponsors . © 2026 OpenReview</p>
</div></details><h2 id="toc-56">29. A Time Series is Worth 64 Words: Long-term Forecasting with...</h2>
<ul>
<li>链接：https://openreview.net/forum?id=Jbdc0vTOcol</li>
<li>来源：bing</li>
<li>摘要：2023年2月1日 · Channel-independent patch time series transformer works very well for long-term forecasting and representation learning.</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">patch时间序列Transformer通过将时间序列分割成子序列级的片段作为输入token，保留了局部语义信息，同时通过将每个通道视为单一的单变量时间序列，减少了计算和内存使用。此外，patch时间序列Transformer使每个通道共享相同的嵌入和Transformer权重，从而实现了跨所有系列的一致性。这种设计不仅允许更长历史的关注，还通过注意力图的计算和内存使用减少，进一步提升了模型的效率。由于channel独立性，patch时间序列Transformer在长期预测准确性上优于SOTA模型。在自监督预训练任务中，该模型表现出色，优于大型数据集上的监督训练。此外，patch时间序列Transformer在不同数据集上的掩码预训练也产生了SOTA预测准确性。因此，patch时间序列Transformer在多个方面展现了其独特优势和卓越性能。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>patch时间序列Transformer通过将时间序列分割成子序列级的片段作为输入token，保留了局部语义信息。</li>
<li>patch时间序列Transformer通过将每个通道视为单一的单变量时间序列，减少了计算和内存使用。</li>
<li>patch时间序列Transformer通过使每个通道共享相同的嵌入和Transformer权重，实现了跨所有系列的一致性。</li>
<li>patch时间序列Transformer通过注意力图的计算和内存使用减少，允许更长的历史关注。</li>
<li>channel独立性使得patch时间序列Transformer在长期预测准确性上优于SOTA模型。</li>
<li>patch时间序列Transformer在自监督预训练任务中表现出色，优于大型数据集上的监督训练。</li>
<li>patch时间序列Transformer在不同数据集上的掩码预训练也产生了SOTA预测准确性。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-57">正文（抓取，非 AI）</h3>
<p>A Time Series is Worth 64 Words: Long-term Forecasting with Transformers | OpenReview Go to ICLR 2023 Conference homepage A Time Series is Worth 64 Words: Long-term Forecasting with Transformers Yuqi Nie , Nam H Nguyen , Phanwadee Sinthong , Jayant Kalagnanam Published: 01 Feb 2023, Last Modified: 14 Jan 2026 ICLR 2023 poster Readers: Everyone Keywords : time series, transformer, forecasting, channel-independence, self-supervised learning, representation learning TL;DR : Channel-independent patch time series transformer works very well for long-term forecasting and representation learning. Abstract : We propose an efficient design of Transformer-based models for multivariate time series forecasting and self-supervised representation learning. It is based on two key components: (i) segmentation of time series into subseries-level patches which are served as input tokens to Transformer; (ii) channel-independence where each channel contains a single univariate time series that shares the same embedding and Transformer weights across all the series. Patching design naturally has three-fold benefit: local semantic information is retained in the embedding; computation and memory usage of the attention maps are quadratically reduced given the same look-back window; and the model can attend longer history. Our channel-independent patch time series Transformer (PatchTST) can improve the long-term forecasting accuracy significantly when compared with that of SOTA Transformer-based models. We also apply our model to self-supervised pre-training tasks and attain excellent fine-tuning performance, which outperforms supervised training on large datasets. Transferring of masked pre-training performed on one dataset to other datasets also produces SOTA forecasting accuracy. Anonymous Url : I certify that there is no URL (e.g., github page) that could be used to find authors’ identity. No Acknowledgement Section : I certify that there is no acknowledgement section in this submission for double blind review. Code Of Ethics : I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics Submission Guidelines : Yes Please Choose The Closest Area That Your Submission Falls Into : Applications (eg, speech processing, computer vision, NLP) Community Implementations : <a href="https://www.catalyzex.com/paper/a-time-series-is-worth-64-words-long-term/code"><img alt="CatalyzeX" src="/images/catalyzex_icon.svg"/> 4 code implementations</a> 21 Replies Loading OpenReview is a long-term project to advance science through improved peer review with legal nonprofit status. We gratefully acknowledge the support of the OpenReview Sponsors . © 2026 OpenReview</p>
</div></details><h2 id="toc-58">30. A Benchmark Study For Limit Order Book (LOB) Models and Time …</h2>
<ul>
<li>链接：https://openreview.net/forum?id=MhD9rLeU31</li>
<li>来源：bing</li>
<li>摘要：2024年9月22日 · Traditional Transformer-based models like PatchTST and iTransformer, while powerful for many time series applications, often struggle with LOB data due to their high noise levels and …</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">传统基于变压器的模型在处理限价订单簿数据时可能表现不佳，因为这些数据的噪声水平较高。基准研究对于评估深度学习模型在限价订单簿数据上的表现至关重要，但现有的限价订单簿模型在中价回报预测任务上尚未进行过基准测试。因此，将限价订单簿模型在专有期货数据集上进行基准测试，可以揭示与股票数据集相比的性能差距。限价订单簿意识模型设计对于在限价订单簿数据集上实现最佳预测性能至关重要。所提出的CVML架构显著提升了限价订单簿数据上的中价回报预测性能。然而，常用的开源股票限价订单簿数据集可能不具有代表性，无法适用于期货数据集。此外，不同限价订单簿数据集之间的性能差距强调了需要针对特定数据集的模型。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>traditional transformer-based models may not perform well with limit order book data due to high noise levels.</li>
<li>benchmark studies are crucial for evaluating the performance of deep learning models on limit order book data.</li>
<li>existing limit order book models have not been benchmarked on mid-price return forecasting tasks before.</li>
<li>benchmarking limit order book models on a proprietary futures dataset highlights performance gaps compared to stock datasets.</li>
<li>limit order book-aware model design is essential for optimal prediction performance on limit order book datasets.</li>
<li>proposed cvml architecture significantly enhances mid-price return forecasting performance on limit order book data.</li>
<li>commonly used open-source stock limit order book datasets may not be representative for futures datasets.</li>
<li>performance gaps exist between different limit order book datasets, emphasizing the need for dataset-specific models.</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-59">正文（抓取，非 AI）</h3>
<p>A Benchmark Study For Limit Order Book (LOB) Models and Time Series Forecasting Models on LOB Data | OpenReview Go to ICLR 2025 Conference homepage A Benchmark Study For Limit Order Book (LOB) Models and Time Series Forecasting Models on LOB Data Weijian Li , Stephen S Cheng , Lining Mao , Jigyasa Kumari , Alex Pyo , Mehak Kawatra , Jialong Li , Jiayi Wang , Ammar Gilani , Jingya Xun , Jerry Yao-Chieh Hu , Han Liu 23 Sept 2024 (modified: 05 Feb 2025) Submitted to ICLR 2025 Everyone Revisions BibTeX CC BY 4.0 Keywords : benchmark, time series forecasting, convolution, deep learning, limit order book, mid-price trend prediction, mid-price return forecasting Abstract : We present a comprehensive benchmark to evaluate the performance of deep learning models on limit order book (LOB) data. Our work makes four significant contributions: (i) We evaluate existing LOB models on a proprietary futures LOB dataset to examine the transferability of LOB model performance between various assets; (ii) We are the first to benchmark existing LOB models on the mid-price return forecasting (MPRF) task. (iii) We present the first benchmark study to evaluate SOTA time series forecasting models on the MPRF task to bridge the two fields of general-purpose time series forecasting and LOB time series forecasting; and (iv) we propose an architecture of convolutional cross-variate mixing layers (CVML) as an add-on to any deep learning multivariate time series model to significantly enhance MPRF performance on LOB data. Our empirical results highlight the value of our benchmark results on our proprietary futures LOB dataset, demonstrating a performance gap between the commonly used open-source stock LOB dataset and our futures dataset. Furthermore, the results demonstrate that LOB-aware model design is essential for achieving optimal prediction performance on LOB datasets. Most importantly, our results show that our proposed CVML architecture brings about an average improvement of 244.9% to various time series models’ mid-price return forecasting performance. Supplementary Material : zip Primary Area : learning on time series and dynamical systems Code Of Ethics : I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics. Submission Guidelines : I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide. Anonymous Url : I certify that there is no URL (e.g., github page) that could be used to find authors’ identity. No Acknowledgement Section : I certify that there is no acknowledgement section in this submission for double blind review. Submission Number : 2746 Loading OpenReview is a long-term project to advance science through improved peer review with legal nonprofit status. We gratefully acknowledge the support of the OpenReview Sponsors . © 2026 OpenReview</p>
</div></details><h2 id="toc-60">31. 环境光遮蔽 - 知乎</h2>
<ul>
<li>链接：https://www.zhihu.com/topic/20712088/intro</li>
<li>来源：bing</li>
<li>摘要：2016年8月21日 · 环境光遮蔽（Ambient Occlusion），简称“AO”，是一种非常复杂的光照技术，通过计算光线在物体上的折射和吸收，在受影响的位置上渲染出适当的阴影，添加渲染深度，从而进一步丰富 …</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">环境光遮蔽并非真实现象，而是3D应用程序中用于进行光线追踪、创建阴影错觉的一种技术。最早在2002年的Siggraph大会上由ILM公司展示，它能够为渲染添加更多真实感，通过创建柔和的全局阴影来模拟环境光对物体的影响。环境光遮蔽是一种全局方法，即对象每个点的亮度值取决于场景中的其他对象，因此可以烘焙到纹理上应用于模型的一部分材质，或烘焙到光照贴图上分析更大空间，了解光线如何移动。尽管环境光遮蔽技术的成本、操作速度和输入数据类型各异，但仅是实现全局光照的一种方式。为了降低计算成本，屏幕空间环境光遮蔽（SSAO）是用于实时近似环境光遮蔽效果的要求最低的技术，而水平基准环境光遮蔽（HBAO）提供了更好的视觉效果，但成本更高。高解析度环境光遮蔽（HDAO）通过增加在计算应该变暗的区域时所使用的样本数量，进一步提升了效果。体素加速环境光遮蔽（VXAO）则更准确地考虑了直射和反射光，尽管成本更高，但其算法包括体素化、体素后处理和锥形追踪三个过程，能够提供更为精确的阴影效果。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>环境光遮蔽并非真实现象，而是3D应用程序用来进行光线追踪，创建阴影错觉的一种技术。</li>
<li>环境光遮蔽最早在2002年的Siggraph大会上由ILM公司展示。</li>
<li>环境光遮蔽可以为渲染添加更多真实感，因为它能够创建柔和的全局阴影。</li>
<li>环境光遮蔽是一种全局方法，即对象每个点的亮度值取决于场景中的其他对象。</li>
<li>环境光遮蔽在KeyShot中被视为附近几何体所产生的阴影。</li>
<li>环境光遮蔽技术的成本、操作速度和输入数据类型不同。</li>
<li>环境光遮蔽烘焙到纹理上可以应用于模型的一部分材质。</li>
<li>环境光遮蔽烘焙到光照贴图上可以分析更大空间，了解光线如何移动。</li>
<li>环境光遮蔽技术仅是实现全局光照的一种方式。</li>
<li>屏幕空间环境光遮蔽（SSAO）是用于实时近似环境光遮蔽效果的要求最低的技术。</li>
<li>水平基准环境光遮蔽（HBAO）提供了更好的视觉效果，但成本比SSAO更高。</li>
<li>高解析度环境光遮蔽（HDAO）增加了在计算应该变暗的区域时所使用的样本数量。</li>
<li>体素加速环境光遮蔽（VXAO）更准确地考虑了直射和反射光，但成本更高。</li>
<li>VXAO算法包括体素化、体素后处理和锥形追踪三个过程。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-61">正文（抓取，非 AI）</h3>
<p>环境光遮蔽 - 知乎 环境光遮蔽 环境光遮蔽（Ambient Occlusion），简称“AO”，是一种非常复杂的光照技术，通过计算光线在物体上的折射和吸收，在受影响的位置上渲染出适当的阴影，添加渲染深度，从而进一步丰富标准光照渲… 查看全部内容 关注话题 ​ 管理 ​ 分享 ​ 百科 讨论 精华 等待回答 详细内容 简介 环境光遮蔽（Ambient Occlusion），简称“ AO ”，是一种非常复杂的光照技术，通过计算光线在物体上的折射和吸收，在受影响的位置上渲染出适当的阴影，添加渲染深度，从而进一步丰富标准光照渲染器的效果。光遮蔽并不是真实的现象，而是3D应用程序用来进行光线追踪，创建阴影错觉的一种光照技术。 环境光遮蔽最早在 2002年 的Siggraph大会上，由ILM工业光魔公司的技术主管Hayden Landis向公众展示。ILM就是拍《星球大战》的乔治·卢卡斯的创建的公司，这不禁让人想到英伟达的RTX（实时光线追踪）演示片也是星球大战。 环境光遮蔽可以为渲染添加更多的真实感，因为它能够创建柔和的全局阴影，从而有助于对象的视觉分离。这是一种全局方法，即对象每个点的亮度值取决于场景中的其他对象。 这张对比图中，左边只有单一的光照和阴影，而右边开启了AO之后，物体接触的地方就有了加深的环境阴影，瞬间就感觉逼真起来了 工作原理 在KeyShot3D渲染软件里，光遮蔽可被认为是附近几何体所产生的阴影，因此光遮蔽表面也是一种紧挨着另一个表面的遮蔽表面。KeyShot作为一种精确的物理光线追踪渲染器，无需依赖光遮蔽来模拟阴影。在KeyShot里，光遮蔽是一种纹理，可用来实现不同的效果。首先，我们来对比观察下下图中的有光遮蔽和没有光遮蔽的物体对象： 想象一下将三个完全相同的盒子放在一面墙前面，每个盒子与墙的距离不一样，盒子与墙的距离决定了它们能否被光遮蔽。如下图所示，蓝色表示光遮蔽。在KeyShot里，光遮蔽程序纹理允许你定义几何体被光遮蔽的距离，下图中的盒子A与墙的距离为1.5cm，因为盒子A与墙的实际距离就是1.5cm，不会被光遮蔽。盒子B和C与墙的距离都小于1.5cm，因此它们的某些面被光遮蔽了。在KeyShot中，几何体被光遮蔽的距离受控于半径滑块，半径单位永远是KeyShot的场景单位。 算法 有一系列的着色算法可以从全局照明中提供不同程度的逼真阴影渲染。它们的成本，操作速度和输入数据类型不同。 其他方法 1.将环境光遮蔽烘焙到纹理上 ：在这种情况下，纹理用作应用于模型的一部分材质。在某些模型中，它将烘焙为Metallic+Roughness纹理的绿/蓝通道。如果将屏幕方法的效果加起来，则将纹理的环境光遮蔽添加到其中，然后获得更详细的阴影。这种方法弥补了另一种方法的缺点：将AO烘焙到光照贴图上。这种方法的缺点是，在烘焙的AO纹理中相邻对象不会有阴影。实际上，在该纹理中只能获得自阴影贴图。 2.将环境光遮蔽烘焙到光照贴图上，前提是光照贴图可用 ： 这种方法“无需成本”，因为光照贴图可用，但它会降低烘培速度，并增加额外的时间来计算环境光遮蔽效果。这种方法更准确，因为在烘焙阶段，不仅可以分析附近表面，而且可以分析更大的空间，从而了解光线是如何移动。主要缺点是光照贴图不是非常详细，而纹理像素的大小是1/10米或1/20米。这意味着烘焙到光照贴图上的AO将仅包含最大部分，而所有次要细节都不会被烘焙。因此，这种方法仅适用于传输非常大的细节。 应用解析 AO技术的最终目的也就是让光影效果更逼真，实现全局光照（AO只是实现方式之一）。它用于表现东西之间对漫反射光线的遮挡，如果人们玩游戏的时候，可能会能看到很多建模本身不那么真实，那么用了AO以后，东西之间距离越近就会有越明显的阴影，从而增加了层次感、真实感，让玩家玩儿起来更投入。其重要应用范围有一下几点： 1.全局光照是未来实现游戏图形画面真实化的一个重要元素（其可以达成光影效果的逼真化，对应的相反关键词为直接光照）。 2.光线追踪是实现全局光照完美效果的最佳技术/方式之一，也是未来的一个发展方向，其技术实现方式最接近现实的物理模型，但是因为效率较低，所以现在的游戏和显卡架构无法承担。 3.网络游戏已经开始实现全局光照的效果，但是并未采用光线追踪的方式，而是采用其它多种实现方式来达成全局光照的效果，其中包括“AO”环境光遮蔽。 4.“AO”环境光遮蔽是实现全局光照中部分物体局部光照和阴影真实化的一种技术方式，其函数实现方式并未严格遵循现实的物理模型，但是效率较高，因此被当前游戏广泛应用。 5. AO环境光遮蔽（包括其变种SSAO、HBAO等）仅是实现全局光照的技术方式之一，当前的游戏应用了多种技术方式共同达成全局光照总体效果的完善和完美。 相关技术概念 屏幕空间环境光遮蔽 SSAO（屏幕空间环境光遮蔽）是用于实时近似环境光遮蔽效果的要求最低的计算机图形技术。但根据开发者实现的效果，实际质量可能因游戏而异。这种效果在弯曲处和凹陷处可见，但在平面和凸面上不可见，因为它们不会阻挡彼此的光线。这种效果非常重要，因为没有它的存在将会令场景缺乏层次感。 水平基准环境光遮蔽（Horizon Based Ambient Occlusion；HBAO） HBAO是一种用于近似环境光遮蔽效果的算法，是英伟达在2008年提出的一种SSAO衍生版本。这一方法提供了更好的视觉效果，但成本比SSAO更高。 高解析度环境光遮蔽（High Definition Ambient Occlusion；HDAO） HDAO由AMD开发。这种算法基于Gather4技术，亦即在一个寄存器中采集四个纹素。HDAO和HBAO大致相同，它们的操作原理类似。HDAO增加了在计算应该变暗的区域时所使用的样本数量，并以全分辨率渲染，从而更准确地呈现AO。 体素加速环境光遮蔽（Voxel Accelerated Ambient Occlusion；VXAO） VXAO是一种适用于体素的方法（体素相当于3D中的像素）。DirectX 12支持在搜索结构中对场景进行回溯和体素化。 通过这样做，我们可以获得一种考虑到更复杂场景几何的，稍微更准确的方法。但这种方法比其他方法成本更高。VXAO是VXGI（体素全局照明）技术的一部分，它更准确地考虑了直射和反射光。 VXAO算法包括三个过程：体素化，体素后处理和锥形追踪。体素化意味着将基于三角形的网格渲染为3D纹理。 它的性能取决于三角形的总数，它们的大小，以及渲染它们所需的绘制调用次数。后处理涉及清除，过滤和下采样体素的过程，其性能取决于体素化过程中产生的体素总数。体素锥形追踪在屏幕空间中进行，其性能取决于着色率，屏幕分辨率和1080p分辨率的锥形追踪通道。 参考资料 GeForce官方网站，在游戏中启用环境光遮蔽(Ambient Occlusion， 在游戏中启用环境光遮蔽（Ambient Occlusion） Ambient Occlusion， Ambient Occlusion ，2018.3 坯子库，Ambient Occlusion（AO渲染器）， Ambient Occlusion（AO渲染器） ， 2016年8月21日 浏览量 36.2 万 讨论量 234</p>
</div></details><h2 id="toc-62">32. TimeMixer++: A General Time Series Pattern Machine for …</h2>
<ul>
<li>链接：https://openreview.net/forum?id=1CLzLXSFNn</li>
<li>来源：bing</li>
<li>摘要：2025年1月22日 · TimeMixer++ achieves consistent improvements over TimesNet and iTransformer, underscoring its contribution to advancing the state of the art in general time series analysis.</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">TimeMixer++ 在时间序列分析中表现出色，其性能超越了 TimesNet 和 iTransformer，证明了其在该领域的先进性。它能够处理多尺度时间序列，通过多分辨率时间成像、时间图像分解、多尺度混合和多分辨率混合来提取复杂的时间序列模式。TimeMixer++ 通过双轴注意力机制捕捉季节性和趋势模式，并通过层次聚合多尺度模式，适应性地整合所有表示。此外，它在8个时间序列分析任务中实现了最先进的性能，包括预测、分类、异常检测和插补，这表明其设计旨在通过强大的表示和模式提取能力在广泛的时间序列任务中表现出色。因此，TimeMixer++ 成为了处理复杂时间序列数据的理想选择。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>TimeMixer++ 的性能超越了 TimesNet 和 iTransformer，表明它在时间序列分析中的先进性。</li>
<li>TimeMixer++ 能够处理多尺度时间序列，通过多分辨率时间成像、时间图像分解、多尺度混合和多分辨率混合来提取复杂的时间序列模式。</li>
<li>TimeMixer++ 在8个时间序列分析任务中实现了最先进的性能，包括预测、分类、异常检测和插补。</li>
<li>TimeMixer++ 通过多分辨率时间成像捕捉时间域和频域中的模式。</li>
<li>TimeMixer++ 使用双轴注意力提取季节性和趋势模式。</li>
<li>TimeMixer++ 通过层次聚合多尺度模式。</li>
<li>TimeMixer++ 适应性地整合所有表示。</li>
<li>TimeMixer++ 的设计旨在通过强大的表示和模式提取能力在广泛的时间序列任务中表现出色。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-63">正文（抓取，非 AI）</h3>
<p>TimeMixer++: A General Time Series Pattern Machine for Universal Predictive Analysis | OpenReview Go to ICLR 2025 Conference homepage TimeMixer++: A General Time Series Pattern Machine for Universal Predictive Analysis Shiyu Wang , Jiawei LI , Xiaoming Shi , Zhou Ye , Baichuan Mo , Wenze Lin , Ju Shengtong , Zhixuan Chu , Ming Jin Published: 22 Jan 2025, Last Modified: 18 May 2025 ICLR 2025 Oral Everyone Revisions BibTeX CC BY 4.0 Keywords : time series, pattern machine, predictive analysis TL;DR : TimeMixer++ is a time series pattern machine that employs multi-scale and multi-resolution pattern extraction to deliver SOTA performance across 8 diverse analytical tasks, including forecasting, classification, anomaly detection, and imputation. Abstract : Time series analysis plays a critical role in numerous applications, supporting tasks such as forecasting, classification, anomaly detection, and imputation. In this work, we present the time series pattern machine (TSPM), a model designed to excel in a broad range of time series tasks through powerful representation and pattern extraction capabilities. Traditional time series models often struggle to capture universal patterns, limiting their effectiveness across diverse tasks. To address this, we define multiple scales in the time domain and various resolutions in the frequency domain, employing various mixing strategies to extract intricate, task-adaptive time series patterns. Specifically, we introduce TimeMixer++, a general-purpose TSPM that processes multi-scale time series using (1) multi-resolution time imaging (MRTI), (2) time image decomposition (TID), (3) multi-scale mixing (MCM), and (4) multi-resolution mixing (MRM) to extract comprehensive temporal patterns. MRTI transforms multi-scale time series into multi-resolution time images, capturing patterns across both temporal and frequency domains. TID leverages dual-axis attention to extract seasonal and trend patterns, while MCM hierarchically aggregates these patterns across scales. MRM adaptively integrates all representations across resolutions. TimeMixer++ achieves state-of-the-art performance across 8 time series analytical tasks, consistently surpassing both general-purpose and task-specific models. Our work marks a promising step toward the next generation of TSPMs, paving the way for further advancements in time series analysis. Primary Area : learning on time series and dynamical systems Code Of Ethics : I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics. Submission Guidelines : I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide. Anonymous Url : I certify that there is no URL (e.g., github page) that could be used to find authors’ identity. No Acknowledgement Section : I certify that there is no acknowledgement section in this submission for double blind review. Submission Number : 9409 Loading OpenReview is a long-term project to advance science through improved peer review with legal nonprofit status. We gratefully acknowledge the support of the OpenReview Sponsors . © 2026 OpenReview</p>
</div></details><h2 id="toc-64">33. Forum | OpenReview</h2>
<ul>
<li>链接：https://openreview.net/forum?id=JePfAI8fah</li>
<li>来源：bing</li>
<li>摘要：2024年1月16日 · The recent boom of linear forecasting models questions the ongoing passion for architectural modifications of Transformer-based forecasters. These forecasters leverage …</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">iTransformer 的设计基于对 Transformer 组件功能的反思，无需对基本组件进行任何修改，而是通过将注意力机制和前馈网络应用于反转维度来实现时间序列预测。这种创新方法使得 iTransformer 能够处理具有较大回溯窗口的时间序列预测，而不会出现性能下降和计算爆炸。此外，iTransformer 通过嵌入时间点为变量令牌来捕捉多变量相关性，并通过为每个变量令牌应用前馈网络来学习非线性表示，从而提升了 Transformer 家族在不同变量上的泛化能力。因此，iTransformer 在现实世界数据集上达到了最先进的性能，并且更好地利用了任意大小的回溯窗口，成为时间序列预测的基本骨干结构的一个不错的选择。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>iTransformer 的设计基于对 Transformer 组件功能的反思，无需对基本组件进行任何修改。</li>
<li>iTransformer 通过将注意力机制和前馈网络应用于反转维度来实现时间序列预测。</li>
<li>iTransformer 能够处理具有较大回溯窗口的时间序列预测，而不会出现性能下降和计算爆炸。</li>
<li>iTransformer 通过嵌入时间点为变量令牌来捕捉多变量相关性。</li>
<li>iTransformer 通过为每个变量令牌应用前馈网络来学习非线性表示。</li>
<li>iTransformer 在现实世界数据集上达到了最先进的性能。</li>
<li>iTransformer 提升了 Transformer 家族在不同变量上的泛化能力。</li>
<li>iTransformer 更好地利用了任意大小的回溯窗口。</li>
<li>iTransformer 成为时间序列预测的基本骨干结构的一个不错的选择。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-65">正文（抓取，非 AI）</h3>
<p>iTransformer: Inverted Transformers Are Effective for Time Series Forecasting | OpenReview Go to ICLR 2024 Conference homepage iTransformer: Inverted Transformers Are Effective for Time Series Forecasting Yong Liu , Tengge Hu , Haoran Zhang , Haixu Wu , Shiyu Wang , Lintao Ma , Mingsheng Long Published: 16 Jan 2024, Last Modified: 14 Mar 2024 ICLR 2024 spotlight Everyone Revisions BibTeX Code Of Ethics : I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics. Keywords : Time Series Forecasting, Transformer Submission Guidelines : I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2024/AuthorGuide. TL;DR : Based on the reflection on the duties of Transformer components, we propose inverted Transformer for time series forecasting, which achieves the SOTA in real-world applications and shows powerful strength on framework generalization. Abstract : The recent boom of linear forecasting models questions the ongoing passion for architectural modifications of Transformer-based forecasters. These forecasters leverage Transformers to model the global dependencies over temporal tokens of time series, with each token formed by multiple variates of the same timestamp. However, Transformers are challenged in forecasting series with larger lookback windows due to performance degradation and computation explosion. Besides, the embedding for each temporal token fuses multiple variates that represent potential delayed events and distinct physical measurements, which may fail in learning variate-centric representations and result in meaningless attention maps. In this work, we reflect on the competent duties of Transformer components and repurpose the Transformer architecture without any modification to the basic components. We propose iTransformer that simply applies the attention and feed-forward network on the inverted dimensions. Specifically, the time points of individual series are embedded into variate tokens which are utilized by the attention mechanism to capture multivariate correlations; meanwhile, the feed-forward network is applied for each variate token to learn nonlinear representations. The iTransformer model achieves state-of-the-art on challenging real-world datasets, which further empowers the Transformer family with promoted performance, generalization ability across different variates, and better utilization of arbitrary lookback windows, making it a nice alternative as the fundamental backbone of time series forecasting. Code is available at this repository: https://github.com/thuml/iTransformer. Anonymous Url : I certify that there is no URL (e.g., github page) that could be used to find authors' identity. No Acknowledgement Section : I certify that there is no acknowledgement section in this submission for double blind review. Primary Area : representation learning for computer vision, audio, language, and other modalities Submission Number : 632 Loading OpenReview is a long-term project to advance science through improved peer review with legal nonprofit status. We gratefully acknowledge the support of the OpenReview Sponsors . © 2026 OpenReview</p>
</div></details><h2 id="toc-66">34. 模仿学习 (Imitation Learning)入门指南</h2>
<ul>
<li>链接：https://www.zhihu.com/tardis/bd/art/140348314</li>
<li>来源：bing</li>
<li>摘要：2020年5月14日 · 模仿学习的思想很直观 (intuitive)。我们在前面所介绍的Model-free, Model-based强化学习方法都是 从零开始 (from scratch) 探索并学习一个使累计回报最大的策略 (policy) 。 Imitation …</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">模仿学习的目标是使模型生成的状态-动作轨迹分布与输入的轨迹分布相匹配，这要求模型能够准确地模仿专家的行为。然而，模仿学习需要大量高质量的示范数据，但好的示范难以收集，这使得数据收集成为一大挑战。在强化学习中，误差会累积，而不仅仅是像图像分类那样，因此模仿学习的方法如DAgger需要额外的传感器来确保模型的准确性，但这也增加了成本。DAgger通过不断询问专家来改进模型，其目标是通过模仿专家的动作序列来提高模型的性能。此外，模仿学习可以通过引入GAN来生成更接近专家的动作序列，或者通过动作捕捉来获取高质量的专家数据。模仿学习不仅限于模仿行走，还可以模仿多种行为，而One-Shot Imitation Learning允许通过少量示范来学习新的行为，Third-Person Imitation Learning则可以让智能体通过观看视频学习。Unsupervised Perceptual Rewards可以无监督地找出关键动作，模仿学习可以通过特征的MSE来保证特征的一致性。因此，模仿学习可以通过观察中学习，通过视频来模仿行为，从而实现从第三视角的示范中学习。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>模仿学习的目标是使模型生成的状态-动作轨迹分布与输入的轨迹分布相匹配。</li>
<li>模仿学习需要大量高质量的示范数据，但好的示范难以收集。</li>
<li>误差在强化学习中会累积，而不仅仅是像图像分类那样。</li>
<li>模仿学习的方法如DAgger需要额外的传感器，成本较高。</li>
<li>DAgger的目标是通过不断询问专家来改进模型。</li>
<li>模仿学习可以通过引入GAN来生成更接近专家的动作序列。</li>
<li>One-Shot Imitation Learning允许通过少量示范来学习新的行为。</li>
<li>Third-Person Imitation Learning可以让智能体通过观看视频学习。</li>
<li>模仿学习可以通过动作捕捉来获取高质量的专家数据。</li>
<li>模仿学习可以模仿多种行为，而不仅仅是行走。</li>
<li>Unsupervised Perceptual Rewards可以无监督地找出关键动作。</li>
<li>模仿学习可以从观察中学习，通过视频来模仿行为。</li>
<li>模仿学习的目标是使智能体模仿第三视角的示范。</li>
<li>模仿学习可以通过特征的MSE来保证特征的一致性。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-67">正文（抓取，非 AI）</h3>
<p>Imitation Learning: An Introduction 模仿学习在机器人学习(Robot Learning)中扮演了比较重要的角色。这其实在之前的paper reading中已经涉及过了: 刘浚嘉：Overcoming Exploration in RL with Demonstrations 模仿学习的思想很直观(intuitive)。我们在前面所介绍的Model-free, Model-based强化学习方法都是 从零开始(from scratch) 探索并学习一个使累计回报最大的策略(policy) 。 Imitation Learning的想法是，借助人类给出的示范(demonstration)，可以快速地达到这个目的。这个示范是多组trajectory轨迹数据 ， 每条轨迹包含 。类比一下人类的例子： 学车，你需要一个暴躁（可能）的教练； 健身，你需要一个动作标准且壮实的私教； 读研，你需要一个mentor（？）。 那么平时人(human)在得到以上的指导、示范(demonstration)之后会怎么做呢？ 记住并试着模仿着做 。 模型的训练目标就是使模型生成的状态-动作轨迹分布 和输入的轨迹分布 相匹配。这种方式叫做 行为克隆(Behavior Cloning) 。 这听起来不就是监督学习或者GAN嘛？还要啥Imitation Learning？名字一个比一个响。进度条告诉我们事情没那么简单。 Challenge Challenge出在两个地方： collection expect demonstrations：费时费力，还需要大量数据样本进行监督学习训练。此外，只能收集到好的示范，会导致事故的示范很难收集。 Error accumulates: 模仿不可能精确地复制示范地动作。 又由于强化学习是序列决策问题，这跟普通的mnist图像分类不同，误差是会累积的！ 以自动驾驶为例，会有下图的问题： 一个简易的解决方案 2016 | Paper | Nvidia | End to End Learning for Self-Driving Cars 引入简单的反馈控制。 实际上，强化学习的即时reward相当于一个反射弧长一点的反馈控制，这在长远决策上是有效的，但在实时的安全性考量上是有欠缺的。然而这里提到的反馈控制，更多地是 辅助传感器（extra sensors） 给出的。还是以自动驾驶为例： 这种方法还是有一定成本的，并且适用范围受限，另外看着也不高大上。 高大上的方法 DAgger (Dataset Aggregation) 2010 | Paper Stéphane Ross, Geoffrey J. Gordon, J. Andrew Bagnell 实际上也不高大上，真的都很朴素。Dataset Aggregation（数据集集成？）其实就是 不懂就问 ，不信你看pseudocode: 人为地打标记的成本也太高，如何自动地代替人完成这个工作呢？ 在不引入extra传感器的前提下，就是一个典型的POMDP问题，我们可以利用历史轨迹信息来解决： 将现在的observation与之前的一些历史obs连接(concat)在一起，经过CNN提取特征 用RNN记录历史信息 强化学习中的模仿学习 RL 的 reward func designing 一直备受诟病，而模仿学习似乎提供了解决方案。通过expert data构造这个reward func就是IL+RL的思路。 Model-Free Imitation Learning with Policy Optimization 2016 | Paper | OpenAI 这篇文章是在吴恩达提出的学徒学习Apprenticeship Learning的基础上进行神经网络化，从而使用Policy Gradient方法来更新网络，基本思想是利用当前策略的样本和专家样本估计出一个Reward函数，然后利用这个Reward进行DRL。然而很多实际场景中的动作好坏与否即使人也很难界定，所以这个方法在复杂场景难以work。 Generative Adversarial Imitation Learning (GAIL) 2016 | Paper | OpenAI 将GAN引入到Imitation Learning： Generator用于生成动作序列，可以使用一些model-free方法； Discriminator则用于区分这个动作序列是否属于expert动作，其输出就可以作为reward使用。 GAIL的目标是使生成的动作和专家动作越来越接近。虽然看起来很新颖，但是也不过是沿用了行为克隆的思想，只是没那么直接了而已。 One-Shot Imitation Learning 2017 | Paper | OpenAI | Blog Yan Duan, Marcin Andrychowicz, Bradly C. Stadie, Jonathan Ho, Jonas Schneider, Ilya Sutskever, Pieter Abbeel, Wojciech Zaremba One-Shot这个词不出所料地出自Abbeel组，之前的Meta-Learning：An Introduction系列涉及了一些。 依旧是meta的思想，训练集是demonstration数据+当前state，输出是action。 Third-Person Imitation Learning, 2017 | Paper | OpenAI | 这个工作的重点在于 使agent可以通过第三视角学习demonstration ，这就可以让它看视频学习了。 思路也很简单，在之前GAIL的基础上又加入了一个GAN，用于判断视角并使其能够在不同视角下提取出相同的feature。 Learning human behaviors from motion capture by adversarial imitation 2017 | Paper | Deepmind | Blog 通过motion capture(动作捕捉)获取expert数据，依然是GAIL的结构，只是Discriminator不需要输入action，只需要state即可 Robust Imitation of Diverse Behaviors 2017 | Paper | Deepmind 上一个工作的延续，不只局限于行走，而是希望模仿多种行为。 思路：仍然是在GAIL基础上，添加了一个VAE encoder从而更好的提取图像特征信息，大幅度提升了信息量，对于一个全新的动作，也能够直接模仿。且不同行为的模仿都只使用同一个Policy 网络。 Unsupervised Perceptual Rewards for Imitation Learning 2017 | Paper | Google Brain 通过使用pretrain的Inception模型来提取示范数据，再使用IRL计算reward。这样可以无监督地找出 倒水 这个过程中的关键动作。 Imitation from Observation：Learning to Imitate Behaviors from Raw Video via Context Translation 2017 | Paper | OpenAI Third Person Imitation Learning的延续，直接将第三人称的demo转成第一人称。方法也很简单： 构造一个带condition的Encoder-Decoder； MSELoss计算预测图像与真实图像的偏差； 为保证特征一直，加入了特征的MSE。 总结 这些非数学性的思想真的真的很naive。归根结底就是那句话： 圣人之道，在于百姓日用 。 Reference RL — Imitation Learning - Medium DAgger - Berkeley 最前沿：机器人学习Robot Learning之模仿学习Imitation Learning的发展 给大家推荐本书，心学圣人王阳明的致良知研究，让我们走近龙场悟道的开端，感悟圣人之道在于百姓日用的精髓。</p>
</div></details><h2 id="toc-68">35. 使用Python实现二阶锥规划（SOCP）问题的高效求解方法</h2>
<ul>
<li>链接：https://www.oryoy.com/news/shi-yong-python-shi-xian-er-jie-zhui-gui-hua-socp-wen-ti-de-gao-xiao-qiu-jie-fang-fa.html</li>
<li>来源：bing</li>
<li>摘要：2024年10月31日 · 使用Python实现二阶锥规划（SOCP）问题的高效求解方法 引言 在优化领域中，二阶锥规划（Second-Order Cone Programming, SOCP）是一种重要的数学优化问题，广 …</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">SOCP（Second-Order Cone Programming）是一种特殊的凸优化问题，其目标函数为线性，约束条件包括线性不等式和二阶锥不等式。二阶锥通常定义为{(t, x) | t ≥ |x|_2}，这种结构使得SOCP在处理金融领域的投资组合优化问题时尤为有效。CVXPY是一个用于凸优化的高级建模框架，可以方便地将实际问题转化为SOCP形式，而ECOS和SCS则是高效的SOCP求解器，选择合适的求解器可以显著提高求解效率。此外，预处理数据可以减少求解过程中的数值问题，利用问题结构可以进一步简化求解过程，提高效率。因此，在投资组合优化中，预期收益最大化是目标之一，而风险约束则是重要条件。通过不断实践和学习，可以更好地掌握SOCP问题的求解方法。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>SOCP是一种特殊的凸优化问题。</li>
<li>SOCP问题的目标函数是线性的。</li>
<li>SOCP问题的约束条件包括线性不等式和二阶锥不等式。</li>
<li>二阶锥通常定义为{(t, x) | t ≥ |x|_2}。</li>
<li>CVXPY是一个用于凸优化的高级建模框架。</li>
<li>ECOS和SCS是高效的SOCP求解器。</li>
<li>选择合适的求解器可以显著提高求解效率。</li>
<li>预处理数据可以减少求解过程中的数值问题。</li>
<li>利用问题结构可以简化求解过程，提高效率。</li>
<li>SOCP在金融领域的投资组合优化中有着广泛应用。</li>
<li>预期收益最大化是投资组合优化的目标之一。</li>
<li>风险约束是投资组合优化中的重要条件。</li>
<li>通过不断实践和学习，可以更好地掌握SOCP问题的求解方法。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-69">正文（抓取，非 AI）</h3>
<p>使用Python实现二阶锥规划（SOCP）问题的高效求解方法 - 云原生实践 使用Python实现二阶锥规划（SOCP）问题的高效求解方法 python 2024-10-31 49° 使用Python实现二阶锥规划（SOCP）问题的高效求解方法 引言 在优化领域中，二阶锥规划（Second-Order Cone Programming, SOCP）是一种重要的数学优化问题，广泛应用于金融、信号处理、机器学习等领域。SOCP问题因其独特的结构和广泛的适用性，受到了广泛关注。本文将详细介绍如何使用Python高效求解SOCP问题，并通过具体实例展示其应用。 什么是二阶锥规划？ 二阶锥规划是一种特殊的凸优化问题，其目标函数是线性的，约束条件包括线性不等式和二阶锥不等式。标准形式的SOCP问题可以表示为： [ \min_{x} \ c^T x ] subject to: [ Ax = b ] [ x \in K ] 其中，( K ) 是一个二阶锥，通常定义为： [ K = \left{ (t, x) \mid t \geq |x|<em x_="x," y="">2 \right} ] Python求解SOCP的工具 在Python中，求解SOCP问题有多种工具可供选择，其中最常用的是CVXPY和ECOS/SCS求解器。CVXPY是一个用于凸优化的高级建模框架，而ECOS和SCS是高效的SOCP求解器。 安装必要的库 首先，我们需要安装CVXPY及其依赖的求解器。可以使用pip进行安装： 示例：求解一个简单的SOCP问题 假设我们有一个简单的SOCP问题： [ \min</em> \ x + y ] subject to: [ x^2 + y^2 \leq 1 ] [ x \geq 0 ] 这个问题可以转化为标准SOCP形式： [ \min_{t, x, y} \ t ] subject to: [ t \geq x + y ] [ (1, x, y) \in K ] [ x \geq 0 ] 下面是使用CVXPY求解该问题的Python代码： 高效求解方法的优化技巧 选择合适的求解器 ：CVXPY支持多种求解器，如ECOS、SCS等。对于不同的问题，选择合适的求解器可以显著提高求解效率。 预处理数据 ：在求解前对数据进行预处理，如归一化、去噪等，可以减少求解过程中的数值问题。 利用问题结构 ：对于具有特殊结构的问题，可以利用其结构特性进行简化，从而提高求解效率。 应用实例：投资组合优化 SOCP在金融领域的投资组合优化中有着广泛应用。假设我们有一组资产，希望构建一个投资组合，使其预期收益最大化，同时满足风险约束。 结论 通过本文的介绍，我们了解了二阶锥规划的基本概念，并学会了如何使用Python中的CVXPY库高效求解SOCP问题。通过具体的实例，我们展示了SOCP在投资组合优化中的应用。希望本文能为读者在解决实际优化问题时提供有益的参考。 参考文献 Boyd, S., &amp; Vandenberghe, L. (2004). Convex Optimization. Cambridge University Press. CVXPY Documentation: https://www.cvxpy.org/ ECOS Solver Documentation: https://github.com/embotech/ecos 通过不断实践和学习，相信读者能够更好地掌握SOCP问题的求解方法，并将其应用于更广泛的领域。 用户名 评论内容 提交评论 重置 相关链接 Python编程中的HTTP劫持防御策略与实践 Python实现高效水质监测系统：从入门到进阶实战指南 Python数据处理入门：掌握高效编程技巧与数据分析方法 Python在保险行业中的应用：自动化理赔与风险评估系统开发实践 Python编程之美：用代码书写诗意人生 Python高效处理大数据：探索COLX库的强大功能与应用实例 Python CenterX：高效数据处理的编程技巧与实践应用 使用Geopandas进行Python地理空间数据分析的进阶技巧与实践案例 使用Python的.drop()方法高效处理数据框中的缺失值与冗余列 Python数据可视化实战：使用Pygal创建交互式图表与图形 Python在Android逆向工程中的应用：解析DEX文件技巧与实践 使用Python编写音乐播放器：轻松实现听歌功能 Python中的LoginManager实现用户认证与权限管理详解 基于Python的CRFSuite实现序列标注与自然语言处理高效实践 Python编程实现图片扫描与处理技术详解 利用Python高效访问和处理Wikidata知识库数据的方法与实践 Python实现人脸识别技术：从基础到进阶完整指南 使用Python实现高效市场波动率指数（VIX）计算与实时监控策略 Python在MesjGrid平台上的高效应用与实战案例解析 Python Lambda函数详解：简化代码与提升效率的最佳实践 使用wraps()装饰器优化Python函数封装与文档保留 Python函数参数解析与最佳实践：提升代码灵活性与可读性 使用Python获取硬件信息：轻松实现系统监控与数据分析 Python Scrappy框架：高效网络爬虫开发实战指南 Python实现全卷积网络（FCN）在图像分割中的应用与实践 高效使用Fabric库实现Python自动化部署与远程任务执行 Python中使用ZSet实现高效数据排序与去重的技巧解析 Python高效编程：掌握getx()函数的高级用法与性能优化技巧 Python中使用.query方法优化数据库查询性能的技巧与实践 高效编程利器：双屏环境下Python开发实践与技巧解析 Python高效编程技巧：掌握常用语句与最佳实践，提升代码质量与效率 Python卡特：掌握Python编程语言的关键技巧与实战案例 Python在SGML解析中的应用与最佳实践指南 使用Python进行文件路径拆分：深入理解os.path.split函数 Python中使用ASArray实现高效数组操作与数据处理技巧详解 Python高效实现addAll函数：探索列表元素累加的最佳实践 Python实战：打造个性化名片管理系统 Python列表解析与Lambda函数结合使用技巧详解 Python Bootstrapping Techniques for Efficient Project Initialization and Development Python中使用.sub()方法实现字符串高效替换与正则表达式应用技巧 Python中"="的用法详解：赋值操作与变量绑定的核心机制 使用Python FiPy库进行偏微分方程数值求解的最佳实践 Python数据处理与算法优化实战：从基础到进阶的数字运算指南 Python中如何使用pandas的dropna方法高效处理缺失数据 Python安全编程实践：防范常见漏洞与提升代码防护能力 Python编程实现高效减法运算与异常处理详解 使用Python和Marsyas库进行音频信号处理的进阶指南 Python群聊：高效编程交流与实战经验分享平台 Python实现WebSocket通信：从入门到实战应用指南 Python中isspace函数详解：如何高效检测空白字符 云原生实践 首页 搜索 搜索 最新文档 揭秘Python中K-L变换高效降维的奥秘：轻松实现复杂数据的维度缩减与可视化 发表于 2026-02-28 从零开始，轻松掌握Python：必备技能与学习路径揭秘 发表于 2026-02-28 轻松实现敏感词过滤：Python代码实战指南 发表于 2026-02-28 Python函数轻松实现温度单位转换，告别繁琐计算，掌握实用技巧！ 发表于 2026-02-28 零基础入门，Python编程视频教程免费学！ 发表于 2026-02-28 轻松掌握：Python编写货币汇率转换器的实用教程 发表于 2026-02-28 揭秘Python轻松转换数字到星期，快速实现日期智能转换技巧 发表于 2026-02-28 轻松掌握Python厘米与英寸转换技巧，一文搞懂单位换算，告别繁琐计算！ 发表于 2026-02-28</p>
</div></details><h2 id="toc-70">36. Utilizing LLMs for Industrial Process Automation</h2>
<ul>
<li>链接：https://arxiv.org/abs/2602.23331v1</li>
<li>来源：arxiv</li>
<li>摘要：A growing number of publications address the best practices to use Large Language Models (LLMs) for software engineering in recent years. However, most of this work focuses on widely-used general purpose programming languages like Python due to their widespread usage training data. The utility of LLMs for software within the industrial process automation domain, with highly-specialized languages that are typically only used in proprietary contexts, remains underexplored. This research aims to utilize and integrate LLMs in the industrial development process, solving real-life programming tasks (e.g., generating a movement routine for a robotic arm) and accelerating the development cycles of manufacturing systems.</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">LLMs（大型语言模型）的研究主要集中在广泛使用的通用编程语言上，因为这些语言的训练数据丰富，而工业过程自动化领域中使用高度专业化的语言较少被探索。这些高度专业化的语言通常仅在专有上下文中使用，导致在工业软件开发中的应用研究仍处于起步阶段。尽管如此，LLMs在工业环境中解决实际编程任务具有潜在价值，能够加速制造系统的开发周期。因此，进一步研究LLMs在工业过程自动化中的应用显得尤为重要，这将有助于推动工业软件开发的进步。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>LLMs的研究多集中在广泛使用的通用编程语言。</li>
<li>工业过程自动化领域使用高度专业化的语言较少被探索。</li>
<li>LLMs在工业软件开发中的应用尚未得到充分研究。</li>
<li>工业环境中解决实际编程任务可加速制造系统的开发周期。</li>
<li>LLMs在工业过程自动化中的应用具有潜在价值。</li>
<li>高度专业化的语言通常仅在专有上下文中使用。</li>
<li>大多数研究集中在广泛使用的编程语言，因为它们的训练数据丰富。</li>
<li>LLMs在工业软件开发中的应用研究仍处于起步阶段。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-71">正文（抓取，非 AI）</h3>
<p>[2602.23331v1] Utilizing LLMs for Industrial Process Automation Computer Science &gt; Software Engineering arXiv:2602.23331v1 (cs) [Submitted on 26 Feb 2026] Title: Utilizing LLMs for Industrial Process Automation Authors: Salim Fares View a PDF of the paper titled Utilizing LLMs for Industrial Process Automation, by Salim Fares View PDF HTML (experimental) Abstract: A growing number of publications address the best practices to use Large Language Models (LLMs) for software engineering in recent years. However, most of this work focuses on widely-used general purpose programming languages like Python due to their widespread usage training data. The utility of LLMs for software within the industrial process automation domain, with highly-specialized languages that are typically only used in proprietary contexts, remains underexplored. This research aims to utilize and integrate LLMs in the industrial development process, solving real-life programming tasks (e.g., generating a movement routine for a robotic arm) and accelerating the development cycles of manufacturing systems. Subjects: Software Engineering (cs.SE) ; Artificial Intelligence (cs.AI) Cite as: arXiv:2602.23331 [cs.SE] (or arXiv:2602.23331v1 [cs.SE] for this version) https://doi.org/10.48550/arXiv.2602.23331 Focus to learn more arXiv-issued DOI via DataCite (pending registration) Submission history From: Salim Fares [ view email ] [v1] Thu, 26 Feb 2026 18:38:00 UTC (55 KB) Full-text links: Access Paper: View a PDF of the paper titled Utilizing LLMs for Industrial Process Automation, by Salim Fares View PDF HTML (experimental) TeX Source view license Current browse context: cs.SE &lt; prev | next &gt; new | recent | 2026-02 Change to browse by: cs cs.AI References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-72">37. 什么是 LLM？— 大型语言模型简介 — AWS</h2>
<ul>
<li>链接：https://aws.amazon.com/cn/what-is/large-language-model/</li>
<li>来源：bing</li>
<li>摘要：2026年2月12日 · 了解什么是大型语言模型以及为什么 LLM 必不可少。了解企业人工智能的好处，以及如何用其创建新的内容和想法，包括文本、对话、图像、视频和音频。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-73">正文（抓取，非 AI）</h3>
<p>什么是 LLM？— 大型语言模型简介 — AWS 跳至主要内容 想了解专为中国区域提供的云产品？请访问 www.amazonaws.cn 。申请中国区域免费套餐请访问 www.amazonaws.cn/free 。 什么是云计算？ › 云计算概念中心 › 生成式人工智能 什么是 LLM（大型语言模型）？ 创建 AWS 账户 什么是大型语言模型？ 为什么大型语言模型如此重要？ 大型语言模型如何运作？ 大型语言模型有哪些应用？ 如何训练大型语言模型？ LLM 的未来前景是什么？ AWS 如何通过 LLM 提供帮助？ 什么是大型语言模型？ 大型语言模型，也称为 LLM，是基于大量数据进行预训练的超大型 深度学习 模型。底层转换器是一组 神经网络 ，这些神经网络由具有自注意力功能的编码器和解码器组成。编码器和解码器从一系列文本中提取含义，并理解其中的单词和短语之间的关系。 转换器 LLM 能够进行无监督的训练，但更精确的解释是转换器可以执行自主学习。通过此过程，转换器可学会理解基本的语法、语言和知识。 与早期按顺序处理输入的循环神经网络（RNN）不同，转换器并行处理整个序列。这可让数据科学家使用 GPU 训练基于转换器的 LLM，从而大幅度缩短训练时间。 借助转换器神经网络架构，您可使用非常大规模的模型，其中通常具有数千亿个参数。如此大规模的模型可以摄取大量数据，这些数据通常来自互联网，但也包括来自Comm on Crawl （包含超过500亿个网页）和维基百科（约有5700万个页面）等来源。 阅读有关神经网络的更多信息 » 阅读有关深度学习的更多信息 » 为什么大型语言模型如此重要？ 大型语言模型非常灵活。一个模型可以执行完全不同的任务，例如回答问题、总结文档、翻译语言和完成语句。LLM 有可能破坏内容创作以及人们使用搜索引擎和虚拟助手的方式。 尽管并不完美，但 LLM 表现出根据相对较少量的提示或输入做出预测的非凡能力。LLM 可用于 生成式 AI （人工智能），根据人类语言的输入提示生成内容。 LLM 非常庞大。它们可以考虑数十亿个参数，并且有许多可能的用途。下面是一些示例： Open AI 的 GPT-3 模型有 1750 亿个参数。类似的产品 ChatGPT 可以从数据中识别模式并生成自然且可读的输出。虽然我们不知道 Claude 2 的规模，但该模型可以在每个提示中输入多达 10 万个令牌，这意味着它可以处理数百页的技术文档，甚至可以处理整本书。 AI21 Labs 的 Jurassic-1 模型具有 1780 亿个参数和由 25 万单词部分组成的令牌词汇表以及类似的对话功能。 Cohere 的 Command 模型具有类似的功能，并且可以使用 100 多种不同的语言开展工作。 LightOn 的 Paradigm 提供根基模型，并且宣称该模型的功能超过 GPT-3。所有这些 LLM 都带有 API，可让开发人员打造独特的生成式人工智能应用程序。 阅读有关生成式 AI 的 更多信息” 阅读有关根基模型的更多信息 » 大型语言模型如何运作？ LLM 运作原理的一个关键因素是它们表示单词的方式。早期的 机器学习 使用数字表来表示每个单词。但是，这种表示形式无法识别单词之间的关系，例如具有相似含义的单词。人们采用如下方式克服此限制：使用多维向量（通常称为单词嵌入）来表示单词，从而使具有相似上下文含义或其他关系的单词在向量空间中彼此接近。 使用单词嵌入，转换器可以通过编码器将文本预处理为数字表示，并理解含义相似的单词和短语的上下文以及单词之间的其他关系，例如语音部分。然后，LLM 就可以通过解码器应用这些语言知识来生成独特的输出。 大型语言模型有哪些应用？ LLM 有很多实际应用。 文案写作 除了 GPT-3 和 ChatGPT 之外，Claude、Llama 2、Cohere Command 和 Jurassic 也可编写原件。AI21 Wordspice 建议修改原始语句以改善风格和语音。 知识库回答 该技术通常称为知识密集型自然语言处理（KI-NLP），是指可以根据数字存档中的信息帮助回答特定问题的 LLM。AI21 Studio playground 能够回答常识性问题就是此类示例。 文本分类 使用集群，LLM 可以对含义或情绪相似的文本进行分类。用途包括衡量客户情绪、确定文本之间的关系和文档搜索。 代码生成 LLM 擅长根据自然语言提示生成代码。例子包括 亚马逊CodeWhispererer 和GitHub Copilot中使用的Open AI的代码，它们可以用Python、JavaScript、Ruby和其他几种编程语言进行编码。其他编码应用包括创建 SQL 查询、编写 Shell 命令和进行网站设计。 了解有关 AI 代码生成的 更多信息。 文本生成 与代码生成类似，文本生成可以完成不完整的语句，编写产品文档，或者像 Alexa Create 一样创作简短的儿童故事。 如何训练大型语言模型？ 基于转换器的神经网络非常庞大。这些网络包含多个节点和层。层中的每个节点都有指向后续层中所有节点的连接，并且每个节点都有权重和偏差。权重和偏差以及嵌入称为模型参数。基于转换器的大型神经网络可以有数十亿个参数。模型的大小通常由模型大小、参数数量和训练数据规模之间的经验关系决定。 使用大量高质量数据执行训练。在训练过程中，模型会迭代调整参数值，直到模型可根据前一个输入令牌序列正确预测下一个令牌。为此，模型使用自学技术，这些技术教导模型调整参数，以最大限度地提高训练示例中正确预测下一个令牌的可能性。 经过训练，LLM 可以很容易地适应使用相对较小的有监督数据集执行多项任务，这一过程称为微调。 存在三种常见的学习模型： 零样本学习；Base LLM 无需明确训练即可响应各种请求，通常是通过提示，但是答案的准确性各不相同。 少量样本学习：通过提供一些相关的训练示例，基础模型在该特定领域的表现显著提升。 微调：这是少量样本学习的扩展，其中数据科学家训练基础模型，使模型使用与特定应用相关的其他数据来调整其参数。 LLM 的未来前景是什么？ 随着 ChatGPT、Claude 2 和 Llama 2 等可以回答问题和生成文本的大型语言模型的引入，我们可以预见令人兴奋的未来前景。可以肯定的是，LLM 会越来越接近人性化的表现，尽管这一过程会较为漫长。这些 LLM 即时取得的成功表明人们对机器人类型 LLM 的浓厚兴趣，这些 LLM 可模仿人类大脑的思维，在某些情况下表现甚至优于人类大脑。以下是一些关于 LLM 未来前景的想法： 增强的功能 尽管 LLM 给人们留下了深刻的印象，但当前的技术水平并不完善，LLM 也并非绝对可靠。然而，随着开发人员学习如何在减少偏见和消除错误答案的同时提高性能，较新的 LLM 版本将提高准确性和增强功能。 视听训练 开发人员使用文本训练大多数 LLM，但有些人已经开始使用视频和音频输入来训练模型。这种形式的训练应该可以加快模型开发速度，并为将 LLM 用于自动驾驶汽车开辟新的可能性。 工作场所转型 LLM 是颠覆性的因素，它将转变工作场所。LLM 可能会采用机器人处理重复性制造任务的相同方式来减少单调和重复的任务。可能减少的任务包括重复的文书任务、客户服务 聊天机器人 和简单的自动文案写作。 对话式 AI LLM 无疑将提高 Alexa、Google Assistant 和 Siri 等自动虚拟助手的性能。这些虚拟助手将能够更妥善地解释用户意图并响应复杂的命令。 阅读有关对话式人工智能的更多信息 AWS 如何通过 LLM 提供帮助？ AWS 为大型语言模型开发人员提供了多种可能性。 Amazon Bedrock 是使用 LLM 构建和扩展 生成式 AI 应用程序的最简单方法。Amazon Bedrock 是一项完全托管的服务，可通过 API 提供来自 Amazon 和领先 AI 初创企业的 LLM，因此您可以从各种 LLM 中进行选择，找到最适合您的应用场景的模型。 Amazon SageMaker JumpStart 是一个机器学习中心，提供基础模型、内置算法和预建的机器学习解决方案，您只需点击几下即可进行部署。使用 SageMaker JumpStart，您可以访问包括基础模型在内的预训练模型，以执行文章摘要和图像生成等任务。您可以使用自己的数据，基于您的使用案例，对预训练模型进行完全定制，还可以使用用户界面或软件开发工具包轻松将它们部署到生产环境中。 立即 创建免费账户，开始在 AWS 上使用 LLM 和 A I。 AWS 上的后续步骤 查看其他与产品相关的资源 通过 AWS 生成式人工智能服务加速创新 注册免费账户 立即享受 AWS Free Tier。 注册 开始在控制台中构建 在 AWS 管理控制台中开始构建。 登录</p>
</div></details><h2 id="toc-74">38. Generalized Rapid Action Value Estimation in Memory-Constrained Environments</h2>
<ul>
<li>链接：https://arxiv.org/abs/2602.23318v1</li>
<li>来源：arxiv</li>
<li>摘要：Generalized Rapid Action Value Estimation (GRAVE) has been shown to be a strong variant within the Monte-Carlo Tree Search (MCTS) family of algorithms for General Game Playing (GGP). However, its reliance on storing additional win/visit statistics at each node makes its use impractical in memory-constrained environments, thereby limiting its applicability in practice. In this paper, we introduce the GRAVE2, GRAVER and GRAVER2 algorithms, which extend GRAVE through two-level search, node recycling, and a combination of both techniques, respectively. We show that these enhancements enable a drastic reduction in the number of stored nodes while matching the playing strength of GRAVE.</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">GRAVE依赖于在每个节点存储额外的胜/访问统计信息，这种依赖性使得它在内存受限的环境中不实用。为了解决这一问题，GRAVE2、GRAVER和GRAVER2通过多级搜索、节点回收以及两者结合的技术对GRAVE进行了扩展。这些增强措施大幅减少了需要存储的节点数量，从而提高了算法的适用性。其中，节点回收技术有助于减少存储的节点数量，而多级搜索技术则有助于提高算法的性能。这些技术的结合不仅能够匹配GRAVE的性能，还能在内存受限的环境中提供更好的表现。因此，通过这些改进，算法能够在保持高效的同时，更好地适应不同的应用场景。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>GRAVE依赖于在每个节点存储额外的胜/访问统计信息。</li>
<li>这种依赖性使得GRAVE在内存受限环境中不实用。</li>
<li>GRAVE2、GRAVER和GRAVER2通过多级搜索、节点回收和两者结合的技术扩展了GRAVE。</li>
<li>这些增强使存储的节点数量大幅减少。</li>
<li>多级搜索、节点回收和两者结合的技术可以提高算法的适用性。</li>
<li>节点回收技术有助于减少存储的节点数量。</li>
<li>多级搜索技术有助于提高算法的性能。</li>
<li>这些技术的结合可以匹配GRAVE的性能。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-75">正文（抓取，非 AI）</h3>
<p>[2602.23318v1] Generalized Rapid Action Value Estimation in Memory-Constrained Environments Computer Science &gt; Artificial Intelligence arXiv:2602.23318v1 (cs) [Submitted on 26 Feb 2026] Title: Generalized Rapid Action Value Estimation in Memory-Constrained Environments Authors: Aloïs Rautureau , Tristan Cazenave , Éric Piette View a PDF of the paper titled Generalized Rapid Action Value Estimation in Memory-Constrained Environments, by Alo\"is Rautureau and 2 other authors View PDF HTML (experimental) Abstract: Generalized Rapid Action Value Estimation (GRAVE) has been shown to be a strong variant within the Monte-Carlo Tree Search (MCTS) family of algorithms for General Game Playing (GGP). However, its reliance on storing additional win/visit statistics at each node makes its use impractical in memory-constrained environments, thereby limiting its applicability in practice. In this paper, we introduce the GRAVE2, GRAVER and GRAVER2 algorithms, which extend GRAVE through two-level search, node recycling, and a combination of both techniques, respectively. We show that these enhancements enable a drastic reduction in the number of stored nodes while matching the playing strength of GRAVE. Subjects: Artificial Intelligence (cs.AI) Cite as: arXiv:2602.23318 [cs.AI] (or arXiv:2602.23318v1 [cs.AI] for this version) https://doi.org/10.48550/arXiv.2602.23318 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Aloïs Rautureau [ view email ] [v1] Thu, 26 Feb 2026 18:25:59 UTC (338 KB) Full-text links: Access Paper: View a PDF of the paper titled Generalized Rapid Action Value Estimation in Memory-Constrained Environments, by Alo\"is Rautureau and 2 other authors View PDF HTML (experimental) TeX Source view license Current browse context: cs.AI &lt; prev | next &gt; new | recent | 2026-02 Change to browse by: cs References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-76">39. Just-In-Time Piecewise-Linear Semantics for ReLU-type Networks</h2>
<ul>
<li>链接：https://arxiv.org/abs/2510.17622v1</li>
<li>来源：arxiv</li>
<li>摘要：We present a JIT PL semantics for ReLU-type networks that compiles models into a guarded CPWL transducer with shared guards. The system adds hyperplanes only when operands are affine on the current cell, maintains global lower/upper envelopes, and uses a budgeted branch-and-bound. We obtain anytime soundness, exactness on fully refined cells, monotone progress, guard-linear complexity (avoiding global $\binom{k}{2}$), dominance pruning, and decidability under finite refinement. The shared carrier supports region extraction, decision complexes, Jacobians, exact/certified Lipschitz, LP/SOCP robustness, and maximal causal influence. A minimal prototype returns certificates or counterexamples with cost proportional to visited subdomains.</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">JIT PL语义仅在当前单元格内操作数为仿射时才添加超平面，系统维护全局的下界和上界包络，预算分支定界技术用于管理这一过程。声性在任何时间点都得到保证，而不仅仅是最终结果，确保在完全细化的单元格上达到精确性。进展是单调的，提供清晰的方向，避免了全局复杂性的需求，通过守卫线性复杂度和主导剪枝，进一步减少了不必要的计算。在有限细化下，决策性得到保证，共享载体支持各种高级操作，成本与访问子域成正比。区域提取和决策复杂性得到支持，雅可比矩阵可以精确计算或认证，Lipschitz常数通过精确或认证结果确定。系统对线性规划/半二次规划具有鲁棒性，能够评估最大因果影响，并以最小成本返回反例或证书。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>JIT PL semantics only adds hyperplanes when operands are affine on the current cell.</li>
<li>Global lower/upper envelopes are maintained by the system.</li>
<li>Budgeted branch-and-bound is used to manage the process.</li>
<li>Soundness is achieved anytime, not just at the end.</li>
<li>Exactness is guaranteed on fully refined cells.</li>
<li>Progress is monotone, ensuring a clear direction.</li>
<li>Guard-linear complexity avoids the need for global $\binom{k}{2}$.</li>
<li>Dominance pruning helps in reducing unnecessary computations.</li>
<li>Decidability is ensured under finite refinement.</li>
<li>Shared carrier supports various advanced operations.</li>
<li>Cost is proportional to visited subdomains in the prototype.</li>
<li>Region extraction and decision complexes are supported.</li>
<li>Jacobians can be computed exactly or certified.</li>
<li>Lipschitz constants are determined with exact or certified results.</li>
<li>Robustness to LP/SOCP can be assessed.</li>
<li>Maximal causal influence can be analyzed.</li>
<li>Counterexamples or certificates are returned with minimal cost.</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-77">正文（抓取，非 AI）</h3>
<p>[2510.17622v1] Just-In-Time Piecewise-Linear Semantics for ReLU-type Networks Computer Science &gt; Logic in Computer Science arXiv:2510.17622v1 (cs) [Submitted on 20 Oct 2025] Title: Just-In-Time Piecewise-Linear Semantics for ReLU-type Networks Authors: Hongyi Duan , Haoyang Liu , Jian'an Zhang , Fengrui Liu , Yiyi Wang View a PDF of the paper titled Just-In-Time Piecewise-Linear Semantics for ReLU-type Networks, by Hongyi Duan and 4 other authors View PDF HTML (experimental) Abstract: We present a JIT PL semantics for ReLU-type networks that compiles models into a guarded CPWL transducer with shared guards. The system adds hyperplanes only when operands are affine on the current cell, maintains global lower/upper envelopes, and uses a budgeted branch-and-bound. We obtain anytime soundness, exactness on fully refined cells, monotone progress, guard-linear complexity (avoiding global $\binom{k}{2}$), dominance pruning, and decidability under finite refinement. The shared carrier supports region extraction, decision complexes, Jacobians, exact/certified Lipschitz, LP/SOCP robustness, and maximal causal influence. A minimal prototype returns certificates or counterexamples with cost proportional to visited subdomains. Subjects: Logic in Computer Science (cs.LO) ; Machine Learning (cs.LG) Cite as: arXiv:2510.17622 [cs.LO] (or arXiv:2510.17622v1 [cs.LO] for this version) https://doi.org/10.48550/arXiv.2510.17622 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Hongyi Duan [ view email ] [v1] Mon, 20 Oct 2025 15:05:14 UTC (4,397 KB) Full-text links: Access Paper: View a PDF of the paper titled Just-In-Time Piecewise-Linear Semantics for ReLU-type Networks, by Hongyi Duan and 4 other authors View PDF HTML (experimental) TeX Source view license Current browse context: cs.LO &lt; prev | next &gt; new | recent | 2025-10 Change to browse by: cs cs.LG References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-78">40. Memory-induced active particle ratchets: Mean currents and large deviations</h2>
<ul>
<li>链接：https://arxiv.org/abs/2602.23327v1</li>
<li>来源：arxiv</li>
<li>摘要：We analyse a continuous-time random walk model with stochastic reversals of direction. There is no external potential but the reorientation mechanism generates a non-zero current from asymmetry in the forward and backward waiting-time distributions (even when they have the same mean); the system can therefore can be considered as a type of active particle ratchet. We derive an explicit expression for the mean ratchet current with exponentially distributed reorientation times and also develop a general renewal-theory framework to obtain the full large deviations, using this to comment on the possibility of dynamical phase transitions.</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">mean currents在非对称等待时间分布的影响下产生，即使在没有外部电势的情况下，系统也能表现出非零电流。reorientation times的指数分布简化了这一分析，使得应用renewal theory成为可能，该理论为理解大偏差现象提供了框架。基于系统的这种行为，动态相变可能是可能的，这使得该模型可以被视为一种活性粒子滑轮。stochastic方向反转是该模型的关键特征，这些特性共同决定了mean currents的产生和维持。因此，通过这些机制，系统能够展现出独特的动力学行为，包括可能的相变和非平衡态的维持。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>mean currents arise from the asymmetry in waiting-time distributions.</li>
<li>the system exhibits non-zero current even without external potential.</li>
<li>reorientation times' exponential distribution simplifies the analysis.</li>
<li>renewal theory provides a framework for understanding large deviations.</li>
<li>dynamical phase transitions are possible based on the system's behavior.</li>
<li>the model can be considered a type of active particle ratchet.</li>
<li>stochastic reversals of direction are a key feature of the model.</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-79">正文（抓取，非 AI）</h3>
<p>[2602.23327v1] Memory-induced active particle ratchets: Mean currents and large deviations Condensed Matter &gt; Statistical Mechanics arXiv:2602.23327v1 (cond-mat) [Submitted on 26 Feb 2026] Title: Memory-induced active particle ratchets: Mean currents and large deviations Authors: Venkata D. Pamulaparthy , Rosemary J. Harris View a PDF of the paper titled Memory-induced active particle ratchets: Mean currents and large deviations, by Venkata D. Pamulaparthy and Rosemary J. Harris View PDF Abstract: We analyse a continuous-time random walk model with stochastic reversals of direction. There is no external potential but the reorientation mechanism generates a non-zero current from asymmetry in the forward and backward waiting-time distributions (even when they have the same mean); the system can therefore can be considered as a type of active particle ratchet. We derive an explicit expression for the mean ratchet current with exponentially distributed reorientation times and also develop a general renewal-theory framework to obtain the full large deviations, using this to comment on the possibility of dynamical phase transitions. Comments: 15 pages, 10 figures Subjects: Statistical Mechanics (cond-mat.stat-mech) Cite as: arXiv:2602.23327 [cond-mat.stat-mech] (or arXiv:2602.23327v1 [cond-mat.stat-mech] for this version) https://doi.org/10.48550/arXiv.2602.23327 Focus to learn more arXiv-issued DOI via DataCite (pending registration) Submission history From: Venkata Dhruva Pamulaparthy [ view email ] [v1] Thu, 26 Feb 2026 18:36:13 UTC (580 KB) Full-text links: Access Paper: View a PDF of the paper titled Memory-induced active particle ratchets: Mean currents and large deviations, by Venkata D. Pamulaparthy and Rosemary J. Harris View PDF TeX Source view license Current browse context: cond-mat.stat-mech &lt; prev | next &gt; new | recent | 2026-02 Change to browse by: cond-mat References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) IArxiv recommender toggle IArxiv Recommender ( What is IArxiv? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-80">41. Spin Glass Concepts in Computer Science, Statistics, and Learning</h2>
<ul>
<li>链接：https://arxiv.org/abs/2602.23326v1</li>
<li>来源：arxiv</li>
<li>摘要：Spin glass theory studies the structure of sublevel sets and minima (or near-minima) of certain classes of random functions in high dimension. Near-minima of random functions also play an important role in high-dimensional statistics and statistical learning, where minimizing the empirical risk (which is a random function of the model parameters) is the method of choice for learning a statistical model from noisy data. Finally, near-minima of random functions are obviously central to average-case analysis of optimization algorithms. Computer science, statistics, and machine learning naturally lead to questions that are traditionally not addressed within physics and mathematical physics. I will try to explain how ideas from spin glass theory have seeded recent developments in these fields.   (This article was written on the occasion of the 2024 Abel Prize to Michel Talagrand.)</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">sublevel sets和随机函数的最小值在高维空间中的研究是重要的，这在高维统计中尤为重要。近似最小值的概念对于优化算法的分析至关重要，而最小化经验风险则是一种学习统计模型的方法。计算机科学、统计学和机器学习领域提出了许多新的问题，而这些领域中的许多问题并未被物理学和数学物理学传统地关注。自旋玻璃理论中的概念对算法的平均情况分析具有重要意义，因此这些理论为理解优化算法的行为提供了新的视角。此外，自旋玻璃理论的思想已经影响了最近的发展，使得优化算法的分析更加深入。因此，这些领域的交叉研究不仅推动了理论的发展，也为实际问题的解决提供了新的思路。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>sublevel sets and minima of random functions are studied in high dimensions.</li>
<li>near-minima of random functions are important in high-dimensional statistics.</li>
<li>minimizing empirical risk is a method for learning statistical models.</li>
<li>near-minima are central to the analysis of optimization algorithms.</li>
<li>computer science, statistics, and machine learning lead to new questions.</li>
<li>ideas from spin glass theory have influenced recent developments.</li>
<li>physics and mathematical physics traditionally do not address these questions.</li>
<li>spin glass concepts are relevant to the average-case analysis of algorithms.</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-81">正文（抓取，非 AI）</h3>
<p>[2602.23326v1] Spin Glass Concepts in Computer Science, Statistics, and Learning Mathematics &gt; Probability arXiv:2602.23326v1 (math) [Submitted on 26 Feb 2026] Title: Spin Glass Concepts in Computer Science, Statistics, and Learning Authors: Andrea Montanari View a PDF of the paper titled Spin Glass Concepts in Computer Science, Statistics, and Learning, by Andrea Montanari View PDF HTML (experimental) Abstract: Spin glass theory studies the structure of sublevel sets and minima (or near-minima) of certain classes of random functions in high dimension. Near-minima of random functions also play an important role in high-dimensional statistics and statistical learning, where minimizing the empirical risk (which is a random function of the model parameters) is the method of choice for learning a statistical model from noisy data. Finally, near-minima of random functions are obviously central to average-case analysis of optimization algorithms. Computer science, statistics, and machine learning naturally lead to questions that are traditionally not addressed within physics and mathematical physics. I will try to explain how ideas from spin glass theory have seeded recent developments in these fields. (This article was written on the occasion of the 2024 Abel Prize to Michel Talagrand.) Comments: 33 pages; 2 pdf figures Subjects: Probability (math.PR) ; Disordered Systems and Neural Networks (cond-mat.dis-nn) Cite as: arXiv:2602.23326 [math.PR] (or arXiv:2602.23326v1 [math.PR] for this version) https://doi.org/10.48550/arXiv.2602.23326 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Andrea Montanari [ view email ] [v1] Thu, 26 Feb 2026 18:35:47 UTC (55 KB) Full-text links: Access Paper: View a PDF of the paper titled Spin Glass Concepts in Computer Science, Statistics, and Learning, by Andrea Montanari View PDF HTML (experimental) TeX Source view license Current browse context: math.PR &lt; prev | next &gt; new | recent | 2026-02 Change to browse by: cond-mat cond-mat.dis-nn math References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-82">42. First-order SDSOS-convex semi-algebraic optimization and exact SOCP relaxations</h2>
<ul>
<li>链接：https://arxiv.org/abs/2509.07418v1</li>
<li>来源：arxiv</li>
<li>摘要：In this paper, we define a new type of nonsmooth convex function, called {\em first-order SDSOS-convex semi-algebraic function}, which is an extension of the previously proposed first-order SDSOS-convex polynomials (Chuong et al. in J Global Optim 75:885--919, 2019). This class of nonsmooth convex functions contains many well-known functions, such as the Euclidean norm, the $\ell_1$-norm commonly used in compressed sensing and sparse optimization, and the least squares function frequently employed in machine learning and regression analysis. We show that, under suitable assumptions, the optimal value and optimal solutions of first-order SDSOS-convex semi-algebraic programs can be found by exactly solving an associated second-order cone programming problem. Finally, an application to robust optimization with first-order SDSOS-convex polynomials is discussed.</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">首先，first-order SDSOS-convex semi-algebraic函数可能包含非光滑凸函数，这扩展了先前的SDSOS-convex多项式。其次，first-order SDSOS-convex polynomials是其前身，即更早的SDSOS-convex多项式。此外，Euclidean norm和$\ell_1$-norm以及least squares函数都是此类函数的具体实例。因此，通过精确求解关联的SOCP问题，可以找到此类函数的最优值和解。由于这些函数的定义扩展了先前的SDSOS-convex多项式，这种方法特别适用于鲁棒优化。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>first-order SDSOS-convex semi-algebraic functions可能包含非光滑凸函数。</li>
<li>first-order SDSOS-convex polynomials是其前身。</li>
<li>Euclidean norm和$\ell_1$-norm是此类函数的例子。</li>
<li>least squares函数也是此类函数的例子。</li>
<li>optimal值和解可以通过精确求解关联的SOCP问题找到。</li>
<li>此优化方法适用于鲁棒优化。</li>
<li>此类函数的定义扩展了先前的SDSOS-convex多项式。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-83">正文（抓取，非 AI）</h3>
<p>[2509.07418v1] First-order SDSOS-convex semi-algebraic optimization and exact SOCP relaxations Mathematics &gt; Optimization and Control arXiv:2509.07418v1 (math) [Submitted on 9 Sep 2025] Title: First-order SDSOS-convex semi-algebraic optimization and exact SOCP relaxations Authors: Chengmiao Yang , Liguo Jiao , Jae Hyoung Lee View a PDF of the paper titled First-order SDSOS-convex semi-algebraic optimization and exact SOCP relaxations, by Chengmiao Yang and Liguo Jiao and Jae Hyoung Lee View PDF HTML (experimental) Abstract: In this paper, we define a new type of nonsmooth convex function, called {\em first-order SDSOS-convex semi-algebraic function}, which is an extension of the previously proposed first-order SDSOS-convex polynomials (Chuong et al. in J Global Optim 75:885--919, 2019). This class of nonsmooth convex functions contains many well-known functions, such as the Euclidean norm, the $\ell_1$-norm commonly used in compressed sensing and sparse optimization, and the least squares function frequently employed in machine learning and regression analysis. We show that, under suitable assumptions, the optimal value and optimal solutions of first-order SDSOS-convex semi-algebraic programs can be found by exactly solving an associated second-order cone programming problem. Finally, an application to robust optimization with first-order SDSOS-convex polynomials is discussed. Comments: 24pages Subjects: Optimization and Control (math.OC) Cite as: arXiv:2509.07418 [math.OC] (or arXiv:2509.07418v1 [math.OC] for this version) https://doi.org/10.48550/arXiv.2509.07418 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Chengmiao Yang [ view email ] [v1] Tue, 9 Sep 2025 06:06:48 UTC (21 KB) Full-text links: Access Paper: View a PDF of the paper titled First-order SDSOS-convex semi-algebraic optimization and exact SOCP relaxations, by Chengmiao Yang and Liguo Jiao and Jae Hyoung Lee View PDF HTML (experimental) TeX Source view license Current browse context: math.OC &lt; prev | next &gt; new | recent | 2025-09 Change to browse by: math References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-84">43. Robust model selection using likelihood as data</h2>
<ul>
<li>链接：https://arxiv.org/abs/2602.23355v1</li>
<li>来源：arxiv</li>
<li>摘要：Model selection is a central task in statistics, but standard methods are not robust in misspecified settings where the true data-generating process (DGP) is not in the set of candidate models. The key limitation is that existing methods -- including information criteria and Bayesian posteriors -- do not quantify uncertainty about how well each candidate model approximates the true DGP. In this paper, we introduce a novel approach to model selection based on modeling the likelihood values themselves. Specifically, given $K$ candidate models and $n$ observations, we view the $n\times K$ matrix of negative log-likelihood values as a random data matrix and observe that the expectation of each row is equal to the vector of Kullback--Leibler divergences between the $K$ models and the true DGP, up to an additive constant. We use a multivariate normal model to estimate and quantify uncertainty in this expectation, providing calibrated inferences for robust model selection under misspecification. The procedure is easy to compute, interpretable, and comes with theoretical guarantees, including consistency.</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">标准方法在模型错配的情况下无法量化不确定性，这使得传统的不确定性评估变得困难。在这种情况下，负对数似然值被视作一个随机数据矩阵，其中每个行的期望值等于库尔贝-莱布尼兹散度，只是在常数项上有所不同。为了估计这种不确定性，研究者采用多元正态模型来进行计算。这一过程不仅能够提供稳健的模型选择的校准推断，还使得计算简便且易于解释。此外，该方法还具备理论上的保证，包括一致性，从而增强了其在实际应用中的可靠性。因此，这种方法不仅在理论上得到了支持，而且在实践中也表现出色，为模型选择提供了可靠的基础。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>standard methods fail to quantify uncertainty in misspecified settings.</li>
<li>negative log-likelihood values are treated as a random data matrix.</li>
<li>the expectation of each row equals Kullback-Leibler divergences up to a constant.</li>
<li>a multivariate normal model is used to estimate uncertainty.</li>
<li>the procedure provides calibrated inferences for robust model selection.</li>
<li>the method is easy to compute and interpretable.</li>
<li>the approach comes with theoretical guarantees, including consistency.</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-85">正文（抓取，非 AI）</h3>
<p>[2602.23355v1] Robust model selection using likelihood as data Statistics &gt; Methodology arXiv:2602.23355v1 (stat) [Submitted on 26 Feb 2026] Title: Robust model selection using likelihood as data Authors: Jongwoo Choi , Neil A. Spencer , Jeffrey W. Miller View a PDF of the paper titled Robust model selection using likelihood as data, by Jongwoo Choi and 2 other authors View PDF Abstract: Model selection is a central task in statistics, but standard methods are not robust in misspecified settings where the true data-generating process (DGP) is not in the set of candidate models. The key limitation is that existing methods -- including information criteria and Bayesian posteriors -- do not quantify uncertainty about how well each candidate model approximates the true DGP. In this paper, we introduce a novel approach to model selection based on modeling the likelihood values themselves. Specifically, given $K$ candidate models and $n$ observations, we view the $n\times K$ matrix of negative log-likelihood values as a random data matrix and observe that the expectation of each row is equal to the vector of Kullback--Leibler divergences between the $K$ models and the true DGP, up to an additive constant. We use a multivariate normal model to estimate and quantify uncertainty in this expectation, providing calibrated inferences for robust model selection under misspecification. The procedure is easy to compute, interpretable, and comes with theoretical guarantees, including consistency. Subjects: Methodology (stat.ME) Cite as: arXiv:2602.23355 [stat.ME] (or arXiv:2602.23355v1 [stat.ME] for this version) https://doi.org/10.48550/arXiv.2602.23355 Focus to learn more arXiv-issued DOI via DataCite (pending registration) Submission history From: Jongwoo Choi [ view email ] [v1] Thu, 26 Feb 2026 18:57:26 UTC (3,599 KB) Full-text links: Access Paper: View a PDF of the paper titled Robust model selection using likelihood as data, by Jongwoo Choi and 2 other authors View PDF TeX Source view license Current browse context: stat.ME &lt; prev | next &gt; new | recent | 2026-02 Change to browse by: stat References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-86">44. Sensor Generalization for Adaptive Sensing in Event-based Object Detection via Joint Distribution Training</h2>
<ul>
<li>链接：https://arxiv.org/abs/2602.23357v1</li>
<li>来源：arxiv</li>
<li>摘要：Bio-inspired event cameras have recently attracted significant research due to their asynchronous and low-latency capabilities. These features provide a high dynamic range and significantly reduce motion blur. However, because of the novelty in the nature of their output signals, there is a gap in the variability of available data and a lack of extensive analysis of the parameters characterizing their signals. This paper addresses these issues by providing readers with an in-depth understanding of how intrinsic parameters affect the performance of a model trained on event data, specifically for object detection. We also use our findings to expand the capabilities of the downstream model towards sensor-agnostic robustness.</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">新型输出信号的特性导致数据变异性不足，这使得基于事件数据训练的模型性能受到影响。内在参数对这些模型的性能有重要影响，但缺乏对生物启发式事件相机信号参数的广泛分析，使得这一问题更加复杂。为了解决这些问题，该研究通过联合分布训练来提升模型的适应性，从而增强其性能。传感器的通用性对于适应性传感至关重要，这不仅能够提升模型的鲁棒性，还能扩展其到传感器无关性的能力，从而在不同传感器之间实现更好的性能一致性。因此，通过这些方法，可以有效提升基于事件数据的模型性能，并增强其在不同传感器环境下的适应性和鲁棒性。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>新型输出信号的特性导致数据变异性不足。</li>
<li>内在参数对基于事件数据训练的模型性能有影响。</li>
<li>缺乏对生物启发式事件相机信号参数的广泛分析。</li>
<li>该研究通过联合分布训练提升模型适应性。</li>
<li>传感器通用性对于适应性传感至关重要。</li>
<li>下游模型的鲁棒性可以扩展到传感器无关性。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-87">正文（抓取，非 AI）</h3>
<p>[2602.23357v1] Sensor Generalization for Adaptive Sensing in Event-based Object Detection via Joint Distribution Training Computer Science &gt; Computer Vision and Pattern Recognition arXiv:2602.23357v1 (cs) [Submitted on 26 Feb 2026] Title: Sensor Generalization for Adaptive Sensing in Event-based Object Detection via Joint Distribution Training Authors: Aheli Saha , René Schuster , Didier Stricker View a PDF of the paper titled Sensor Generalization for Adaptive Sensing in Event-based Object Detection via Joint Distribution Training, by Aheli Saha and 2 other authors View PDF HTML (experimental) Abstract: Bio-inspired event cameras have recently attracted significant research due to their asynchronous and low-latency capabilities. These features provide a high dynamic range and significantly reduce motion blur. However, because of the novelty in the nature of their output signals, there is a gap in the variability of available data and a lack of extensive analysis of the parameters characterizing their signals. This paper addresses these issues by providing readers with an in-depth understanding of how intrinsic parameters affect the performance of a model trained on event data, specifically for object detection. We also use our findings to expand the capabilities of the downstream model towards sensor-agnostic robustness. Comments: 12 pages, International Conference on Pattern Recognition Applications and Methods Subjects: Computer Vision and Pattern Recognition (cs.CV) Cite as: arXiv:2602.23357 [cs.CV] (or arXiv:2602.23357v1 [cs.CV] for this version) https://doi.org/10.48550/arXiv.2602.23357 Focus to learn more arXiv-issued DOI via DataCite (pending registration) Submission history From: Aheli Saha [ view email ] [v1] Thu, 26 Feb 2026 18:57:52 UTC (2,819 KB) Full-text links: Access Paper: View a PDF of the paper titled Sensor Generalization for Adaptive Sensing in Event-based Object Detection via Joint Distribution Training, by Aheli Saha and 2 other authors View PDF HTML (experimental) TeX Source view license Current browse context: cs.CV &lt; prev | next &gt; new | recent | 2026-02 Change to browse by: cs References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-88">45. On the embedding transformation for optimal control of multi-mode switched systems</h2>
<ul>
<li>链接：https://arxiv.org/abs/2512.12883v1</li>
<li>来源：arxiv</li>
<li>摘要：This paper develops an embedding-based approach to solve switched optimal control problems (SOCPs) with an arbitrary number of subsystems. Initially, the discrete switching signal is represented by a set of binary variables, encoding each mode in binary format. An embedded optimal control problem (EOCP) is then formulated by replacing these binary variables with continuous embedded variables that can take intermediate values between zero and one. Although embedding allows SOCPs to be addressed using conventional techniques, the optimal solutions of EOCPs often yield intermediate values for binary variables, which may not be feasible for the original SOCP. To address this challenge, a modified EOCP (MEOCP) is introduced by adding a concave auxiliary cost function of appropriate dimensionality to the main cost function. This addition ensures that the optimal solution of the EOCP is bang-bang, and as a result, feasible for the original SOCP.</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">嵌入变量可以取介于0和1之间的中间值，而最优解的扩展最优控制问题（EOCP）通常也会产生二元变量的中间值。然而，这些中间值可能不满足原始的二次优化问题（SOCP）的要求。为了解决这一问题，通常会添加一个凹辅助成本函数，以确保得到开-关（bang-bang）解。这种开-关解是可行的，并且满足原始的SOCP。因此，通过引入凹辅助成本函数，可以确保解的可行性，从而解决中间值不可行的问题。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>embedding variables can take intermediate values between zero and one.</li>
<li>optimal solutions of EOCPs often yield intermediate values for binary variables.</li>
<li>these intermediate values may not be feasible for the original SOCP.</li>
<li>a concave auxiliary cost function is added to ensure bang-bang solutions.</li>
<li>bang-bang solutions are feasible for the original SOCP.</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-89">正文（抓取，非 AI）</h3>
<p>[2512.12883v1] On the embedding transformation for optimal control of multi-mode switched systems Electrical Engineering and Systems Science &gt; Systems and Control arXiv:2512.12883v1 (eess) [Submitted on 14 Dec 2025] Title: On the embedding transformation for optimal control of multi-mode switched systems Authors: Masoud S. Sakha , Rushikesh Kamalapurkar View a PDF of the paper titled On the embedding transformation for optimal control of multi-mode switched systems, by Masoud S. Sakha and 1 other authors View PDF HTML (experimental) Abstract: This paper develops an embedding-based approach to solve switched optimal control problems (SOCPs) with an arbitrary number of subsystems. Initially, the discrete switching signal is represented by a set of binary variables, encoding each mode in binary format. An embedded optimal control problem (EOCP) is then formulated by replacing these binary variables with continuous embedded variables that can take intermediate values between zero and one. Although embedding allows SOCPs to be addressed using conventional techniques, the optimal solutions of EOCPs often yield intermediate values for binary variables, which may not be feasible for the original SOCP. To address this challenge, a modified EOCP (MEOCP) is introduced by adding a concave auxiliary cost function of appropriate dimensionality to the main cost function. This addition ensures that the optimal solution of the EOCP is bang-bang, and as a result, feasible for the original SOCP. Subjects: Systems and Control (eess.SY) ; Optimization and Control (math.OC) Cite as: arXiv:2512.12883 [eess.SY] (or arXiv:2512.12883v1 [eess.SY] for this version) https://doi.org/10.48550/arXiv.2512.12883 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Masoud Sakha [ view email ] [v1] Sun, 14 Dec 2025 23:56:07 UTC (928 KB) Full-text links: Access Paper: View a PDF of the paper titled On the embedding transformation for optimal control of multi-mode switched systems, by Masoud S. Sakha and 1 other authors View PDF HTML (experimental) TeX Source view license Current browse context: eess.SY &lt; prev | next &gt; new | recent | 2025-12 Change to browse by: cs cs.SY eess math math.OC References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-90">46. VGG-T$^3$: Offline Feed-Forward 3D Reconstruction at Scale</h2>
<ul>
<li>链接：https://arxiv.org/abs/2602.23361v1</li>
<li>来源：arxiv</li>
<li>摘要：We present a scalable 3D reconstruction model that addresses a critical limitation in offline feed-forward methods: their computational and memory requirements grow quadratically w.r.t. the number of input images. Our approach is built on the key insight that this bottleneck stems from the varying-length Key-Value (KV) space representation of scene geometry, which we distill into a fixed-size Multi-Layer Perceptron (MLP) via test-time training. VGG-T$^3$ (Visual Geometry Grounded Test Time Training) scales linearly w.r.t. the number of input views, similar to online models, and reconstructs a $1k$ image collection in just $54$ seconds, achieving a $11.6\times$ speed-up over baselines that rely on softmax attention. Since our method retains global scene aggregation capability, our point map reconstruction error outperforming other linear-time methods by large margins. Finally, we demonstrate visual localization capabilities of our model by querying the scene representation with unseen images.</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">offline feed-forward方法的计算和内存需求随输入图像数量的平方增长，这使得其在处理大规模图像集合时效率低下。相比之下，VGG-T$^3$通过测试时训练将可变长度的Key-Value空间表示转化为固定大小的多层感知机，从而使得其计算复杂度和内存需求与输入视图的数量成线性关系，显著提高了处理效率。具体而言，VGG-T$^3$能够在54秒内重建1000张图像集合，比依赖softmax注意力的基线方法快11.6倍。此外，VGG-T$^3$保留了全局场景聚合能力，其点图重建误差优于其他线性时间方法，同时通过查询场景表示进行视觉定位，展示了其在未见图像上的定位能力。因此，VGG-T$^3$不仅提高了处理效率，还保持了良好的重建精度和定位能力。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>offline feed-forward方法的计算和内存需求随输入图像数量的平方增长。</li>
<li>VGG-T$^3$通过测试时训练将可变长度的Key-Value空间表示转化为固定大小的多层感知机。</li>
<li>VGG-T$^3$的计算复杂度和内存需求与输入视图的数量成线性关系。</li>
<li>VGG-T$^3$在54秒内重建1000张图像集合，比依赖softmax注意力的基线方法快11.6倍。</li>
<li>VGG-T$^3$保留了全局场景聚合能力，其点图重建误差优于其他线性时间方法。</li>
<li>VGG-T$^3$通过查询场景表示进行视觉定位，展示其未见图像的定位能力。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-91">正文（抓取，非 AI）</h3>
<p>[2602.23361v1] VGG-T$^3$: Offline Feed-Forward 3D Reconstruction at Scale Computer Science &gt; Computer Vision and Pattern Recognition arXiv:2602.23361v1 (cs) [Submitted on 26 Feb 2026] Title: VGG-T$^3$: Offline Feed-Forward 3D Reconstruction at Scale Authors: Sven Elflein , Ruilong Li , Sérgio Agostinho , Zan Gojcic , Laura Leal-Taixé , Qunjie Zhou , Aljosa Osep View a PDF of the paper titled VGG-T$^3$: Offline Feed-Forward 3D Reconstruction at Scale, by Sven Elflein and 6 other authors View PDF HTML (experimental) Abstract: We present a scalable 3D reconstruction model that addresses a critical limitation in offline feed-forward methods: their computational and memory requirements grow quadratically w.r.t. the number of input images. Our approach is built on the key insight that this bottleneck stems from the varying-length Key-Value (KV) space representation of scene geometry, which we distill into a fixed-size Multi-Layer Perceptron (MLP) via test-time training. VGG-T$^3$ (Visual Geometry Grounded Test Time Training) scales linearly w.r.t. the number of input views, similar to online models, and reconstructs a $1k$ image collection in just $54$ seconds, achieving a $11.6\times$ speed-up over baselines that rely on softmax attention. Since our method retains global scene aggregation capability, our point map reconstruction error outperforming other linear-time methods by large margins. Finally, we demonstrate visual localization capabilities of our model by querying the scene representation with unseen images. Comments: CVPR 2026, Project page: this https URL Subjects: Computer Vision and Pattern Recognition (cs.CV) Cite as: arXiv:2602.23361 [cs.CV] (or arXiv:2602.23361v1 [cs.CV] for this version) https://doi.org/10.48550/arXiv.2602.23361 Focus to learn more arXiv-issued DOI via DataCite (pending registration) Submission history From: Sven Elflein [ view email ] [v1] Thu, 26 Feb 2026 18:59:33 UTC (23,221 KB) Full-text links: Access Paper: View a PDF of the paper titled VGG-T$^3$: Offline Feed-Forward 3D Reconstruction at Scale, by Sven Elflein and 6 other authors View PDF HTML (experimental) TeX Source view license Current browse context: cs.CV &lt; prev | next &gt; new | recent | 2026-02 Change to browse by: cs References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-92">47. EMAformer: Enhancing Transformer through Embedding Armor for Time Series Forecasting</h2>
<ul>
<li>链接：https://arxiv.org/abs/2511.08396v1</li>
<li>来源：arxiv</li>
<li>摘要：Multivariate time series forecasting is crucial across a wide range of domains. While presenting notable progress for the Transformer architecture, iTransformer still lags behind the latest MLP-based models. We attribute this performance gap to unstable inter-channel relationships. To bridge this gap, we propose EMAformer, a simple yet effective model that enhances the Transformer with an auxiliary embedding suite, akin to armor that reinforces its ability. By introducing three key inductive biases, i.e., \textit{global stability}, \textit{phase sensitivity}, and \textit{cross-axis specificity}, EMAformer unlocks the further potential of the Transformer architecture, achieving state-of-the-art performance on 12 real-world benchmarks and reducing forecasting errors by an average of 2.73\% in MSE and 5.15\% in MAE. This significantly advances the practical applicability of Transformer-based approaches for multivariate time series forecasting. The code is available on https://github.com/PlanckChang/EMAformer.</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">首先，易漏但重要的「EMAformer」通过引入三项关键归纳偏置，显著提升了 Transformer 的性能。这些归纳偏置包括：通过不稳定通道间关系来解释 performance gap，从而捕捉相位变化的 phase sensitivity，以及区分不同轴的 cross-axis specificity。其次，这些改进使得 Transformer 在 12 个实际基准上的表现达到了 state-of-the-art，具体表现为 MSE 和 MAE 分别平均降低了 2.73% 和 5.15%。此外，代码在 GitHub 上公开，这不仅有助于其他研究者复现和验证结果，还促进了该方法在实际应用中的进一步推广，从而显著提升了 Transformer 方法的适用性。因此，通过这些改进，「EMAformer」不仅在理论上取得了突破，还在实际应用中展现了显著的优势。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>performance gap 通常归因于不稳定通道间关系。</li>
<li>global stability 有助于增强整体稳定性。</li>
<li>phase sensitivity 有助于捕捉相位变化。</li>
<li>cross-axis specificity 有助于区分不同轴。</li>
<li>EMAformer 通过引入三项关键归纳偏置，显著提升了 Transformer 的性能。</li>
<li>state-of-the-art 表现在 12 个实际基准上。</li>
<li>MSE 和 MAE 分别平均降低了 2.73% 和 5.15%。</li>
<li>代码在 GitHub 上公开。</li>
<li>实际应用中 Transformer 方法的适用性得到了显著提升。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-93">正文（抓取，非 AI）</h3>
<p>[2511.08396v1] EMAformer: Enhancing Transformer through Embedding Armor for Time Series Forecasting Computer Science &gt; Machine Learning arXiv:2511.08396v1 (cs) [Submitted on 11 Nov 2025] Title: EMAformer: Enhancing Transformer through Embedding Armor for Time Series Forecasting Authors: Zhiwei Zhang , Xinyi Du , Xuanchi Guo , Weihao Wang , Wenjuan Han View a PDF of the paper titled EMAformer: Enhancing Transformer through Embedding Armor for Time Series Forecasting, by Zhiwei Zhang and 4 other authors View PDF HTML (experimental) Abstract: Multivariate time series forecasting is crucial across a wide range of domains. While presenting notable progress for the Transformer architecture, iTransformer still lags behind the latest MLP-based models. We attribute this performance gap to unstable inter-channel relationships. To bridge this gap, we propose EMAformer, a simple yet effective model that enhances the Transformer with an auxiliary embedding suite, akin to armor that reinforces its ability. By introducing three key inductive biases, i.e., \textit{global stability}, \textit{phase sensitivity}, and \textit{cross-axis specificity}, EMAformer unlocks the further potential of the Transformer architecture, achieving state-of-the-art performance on 12 real-world benchmarks and reducing forecasting errors by an average of 2.73\% in MSE and 5.15\% in MAE. This significantly advances the practical applicability of Transformer-based approaches for multivariate time series forecasting. The code is available on this https URL . Comments: 14 pages, 9 figures, 6 tables, accepted by AAAI2026 Subjects: Machine Learning (cs.LG) Cite as: arXiv:2511.08396 [cs.LG] (or arXiv:2511.08396v1 [cs.LG] for this version) https://doi.org/10.48550/arXiv.2511.08396 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Zhiwei Zhang [ view email ] [v1] Tue, 11 Nov 2025 16:12:44 UTC (402 KB) Full-text links: Access Paper: View a PDF of the paper titled EMAformer: Enhancing Transformer through Embedding Armor for Time Series Forecasting, by Zhiwei Zhang and 4 other authors View PDF HTML (experimental) TeX Source view license Current browse context: cs.LG &lt; prev | next &gt; new | recent | 2025-11 Change to browse by: cs References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) IArxiv recommender toggle IArxiv Recommender ( What is IArxiv? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-94">48. CXReasonAgent: Evidence-Grounded Diagnostic Reasoning Agent for Chest X-rays</h2>
<ul>
<li>链接：https://arxiv.org/abs/2602.23276v1</li>
<li>来源：arxiv</li>
<li>摘要：Chest X-ray plays a central role in thoracic diagnosis, and its interpretation inherently requires multi-step, evidence-grounded reasoning. However, large vision-language models (LVLMs) often generate plausible responses that are not faithfully grounded in diagnostic evidence and provide limited visual evidence for verification, while also requiring costly retraining to support new diagnostic tasks, limiting their reliability and adaptability in clinical settings. To address these limitations, we present CXReasonAgent, a diagnostic agent that integrates a large language model (LLM) with clinically grounded diagnostic tools to perform evidence-grounded diagnostic reasoning using image-derived diagnostic and visual evidence. To evaluate these capabilities, we introduce CXReasonDial, a multi-turn dialogue benchmark with 1,946 dialogues across 12 diagnostic tasks, and show that CXReasonAgent produces faithfully grounded responses, enabling more reliable and verifiable diagnostic reasoning than LVLMs. These findings highlight the importance of integrating clinically grounded diagnostic tools, particularly in safety-critical clinical settings.</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">LVLMs生成的响应可能缺乏对诊断证据的忠实性，这使得它们在支持新诊断任务时需要昂贵的重新训练。相比之下，CXReasonAgent通过结合LLM和临床诊断工具，提高了诊断的可靠性，并能够产生忠实于诊断证据的响应。CXReasonDial作为一个包含1,946个对话、覆盖12个诊断任务的多轮对话基准，用于评估诊断能力，进一步证明了CXReasonAgent在临床环境中更具可靠性和适应性。因此，在临床环境中，特别是安全关键的环境中，应整合临床诊断工具，以确保诊断的准确性和可靠性。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>LVLMs生成的响应可能缺乏对诊断证据的忠实性。</li>
<li>CXReasonAgent通过结合LLM和临床诊断工具，提高了诊断的可靠性。</li>
<li>CXReasonDial是一个多轮对话基准，用于评估诊断能力。</li>
<li>CXReasonAgent能够产生忠实于诊断证据的响应。</li>
<li>LVLMs在支持新诊断任务时需要昂贵的重新训练。</li>
<li>CXReasonAgent在临床环境中更具可靠性和适应性。</li>
<li>临床环境中应整合临床诊断工具，特别是在安全关键的环境中。</li>
<li>CXReasonDial包含1,946个对话，覆盖12个诊断任务。</li>
<li>CXReasonAgent的诊断推理比LVLMs更可靠和可验证。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-95">正文（抓取，非 AI）</h3>
<p>[2602.23276v1] CXReasonAgent: Evidence-Grounded Diagnostic Reasoning Agent for Chest X-rays Computer Science &gt; Artificial Intelligence arXiv:2602.23276v1 (cs) [Submitted on 26 Feb 2026] Title: CXReasonAgent: Evidence-Grounded Diagnostic Reasoning Agent for Chest X-rays Authors: Hyungyung Lee , Hangyul Yoon , Edward Choi View a PDF of the paper titled CXReasonAgent: Evidence-Grounded Diagnostic Reasoning Agent for Chest X-rays, by Hyungyung Lee and 2 other authors View PDF HTML (experimental) Abstract: Chest X-ray plays a central role in thoracic diagnosis, and its interpretation inherently requires multi-step, evidence-grounded reasoning. However, large vision-language models (LVLMs) often generate plausible responses that are not faithfully grounded in diagnostic evidence and provide limited visual evidence for verification, while also requiring costly retraining to support new diagnostic tasks, limiting their reliability and adaptability in clinical settings. To address these limitations, we present CXReasonAgent, a diagnostic agent that integrates a large language model (LLM) with clinically grounded diagnostic tools to perform evidence-grounded diagnostic reasoning using image-derived diagnostic and visual evidence. To evaluate these capabilities, we introduce CXReasonDial, a multi-turn dialogue benchmark with 1,946 dialogues across 12 diagnostic tasks, and show that CXReasonAgent produces faithfully grounded responses, enabling more reliable and verifiable diagnostic reasoning than LVLMs. These findings highlight the importance of integrating clinically grounded diagnostic tools, particularly in safety-critical clinical settings. Subjects: Artificial Intelligence (cs.AI) Cite as: arXiv:2602.23276 [cs.AI] (or arXiv:2602.23276v1 [cs.AI] for this version) https://doi.org/10.48550/arXiv.2602.23276 Focus to learn more arXiv-issued DOI via DataCite (pending registration) Submission history From: Hyungyung Lee [ view email ] [v1] Thu, 26 Feb 2026 17:51:21 UTC (18,344 KB) Full-text links: Access Paper: View a PDF of the paper titled CXReasonAgent: Evidence-Grounded Diagnostic Reasoning Agent for Chest X-rays, by Hyungyung Lee and 2 other authors View PDF HTML (experimental) TeX Source view license Current browse context: cs.AI &lt; prev | next &gt; new | recent | 2026-02 Change to browse by: cs References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-96">49. Butterfly Echo Protocol for Axis-Agnostic Heisenberg-Limited Metrology</h2>
<ul>
<li>链接：https://arxiv.org/abs/2602.23332v1</li>
<li>来源：arxiv</li>
<li>摘要：The extreme sensitivity of chaotic systems to external perturbations makes them natural candidates for sensing applications. We propose a single-shot echo-based protocol for estimating small rotations about an unknown axis that leverages random symmetric probe states prepared via chaotic dynamics. In contrast to previous protocols for this axis-agnostic rotation sensing problem that depend on difficult-to-prepare anticoherent states, the random probe states used in our protocol can be prepared via constant-depth chaotic circuits composed of random one-axis twisting pulses. We demonstrate analytically that our protocol achieves Heisenberg scaling relative to an arbitrary rotation axis that need not be a priori known. We also investigate the effects of collective and single-particle dephasing in our protocol using analytical and numerical tools. While the requirements on dephasing rates to maintain Heisenberg sensitivity are strict, they are achievable in near-term experiments, for instance, for magnetometric rotosensing with high-spin lanthanide atoms such as dysprosium-164.</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">混沌系统对外部扰动的极端敏感性使其成为传感应用的理想选择。为此，一种协议利用了通过混沌动力学准备的随机对称探针态。该协议能够在单次测量中估计关于未知轴的小旋转，并且相对于任意旋转轴实现了海森堡缩放。该协议可以通过常深混沌电路来实现，但其性能受到集体和单粒子相位散射的影响。为了保持海森堡敏感性，必须满足严格的相位散射速率要求。因此，该协议对于使用高自旋镧系元素原子进行近期内实验是可行的。例如，镝-164就是一种高自旋镧系元素原子，适用于磁性旋转传感。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>chaotic systems' extreme sensitivity to external perturbations makes them suitable for sensing applications.</li>
<li>the protocol uses random symmetric probe states prepared via chaotic dynamics.</li>
<li>the protocol can estimate small rotations about an unknown axis in a single shot.</li>
<li>the protocol achieves Heisenberg scaling relative to an arbitrary rotation axis.</li>
<li>the protocol can be implemented using constant-depth chaotic circuits.</li>
<li>the protocol's performance is affected by collective and single-particle dephasing.</li>
<li>maintaining Heisenberg sensitivity requires strict dephasing rate requirements.</li>
<li>the protocol is feasible for near-term experiments with high-spin lanthanide atoms.</li>
<li>dysprosium-164 is an example of a high-spin lanthanide atom for magnetometric rotosensing.</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-97">正文（抓取，非 AI）</h3>
<p>[2602.23332v1] Butterfly Echo Protocol for Axis-Agnostic Heisenberg-Limited Metrology Quantum Physics arXiv:2602.23332v1 (quant-ph) [Submitted on 26 Feb 2026] Title: Butterfly Echo Protocol for Axis-Agnostic Heisenberg-Limited Metrology Authors: Jacob Bringewatt , Leon Zaporski , Matthew Radzihovsky , Jasmine Albert , Alexey V. Gorshkov , Vladan Vuletic , Gregory Bentsen View a PDF of the paper titled Butterfly Echo Protocol for Axis-Agnostic Heisenberg-Limited Metrology, by Jacob Bringewatt and 6 other authors View PDF HTML (experimental) Abstract: The extreme sensitivity of chaotic systems to external perturbations makes them natural candidates for sensing applications. We propose a single-shot echo-based protocol for estimating small rotations about an unknown axis that leverages random symmetric probe states prepared via chaotic dynamics. In contrast to previous protocols for this axis-agnostic rotation sensing problem that depend on difficult-to-prepare anticoherent states, the random probe states used in our protocol can be prepared via constant-depth chaotic circuits composed of random one-axis twisting pulses. We demonstrate analytically that our protocol achieves Heisenberg scaling relative to an arbitrary rotation axis that need not be a priori known. We also investigate the effects of collective and single-particle dephasing in our protocol using analytical and numerical tools. While the requirements on dephasing rates to maintain Heisenberg sensitivity are strict, they are achievable in near-term experiments, for instance, for magnetometric rotosensing with high-spin lanthanide atoms such as dysprosium-164. Comments: 7 pages, 4 figures + 16 page supplement Subjects: Quantum Physics (quant-ph) Cite as: arXiv:2602.23332 [quant-ph] (or arXiv:2602.23332v1 [quant-ph] for this version) https://doi.org/10.48550/arXiv.2602.23332 Focus to learn more arXiv-issued DOI via DataCite (pending registration) Submission history From: Gregory Bentsen [ view email ] [v1] Thu, 26 Feb 2026 18:38:01 UTC (5,623 KB) Full-text links: Access Paper: View a PDF of the paper titled Butterfly Echo Protocol for Axis-Agnostic Heisenberg-Limited Metrology, by Jacob Bringewatt and 6 other authors View PDF HTML (experimental) TeX Source view license Current browse context: quant-ph &lt; prev | next &gt; new | recent | 2026-02 References &amp; Citations INSPIRE HEP NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-98">50. Differentiable Zero-One Loss via Hypersimplex Projections</h2>
<ul>
<li>链接：https://arxiv.org/abs/2602.23336v1</li>
<li>来源：arxiv</li>
<li>摘要：Recent advances in machine learning have emphasized the integration of structured optimization components into end-to-end differentiable models, enabling richer inductive biases and tighter alignment with task-specific objectives. In this work, we introduce a novel differentiable approximation to the zero-one loss-long considered the gold standard for classification performance, yet incompatible with gradient-based optimization due to its non-differentiability. Our method constructs a smooth, order-preserving projection onto the n,k-dimensional hypersimplex through a constrained optimization framework, leading to a new operator we term Soft-Binary-Argmax. After deriving its mathematical properties, we show how its Jacobian can be efficiently computed and integrated into binary and multiclass learning systems. Empirically, our approach achieves significant improvements in generalization under large-batch training by imposing geometric consistency constraints on the output logits, thereby narrowing the performance gap traditionally observed in large-batch training.</li>
<li><strong>易漏细节梳理：</strong></li>
</ul>
<p class="insight-summary">首先，易漏但重要的end-to-end不同iable模型允许将结构化优化组件直接集成进来，这为模型的优化提供了更大的灵活性。然而，传统的零一损失（zero-one loss）作为分类性能的黄金标准，却因其非可微性而无法与梯度优化方法兼容，这成为模型优化的一大障碍。为了解决这一问题，研究者引入了Soft-Binary-Argmax这一新的操作符，它能够通过光滑地投影到n,k维超单纯形上来替代零一损失，从而使得模型的优化过程更加平滑和连续。此外，几何一致性约束在大批次训练中起到了关键作用，它有助于提高模型的泛化能力，确保模型在不同数据集上的表现更为稳定。因此，Soft-Binary-Argmax的雅可比可以高效计算并集成到二元和多元学习系统中，进一步提升了模型的优化效率和性能。</p>
<ul>
<li><strong>关键点（易漏细节）：</strong></li>
</ul>
<ul class="insight-detail">
<li>end-to-end differentiable models允许将结构化优化组件集成进来；</li>
<li>zero-one loss是分类性能的黄金标准，但因其非可微性而不兼容梯度优化；</li>
<li>Soft-Binary-Argmax是一个新的操作符，用于光滑地投影到n,k维超单纯形上；</li>
<li>几何一致性约束有助于在大批次训练中提高泛化能力；</li>
<li>Soft-Binary-Argmax的雅可比可以高效计算并集成到二元和多元学习系统中。</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-99">正文（抓取，非 AI）</h3>
<p>[2602.23336v1] Differentiable Zero-One Loss via Hypersimplex Projections Computer Science &gt; Machine Learning arXiv:2602.23336v1 (cs) [Submitted on 26 Feb 2026] Title: Differentiable Zero-One Loss via Hypersimplex Projections Authors: Camilo Gomez , Pengyang Wang , Liansheng Tang View a PDF of the paper titled Differentiable Zero-One Loss via Hypersimplex Projections, by Camilo Gomez and 2 other authors View PDF HTML (experimental) Abstract: Recent advances in machine learning have emphasized the integration of structured optimization components into end-to-end differentiable models, enabling richer inductive biases and tighter alignment with task-specific objectives. In this work, we introduce a novel differentiable approximation to the zero-one loss-long considered the gold standard for classification performance, yet incompatible with gradient-based optimization due to its non-differentiability. Our method constructs a smooth, order-preserving projection onto the n,k-dimensional hypersimplex through a constrained optimization framework, leading to a new operator we term Soft-Binary-Argmax. After deriving its mathematical properties, we show how its Jacobian can be efficiently computed and integrated into binary and multiclass learning systems. Empirically, our approach achieves significant improvements in generalization under large-batch training by imposing geometric consistency constraints on the output logits, thereby narrowing the performance gap traditionally observed in large-batch training. Comments: To appear in PAKDD 2026 (Pacific-Asia Conference on Knowledge Discovery and Data Mining), 12 pages Subjects: Machine Learning (cs.LG) ; Machine Learning (stat.ML) Cite as: arXiv:2602.23336 [cs.LG] (or arXiv:2602.23336v1 [cs.LG] for this version) https://doi.org/10.48550/arXiv.2602.23336 Focus to learn more arXiv-issued DOI via DataCite (pending registration) Submission history From: Camilo Gomez [ view email ] [v1] Thu, 26 Feb 2026 18:41:31 UTC (100 KB) Full-text links: Access Paper: View a PDF of the paper titled Differentiable Zero-One Loss via Hypersimplex Projections, by Camilo Gomez and 2 other authors View PDF HTML (experimental) TeX Source view license Current browse context: cs.LG &lt; prev | next &gt; new | recent | 2026-02 Change to browse by: cs stat stat.ML References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) IArxiv recommender toggle IArxiv Recommender ( What is IArxiv? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-100">51. An Optimization Method for Autoregressive Time Series Forecasting</h2>
<ul>
<li>链接：https://arxiv.org/abs/2602.02288v1</li>
<li>来源：arxiv</li>
<li>摘要：Current time-series forecasting models are primarily based on transformer-style neural networks. These models achieve long-term forecasting mainly by scaling up the model size rather than through genuinely autoregressive (AR) rollout. From the perspective of large language model training, the traditional training process for time-series forecasting models ignores temporal causality. In this paper, we propose a novel training method for time-series forecasting that enforces two key properties: (1) AR prediction errors should increase with the forecasting horizon. Any violation of this principle is considered random guessing and is explicitly penalized in the loss function, and (2) the method enables models to concatenate short-term AR predictions for forming flexible long-term forecasts. Empirical results demonstrate that our method establishes a new state-of-the-art across multiple benchmarks, achieving an MSE reduction of more than 10% compared to iTransformer and other recent strong baselines. Furthermore, it enables short-horizon forecasting models to perform reliable long-term predictions at horizons over 7.5 times longer. Code is available at https://github.com/LizhengMathAi/A</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-101">正文（抓取，非 AI）</h3>
<p>[2602.02288v1] An Optimization Method for Autoregressive Time Series Forecasting Computer Science &gt; Machine Learning arXiv:2602.02288v1 (cs) [Submitted on 2 Feb 2026] Title: An Optimization Method for Autoregressive Time Series Forecasting Authors: Zheng Li , Jerry Cheng , Huanying Gu View a PDF of the paper titled An Optimization Method for Autoregressive Time Series Forecasting, by Zheng Li and 2 other authors View PDF HTML (experimental) Abstract: Current time-series forecasting models are primarily based on transformer-style neural networks. These models achieve long-term forecasting mainly by scaling up the model size rather than through genuinely autoregressive (AR) rollout. From the perspective of large language model training, the traditional training process for time-series forecasting models ignores temporal causality. In this paper, we propose a novel training method for time-series forecasting that enforces two key properties: (1) AR prediction errors should increase with the forecasting horizon. Any violation of this principle is considered random guessing and is explicitly penalized in the loss function, and (2) the method enables models to concatenate short-term AR predictions for forming flexible long-term forecasts. Empirical results demonstrate that our method establishes a new state-of-the-art across multiple benchmarks, achieving an MSE reduction of more than 10% compared to iTransformer and other recent strong baselines. Furthermore, it enables short-horizon forecasting models to perform reliable long-term predictions at horizons over 7.5 times longer. Code is available at this https URL Comments: 10 pages, 2 figures, 2 tables Subjects: Machine Learning (cs.LG) ; Artificial Intelligence (cs.AI) Cite as: arXiv:2602.02288 [cs.LG] (or arXiv:2602.02288v1 [cs.LG] for this version) https://doi.org/10.48550/arXiv.2602.02288 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Zheng Li [ view email ] [v1] Mon, 2 Feb 2026 16:28:00 UTC (242 KB) Full-text links: Access Paper: View a PDF of the paper titled An Optimization Method for Autoregressive Time Series Forecasting, by Zheng Li and 2 other authors View PDF HTML (experimental) TeX Source view license Current browse context: cs.LG &lt; prev | next &gt; new | recent | 2026-02 Change to browse by: cs cs.AI References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) IArxiv recommender toggle IArxiv Recommender ( What is IArxiv? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-102">52. 最优化函数 socp 的使用及转化案例</h2>
<ul>
<li>链接：https://docs.dolphindb.cn/zh/tutorials/socp_usage_case.html</li>
<li>来源：bing</li>
<li>摘要：3 天之前 · socp 函数用于求解二阶锥规划（SOCP, Second Order Cone Programming）问题，其数学模型具有较强的通用性，有很广泛的适用场景。针对线性规划（LP）、二次规划（QP） …</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-103">正文（抓取，非 AI）</h3>
<p>最优化函数 socp 的使用及转化案例 跳转到主要内容 最优化函数 socp 的使用及转化案例 1. SOCP 问题的背景介绍 在金融量化领域，最优化函数能够根据最大化或最小化特定目标（如利润、收益、风险），有效解决各种复杂的决策问题，包括投资组合优化、风险管理以及资产和衍生品定价等。为了满足不同应用场景的需求，DolphinDB 提供了一系列最优化函数，包括 linprog 、 quadprog 、 qclp 、 socp ，帮助用户将业务问题转化为数学规划模型，并利用合适的求解器寻求最优解或可行解，从而辅助做出更明智的决策。 其中，二阶锥规划（socp）函数在金融问题求解中展现了独特的优势，特别是在处理不确定性、非线性约束以及复杂的风险控制问题时。例如，在风险管理中，socp 可以处理方差、标准差等二次项约束；在投资组合优化中，它能够有效应对杠杆限制、市场约束和资本要求等复杂条件。此外，socp 在应对大规模数据问题时具有高效的求解能力，以确保快速找到全局最优解。本文将详细介绍 socp 函数的使用，并通过实际的投资组合优化案例，展示如何将实际问题转化为二阶锥规划形式进行求解。 2. SOCP 问题的数学模型 socp 函数用于求解二阶锥规划（SOCP, Second Order Cone Programming）问题，其数学模型具有较强的通用性，有很广泛的适用场景。针对线性规划（LP）、二次规划（QP）、二次约束线性规划（QCLP）， DolphinDB 提供了对应的最优化求解函数来求解，除此之外，这些规划问题都可以转化成更一般的二阶锥规划问题，从而通过 socp 函数进行求解，具体的转换问题方式将在下一节详细介绍。为了更好地理解 socp 函数的各个参数以及如何将规划问题转换为二阶锥形式，接下来我们将简要介绍 SOCP 问题的数学模型，并通过数学模型说明 socp 函数适用的优化问题类型。 2.1 SOCP 问题的锥表述形式 其中 K 为锥， s 为松弛变量，其值在优化过程中会被确定。 f 是数值型向量，表示目标函数的系数向量。 G 是数值型矩阵，表示锥约束的系数矩阵。 h 是数值型向量，表示锥约束的右端向量。 Aeq 是数值型矩阵，表示等式约束的系数矩阵。 beq 是数值型向量，表示等式约束的右端向量。 Gx + s = h 包含了 SOCP 问题中所有的不等式约束条件，包括线性不等式以及二阶锥约束。 socp 函数中的参数 l 表示线性不等式约束个数的维度的整数标量；q 表示各个二阶锥约束的维度大小的正数向量，形式为 [r0, r1,…, rN-1]。注意： q 中每个二阶锥维度要 +1，因为除了二阶锥不等式约束左边的二范数维度，还要加上右端的常数式子。所以最后 l+sum(q) 应该等于矩阵 G 的行数。 SOCP 问题的标准形式都可根据此规则转换为上述锥表述形式，从而通过 socp 函数求解。 2.2 SOCP 问题的标准形式 SOCP 问题的标准形式可以转化成上述的锥表述形式，从而确定 socp 函数中对应的参数 G, h,l, q 进行求解。下面将从数学原理的角度出发说明如何由该标准形式得到锥表述形式，以帮助读者更好地理解。 重写二阶锥约束：将该问题中的二阶锥约束进行重写 对约束进行整合，并引入约束变量 s 因此矩阵 G 以及向量 h 的形式分别为： 2.3 socp 函数适用的优化问题 从上述数学模型可以看出，SOCP 二阶锥问题是一种凸优化问题，其目标函数为线性函数，约束条件是线性约束和二阶锥约束的组合，符合上述模型的问题均可通过 DolphinDB 中的 socp 函数求解。另外，该问题具有一定的局限性，主要体现在非凸性以及非线性上，如非凸优化问题、整数规划问题、非线性规划问题、半定规划问题等，这些问题将无法通过 socp 函数求解。 3. 常见优化问题的二阶锥规划形式转换 在使用 SOCP（二阶锥规划）求解规划问题时，首先需要将原问题转换为 SOCP 的标准形式。这个过程依赖于二阶锥的几何特性及其与标准形式的对应关系，从而确定 socp 函数所需的参数。接下来我们将详细介绍如何将不同的目标函数和约束条件转化为二阶锥形式，包括线性规划问题、二次规划问题、二次约束规划问题，并提供相应问题形式的转化示例。 3.1 线性规划的二阶锥形式 线性规划问题为线性约束条件下的线性目标函数的最优化问题，可以直接通过 DolphinDB 中的 linprog 或转换为二阶锥问题进行求解。原问题形式为： 上述数学模型中，对于线性不等式约束： 移项可得： 同理对于 x 上下界的约束： 可以转化为： 对应二阶锥的标准形式为： 其中 E 是维度与 x 相同的单位矩阵。 因此得到二阶锥参数： 3.2 二次不等式约束的二阶锥形式 二次不等式约束问题为二次线性约束条件下的线性目标函数的最优化问题，可以直接通过 DolphinDB 中的 qclp 或转换为二阶锥问题进行求解。原问题形式为： 对于二次不等式约束： 其中 V 为正定矩阵，k 为正标量。V 可以通过 cholesky 分解得到： 则原约束可转化为： 即可得到二阶锥的标准形式： 得到锥形式参数： 其中 n 为变量 x 的长度 3.3 绝对值约束的二阶锥形式 绝对值约束问题为线性约束条件下（包含绝对值约束）的线性目标函数的最优化问题。原问题形式为： 对于包含绝对值约束的线性规划： 其中 1 范数约束即为绝对值约束： 引入辅助变量： 将原问题改写为关于 [xT, u1, ··· , un]T 的优化问题，即可转换为二阶锥规划的形式： 相对应的锥形式参数为： 3.4 二次规划的二阶锥形式 二次规划问题为线性约束条件下的二次目标函数的最优化问题，可以直接通过 DolphinDB 中的 quadprog 或转换为二阶锥问题进行求解。原问题形式为： 对于目标函数中含有二次项的规划问题，引入辅助变量 y： 由于 H 为正定矩阵，可以通过 cholesky 分解得到： 原问题改写为： 通过转化代入到问题二次约束中即可得到： 最终得到与原问题等价的规划问题： 其中， 相对应的锥形式参数为： 4. 应用场景示例 本节将通过两个经典的金融场景问题，详细介绍如何将具体的投资组合优化问题转换成二阶锥规划问题，并通过 socp 函数求最优解。 4.1 基于线性规划模型的股票投资组合优化问题 本案例将演示如何将包含多重权重约束条件的线性股票投资组合优化问题转换为二阶锥规划形式，并在确定相应参数后，通过 socp 函数进行求解，以实现期望收益率的最大化。 4.1.1 建立数学规划模型 将上述问题转化为数学模型后，其目标函数（期望收益率最大化）为： 其中： ω 为投资组合中各股票的目标的权重 ƒ 为投资组合中各股票的预期收益率 约束条件为： 上述七项约束条件分别为：个股权重限制、成分股权重限制、成分股偏离限制、市值权重限制、行业权重限制、公司禁投股票限制以及目标个股权重限制。其中前六项约束条件均为线性不等式约束，可直接按照章节3.1 中的步骤进行转化，最后一个目标个股权重限制的约束则需通过 socp 中的 Aeq 和 beq 来实现。 4.1.2 转换 socp 所需参数 DolphinDB 中 socp 函数的使用语法如下，接下来我们将按照该语法逐步转换并确定相关参数： socp(f, [G], [h], [l], [q], [A], [b]) 表示目标函数的系数向量 f ：由于 socp 标准形式中的目标函数为最小化，所以此处需要对原始预期收益率取负数。 表示锥约束的系数矩阵 G 表示锥约束的右端向量 h 表示非负象限约束维度标量 l： 本例中所有约束均为线性，所以 l 即为锥约束向量 h 的个数 表示各个二阶锥约束维度大小的向量 q： 本例中无二阶锥约束，因此可设置 q 为空向量 表示等式约束的系数矩阵 Aeq 以及表示等式约束的右端向量 beq 4.1.3 调用 socp 函数进行求解 4.2 包含换手率约束及平衡收益与风险的投资组合优化问题 本案例基于 MVO（均值-方差优化） 模型，通过优化投资组合来最大化收益并最小化风险。除了延续上一个案例的权重约束条件外，还引入了换手率以及跟踪误差的约束条件。 4.2.1 建立数学规划模型 MVO 模型的基本目标是通过最大化预期收益和最小化风险来优化投资组合。在上一案例的约束条件基础上，添加换手率约束条件，可以在优化组合时，避免过度交易，以帮助管理交易成本。将该问题转化为数学模型后的目标函数为： 其中： ∑ 为收益率协方差矩阵 ω 0 为上次持仓即上次的个股权重 C T 为换手成本系数 λ 为风险系数 约束条件为： 4.2.2 对目标函数与约束条件进行二阶锥转换 基于上述数学模型，我们进一步将其转化为二阶锥规划的标准形式，方便后续函数参数的确定。首先对于目标函数中的二次项的规划问题，参考章节 2.4 的方法，我们可引入辅助变量 y，而针对带有绝对值的换手率规划问题，我们同样引入辅助变量 u： 因此原目标函数可优化为： 对于该问题的约束条件，其中前六项约束条件与上一案例类似，均为线性约束，此处不做重复赘述。 第七项约束为换手率约束，为一范数约束，即绝对值约束。 基于上述引入的辅助变量 u，原约束条件可转换为如下二阶锥规划的形式： 第八项约束条件为跟踪误差约束，即投资组合相对于基准波动率的限制条件，为二次不等式约束，可通过 cholesky 分解得到可以得到： 则原约束条件可以转化为如下二阶锥规划的形式（具体推导步骤可参考章节 2.2）： 4.2.3 转换 socp 所需参数 根据上述转化，下面我们将具体介绍 socp 函数各个参数生成转换的具体过程。 表示目标函数的系数向量 f ：上述二阶锥转换中我们引入了新的目标函数的求解变量 表示锥约束的系数矩阵 G 表示锥约束的右端向量 h 以及表示非负象限约束维度标量 l 表示各个二阶锥约束维度大小的向量 q 表示等式约束的系数矩阵 Aeq 以及表示等式约束的右端向量 beq 4.2.4 调用 socp 函数进行求解 5. 小结 socp 是 DolphinDB 最优化求解系列函数中应用最广泛的优化问题求解器。许多复杂的凸优化问题都可以通过转换为二阶锥规划（SOCP）来进行求解。由于二阶锥规划具备成熟且高效的求解算法，其在处理大规模优化问题时展现出优越的计算性能和极高的求解效率。</p>
</div></details><h2 id="toc-104">53. Towards Joint Optimization for UAV-Integrated RIS-Assisted Fluid Antenna Systems</h2>
<ul>
<li>链接：https://arxiv.org/abs/2601.22109v1</li>
<li>来源：arxiv</li>
<li>摘要：Unmanned aerial vehicles (UAVs) integrated into cellular networks face significant challenges from air-to-ground interference. To address this, we propose a downlink UAV communication system that leverages a fluid antenna system (FAS)- assisted reconfigurable intelligent surface (RIS) to enhance signal quality. By jointly optimizing the FAS port positions and RIS phase shifts, we maximize the achievable rate. The resulting nonconvex optimization problem is solved using successive convex approximation (SCA) based on second-order cone programming (SOCP), which reformulates the constraints into a tractable form. Simulation results show that the proposed algorithm significantly improves both outage probability and achievable rate over conventional fixed-position antenna (FPA) schemes, with particularly large gains in large-scale RIS configurations. Moreover, the algorithm converges rapidly, making it suitable for real-time applications</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-105">正文（抓取，非 AI）</h3>
<p>[2601.22109v1] Towards Joint Optimization for UAV-Integrated RIS-Assisted Fluid Antenna Systems Electrical Engineering and Systems Science &gt; Signal Processing arXiv:2601.22109v1 (eess) [Submitted on 29 Jan 2026] Title: Towards Joint Optimization for UAV-Integrated RIS-Assisted Fluid Antenna Systems Authors: Ali Reda , Tamer Mekkawy , Theodoros A. Tsiftsis , Chan-Byoung Chae , Kai-Kit Wong View a PDF of the paper titled Towards Joint Optimization for UAV-Integrated RIS-Assisted Fluid Antenna Systems, by Ali Reda and 4 other authors View PDF HTML (experimental) Abstract: Unmanned aerial vehicles (UAVs) integrated into cellular networks face significant challenges from air-to-ground interference. To address this, we propose a downlink UAV communication system that leverages a fluid antenna system (FAS)- assisted reconfigurable intelligent surface (RIS) to enhance signal quality. By jointly optimizing the FAS port positions and RIS phase shifts, we maximize the achievable rate. The resulting nonconvex optimization problem is solved using successive convex approximation (SCA) based on second-order cone programming (SOCP), which reformulates the constraints into a tractable form. Simulation results show that the proposed algorithm significantly improves both outage probability and achievable rate over conventional fixed-position antenna (FPA) schemes, with particularly large gains in large-scale RIS configurations. Moreover, the algorithm converges rapidly, making it suitable for real-time applications Comments: 11 pages, 8 figures Subjects: Signal Processing (eess.SP) Cite as: arXiv:2601.22109 [eess.SP] (or arXiv:2601.22109v1 [eess.SP] for this version) https://doi.org/10.48550/arXiv.2601.22109 Focus to learn more arXiv-issued DOI via DataCite (pending registration) Journal reference: IEEE Transactions on Vehicular Technology, 2026 Related DOI : https://doi.org/10.1109/TVT.2026.3658088 Focus to learn more DOI(s) linking to related resources Submission history From: Theodoros Tsiftsis Prof. [ view email ] [v1] Thu, 29 Jan 2026 18:38:38 UTC (536 KB) Full-text links: Access Paper: View a PDF of the paper titled Towards Joint Optimization for UAV-Integrated RIS-Assisted Fluid Antenna Systems, by Ali Reda and 4 other authors View PDF HTML (experimental) TeX Source view license Current browse context: eess.SP &lt; prev | next &gt; new | recent | 2026-01 Change to browse by: eess References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-106">54. Selective Learning for Deep Time Series Forecasting</h2>
<ul>
<li>链接：https://arxiv.org/abs/2510.25207v1</li>
<li>来源：arxiv</li>
<li>摘要：Benefiting from high capacity for capturing complex temporal patterns, deep learning (DL) has significantly advanced time series forecasting (TSF). However, deep models tend to suffer from severe overfitting due to the inherent vulnerability of time series to noise and anomalies. The prevailing DL paradigm uniformly optimizes all timesteps through the MSE loss and learns those uncertain and anomalous timesteps without difference, ultimately resulting in overfitting. To address this, we propose a novel selective learning strategy for deep TSF. Specifically, selective learning screens a subset of the whole timesteps to calculate the MSE loss in optimization, guiding the model to focus on generalizable timesteps while disregarding non-generalizable ones. Our framework introduces a dual-mask mechanism to target timesteps: (1) an uncertainty mask leveraging residual entropy to filter uncertain timesteps, and (2) an anomaly mask employing residual lower bound estimation to exclude anomalous timesteps. Extensive experiments across eight real-world datasets demonstrate that selective learning can significantly improve the predictive performance for typical state-of-the-art deep models, inc</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-107">正文（抓取，非 AI）</h3>
<p>[2510.25207v1] Selective Learning for Deep Time Series Forecasting Computer Science &gt; Machine Learning arXiv:2510.25207v1 (cs) [Submitted on 29 Oct 2025] Title: Selective Learning for Deep Time Series Forecasting Authors: Yisong Fu , Zezhi Shao , Chengqing Yu , Yujie Li , Zhulin An , Qi Wang , Yongjun Xu , Fei Wang View a PDF of the paper titled Selective Learning for Deep Time Series Forecasting, by Yisong Fu and 7 other authors View PDF Abstract: Benefiting from high capacity for capturing complex temporal patterns, deep learning (DL) has significantly advanced time series forecasting (TSF). However, deep models tend to suffer from severe overfitting due to the inherent vulnerability of time series to noise and anomalies. The prevailing DL paradigm uniformly optimizes all timesteps through the MSE loss and learns those uncertain and anomalous timesteps without difference, ultimately resulting in overfitting. To address this, we propose a novel selective learning strategy for deep TSF. Specifically, selective learning screens a subset of the whole timesteps to calculate the MSE loss in optimization, guiding the model to focus on generalizable timesteps while disregarding non-generalizable ones. Our framework introduces a dual-mask mechanism to target timesteps: (1) an uncertainty mask leveraging residual entropy to filter uncertain timesteps, and (2) an anomaly mask employing residual lower bound estimation to exclude anomalous timesteps. Extensive experiments across eight real-world datasets demonstrate that selective learning can significantly improve the predictive performance for typical state-of-the-art deep models, including 37.4% MSE reduction for Informer, 8.4% for TimesNet, and 6.5% for iTransformer. Comments: Accepted by NeurIPS 2025 Subjects: Machine Learning (cs.LG) Cite as: arXiv:2510.25207 [cs.LG] (or arXiv:2510.25207v1 [cs.LG] for this version) https://doi.org/10.48550/arXiv.2510.25207 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Yisong Fu [ view email ] [v1] Wed, 29 Oct 2025 06:18:52 UTC (2,743 KB) Full-text links: Access Paper: View a PDF of the paper titled Selective Learning for Deep Time Series Forecasting, by Yisong Fu and 7 other authors View PDF TeX Source view license Current browse context: cs.LG &lt; prev | next &gt; new | recent | 2025-10 Change to browse by: cs References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) IArxiv recommender toggle IArxiv Recommender ( What is IArxiv? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-108">55. TwinFormer: A Dual-Level Transformer for Long-Sequence Time-Series Forecasting</h2>
<ul>
<li>链接：https://arxiv.org/abs/2512.12301v1</li>
<li>来源：arxiv</li>
<li>摘要：TwinFormer is a hierarchical Transformer for long-sequence time-series forecasting. It divides the input into non-overlapping temporal patches and processes them in two stages: (1) a Local Informer with top-$k$ Sparse Attention models intra-patch dynamics, followed by mean pooling; (2) a Global Informer captures long-range inter-patch dependencies using the same top-$k$ attention. A lightweight GRU aggregates the globally contextualized patch tokens for direct multi-horizon prediction. The resulting architecture achieves linear $O(kLd)$ time and memory complexity. On eight real-world benchmarking datasets from six different domains, including weather, stock price, temperature, power consumption, electricity, and disease, and forecasting horizons $96-720$, TwinFormer secures $27$ positions in the top two out of $34$. Out of the $27$, it achieves the best performance on MAE and RMSE at $17$ places and $10$ at the second-best place on MAE and RMSE. This consistently outperforms PatchTST, iTransformer, FEDformer, Informer, and vanilla Transformers. Ablations confirm the superiority of top-$k$ Sparse Attention over ProbSparse and the effectiveness of GRU-based aggregation. Code is avail</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-109">正文（抓取，非 AI）</h3>
<p>[2512.12301v1] TwinFormer: A Dual-Level Transformer for Long-Sequence Time-Series Forecasting Computer Science &gt; Machine Learning arXiv:2512.12301v1 (cs) [Submitted on 13 Dec 2025] Title: TwinFormer: A Dual-Level Transformer for Long-Sequence Time-Series Forecasting Authors: Mahima Kumavat , Aditya Maheshwari View a PDF of the paper titled TwinFormer: A Dual-Level Transformer for Long-Sequence Time-Series Forecasting, by Mahima Kumavat and Aditya Maheshwari View PDF HTML (experimental) Abstract: TwinFormer is a hierarchical Transformer for long-sequence time-series forecasting. It divides the input into non-overlapping temporal patches and processes them in two stages: (1) a Local Informer with top-$k$ Sparse Attention models intra-patch dynamics, followed by mean pooling; (2) a Global Informer captures long-range inter-patch dependencies using the same top-$k$ attention. A lightweight GRU aggregates the globally contextualized patch tokens for direct multi-horizon prediction. The resulting architecture achieves linear $O(kLd)$ time and memory complexity. On eight real-world benchmarking datasets from six different domains, including weather, stock price, temperature, power consumption, electricity, and disease, and forecasting horizons $96-720$, TwinFormer secures $27$ positions in the top two out of $34$. Out of the $27$, it achieves the best performance on MAE and RMSE at $17$ places and $10$ at the second-best place on MAE and RMSE. This consistently outperforms PatchTST, iTransformer, FEDformer, Informer, and vanilla Transformers. Ablations confirm the superiority of top-$k$ Sparse Attention over ProbSparse and the effectiveness of GRU-based aggregation. Code is available at this repository: this https URL . Comments: 14 pages, 4 figures Subjects: Machine Learning (cs.LG) Cite as: arXiv:2512.12301 [cs.LG] (or arXiv:2512.12301v1 [cs.LG] for this version) https://doi.org/10.48550/arXiv.2512.12301 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Aditya Maheshwari [ view email ] [v1] Sat, 13 Dec 2025 11:50:18 UTC (964 KB) Full-text links: Access Paper: View a PDF of the paper titled TwinFormer: A Dual-Level Transformer for Long-Sequence Time-Series Forecasting, by Mahima Kumavat and Aditya Maheshwari View PDF HTML (experimental) TeX Source view license Current browse context: cs.LG &lt; prev | next &gt; new | recent | 2025-12 Change to browse by: cs References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) IArxiv recommender toggle IArxiv Recommender ( What is IArxiv? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-110">56. Constrained Policy Optimization via Sampling-Based Weight-Space Projection</h2>
<ul>
<li>链接：https://arxiv.org/abs/2512.13788v1</li>
<li>来源：arxiv</li>
<li>摘要：Safety-critical learning requires policies that improve performance without leaving the safe operating regime. We study constrained policy learning where model parameters must satisfy unknown, rollout-based safety constraints. We propose SCPO, a sampling-based weight-space projection method that enforces safety directly in parameter space without requiring gradient access to the constraint functions. Our approach constructs a local safe region by combining trajectory rollouts with smoothness bounds that relate parameter changes to shifts in safety metrics. Each gradient update is then projected via a convex SOCP, producing a safe first-order step. We establish a safe-by-induction guarantee: starting from any safe initialization, all intermediate policies remain safe given feasible projections. In constrained control settings with a stabilizing backup policy, our approach further ensures closed-loop stability and enables safe adaptation beyond the conservative backup. On regression with harmful supervision and a constrained double-integrator task with malicious expert, our approach consistently rejects unsafe updates, maintains feasibility throughout training, and achieves meaningfu</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-111">正文（抓取，非 AI）</h3>
<p>[2512.13788v1] Constrained Policy Optimization via Sampling-Based Weight-Space Projection Computer Science &gt; Machine Learning arXiv:2512.13788v1 (cs) [Submitted on 15 Dec 2025] Title: Constrained Policy Optimization via Sampling-Based Weight-Space Projection Authors: Shengfan Cao , Francesco Borrelli View a PDF of the paper titled Constrained Policy Optimization via Sampling-Based Weight-Space Projection, by Shengfan Cao and 1 other authors View PDF HTML (experimental) Abstract: Safety-critical learning requires policies that improve performance without leaving the safe operating regime. We study constrained policy learning where model parameters must satisfy unknown, rollout-based safety constraints. We propose SCPO, a sampling-based weight-space projection method that enforces safety directly in parameter space without requiring gradient access to the constraint functions. Our approach constructs a local safe region by combining trajectory rollouts with smoothness bounds that relate parameter changes to shifts in safety metrics. Each gradient update is then projected via a convex SOCP, producing a safe first-order step. We establish a safe-by-induction guarantee: starting from any safe initialization, all intermediate policies remain safe given feasible projections. In constrained control settings with a stabilizing backup policy, our approach further ensures closed-loop stability and enables safe adaptation beyond the conservative backup. On regression with harmful supervision and a constrained double-integrator task with malicious expert, our approach consistently rejects unsafe updates, maintains feasibility throughout training, and achieves meaningful primal objective improvement. Comments: Submitted to IFAC World Congress 2026 Subjects: Machine Learning (cs.LG) ; Robotics (cs.RO) Cite as: arXiv:2512.13788 [cs.LG] (or arXiv:2512.13788v1 [cs.LG] for this version) https://doi.org/10.48550/arXiv.2512.13788 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Shengfan Cao [ view email ] [v1] Mon, 15 Dec 2025 19:00:01 UTC (4,002 KB) Full-text links: Access Paper: View a PDF of the paper titled Constrained Policy Optimization via Sampling-Based Weight-Space Projection, by Shengfan Cao and 1 other authors View PDF HTML (experimental) TeX Source view license Current browse context: cs.LG &lt; prev | next &gt; new | recent | 2025-12 Change to browse by: cs cs.RO References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) IArxiv recommender toggle IArxiv Recommender ( What is IArxiv? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-112">57. Retrieve and Segment: Are a Few Examples Enough to Bridge the Supervision Gap in Open-Vocabulary Segmentation?</h2>
<ul>
<li>链接：https://arxiv.org/abs/2602.23339v1</li>
<li>来源：arxiv</li>
<li>摘要：Open-vocabulary segmentation (OVS) extends the zero-shot recognition capabilities of vision-language models (VLMs) to pixel-level prediction, enabling segmentation of arbitrary categories specified by text prompts. Despite recent progress, OVS lags behind fully supervised approaches due to two challenges: the coarse image-level supervision used to train VLMs and the semantic ambiguity of natural language. We address these limitations by introducing a few-shot setting that augments textual prompts with a support set of pixel-annotated images. Building on this, we propose a retrieval-augmented test-time adapter that learns a lightweight, per-image classifier by fusing textual and visual support features. Unlike prior methods relying on late, hand-crafted fusion, our approach performs learned, per-query fusion, achieving stronger synergy between modalities. The method supports continually expanding support sets, and applies to fine-grained tasks such as personalized segmentation. Experiments show that we significantly narrow the gap between zero-shot and supervised segmentation while preserving open-vocabulary ability.</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-113">正文（抓取，非 AI）</h3>
<p>[2602.23339v1] Retrieve and Segment: Are a Few Examples Enough to Bridge the Supervision Gap in Open-Vocabulary Segmentation? Computer Science &gt; Computer Vision and Pattern Recognition arXiv:2602.23339v1 (cs) [Submitted on 26 Feb 2026] Title: Retrieve and Segment: Are a Few Examples Enough to Bridge the Supervision Gap in Open-Vocabulary Segmentation? Authors: Tilemachos Aravanis , Vladan Stojnić , Bill Psomas , Nikos Komodakis , Giorgos Tolias View a PDF of the paper titled Retrieve and Segment: Are a Few Examples Enough to Bridge the Supervision Gap in Open-Vocabulary Segmentation?, by Tilemachos Aravanis and 4 other authors View PDF Abstract: Open-vocabulary segmentation (OVS) extends the zero-shot recognition capabilities of vision-language models (VLMs) to pixel-level prediction, enabling segmentation of arbitrary categories specified by text prompts. Despite recent progress, OVS lags behind fully supervised approaches due to two challenges: the coarse image-level supervision used to train VLMs and the semantic ambiguity of natural language. We address these limitations by introducing a few-shot setting that augments textual prompts with a support set of pixel-annotated images. Building on this, we propose a retrieval-augmented test-time adapter that learns a lightweight, per-image classifier by fusing textual and visual support features. Unlike prior methods relying on late, hand-crafted fusion, our approach performs learned, per-query fusion, achieving stronger synergy between modalities. The method supports continually expanding support sets, and applies to fine-grained tasks such as personalized segmentation. Experiments show that we significantly narrow the gap between zero-shot and supervised segmentation while preserving open-vocabulary ability. Subjects: Computer Vision and Pattern Recognition (cs.CV) Cite as: arXiv:2602.23339 [cs.CV] (or arXiv:2602.23339v1 [cs.CV] for this version) https://doi.org/10.48550/arXiv.2602.23339 Focus to learn more arXiv-issued DOI via DataCite (pending registration) Submission history From: Tilemachos Aravanis [ view email ] [v1] Thu, 26 Feb 2026 18:45:33 UTC (10,381 KB) Full-text links: Access Paper: View a PDF of the paper titled Retrieve and Segment: Are a Few Examples Enough to Bridge the Supervision Gap in Open-Vocabulary Segmentation?, by Tilemachos Aravanis and 4 other authors View PDF TeX Source view license Current browse context: cs.CV &lt; prev | next &gt; new | recent | 2026-02 Change to browse by: cs References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-114">58. Real-Time Stream Compaction for Sparse Machine Learning on FPGAs</h2>
<ul>
<li>链接：https://arxiv.org/abs/2602.23281v1</li>
<li>来源：arxiv</li>
<li>摘要：Machine learning algorithms are being used more frequently in the first-level triggers in collider experiments, with Graph Neural Networks pushing the hardware requirements of FPGA-based triggers beyond the current state of the art. To meet the stringent demands of high-throughput and low-latency environments, we propose a concept for latency-optimized preprocessing of sparse sensor data, enabling efficient GNN hardware acceleration by removing dynamic input sparsity. Our approach rearranges data coming from a large number of First-In-First-Out interfaces, typically sensor frontends, to a smaller number of FIFO interfaces connected to a machine learning hardware accelerator. In order to achieve high throughput while minimizing the hardware utilization, we developed a hierarchical sparsity compression pipeline optimized for FPGAs. We implemented our concept in the Chisel design language as an open-source hardware generator. For demonstration, we implemented one configuration of our module as preprocessing stage in a GNN-based first-level trigger for the Electromagnetic Calorimeter inside the Belle II detector. Additionally we evaluate latency, throughput, resource utilization, and s</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-115">正文（抓取，非 AI）</h3>
<p>[2602.23281v1] Real-Time Stream Compaction for Sparse Machine Learning on FPGAs High Energy Physics - Experiment arXiv:2602.23281v1 (hep-ex) [Submitted on 26 Feb 2026] Title: Real-Time Stream Compaction for Sparse Machine Learning on FPGAs Authors: Marc Neu , Isabel Haide , Torben Ferber , Jürgen Becker View a PDF of the paper titled Real-Time Stream Compaction for Sparse Machine Learning on FPGAs, by Marc Neu and 3 other authors View PDF HTML (experimental) Abstract: Machine learning algorithms are being used more frequently in the first-level triggers in collider experiments, with Graph Neural Networks pushing the hardware requirements of FPGA-based triggers beyond the current state of the art. To meet the stringent demands of high-throughput and low-latency environments, we propose a concept for latency-optimized preprocessing of sparse sensor data, enabling efficient GNN hardware acceleration by removing dynamic input sparsity. Our approach rearranges data coming from a large number of First-In-First-Out interfaces, typically sensor frontends, to a smaller number of FIFO interfaces connected to a machine learning hardware accelerator. In order to achieve high throughput while minimizing the hardware utilization, we developed a hierarchical sparsity compression pipeline optimized for FPGAs. We implemented our concept in the Chisel design language as an open-source hardware generator. For demonstration, we implemented one configuration of our module as preprocessing stage in a GNN-based first-level trigger for the Electromagnetic Calorimeter inside the Belle II detector. Additionally we evaluate latency, throughput, resource utilization, and scalability for a wide range of parameters, to enable broader use for other large scale scientific experiments. Comments: 8 pages Subjects: High Energy Physics - Experiment (hep-ex) ; Hardware Architecture (cs.AR) Cite as: arXiv:2602.23281 [hep-ex] (or arXiv:2602.23281v1 [hep-ex] for this version) https://doi.org/10.48550/arXiv.2602.23281 Focus to learn more arXiv-issued DOI via DataCite (pending registration) Submission history From: Marc Neu [ view email ] [v1] Thu, 26 Feb 2026 17:54:32 UTC (3,321 KB) Full-text links: Access Paper: View a PDF of the paper titled Real-Time Stream Compaction for Sparse Machine Learning on FPGAs, by Marc Neu and 3 other authors View PDF HTML (experimental) TeX Source view license Current browse context: hep-ex &lt; prev | next &gt; new | recent | 2026-02 Change to browse by: cs cs.AR References &amp; Citations INSPIRE HEP NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-116">59. Non-Attainment of Minima in Non-Polyhedral Conic Optimization: A Robust SOCP Example</h2>
<ul>
<li>链接：https://arxiv.org/abs/2510.00318v2</li>
<li>来源：arxiv</li>
<li>摘要：A fundamental theorem of linear programming states that a feasible linear program is solvable if and only if its objective function is copositive with respect to the recession cone of its feasible set. This paper demonstrates that this crucial guarantee does not extend to Second-Order Cone Programs (SOCPs), a workhorse model in robust and convex optimization. We construct and analyze a rigorous counterexample derived from a robust linear optimization problem with ellipsoidal uncertainty. The resulting SOCP possesses a non-empty feasible set, a bounded objective, and an objective function that is copositive on its recession cone. Despite satisfying these classical conditions for solvability, the problem admits no optimal solution; its infimum is finite but unattainable. We trace this pathology directly to the non-polyhedral geometry of the second-order cone, which causes the image of the feasible set under the linear objective to be non-closed. We interpret the example explicitly within the context of robust optimization, discuss its significant practical implications for modeling and computation, and propose effective mitigation strategies via polyhedral approximation or regulariza</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-117">正文（抓取，非 AI）</h3>
<p>[2510.00318v2] Non-Attainment of Minima in Non-Polyhedral Conic Optimization: A Robust SOCP Example Mathematics &gt; Optimization and Control arXiv:2510.00318v2 (math) [Submitted on 30 Sep 2025 ( v1 ), last revised 30 Dec 2025 (this version, v2)] Title: Non-Attainment of Minima in Non-Polyhedral Conic Optimization: A Robust SOCP Example Authors: Vinh Nguyen View a PDF of the paper titled Non-Attainment of Minima in Non-Polyhedral Conic Optimization: A Robust SOCP Example, by Vinh Nguyen View PDF HTML (experimental) Abstract: A fundamental theorem of linear programming states that a feasible linear program is solvable if and only if its objective function is copositive with respect to the recession cone of its feasible set. This paper demonstrates that this crucial guarantee does not extend to Second-Order Cone Programs (SOCPs), a workhorse model in robust and convex optimization. We construct and analyze a rigorous counterexample derived from a robust linear optimization problem with ellipsoidal uncertainty. The resulting SOCP possesses a non-empty feasible set, a bounded objective, and an objective function that is copositive on its recession cone. Despite satisfying these classical conditions for solvability, the problem admits no optimal solution; its infimum is finite but unattainable. We trace this pathology directly to the non-polyhedral geometry of the second-order cone, which causes the image of the feasible set under the linear objective to be non-closed. We interpret the example explicitly within the context of robust optimization, discuss its significant practical implications for modeling and computation, and propose effective mitigation strategies via polyhedral approximation or regularization. Comments: minor change to title and adding some remarks Subjects: Optimization and Control (math.OC) MSC classes: 90C05, 90C17, 90C22, 90C25, 90C46 Cite as: arXiv:2510.00318 [math.OC] (or arXiv:2510.00318v2 [math.OC] for this version) https://doi.org/10.48550/arXiv.2510.00318 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Vinh Nguyen [ view email ] [v1] Tue, 30 Sep 2025 22:22:11 UTC (273 KB) [v2] Tue, 30 Dec 2025 18:35:43 UTC (274 KB) Full-text links: Access Paper: View a PDF of the paper titled Non-Attainment of Minima in Non-Polyhedral Conic Optimization: A Robust SOCP Example, by Vinh Nguyen View PDF HTML (experimental) TeX Source view license Current browse context: math.OC &lt; prev | next &gt; new | recent | 2025-10 Change to browse by: math References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-118">60. FlashOptim: Optimizers for Memory Efficient Training</h2>
<ul>
<li>链接：https://arxiv.org/abs/2602.23349v1</li>
<li>来源：arxiv</li>
<li>摘要：Standard mixed-precision training of neural networks requires many bytes of accelerator memory for each model parameter. These bytes reflect not just the parameter itself, but also its gradient and one or more optimizer state variables. With each of these values typically requiring 4 bytes, training even a 7 billion parameter model can be impractical for researchers with less than 100GB of accelerator memory.   We introduce FlashOptim, a suite of optimizations that reduces per-parameter memory by over 50% while preserving model quality and API compatibility. Our approach introduces two key techniques. First, we improve master weight splitting by finding and exploiting a tight bound on its quantization error. Second, we design companding functions that greatly reduce the error in 8-bit optimizer state quantization. Together with 16-bit gradients, these techniques reduce AdamW memory from 16 bytes to 7 bytes per parameter, or 5 bytes with gradient release. They also cut model checkpoint sizes by more than half.   Experiments with FlashOptim applied to SGD, AdamW, and Lion show no measurable quality degradation on any task from a collection of standard vision and language benchmarks, </li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-119">正文（抓取，非 AI）</h3>
<p>[2602.23349v1] FlashOptim: Optimizers for Memory Efficient Training Computer Science &gt; Machine Learning arXiv:2602.23349v1 (cs) [Submitted on 26 Feb 2026] Title: FlashOptim: Optimizers for Memory Efficient Training Authors: Jose Javier Gonzalez Ortiz , Abhay Gupta , Chris Renard , Davis Blalock View a PDF of the paper titled FlashOptim: Optimizers for Memory Efficient Training, by Jose Javier Gonzalez Ortiz and 3 other authors View PDF HTML (experimental) Abstract: Standard mixed-precision training of neural networks requires many bytes of accelerator memory for each model parameter. These bytes reflect not just the parameter itself, but also its gradient and one or more optimizer state variables. With each of these values typically requiring 4 bytes, training even a 7 billion parameter model can be impractical for researchers with less than 100GB of accelerator memory. We introduce FlashOptim, a suite of optimizations that reduces per-parameter memory by over 50% while preserving model quality and API compatibility. Our approach introduces two key techniques. First, we improve master weight splitting by finding and exploiting a tight bound on its quantization error. Second, we design companding functions that greatly reduce the error in 8-bit optimizer state quantization. Together with 16-bit gradients, these techniques reduce AdamW memory from 16 bytes to 7 bytes per parameter, or 5 bytes with gradient release. They also cut model checkpoint sizes by more than half. Experiments with FlashOptim applied to SGD, AdamW, and Lion show no measurable quality degradation on any task from a collection of standard vision and language benchmarks, including Llama-3.1-8B finetuning. Comments: Source code is available at this https URL Subjects: Machine Learning (cs.LG) ; Artificial Intelligence (cs.AI) Cite as: arXiv:2602.23349 [cs.LG] (or arXiv:2602.23349v1 [cs.LG] for this version) https://doi.org/10.48550/arXiv.2602.23349 Focus to learn more arXiv-issued DOI via DataCite (pending registration) Submission history From: Jose Javier Gonzalez Ortiz [ view email ] [v1] Thu, 26 Feb 2026 18:52:22 UTC (484 KB) Full-text links: Access Paper: View a PDF of the paper titled FlashOptim: Optimizers for Memory Efficient Training, by Jose Javier Gonzalez Ortiz and 3 other authors View PDF HTML (experimental) TeX Source view license Current browse context: cs.LG &lt; prev | next &gt; new | recent | 2026-02 Change to browse by: cs cs.AI References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) IArxiv recommender toggle IArxiv Recommender ( What is IArxiv? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-120">61. Physics Informed Viscous Value Representations</h2>
<ul>
<li>链接：https://arxiv.org/abs/2602.23280v1</li>
<li>来源：arxiv</li>
<li>摘要：Offline goal-conditioned reinforcement learning (GCRL) learns goal-conditioned policies from static pre-collected datasets. However, accurate value estimation remains a challenge due to the limited coverage of the state-action space. Recent physics-informed approaches have sought to address this by imposing physical and geometric constraints on the value function through regularization defined over first-order partial differential equations (PDEs), such as the Eikonal equation. However, these formulations can often be ill-posed in complex, high-dimensional environments. In this work, we propose a physics-informed regularization derived from the viscosity solution of the Hamilton-Jacobi-Bellman (HJB) equation. By providing a physics-based inductive bias, our approach grounds the learning process in optimal control theory, explicitly regularizing and bounding updates during value iterations. Furthermore, we leverage the Feynman-Kac theorem to recast the PDE solution as an expectation, enabling a tractable Monte Carlo estimation of the objective that avoids numerical instability in higher-order gradients. Experiments demonstrate that our method improves geometric consistency, making i</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-121">正文（抓取，非 AI）</h3>
<p>[2602.23280v1] Physics Informed Viscous Value Representations Computer Science &gt; Machine Learning arXiv:2602.23280v1 (cs) [Submitted on 26 Feb 2026] Title: Physics Informed Viscous Value Representations Authors: Hrishikesh Viswanath , Juanwu Lu , S. Talha Bukhari , Damon Conover , Ziran Wang , Aniket Bera View a PDF of the paper titled Physics Informed Viscous Value Representations, by Hrishikesh Viswanath and 5 other authors View PDF HTML (experimental) Abstract: Offline goal-conditioned reinforcement learning (GCRL) learns goal-conditioned policies from static pre-collected datasets. However, accurate value estimation remains a challenge due to the limited coverage of the state-action space. Recent physics-informed approaches have sought to address this by imposing physical and geometric constraints on the value function through regularization defined over first-order partial differential equations (PDEs), such as the Eikonal equation. However, these formulations can often be ill-posed in complex, high-dimensional environments. In this work, we propose a physics-informed regularization derived from the viscosity solution of the Hamilton-Jacobi-Bellman (HJB) equation. By providing a physics-based inductive bias, our approach grounds the learning process in optimal control theory, explicitly regularizing and bounding updates during value iterations. Furthermore, we leverage the Feynman-Kac theorem to recast the PDE solution as an expectation, enabling a tractable Monte Carlo estimation of the objective that avoids numerical instability in higher-order gradients. Experiments demonstrate that our method improves geometric consistency, making it broadly applicable to navigation and high-dimensional, complex manipulation tasks. Open-source codes are available at this https URL . Subjects: Machine Learning (cs.LG) ; Robotics (cs.RO) Cite as: arXiv:2602.23280 [cs.LG] (or arXiv:2602.23280v1 [cs.LG] for this version) https://doi.org/10.48550/arXiv.2602.23280 Focus to learn more arXiv-issued DOI via DataCite (pending registration) Submission history From: Hrishikesh Viswanath [ view email ] [v1] Thu, 26 Feb 2026 17:53:46 UTC (8,162 KB) Full-text links: Access Paper: View a PDF of the paper titled Physics Informed Viscous Value Representations, by Hrishikesh Viswanath and 5 other authors View PDF HTML (experimental) TeX Source view license Current browse context: cs.LG &lt; prev | next &gt; new | recent | 2026-02 Change to browse by: cs cs.RO References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) IArxiv recommender toggle IArxiv Recommender ( What is IArxiv? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-122">62. ParamMem: Augmenting Language Agents with Parametric Reflective Memory</h2>
<ul>
<li>链接：https://arxiv.org/abs/2602.23320v1</li>
<li>来源：arxiv</li>
<li>摘要：Self-reflection enables language agents to iteratively refine solutions, yet often produces repetitive outputs that limit reasoning performance. Recent studies have attempted to address this limitation through various approaches, among which increasing reflective diversity has shown promise. Our empirical analysis reveals a strong positive correlation between reflective diversity and task success, further motivating the need for diverse reflection signals. We introduce ParamMem, a parametric memory module that encodes cross-sample reflection patterns into model parameters, enabling diverse reflection generation through temperature-controlled sampling. Building on this module, we propose ParamAgent, a reflection-based agent framework that integrates parametric memory with episodic and cross-sample memory. Extensive experiments on code generation, mathematical reasoning, and multi-hop question answering demonstrate consistent improvements over state-of-the-art baselines. Further analysis reveals that ParamMem is sample-efficient, enables weak-to-strong transfer across model scales, and supports self-improvement without reliance on stronger external model, highlighting the potential o</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-123">正文（抓取，非 AI）</h3>
<p>[2602.23320v1] ParamMem: Augmenting Language Agents with Parametric Reflective Memory Computer Science &gt; Machine Learning arXiv:2602.23320v1 (cs) [Submitted on 26 Feb 2026] Title: ParamMem: Augmenting Language Agents with Parametric Reflective Memory Authors: Tianjun Yao , Yongqiang Chen , Yujia Zheng , Pan Li , Zhiqiang Shen , Kun Zhang View a PDF of the paper titled ParamMem: Augmenting Language Agents with Parametric Reflective Memory, by Tianjun Yao and 5 other authors View PDF HTML (experimental) Abstract: Self-reflection enables language agents to iteratively refine solutions, yet often produces repetitive outputs that limit reasoning performance. Recent studies have attempted to address this limitation through various approaches, among which increasing reflective diversity has shown promise. Our empirical analysis reveals a strong positive correlation between reflective diversity and task success, further motivating the need for diverse reflection signals. We introduce ParamMem, a parametric memory module that encodes cross-sample reflection patterns into model parameters, enabling diverse reflection generation through temperature-controlled sampling. Building on this module, we propose ParamAgent, a reflection-based agent framework that integrates parametric memory with episodic and cross-sample memory. Extensive experiments on code generation, mathematical reasoning, and multi-hop question answering demonstrate consistent improvements over state-of-the-art baselines. Further analysis reveals that ParamMem is sample-efficient, enables weak-to-strong transfer across model scales, and supports self-improvement without reliance on stronger external model, highlighting the potential of ParamMem as an effective component for enhancing language agents. Comments: 20 pages Subjects: Machine Learning (cs.LG) ; Multiagent Systems (cs.MA) ACM classes: I.2.6 Cite as: arXiv:2602.23320 [cs.LG] (or arXiv:2602.23320v1 [cs.LG] for this version) https://doi.org/10.48550/arXiv.2602.23320 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Tianjun Yao [ view email ] [v1] Thu, 26 Feb 2026 18:28:04 UTC (2,463 KB) Full-text links: Access Paper: View a PDF of the paper titled ParamMem: Augmenting Language Agents with Parametric Reflective Memory, by Tianjun Yao and 5 other authors View PDF HTML (experimental) TeX Source view license Current browse context: cs.LG &lt; prev | next &gt; new | recent | 2026-02 Change to browse by: cs cs.MA References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) IArxiv recommender toggle IArxiv Recommender ( What is IArxiv? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-124">63. Close-enough general routing problem for multiple unmanned aerial vehicles in monitoring missions</h2>
<ul>
<li>链接：https://arxiv.org/abs/2602.15841v1</li>
<li>来源：arxiv</li>
<li>摘要：In this paper, we introduce a close-enough multi-UAV general routing problem (CEMUAVGRP) where a fleet of homogeneous UAVs conduct monitoring tasks containing nodes, each of which has its disk neighborhood, and edges, aiming to minimize the total distance. A two-phase iterative method is proposed, partitioning the CEMUAVGRP into a general routing phase where a satisfactory route including required nodes and edges for each UAV is obtained without considering the disk neighborhoods of required nodes, and a close-enough routing phase where representative points are optimized for each required node in the determined route. To be specific, a variable neighborhood descent (VND) heuristic is proposed for the general routing phase, while a second-order cone programming (SOCP) procedure is applied in the close-enough routing phase. These two phases are performed in an iterative fashion under the framework of an adaptive iterated local search (AILS) algorithm until the predefined termination criteria are satisfied. Extensive experiments and comparative studies are conducted, demonstrating the efficiency of the proposed AILS-VND-SOCP algorithm and the superiority of disk neighborhoods.</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-125">正文（抓取，非 AI）</h3>
<p>[2602.15841v1] Close-enough general routing problem for multiple unmanned aerial vehicles in monitoring missions Electrical Engineering and Systems Science &gt; Systems and Control arXiv:2602.15841v1 (eess) [Submitted on 21 Jan 2026] Title: Close-enough general routing problem for multiple unmanned aerial vehicles in monitoring missions Authors: Huan Liu , Michel Gendreau , Binjie Xu , Guohua Wu , Yi Gu View a PDF of the paper titled Close-enough general routing problem for multiple unmanned aerial vehicles in monitoring missions, by Huan Liu and 4 other authors View PDF HTML (experimental) Abstract: In this paper, we introduce a close-enough multi-UAV general routing problem (CEMUAVGRP) where a fleet of homogeneous UAVs conduct monitoring tasks containing nodes, each of which has its disk neighborhood, and edges, aiming to minimize the total distance. A two-phase iterative method is proposed, partitioning the CEMUAVGRP into a general routing phase where a satisfactory route including required nodes and edges for each UAV is obtained without considering the disk neighborhoods of required nodes, and a close-enough routing phase where representative points are optimized for each required node in the determined route. To be specific, a variable neighborhood descent (VND) heuristic is proposed for the general routing phase, while a second-order cone programming (SOCP) procedure is applied in the close-enough routing phase. These two phases are performed in an iterative fashion under the framework of an adaptive iterated local search (AILS) algorithm until the predefined termination criteria are satisfied. Extensive experiments and comparative studies are conducted, demonstrating the efficiency of the proposed AILS-VND-SOCP algorithm and the superiority of disk neighborhoods. Subjects: Systems and Control (eess.SY) ; Networking and Internet Architecture (cs.NI); Optimization and Control (math.OC) Cite as: arXiv:2602.15841 [eess.SY] (or arXiv:2602.15841v1 [eess.SY] for this version) https://doi.org/10.48550/arXiv.2602.15841 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Huan Liu [ view email ] [v1] Wed, 21 Jan 2026 01:43:00 UTC (2,082 KB) Full-text links: Access Paper: View a PDF of the paper titled Close-enough general routing problem for multiple unmanned aerial vehicles in monitoring missions, by Huan Liu and 4 other authors View PDF HTML (experimental) TeX Source view license Current browse context: eess.SY &lt; prev | next &gt; new | recent | 2026-02 Change to browse by: cs cs.NI cs.SY eess math math.OC References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-126">64. Interface-Aware Trajectory Reconstruction of Limited Demonstrations for Robot Learning</h2>
<ul>
<li>链接：https://arxiv.org/abs/2602.23287v1</li>
<li>来源：arxiv</li>
<li>摘要：Assistive robots offer agency to humans with severe motor impairments. Often, these users control high-DoF robots through low-dimensional interfaces, such as using a 1-D sip-and-puff interface to operate a 6-DoF robotic arm. This mismatch results in having access to only a subset of control dimensions at a given time, imposing unintended and artificial constraints on robot motion. As a result, interface-limited demonstrations embed suboptimal motions that reflect interface restrictions rather than user intent. To address this, we present a trajectory reconstruction algorithm that reasons about task, environment, and interface constraints to lift demonstrations into the robot's full control space. We evaluate our approach using real-world demonstrations of ADL-inspired tasks performed via a 2-D joystick and 1-D sip-and-puff control interface, teleoperating two distinct 7-DoF robotic arms. Analyses of the reconstructed demonstrations and derived control policies show that lifted trajectories are faster and more efficient than their interface-constrained counterparts while respecting user preferences.</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-127">正文（抓取，非 AI）</h3>
<p>[2602.23287v1] Interface-Aware Trajectory Reconstruction of Limited Demonstrations for Robot Learning Computer Science &gt; Robotics arXiv:2602.23287v1 (cs) [Submitted on 26 Feb 2026] Title: Interface-Aware Trajectory Reconstruction of Limited Demonstrations for Robot Learning Authors: Demiana R. Barsoum , Mahdieh Nejati Javaremi , Larisa Y.C. Loke , Brenna D. Argall View a PDF of the paper titled Interface-Aware Trajectory Reconstruction of Limited Demonstrations for Robot Learning, by Demiana R. Barsoum and 3 other authors View PDF HTML (experimental) Abstract: Assistive robots offer agency to humans with severe motor impairments. Often, these users control high-DoF robots through low-dimensional interfaces, such as using a 1-D sip-and-puff interface to operate a 6-DoF robotic arm. This mismatch results in having access to only a subset of control dimensions at a given time, imposing unintended and artificial constraints on robot motion. As a result, interface-limited demonstrations embed suboptimal motions that reflect interface restrictions rather than user intent. To address this, we present a trajectory reconstruction algorithm that reasons about task, environment, and interface constraints to lift demonstrations into the robot's full control space. We evaluate our approach using real-world demonstrations of ADL-inspired tasks performed via a 2-D joystick and 1-D sip-and-puff control interface, teleoperating two distinct 7-DoF robotic arms. Analyses of the reconstructed demonstrations and derived control policies show that lifted trajectories are faster and more efficient than their interface-constrained counterparts while respecting user preferences. Comments: 13 pages, 8 figures, to appear in the proceedings of the 2026 Human-Robot Interaction (HRI) Conference Subjects: Robotics (cs.RO) Cite as: arXiv:2602.23287 [cs.RO] (or arXiv:2602.23287v1 [cs.RO] for this version) https://doi.org/10.48550/arXiv.2602.23287 Focus to learn more arXiv-issued DOI via DataCite (pending registration) Related DOI : https://doi.org/10.1145/3757279.3788671 Focus to learn more DOI(s) linking to related resources Submission history From: Demiana Barsoum [ view email ] [v1] Thu, 26 Feb 2026 18:01:25 UTC (4,278 KB) Full-text links: Access Paper: View a PDF of the paper titled Interface-Aware Trajectory Reconstruction of Limited Demonstrations for Robot Learning, by Demiana R. Barsoum and 3 other authors View PDF HTML (experimental) TeX Source view license Current browse context: cs.RO &lt; prev | next &gt; new | recent | 2026-02 Change to browse by: cs References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-128">65. Efficient Deep Learning for Short-Term Solar Irradiance Time Series Forecasting: A Benchmark Study in Ho Chi Minh City</h2>
<ul>
<li>链接：https://arxiv.org/abs/2512.23898v1</li>
<li>来源：arxiv</li>
<li>摘要：Reliable forecasting of Global Horizontal Irradiance (GHI) is essential for mitigating the variability of solar energy in power grids. This study presents a comprehensive benchmark of ten deep learning architectures for short-term (1-hour ahead) GHI time series forecasting in Ho Chi Minh City, leveraging high-resolution NSRDB satellite data (2011-2020) to compare established baselines (e.g. LSTM, TCN) against emerging state-of-the-art architectures, including Transformer, Informer, iTransformer, TSMixer, and Mamba. Experimental results identify the Transformer as the superior architecture, achieving the highest predictive accuracy with an R^2 of 0.9696. The study further utilizes SHAP analysis to contrast the temporal reasoning of these architectures, revealing that Transformers exhibit a strong "recency bias" focused on immediate atmospheric conditions, whereas Mamba explicitly leverages 24-hour periodic dependencies to inform predictions. Furthermore, we demonstrate that Knowledge Distillation can compress the high-performance Transformer by 23.5% while surprisingly reducing error (MAE: 23.78 W/m^2), offering a proven pathway for deploying sophisticated, low-latency forecasting o</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-129">正文（抓取，非 AI）</h3>
<p>[2512.23898v1] Efficient Deep Learning for Short-Term Solar Irradiance Time Series Forecasting: A Benchmark Study in Ho Chi Minh City Computer Science &gt; Machine Learning arXiv:2512.23898v1 (cs) [Submitted on 29 Dec 2025] Title: Efficient Deep Learning for Short-Term Solar Irradiance Time Series Forecasting: A Benchmark Study in Ho Chi Minh City Authors: Tin Hoang View a PDF of the paper titled Efficient Deep Learning for Short-Term Solar Irradiance Time Series Forecasting: A Benchmark Study in Ho Chi Minh City, by Tin Hoang View PDF HTML (experimental) Abstract: Reliable forecasting of Global Horizontal Irradiance (GHI) is essential for mitigating the variability of solar energy in power grids. This study presents a comprehensive benchmark of ten deep learning architectures for short-term (1-hour ahead) GHI time series forecasting in Ho Chi Minh City, leveraging high-resolution NSRDB satellite data (2011-2020) to compare established baselines (e.g. LSTM, TCN) against emerging state-of-the-art architectures, including Transformer, Informer, iTransformer, TSMixer, and Mamba. Experimental results identify the Transformer as the superior architecture, achieving the highest predictive accuracy with an R^2 of 0.9696. The study further utilizes SHAP analysis to contrast the temporal reasoning of these architectures, revealing that Transformers exhibit a strong "recency bias" focused on immediate atmospheric conditions, whereas Mamba explicitly leverages 24-hour periodic dependencies to inform predictions. Furthermore, we demonstrate that Knowledge Distillation can compress the high-performance Transformer by 23.5% while surprisingly reducing error (MAE: 23.78 W/m^2), offering a proven pathway for deploying sophisticated, low-latency forecasting on resource-constrained edge devices. Comments: preprint, 40 pages Subjects: Machine Learning (cs.LG) ; Artificial Intelligence (cs.AI) Cite as: arXiv:2512.23898 [cs.LG] (or arXiv:2512.23898v1 [cs.LG] for this version) https://doi.org/10.48550/arXiv.2512.23898 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Tin Hoang [ view email ] [v1] Mon, 29 Dec 2025 23:22:25 UTC (3,287 KB) Full-text links: Access Paper: View a PDF of the paper titled Efficient Deep Learning for Short-Term Solar Irradiance Time Series Forecasting: A Benchmark Study in Ho Chi Minh City, by Tin Hoang View PDF HTML (experimental) TeX Source view license Current browse context: cs.LG &lt; prev | next &gt; new | recent | 2025-12 Change to browse by: cs cs.AI References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) IArxiv recommender toggle IArxiv Recommender ( What is IArxiv? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-130">66. Lossless Compression: A New Benchmark for Time Series Model Evaluation</h2>
<ul>
<li>链接：https://arxiv.org/abs/2509.21002v1</li>
<li>来源：arxiv</li>
<li>摘要：The evaluation of time series models has traditionally focused on four canonical tasks: forecasting, imputation, anomaly detection, and classification. While these tasks have driven significant progress, they primarily assess task-specific performance and do not rigorously measure whether a model captures the full generative distribution of the data. We introduce lossless compression as a new paradigm for evaluating time series models, grounded in Shannon's source coding theorem. This perspective establishes a direct equivalence between optimal compression length and the negative log-likelihood, providing a strict and unified information-theoretic criterion for modeling capacity. Then We define a standardized evaluation protocol and metrics. We further propose and open-source a comprehensive evaluation framework TSCom-Bench, which enables the rapid adaptation of time series models as backbones for lossless compression. Experiments across diverse datasets on state-of-the-art models, including TimeXer, iTransformer, and PatchTST, demonstrate that compression reveals distributional weaknesses overlooked by classic benchmarks. These findings position lossless compression as a principle</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-131">正文（抓取，非 AI）</h3>
<p>[2509.21002v1] Lossless Compression: A New Benchmark for Time Series Model Evaluation Computer Science &gt; Machine Learning arXiv:2509.21002v1 (cs) [Submitted on 25 Sep 2025] Title: Lossless Compression: A New Benchmark for Time Series Model Evaluation Authors: Meng Wan , Benxi Tian , Jue Wang , Cui Hui , Ningming Nie , Tiantian Liu , Zongguo Wang , Cao Rongqiang , Peng Shi , Yangang Wang View a PDF of the paper titled Lossless Compression: A New Benchmark for Time Series Model Evaluation, by Meng Wan and 9 other authors View PDF HTML (experimental) Abstract: The evaluation of time series models has traditionally focused on four canonical tasks: forecasting, imputation, anomaly detection, and classification. While these tasks have driven significant progress, they primarily assess task-specific performance and do not rigorously measure whether a model captures the full generative distribution of the data. We introduce lossless compression as a new paradigm for evaluating time series models, grounded in Shannon's source coding theorem. This perspective establishes a direct equivalence between optimal compression length and the negative log-likelihood, providing a strict and unified information-theoretic criterion for modeling capacity. Then We define a standardized evaluation protocol and metrics. We further propose and open-source a comprehensive evaluation framework TSCom-Bench, which enables the rapid adaptation of time series models as backbones for lossless compression. Experiments across diverse datasets on state-of-the-art models, including TimeXer, iTransformer, and PatchTST, demonstrate that compression reveals distributional weaknesses overlooked by classic benchmarks. These findings position lossless compression as a principled task that complements and extends existing evaluation for time series modeling. Comments: 24 pages Subjects: Machine Learning (cs.LG) ; Artificial Intelligence (cs.AI) Cite as: arXiv:2509.21002 [cs.LG] (or arXiv:2509.21002v1 [cs.LG] for this version) https://doi.org/10.48550/arXiv.2509.21002 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Meng Wan [ view email ] [v1] Thu, 25 Sep 2025 10:52:48 UTC (3,041 KB) Full-text links: Access Paper: View a PDF of the paper titled Lossless Compression: A New Benchmark for Time Series Model Evaluation, by Meng Wan and 9 other authors View PDF HTML (experimental) TeX Source view license Current browse context: cs.LG &lt; prev | next &gt; new | recent | 2025-09 Change to browse by: cs cs.AI References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) IArxiv recommender toggle IArxiv Recommender ( What is IArxiv? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-132">67. Transformer-Based Modeling of User Interaction Sequences for Dwell Time Prediction in Human-Computer Interfaces</h2>
<ul>
<li>链接：https://arxiv.org/abs/2512.17149v1</li>
<li>来源：arxiv</li>
<li>摘要：This study investigates the task of dwell time prediction and proposes a Transformer framework based on interaction behavior modeling. The method first represents user interaction sequences on the interface by integrating dwell duration, click frequency, scrolling behavior, and contextual features, which are mapped into a unified latent space through embedding and positional encoding. On this basis, a multi-head self-attention mechanism is employed to capture long-range dependencies, while a feed-forward network performs deep nonlinear transformations to model the dynamic patterns of dwell time. Multiple comparative experiments are conducted with BILSTM, DRFormer, FedFormer, and iTransformer as baselines under the same conditions. The results show that the proposed method achieves the best performance in terms of MSE, RMSE, MAPE, and RMAE, and more accurately captures the complex patterns in interaction behavior. In addition, sensitivity experiments are carried out on hyperparameters and environments to examine the impact of the number of attention heads, sequence window length, and device environment on prediction performance, which further demonstrates the robustness and adaptabi</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-133">正文（抓取，非 AI）</h3>
<p>[2512.17149v1] Transformer-Based Modeling of User Interaction Sequences for Dwell Time Prediction in Human-Computer Interfaces Computer Science &gt; Human-Computer Interaction arXiv:2512.17149v1 (cs) [Submitted on 19 Dec 2025] Title: Transformer-Based Modeling of User Interaction Sequences for Dwell Time Prediction in Human-Computer Interfaces Authors: Rui Liu , Runsheng Zhang , Shixiao Wang View a PDF of the paper titled Transformer-Based Modeling of User Interaction Sequences for Dwell Time Prediction in Human-Computer Interfaces, by Rui Liu and 2 other authors View PDF Abstract: This study investigates the task of dwell time prediction and proposes a Transformer framework based on interaction behavior modeling. The method first represents user interaction sequences on the interface by integrating dwell duration, click frequency, scrolling behavior, and contextual features, which are mapped into a unified latent space through embedding and positional encoding. On this basis, a multi-head self-attention mechanism is employed to capture long-range dependencies, while a feed-forward network performs deep nonlinear transformations to model the dynamic patterns of dwell time. Multiple comparative experiments are conducted with BILSTM, DRFormer, FedFormer, and iTransformer as baselines under the same conditions. The results show that the proposed method achieves the best performance in terms of MSE, RMSE, MAPE, and RMAE, and more accurately captures the complex patterns in interaction behavior. In addition, sensitivity experiments are carried out on hyperparameters and environments to examine the impact of the number of attention heads, sequence window length, and device environment on prediction performance, which further demonstrates the robustness and adaptability of the method. Overall, this study provides a new solution for dwell time prediction from both theoretical and methodological perspectives and verifies its effectiveness in multiple aspects. Subjects: Human-Computer Interaction (cs.HC) Cite as: arXiv:2512.17149 [cs.HC] (or arXiv:2512.17149v1 [cs.HC] for this version) https://doi.org/10.48550/arXiv.2512.17149 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Rui Liu [ view email ] [v1] Fri, 19 Dec 2025 00:55:14 UTC (362 KB) Full-text links: Access Paper: View a PDF of the paper titled Transformer-Based Modeling of User Interaction Sequences for Dwell Time Prediction in Human-Computer Interfaces, by Rui Liu and 2 other authors View PDF view license Current browse context: cs.HC &lt; prev | next &gt; new | recent | 2025-12 Change to browse by: cs References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-134">68. ThinkOmni: Lifting Textual Reasoning to Omni-modal Scenarios via Guidance Decoding</h2>
<ul>
<li>链接：https://arxiv.org/abs/2602.23306v1</li>
<li>来源：arxiv</li>
<li>摘要：Omni-modal reasoning is essential for intelligent systems to understand and draw inferences from diverse data sources. While existing omni-modal large language models (OLLM) excel at perceiving diverse modalities, they lack the complex reasoning abilities of recent large reasoning models (LRM). However, enhancing the reasoning ability of OLLMs through additional training presents significant challenges, including the need for high-quality data, task-specific adaptation, and substantial computational costs. To address these limitations, we propose ThinkOmni, a training-free and data-free framework that lifts textual reasoning to omni-modal scenarios. ThinkOmni introduces two key components: 1) LRM-as-a-Guide, which leverages off-the-shelf LRMs to guide the OLLM decoding process; 2) Stepwise Contrastive Scaling, which adaptively balances perception and reasoning signals without manual hyperparameter tuning. Experiments on six multi-modal reasoning benchmarks demonstrate that ThinkOmni consistently delivers performance improvements, with main results achieving 70.2 on MathVista and 75.5 on MMAU. Overall, ThinkOmni offers a flexible and generalizable solution for omni-modal reasoning a</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-135">正文（抓取，非 AI）</h3>
<p>[2602.23306v1] ThinkOmni: Lifting Textual Reasoning to Omni-modal Scenarios via Guidance Decoding Computer Science &gt; Computer Vision and Pattern Recognition arXiv:2602.23306v1 (cs) [Submitted on 26 Feb 2026] Title: ThinkOmni: Lifting Textual Reasoning to Omni-modal Scenarios via Guidance Decoding Authors: Yiran Guan , Sifan Tu , Dingkang Liang , Linghao Zhu , Jianzhong Ju , Zhenbo Luo , Jian Luan , Yuliang Liu , Xiang Bai View a PDF of the paper titled ThinkOmni: Lifting Textual Reasoning to Omni-modal Scenarios via Guidance Decoding, by Yiran Guan and 8 other authors View PDF HTML (experimental) Abstract: Omni-modal reasoning is essential for intelligent systems to understand and draw inferences from diverse data sources. While existing omni-modal large language models (OLLM) excel at perceiving diverse modalities, they lack the complex reasoning abilities of recent large reasoning models (LRM). However, enhancing the reasoning ability of OLLMs through additional training presents significant challenges, including the need for high-quality data, task-specific adaptation, and substantial computational costs. To address these limitations, we propose ThinkOmni, a training-free and data-free framework that lifts textual reasoning to omni-modal scenarios. ThinkOmni introduces two key components: 1) LRM-as-a-Guide, which leverages off-the-shelf LRMs to guide the OLLM decoding process; 2) Stepwise Contrastive Scaling, which adaptively balances perception and reasoning signals without manual hyperparameter tuning. Experiments on six multi-modal reasoning benchmarks demonstrate that ThinkOmni consistently delivers performance improvements, with main results achieving 70.2 on MathVista and 75.5 on MMAU. Overall, ThinkOmni offers a flexible and generalizable solution for omni-modal reasoning and provides new insights into the generalization and application of reasoning capabilities. Comments: Accept by ICLR 2026 Subjects: Computer Vision and Pattern Recognition (cs.CV) Cite as: arXiv:2602.23306 [cs.CV] (or arXiv:2602.23306v1 [cs.CV] for this version) https://doi.org/10.48550/arXiv.2602.23306 Focus to learn more arXiv-issued DOI via DataCite (pending registration) Submission history From: Yiran Guan [ view email ] [v1] Thu, 26 Feb 2026 18:10:41 UTC (4,428 KB) Full-text links: Access Paper: View a PDF of the paper titled ThinkOmni: Lifting Textual Reasoning to Omni-modal Scenarios via Guidance Decoding, by Yiran Guan and 8 other authors View PDF HTML (experimental) TeX Source view license Current browse context: cs.CV &lt; prev | next &gt; new | recent | 2026-02 Change to browse by: cs References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-136">69. SOTAlign: Semi-Supervised Alignment of Unimodal Vision and Language Models via Optimal Transport</h2>
<ul>
<li>链接：https://arxiv.org/abs/2602.23353v1</li>
<li>来源：arxiv</li>
<li>摘要：The Platonic Representation Hypothesis posits that neural networks trained on different modalities converge toward a shared statistical model of the world. Recent work exploits this convergence by aligning frozen pretrained vision and language models with lightweight alignment layers, but typically relies on contrastive losses and millions of paired samples. In this work, we ask whether meaningful alignment can be achieved with substantially less supervision. We introduce a semi-supervised setting in which pretrained unimodal encoders are aligned using a small number of image-text pairs together with large amounts of unpaired data. To address this challenge, we propose SOTAlign, a two-stage framework that first recovers a coarse shared geometry from limited paired data using a linear teacher, then refines the alignment on unpaired samples via an optimal-transport-based divergence that transfers relational structure without overconstraining the target space. Unlike existing semi-supervised methods, SOTAlign effectively leverages unpaired images and text, learning robust joint embeddings across datasets and encoder pairs, and significantly outperforming supervised and semi-supervised</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-137">正文（抓取，非 AI）</h3>
<p>[2602.23353v1] SOTAlign: Semi-Supervised Alignment of Unimodal Vision and Language Models via Optimal Transport Computer Science &gt; Machine Learning arXiv:2602.23353v1 (cs) [Submitted on 26 Feb 2026] Title: SOTAlign: Semi-Supervised Alignment of Unimodal Vision and Language Models via Optimal Transport Authors: Simon Roschmann , Paul Krzakala , Sonia Mazelet , Quentin Bouniot , Zeynep Akata View a PDF of the paper titled SOTAlign: Semi-Supervised Alignment of Unimodal Vision and Language Models via Optimal Transport, by Simon Roschmann and 4 other authors View PDF HTML (experimental) Abstract: The Platonic Representation Hypothesis posits that neural networks trained on different modalities converge toward a shared statistical model of the world. Recent work exploits this convergence by aligning frozen pretrained vision and language models with lightweight alignment layers, but typically relies on contrastive losses and millions of paired samples. In this work, we ask whether meaningful alignment can be achieved with substantially less supervision. We introduce a semi-supervised setting in which pretrained unimodal encoders are aligned using a small number of image-text pairs together with large amounts of unpaired data. To address this challenge, we propose SOTAlign, a two-stage framework that first recovers a coarse shared geometry from limited paired data using a linear teacher, then refines the alignment on unpaired samples via an optimal-transport-based divergence that transfers relational structure without overconstraining the target space. Unlike existing semi-supervised methods, SOTAlign effectively leverages unpaired images and text, learning robust joint embeddings across datasets and encoder pairs, and significantly outperforming supervised and semi-supervised baselines. Comments: Preprint Subjects: Machine Learning (cs.LG) ; Artificial Intelligence (cs.AI) Cite as: arXiv:2602.23353 [cs.LG] (or arXiv:2602.23353v1 [cs.LG] for this version) https://doi.org/10.48550/arXiv.2602.23353 Focus to learn more arXiv-issued DOI via DataCite (pending registration) Submission history From: Simon Roschmann [ view email ] [v1] Thu, 26 Feb 2026 18:55:06 UTC (4,133 KB) Full-text links: Access Paper: View a PDF of the paper titled SOTAlign: Semi-Supervised Alignment of Unimodal Vision and Language Models via Optimal Transport, by Simon Roschmann and 4 other authors View PDF HTML (experimental) TeX Source view license Current browse context: cs.LG &lt; prev | next &gt; new | recent | 2026-02 Change to browse by: cs cs.AI References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) IArxiv recommender toggle IArxiv Recommender ( What is IArxiv? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-138">70. Mean Estimation from Coarse Data: Characterizations and Efficient Algorithms</h2>
<ul>
<li>链接：https://arxiv.org/abs/2602.23341v1</li>
<li>来源：arxiv</li>
<li>摘要：Coarse data arise when learners observe only partial information about samples; namely, a set containing the sample rather than its exact value. This occurs naturally through measurement rounding, sensor limitations, and lag in economic systems. We study Gaussian mean estimation from coarse data, where each true sample $x$ is drawn from a $d$-dimensional Gaussian distribution with identity covariance, but is revealed only through the set of a partition containing $x$. When the coarse samples, roughly speaking, have ``low'' information, the mean cannot be uniquely recovered from observed samples (i.e., the problem is not identifiable). Recent work by Fotakis, Kalavasis, Kontonis, and Tzamos [FKKT21] established that sample-efficient mean estimation is possible when the unknown mean is identifiable and the partition consists of only convex sets. Moreover, they showed that without convexity, mean estimation becomes NP-hard. However, two fundamental questions remained open: (1) When is the mean identifiable under convex partitions? (2) Is computationally efficient estimation possible under identifiability and convex partitions? This work resolves both questions. [...]</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-139">正文（抓取，非 AI）</h3>
<p>[2602.23341v1] Mean Estimation from Coarse Data: Characterizations and Efficient Algorithms Computer Science &gt; Machine Learning arXiv:2602.23341v1 (cs) [Submitted on 26 Feb 2026] Title: Mean Estimation from Coarse Data: Characterizations and Efficient Algorithms Authors: Alkis Kalavasis , Anay Mehrotra , Manolis Zampetakis , Felix Zhou , Ziyu Zhu View a PDF of the paper titled Mean Estimation from Coarse Data: Characterizations and Efficient Algorithms, by Alkis Kalavasis and 4 other authors View PDF HTML (experimental) Abstract: Coarse data arise when learners observe only partial information about samples; namely, a set containing the sample rather than its exact value. This occurs naturally through measurement rounding, sensor limitations, and lag in economic systems. We study Gaussian mean estimation from coarse data, where each true sample $x$ is drawn from a $d$-dimensional Gaussian distribution with identity covariance, but is revealed only through the set of a partition containing $x$. When the coarse samples, roughly speaking, have ``low'' information, the mean cannot be uniquely recovered from observed samples (i.e., the problem is not identifiable). Recent work by Fotakis, Kalavasis, Kontonis, and Tzamos [FKKT21] established that sample-efficient mean estimation is possible when the unknown mean is identifiable and the partition consists of only convex sets. Moreover, they showed that without convexity, mean estimation becomes NP-hard. However, two fundamental questions remained open: (1) When is the mean identifiable under convex partitions? (2) Is computationally efficient estimation possible under identifiability and convex partitions? This work resolves both questions. [...] Comments: Abstract truncated to arXiv limits. To appear in ICLR'26 Subjects: Machine Learning (cs.LG) ; Data Structures and Algorithms (cs.DS); Statistics Theory (math.ST); Machine Learning (stat.ML) Cite as: arXiv:2602.23341 [cs.LG] (or arXiv:2602.23341v1 [cs.LG] for this version) https://doi.org/10.48550/arXiv.2602.23341 Focus to learn more arXiv-issued DOI via DataCite (pending registration) Submission history From: Felix Zhou [ view email ] [v1] Thu, 26 Feb 2026 18:47:06 UTC (1,020 KB) Full-text links: Access Paper: View a PDF of the paper titled Mean Estimation from Coarse Data: Characterizations and Efficient Algorithms, by Alkis Kalavasis and 4 other authors View PDF HTML (experimental) TeX Source view license Current browse context: cs.LG &lt; prev | next &gt; new | recent | 2026-02 Change to browse by: cs cs.DS math math.ST stat stat.ML stat.TH References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) IArxiv recommender toggle IArxiv Recommender ( What is IArxiv? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-140">71. Discourse-Aware Dual-Track Streaming Response for Low-Latency Spoken Dialogue Systems</h2>
<ul>
<li>链接：https://arxiv.org/abs/2602.23266v1</li>
<li>来源：arxiv</li>
<li>摘要：Achieving human-like responsiveness is a critical yet challenging goal for cascaded spoken dialogue systems. Conventional ASR-LLM-TTS pipelines follow a strictly sequential paradigm, requiring complete transcription and full reasoning before speech synthesis can begin, which results in high response latency. We propose the Discourse-Aware Dual-Track Streaming Response (DDTSR) framework, a low-latency architecture that enables listen-while-thinking and speak-while-thinking. DDTSR is built upon three key mechanisms: (1) connective-guided small-large model synergy, where an auxiliary small model generates minimal-committal discourse connectives while a large model performs knowledge-intensive reasoning in parallel; (2) streaming-based cross-modal collaboration, which dynamically overlaps ASR, LLM inference, and TTS to advance the earliest speakable moment; and (3) curriculum-learning-based discourse continuity enhancement, which maintains coherence and logical consistency between early responses and subsequent reasoning outputs. Experiments on two spoken dialogue benchmarks demonstrate that DDTSR reduces response latency by 19%-51% while preserving discourse quality. Further analysis </li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-141">正文（抓取，非 AI）</h3>
<p>[2602.23266v1] Discourse-Aware Dual-Track Streaming Response for Low-Latency Spoken Dialogue Systems Computer Science &gt; Computation and Language arXiv:2602.23266v1 (cs) [Submitted on 26 Feb 2026] Title: Discourse-Aware Dual-Track Streaming Response for Low-Latency Spoken Dialogue Systems Authors: Siyuan Liu , Jiahui Xu , Feng Jiang , Kuang Wang , Zefeng Zhao , Chu-Ren Huang , Jinghang Gu , Changqing Yin , Haizhou Li View a PDF of the paper titled Discourse-Aware Dual-Track Streaming Response for Low-Latency Spoken Dialogue Systems, by Siyuan Liu and 8 other authors View PDF HTML (experimental) Abstract: Achieving human-like responsiveness is a critical yet challenging goal for cascaded spoken dialogue systems. Conventional ASR-LLM-TTS pipelines follow a strictly sequential paradigm, requiring complete transcription and full reasoning before speech synthesis can begin, which results in high response latency. We propose the Discourse-Aware Dual-Track Streaming Response (DDTSR) framework, a low-latency architecture that enables listen-while-thinking and speak-while-thinking. DDTSR is built upon three key mechanisms: (1) connective-guided small-large model synergy, where an auxiliary small model generates minimal-committal discourse connectives while a large model performs knowledge-intensive reasoning in parallel; (2) streaming-based cross-modal collaboration, which dynamically overlaps ASR, LLM inference, and TTS to advance the earliest speakable moment; and (3) curriculum-learning-based discourse continuity enhancement, which maintains coherence and logical consistency between early responses and subsequent reasoning outputs. Experiments on two spoken dialogue benchmarks demonstrate that DDTSR reduces response latency by 19%-51% while preserving discourse quality. Further analysis shows that DDTSR functions as a plug-and-play module compatible with diverse LLM backbones, and remains robust across varying utterance lengths, indicating strong practicality and scalability for real-time spoken interaction. Subjects: Computation and Language (cs.CL) Cite as: arXiv:2602.23266 [cs.CL] (or arXiv:2602.23266v1 [cs.CL] for this version) https://doi.org/10.48550/arXiv.2602.23266 Focus to learn more arXiv-issued DOI via DataCite (pending registration) Submission history From: Siyuan Liu [ view email ] [v1] Thu, 26 Feb 2026 17:39:56 UTC (1,012 KB) Full-text links: Access Paper: View a PDF of the paper titled Discourse-Aware Dual-Track Streaming Response for Low-Latency Spoken Dialogue Systems, by Siyuan Liu and 8 other authors View PDF HTML (experimental) TeX Source view license Current browse context: cs.CL &lt; prev | next &gt; new | recent | 2026-02 Change to browse by: cs References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-142">72. The Procrustean Bed of Time Series: The Optimization Bias of Point-wise Loss</h2>
<ul>
<li>链接：https://arxiv.org/abs/2512.18610v2</li>
<li>来源：arxiv</li>
<li>摘要：Optimizing time series models via point-wise loss functions (e.g., MSE) relying on a heuristic point-wise i.i.d. assumption disregards the causal temporal structure. Focusing on the core independence issue under covariance stationarity, this paper aims to provide a first-principles analysis of the Expectation of Optimization Bias (EOB). Our analysis reveals a fundamental paradigm paradox: The more deterministic and structured the time series, the more severe the bias incurred by point-wise loss function. We derive the first closed-form quantification for the non-deterministic EOB across linear and non-linear systems, and prove EOB is an intrinsic data property, governed exclusively by sequence length and the defined Structural Signal-to-Noise Ratio. This theoretical discovery motivates our principled debiasing program that eliminates the bias through sequence length reduction and structural orthogonalization. We present a concrete solution via DFT or DWT, and propose a novel harmonized $\ell_p$ norm framework to rectify gradient optimization pathologies of high-variance sequences. Extensive experiments validate EOB Theory's generality and the superior performance of debiasing progr</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-143">正文（抓取，非 AI）</h3>
<p>[2512.18610v2] The Procrustean Bed of Time Series: The Optimization Bias of Point-wise Loss Computer Science &gt; Machine Learning arXiv:2512.18610v2 (cs) [Submitted on 21 Dec 2025 ( v1 ), last revised 1 Feb 2026 (this version, v2)] Title: The Procrustean Bed of Time Series: The Optimization Bias of Point-wise Loss Authors: Rongyao Cai , Yuxi Wan , Kexin Zhang , Ming Jin , Hao Wang , Zhiqiang Ge , Daoyi Dong , Yong Liu , Qingsong Wen View a PDF of the paper titled The Procrustean Bed of Time Series: The Optimization Bias of Point-wise Loss, by Rongyao Cai and 8 other authors View PDF HTML (experimental) Abstract: Optimizing time series models via point-wise loss functions (e.g., MSE) relying on a heuristic point-wise i.i.d. assumption disregards the causal temporal structure. Focusing on the core independence issue under covariance stationarity, this paper aims to provide a first-principles analysis of the Expectation of Optimization Bias (EOB). Our analysis reveals a fundamental paradigm paradox: The more deterministic and structured the time series, the more severe the bias incurred by point-wise loss function. We derive the first closed-form quantification for the non-deterministic EOB across linear and non-linear systems, and prove EOB is an intrinsic data property, governed exclusively by sequence length and the defined Structural Signal-to-Noise Ratio. This theoretical discovery motivates our principled debiasing program that eliminates the bias through sequence length reduction and structural orthogonalization. We present a concrete solution via DFT or DWT, and propose a novel harmonized $\ell_p$ norm framework to rectify gradient optimization pathologies of high-variance sequences. Extensive experiments validate EOB Theory's generality and the superior performance of debiasing program, achieving 5.2% and 5.1% average improvement of MSE and MAE conducted on the iTransformer across 11 datasets, respectively. Comments: 54 pages Subjects: Machine Learning (cs.LG) Cite as: arXiv:2512.18610 [cs.LG] (or arXiv:2512.18610v2 [cs.LG] for this version) https://doi.org/10.48550/arXiv.2512.18610 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Rongyao Cai [ view email ] [v1] Sun, 21 Dec 2025 06:08:22 UTC (27,328 KB) [v2] Sun, 1 Feb 2026 14:34:29 UTC (34,944 KB) Full-text links: Access Paper: View a PDF of the paper titled The Procrustean Bed of Time Series: The Optimization Bias of Point-wise Loss, by Rongyao Cai and 8 other authors View PDF HTML (experimental) TeX Source view license Current browse context: cs.LG &lt; prev | next &gt; new | recent | 2025-12 Change to browse by: cs References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) IArxiv recommender toggle IArxiv Recommender ( What is IArxiv? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-144">73. PRIMA: Pre-training with Risk-integrated Image-Metadata Alignment for Medical Diagnosis via LLM</h2>
<ul>
<li>链接：https://arxiv.org/abs/2602.23297v1</li>
<li>来源：arxiv</li>
<li>摘要：Medical diagnosis requires the effective synthesis of visual manifestations and clinical metadata. However, existing methods often treat metadata as isolated tags, failing to exploit the rich semantic knowledge embedded in clinical descriptions. We propose PRIMA (Pre-training with Risk-integrated Image-Metadata Alignment), a framework that integrates domain-specific knowledge into multi-modal representation learning. We first curate an expert corpus of risk-disease correlations via Retrieval-Augmented Generation (RAG) to refine Clinical ModernBERT, embedding diagnostic priors into the text encoder. To bridge the modality gap, we introduce a dual-encoder pre-training strategy utilizing DINOv3 and our refined BERT, optimized by a suite of four complementary loss functions. These losses are designed to capture multi-granular semantic alignment and handle the ambiguity of clinical correlations through soft labels. Finally, we leverage Qwen-3 to fuse these aligned features for precise disease classification. Extensive experiments demonstrate that PRIMA effectively harmonizes pixel-level features with abstract clinical expertise, significantly outperforming other state-of-the-art methods</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-145">正文（抓取，非 AI）</h3>
<p>[2602.23297v1] PRIMA: Pre-training with Risk-integrated Image-Metadata Alignment for Medical Diagnosis via LLM Computer Science &gt; Computer Vision and Pattern Recognition arXiv:2602.23297v1 (cs) [Submitted on 26 Feb 2026] Title: PRIMA: Pre-training with Risk-integrated Image-Metadata Alignment for Medical Diagnosis via LLM Authors: Yiqing Wang , Chunming He , Ming-Chen Lu , Mercy Pawar , Leslie Niziol , Maria Woodward , Sina Farsiu View a PDF of the paper titled PRIMA: Pre-training with Risk-integrated Image-Metadata Alignment for Medical Diagnosis via LLM, by Yiqing Wang and 6 other authors View PDF HTML (experimental) Abstract: Medical diagnosis requires the effective synthesis of visual manifestations and clinical metadata. However, existing methods often treat metadata as isolated tags, failing to exploit the rich semantic knowledge embedded in clinical descriptions. We propose PRIMA (Pre-training with Risk-integrated Image-Metadata Alignment), a framework that integrates domain-specific knowledge into multi-modal representation learning. We first curate an expert corpus of risk-disease correlations via Retrieval-Augmented Generation (RAG) to refine Clinical ModernBERT, embedding diagnostic priors into the text encoder. To bridge the modality gap, we introduce a dual-encoder pre-training strategy utilizing DINOv3 and our refined BERT, optimized by a suite of four complementary loss functions. These losses are designed to capture multi-granular semantic alignment and handle the ambiguity of clinical correlations through soft labels. Finally, we leverage Qwen-3 to fuse these aligned features for precise disease classification. Extensive experiments demonstrate that PRIMA effectively harmonizes pixel-level features with abstract clinical expertise, significantly outperforming other state-of-the-art methods. Notably, our framework achieves superior robustness without the need for massive data collection or exhaustive computational resources. Our code will be made public upon acceptance. Subjects: Computer Vision and Pattern Recognition (cs.CV) Cite as: arXiv:2602.23297 [cs.CV] (or arXiv:2602.23297v1 [cs.CV] for this version) https://doi.org/10.48550/arXiv.2602.23297 Focus to learn more arXiv-issued DOI via DataCite (pending registration) Submission history From: Yiqing Wang [ view email ] [v1] Thu, 26 Feb 2026 18:07:52 UTC (521 KB) Full-text links: Access Paper: View a PDF of the paper titled PRIMA: Pre-training with Risk-integrated Image-Metadata Alignment for Medical Diagnosis via LLM, by Yiqing Wang and 6 other authors View PDF HTML (experimental) TeX Source view license Current browse context: cs.CV &lt; prev | next &gt; new | recent | 2026-02 Change to browse by: cs References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-146">74. Scale Can't Overcome Pragmatics: The Impact of Reporting Bias on Vision-Language Reasoning</h2>
<ul>
<li>链接：https://arxiv.org/abs/2602.23351v1</li>
<li>来源：arxiv</li>
<li>摘要：The lack of reasoning capabilities in Vision-Language Models (VLMs) has remained at the forefront of research discourse. We posit that this behavior stems from a reporting bias in their training data. That is, how people communicate about visual content by default omits tacit information needed to supervise some types of reasoning; e.g., "at the game today!" is a more likely caption than "a photo of 37 people standing behind a field". We investigate the data underlying the popular VLMs OpenCLIP, LLaVA-1.5 and Molmo through the lens of theories from pragmatics, and find that reporting bias results in insufficient representation of four reasoning skills (spatial, temporal, negation, and counting), despite the corpora being of web-scale, and/or synthetically generated. With a set of curated benchmarks, we demonstrate that: (i) VLMs perform poorly on the aforementioned types of reasoning suppressed in the training data by reporting bias; (ii) contrary to popular belief, scaling data size, model size, and to multiple languages does not result in emergence of these skills by default; but, promisingly, (iii) incorporating annotations specifically collected to obtain tacit information is e</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-147">正文（抓取，非 AI）</h3>
<p>[2602.23351v1] Scale Can't Overcome Pragmatics: The Impact of Reporting Bias on Vision-Language Reasoning Computer Science &gt; Computation and Language arXiv:2602.23351v1 (cs) [Submitted on 26 Feb 2026] Title: Scale Can't Overcome Pragmatics: The Impact of Reporting Bias on Vision-Language Reasoning Authors: Amita Kamath , Jack Hessel , Khyathi Chandu , Jena D. Hwang , Kai-Wei Chang , Ranjay Krishna View a PDF of the paper titled Scale Can't Overcome Pragmatics: The Impact of Reporting Bias on Vision-Language Reasoning, by Amita Kamath and 5 other authors View PDF HTML (experimental) Abstract: The lack of reasoning capabilities in Vision-Language Models (VLMs) has remained at the forefront of research discourse. We posit that this behavior stems from a reporting bias in their training data. That is, how people communicate about visual content by default omits tacit information needed to supervise some types of reasoning; e.g., "at the game today!" is a more likely caption than "a photo of 37 people standing behind a field". We investigate the data underlying the popular VLMs OpenCLIP, LLaVA-1.5 and Molmo through the lens of theories from pragmatics, and find that reporting bias results in insufficient representation of four reasoning skills (spatial, temporal, negation, and counting), despite the corpora being of web-scale, and/or synthetically generated. With a set of curated benchmarks, we demonstrate that: (i) VLMs perform poorly on the aforementioned types of reasoning suppressed in the training data by reporting bias; (ii) contrary to popular belief, scaling data size, model size, and to multiple languages does not result in emergence of these skills by default; but, promisingly, (iii) incorporating annotations specifically collected to obtain tacit information is effective. Our findings highlight the need for more intentional training data curation methods, rather than counting on scale for emergence of reasoning capabilities. Comments: TACL 2026 Subjects: Computation and Language (cs.CL) ; Computer Vision and Pattern Recognition (cs.CV) Cite as: arXiv:2602.23351 [cs.CL] (or arXiv:2602.23351v1 [cs.CL] for this version) https://doi.org/10.48550/arXiv.2602.23351 Focus to learn more arXiv-issued DOI via DataCite (pending registration) Submission history From: Amita Kamath [ view email ] [v1] Thu, 26 Feb 2026 18:54:06 UTC (15,345 KB) Full-text links: Access Paper: View a PDF of the paper titled Scale Can't Overcome Pragmatics: The Impact of Reporting Bias on Vision-Language Reasoning, by Amita Kamath and 5 other authors View PDF HTML (experimental) TeX Source view license Current browse context: cs.CL &lt; prev | next &gt; new | recent | 2026-02 Change to browse by: cs cs.CV References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-148">75. Why Diffusion Language Models Struggle with Truly Parallel (Non-Autoregressive) Decoding?</h2>
<ul>
<li>链接：https://arxiv.org/abs/2602.23225v1</li>
<li>来源：arxiv</li>
<li>摘要：Diffusion Language Models (DLMs) are often advertised as enabling parallel token generation, yet practical fast DLMs frequently converge to left-to-right, autoregressive (AR)-like decoding dynamics. In contrast, genuinely non-AR generation is promising because it removes AR's sequential bottleneck, better exploiting parallel hardware to reduce synchronization/communication overhead and improve latency scaling with output length. We argue that a primary driver of AR-like decoding is a mismatch between DLM objectives and the highly sequential structure of widely used training data, including standard pretraining corpora and long chain-of-thought (CoT) supervision. Motivated by this diagnosis, we propose NAP (Non-Autoregressive Parallel DLMs), a proof-of-concept, data-centric approach that better aligns supervision with non-AR parallel decoding. NAP curates examples as multiple independent reasoning trajectories and couples them with a parallel-forced decoding strategy that encourages multi-token parallel updates. Across math reasoning benchmarks, NAP yields stronger performance under parallel decoding than DLMs trained on standard long CoT data, with gains growing as parallelism incr</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-149">正文（抓取，非 AI）</h3>
<p>[2602.23225v1] Why Diffusion Language Models Struggle with Truly Parallel (Non-Autoregressive) Decoding? Computer Science &gt; Computation and Language arXiv:2602.23225v1 (cs) [Submitted on 26 Feb 2026] Title: Why Diffusion Language Models Struggle with Truly Parallel (Non-Autoregressive) Decoding? Authors: Pengxiang Li , Dilxat Muhtar , Lu Yin , Tianlong Chen , Shiwei Liu View a PDF of the paper titled Why Diffusion Language Models Struggle with Truly Parallel (Non-Autoregressive) Decoding?, by Pengxiang Li and 4 other authors View PDF HTML (experimental) Abstract: Diffusion Language Models (DLMs) are often advertised as enabling parallel token generation, yet practical fast DLMs frequently converge to left-to-right, autoregressive (AR)-like decoding dynamics. In contrast, genuinely non-AR generation is promising because it removes AR's sequential bottleneck, better exploiting parallel hardware to reduce synchronization/communication overhead and improve latency scaling with output length. We argue that a primary driver of AR-like decoding is a mismatch between DLM objectives and the highly sequential structure of widely used training data, including standard pretraining corpora and long chain-of-thought (CoT) supervision. Motivated by this diagnosis, we propose NAP (Non-Autoregressive Parallel DLMs), a proof-of-concept, data-centric approach that better aligns supervision with non-AR parallel decoding. NAP curates examples as multiple independent reasoning trajectories and couples them with a parallel-forced decoding strategy that encourages multi-token parallel updates. Across math reasoning benchmarks, NAP yields stronger performance under parallel decoding than DLMs trained on standard long CoT data, with gains growing as parallelism increases. Our results suggest that revisiting data and supervision is a principled direction for mitigating AR-like behavior and moving toward genuinely non-autoregressive parallel generation in DLMs. Our code is available at this https URL . Subjects: Computation and Language (cs.CL) ; Artificial Intelligence (cs.AI) Cite as: arXiv:2602.23225 [cs.CL] (or arXiv:2602.23225v1 [cs.CL] for this version) https://doi.org/10.48550/arXiv.2602.23225 Focus to learn more arXiv-issued DOI via DataCite (pending registration) Submission history From: Pengxiang Li [ view email ] [v1] Thu, 26 Feb 2026 17:04:57 UTC (782 KB) Full-text links: Access Paper: View a PDF of the paper titled Why Diffusion Language Models Struggle with Truly Parallel (Non-Autoregressive) Decoding?, by Pengxiang Li and 4 other authors View PDF HTML (experimental) TeX Source view license Current browse context: cs.CL &lt; prev | next &gt; new | recent | 2026-02 Change to browse by: cs cs.AI References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-150">76. Scaling Search Relevance: Augmenting App Store Ranking with LLM-Generated Judgments</h2>
<ul>
<li>链接：https://arxiv.org/abs/2602.23234v1</li>
<li>来源：arxiv</li>
<li>摘要：Large-scale commercial search systems optimize for relevance to drive successful sessions that help users find what they are looking for. To maximize relevance, we leverage two complementary objectives: behavioral relevance (results users tend to click or download) and textual relevance (a result's semantic fit to the query). A persistent challenge is the scarcity of expert-provided textual relevance labels relative to abundant behavioral relevance labels. We first address this by systematically evaluating LLM configurations, finding that a specialized, fine-tuned model significantly outperforms a much larger pre-trained one in providing highly relevant labels. Using this optimal model as a force multiplier, we generate millions of textual relevance labels to overcome the data scarcity. We show that augmenting our production ranker with these textual relevance labels leads to a significant outward shift of the Pareto frontier: offline NDCG improves for behavioral relevance while simultaneously increasing for textual relevance. These offline gains were validated by a worldwide A/B test on the App Store ranker, which demonstrated a statistically significant +0.24% increase in convers</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-151">正文（抓取，非 AI）</h3>
<p>[2602.23234v1] Scaling Search Relevance: Augmenting App Store Ranking with LLM-Generated Judgments Computer Science &gt; Information Retrieval arXiv:2602.23234v1 (cs) [Submitted on 26 Feb 2026] Title: Scaling Search Relevance: Augmenting App Store Ranking with LLM-Generated Judgments Authors: Evangelia Christakopoulou , Vivekkumar Patel , Hemanth Velaga , Sandip Gaikwad View a PDF of the paper titled Scaling Search Relevance: Augmenting App Store Ranking with LLM-Generated Judgments, by Evangelia Christakopoulou and 3 other authors View PDF HTML (experimental) Abstract: Large-scale commercial search systems optimize for relevance to drive successful sessions that help users find what they are looking for. To maximize relevance, we leverage two complementary objectives: behavioral relevance (results users tend to click or download) and textual relevance (a result's semantic fit to the query). A persistent challenge is the scarcity of expert-provided textual relevance labels relative to abundant behavioral relevance labels. We first address this by systematically evaluating LLM configurations, finding that a specialized, fine-tuned model significantly outperforms a much larger pre-trained one in providing highly relevant labels. Using this optimal model as a force multiplier, we generate millions of textual relevance labels to overcome the data scarcity. We show that augmenting our production ranker with these textual relevance labels leads to a significant outward shift of the Pareto frontier: offline NDCG improves for behavioral relevance while simultaneously increasing for textual relevance. These offline gains were validated by a worldwide A/B test on the App Store ranker, which demonstrated a statistically significant +0.24% increase in conversion rate, with the most substantial performance gains occurring in tail queries, where the new textual relevance labels provide a robust signal in the absence of reliable behavioral relevance labels. Subjects: Information Retrieval (cs.IR) ; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) Cite as: arXiv:2602.23234 [cs.IR] (or arXiv:2602.23234v1 [cs.IR] for this version) https://doi.org/10.48550/arXiv.2602.23234 Focus to learn more arXiv-issued DOI via DataCite (pending registration) Submission history From: Hemanth Velaga [ view email ] [v1] Thu, 26 Feb 2026 17:11:26 UTC (50 KB) Full-text links: Access Paper: View a PDF of the paper titled Scaling Search Relevance: Augmenting App Store Ranking with LLM-Generated Judgments, by Evangelia Christakopoulou and 3 other authors View PDF HTML (experimental) TeX Source view license Current browse context: cs.IR &lt; prev | next &gt; new | recent | 2026-02 Change to browse by: cs cs.AI cs.LG References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-152">77. Conformalized Neural Networks for Federated Uncertainty Quantification under Dual Heterogeneity</h2>
<ul>
<li>链接：https://arxiv.org/abs/2602.23296v1</li>
<li>来源：arxiv</li>
<li>摘要：Federated learning (FL) faces challenges in uncertainty quantification (UQ). Without reliable UQ, FL systems risk deploying overconfident models at under-resourced agents, leading to silent local failures despite seemingly satisfactory global performance. Existing federated UQ approaches often address data heterogeneity or model heterogeneity in isolation, overlooking their joint effect on coverage reliability across agents. Conformal prediction is a widely used distribution-free UQ framework, yet its applications in heterogeneous FL settings remains underexplored. We provide FedWQ-CP, a simple yet effective approach that balances empirical coverage performance with efficiency at both global and agent levels under the dual heterogeneity. FedWQ-CP performs agent-server calibration in a single communication round. On each agent, conformity scores are computed on calibration data and a local quantile threshold is derived. Each agent then transmits only its quantile threshold and calibration sample size to the server. The server simply aggregates these thresholds through a weighted average to produce a global threshold. Experimental results on seven public datasets for both classificat</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-153">正文（抓取，非 AI）</h3>
<p>[2602.23296v1] Conformalized Neural Networks for Federated Uncertainty Quantification under Dual Heterogeneity Computer Science &gt; Machine Learning arXiv:2602.23296v1 (cs) [Submitted on 26 Feb 2026] Title: Conformalized Neural Networks for Federated Uncertainty Quantification under Dual Heterogeneity Authors: Quang-Huy Nguyen , Jiaqi Wang , Wei-Shinn Ku View a PDF of the paper titled Conformalized Neural Networks for Federated Uncertainty Quantification under Dual Heterogeneity, by Quang-Huy Nguyen and Jiaqi Wang and Wei-Shinn Ku View PDF HTML (experimental) Abstract: Federated learning (FL) faces challenges in uncertainty quantification (UQ). Without reliable UQ, FL systems risk deploying overconfident models at under-resourced agents, leading to silent local failures despite seemingly satisfactory global performance. Existing federated UQ approaches often address data heterogeneity or model heterogeneity in isolation, overlooking their joint effect on coverage reliability across agents. Conformal prediction is a widely used distribution-free UQ framework, yet its applications in heterogeneous FL settings remains underexplored. We provide FedWQ-CP, a simple yet effective approach that balances empirical coverage performance with efficiency at both global and agent levels under the dual heterogeneity. FedWQ-CP performs agent-server calibration in a single communication round. On each agent, conformity scores are computed on calibration data and a local quantile threshold is derived. Each agent then transmits only its quantile threshold and calibration sample size to the server. The server simply aggregates these thresholds through a weighted average to produce a global threshold. Experimental results on seven public datasets for both classification and regression demonstrate that FedWQ-CP empirically maintains agent-wise and global coverage while producing the smallest prediction sets or intervals. Subjects: Machine Learning (cs.LG) ; Artificial Intelligence (cs.AI) Cite as: arXiv:2602.23296 [cs.LG] (or arXiv:2602.23296v1 [cs.LG] for this version) https://doi.org/10.48550/arXiv.2602.23296 Focus to learn more arXiv-issued DOI via DataCite (pending registration) Submission history From: Quang Huy Nguyen [ view email ] [v1] Thu, 26 Feb 2026 18:07:45 UTC (95 KB) Full-text links: Access Paper: View a PDF of the paper titled Conformalized Neural Networks for Federated Uncertainty Quantification under Dual Heterogeneity, by Quang-Huy Nguyen and Jiaqi Wang and Wei-Shinn Ku View PDF HTML (experimental) TeX Source view license Current browse context: cs.LG &lt; prev | next &gt; new | recent | 2026-02 Change to browse by: cs cs.AI References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) IArxiv recommender toggle IArxiv Recommender ( What is IArxiv? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-154">78. AgentDropoutV2: Optimizing Information Flow in Multi-Agent Systems via Test-Time Rectify-or-Reject Pruning</h2>
<ul>
<li>链接：https://arxiv.org/abs/2602.23258v1</li>
<li>来源：arxiv</li>
<li>摘要：While Multi-Agent Systems (MAS) excel in complex reasoning, they suffer from the cascading impact of erroneous information generated by individual participants. Current solutions often resort to rigid structural engineering or expensive fine-tuning, limiting their deployability and adaptability. We propose AgentDropoutV2, a test-time rectify-or-reject pruning framework designed to dynamically optimize MAS information flow without retraining. Our approach acts as an active firewall, intercepting agent outputs and employing a retrieval-augmented rectifier to iteratively correct errors based on a failure-driven indicator pool. This mechanism allows for the precise identification of potential errors using distilled failure patterns as prior knowledge. Irreparable outputs are subsequently pruned to prevent error propagation, while a fallback strategy preserves system integrity. Empirical results on extensive math benchmarks show that AgentDropoutV2 significantly boosts the MAS's task performance, achieving an average accuracy gain of 6.3 percentage points on math benchmarks. Furthermore, the system exhibits robust generalization and adaptivity, dynamically modulating rectification effor</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-155">正文（抓取，非 AI）</h3>
<p>[2602.23258v1] AgentDropoutV2: Optimizing Information Flow in Multi-Agent Systems via Test-Time Rectify-or-Reject Pruning Computer Science &gt; Artificial Intelligence arXiv:2602.23258v1 (cs) [Submitted on 26 Feb 2026] Title: AgentDropoutV2: Optimizing Information Flow in Multi-Agent Systems via Test-Time Rectify-or-Reject Pruning Authors: Yutong Wang , Siyuan Xiong , Xuebo Liu , Wenkang Zhou , Liang Ding , Miao Zhang , Min Zhang View a PDF of the paper titled AgentDropoutV2: Optimizing Information Flow in Multi-Agent Systems via Test-Time Rectify-or-Reject Pruning, by Yutong Wang and 6 other authors View PDF HTML (experimental) Abstract: While Multi-Agent Systems (MAS) excel in complex reasoning, they suffer from the cascading impact of erroneous information generated by individual participants. Current solutions often resort to rigid structural engineering or expensive fine-tuning, limiting their deployability and adaptability. We propose AgentDropoutV2, a test-time rectify-or-reject pruning framework designed to dynamically optimize MAS information flow without retraining. Our approach acts as an active firewall, intercepting agent outputs and employing a retrieval-augmented rectifier to iteratively correct errors based on a failure-driven indicator pool. This mechanism allows for the precise identification of potential errors using distilled failure patterns as prior knowledge. Irreparable outputs are subsequently pruned to prevent error propagation, while a fallback strategy preserves system integrity. Empirical results on extensive math benchmarks show that AgentDropoutV2 significantly boosts the MAS's task performance, achieving an average accuracy gain of 6.3 percentage points on math benchmarks. Furthermore, the system exhibits robust generalization and adaptivity, dynamically modulating rectification efforts based on task difficulty while leveraging context-aware indicators to resolve a wide spectrum of error patterns. Our code and dataset are released at this https URL . Subjects: Artificial Intelligence (cs.AI) ; Computation and Language (cs.CL) Cite as: arXiv:2602.23258 [cs.AI] (or arXiv:2602.23258v1 [cs.AI] for this version) https://doi.org/10.48550/arXiv.2602.23258 Focus to learn more arXiv-issued DOI via DataCite (pending registration) Submission history From: Yutong Wang [ view email ] [v1] Thu, 26 Feb 2026 17:31:43 UTC (626 KB) Full-text links: Access Paper: View a PDF of the paper titled AgentDropoutV2: Optimizing Information Flow in Multi-Agent Systems via Test-Time Rectify-or-Reject Pruning, by Yutong Wang and 6 other authors View PDF HTML (experimental) TeX Source view license Current browse context: cs.AI &lt; prev | next &gt; new | recent | 2026-02 Change to browse by: cs cs.CL References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-156">79. A Mixture-of-Experts Model for Multimodal Emotion Recognition in Conversations</h2>
<ul>
<li>链接：https://arxiv.org/abs/2602.23300v1</li>
<li>来源：arxiv</li>
<li>摘要：Emotion Recognition in Conversations (ERC) presents unique challenges, requiring models to capture the temporal flow of multi-turn dialogues and to effectively integrate cues from multiple modalities. We propose Mixture of Speech-Text Experts for Recognition of Emotions (MiSTER-E), a modular Mixture-of-Experts (MoE) framework designed to decouple two core challenges in ERC: modality-specific context modeling and multimodal information fusion. MiSTER-E leverages large language models (LLMs) fine-tuned for both speech and text to provide rich utterance-level embeddings, which are then enhanced through a convolutional-recurrent context modeling layer. The system integrates predictions from three experts-speech-only, text-only, and cross-modal-using a learned gating mechanism that dynamically weighs their outputs. To further encourage consistency and alignment across modalities, we introduce a supervised contrastive loss between paired speech-text representations and a KL-divergence-based regulariza-tion across expert predictions. Importantly, MiSTER-E does not rely on speaker identity at any stage. Experiments on three benchmark datasets-IEMOCAP, MELD, and MOSI-show that our proposal </li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-157">正文（抓取，非 AI）</h3>
<p>[2602.23300v1] A Mixture-of-Experts Model for Multimodal Emotion Recognition in Conversations Computer Science &gt; Computation and Language arXiv:2602.23300v1 (cs) [Submitted on 26 Feb 2026] Title: A Mixture-of-Experts Model for Multimodal Emotion Recognition in Conversations Authors: Soumya Dutta , Smruthi Balaji , Sriram Ganapathy View a PDF of the paper titled A Mixture-of-Experts Model for Multimodal Emotion Recognition in Conversations, by Soumya Dutta and 2 other authors View PDF HTML (experimental) Abstract: Emotion Recognition in Conversations (ERC) presents unique challenges, requiring models to capture the temporal flow of multi-turn dialogues and to effectively integrate cues from multiple modalities. We propose Mixture of Speech-Text Experts for Recognition of Emotions (MiSTER-E), a modular Mixture-of-Experts (MoE) framework designed to decouple two core challenges in ERC: modality-specific context modeling and multimodal information fusion. MiSTER-E leverages large language models (LLMs) fine-tuned for both speech and text to provide rich utterance-level embeddings, which are then enhanced through a convolutional-recurrent context modeling layer. The system integrates predictions from three experts-speech-only, text-only, and cross-modal-using a learned gating mechanism that dynamically weighs their outputs. To further encourage consistency and alignment across modalities, we introduce a supervised contrastive loss between paired speech-text representations and a KL-divergence-based regulariza-tion across expert predictions. Importantly, MiSTER-E does not rely on speaker identity at any stage. Experiments on three benchmark datasets-IEMOCAP, MELD, and MOSI-show that our proposal achieves 70.9%, 69.5%, and 87.9% weighted F1-scores respectively, outperforming several baseline speech-text ERC systems. We also provide various ablations to highlight the contributions made in the proposed approach. Comments: Accepted to Elsevier Computer Speech and Language. 30 pages, 9 figures, 5 tables Subjects: Computation and Language (cs.CL) ; Audio and Speech Processing (eess.AS) Cite as: arXiv:2602.23300 [cs.CL] (or arXiv:2602.23300v1 [cs.CL] for this version) https://doi.org/10.48550/arXiv.2602.23300 Focus to learn more arXiv-issued DOI via DataCite (pending registration) Submission history From: Soumya Dutta Mr [ view email ] [v1] Thu, 26 Feb 2026 18:08:40 UTC (7,797 KB) Full-text links: Access Paper: View a PDF of the paper titled A Mixture-of-Experts Model for Multimodal Emotion Recognition in Conversations, by Soumya Dutta and 2 other authors View PDF HTML (experimental) TeX Source view license Current browse context: cs.CL &lt; prev | next &gt; new | recent | 2026-02 Change to browse by: cs eess eess.AS References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-158">80. Model Agreement via Anchoring</h2>
<ul>
<li>链接：https://arxiv.org/abs/2602.23360v1</li>
<li>来源：arxiv</li>
<li>摘要：Numerous lines of aim to control $\textit{model disagreement}$ -- the extent to which two machine learning models disagree in their predictions. We adopt a simple and standard notion of model disagreement in real-valued prediction problems, namely the expected squared difference in predictions between two models trained on independent samples, without any coordination of the training processes. We would like to be able to drive disagreement to zero with some natural parameter(s) of the training procedure using analyses that can be applied to existing training methodologies.   We develop a simple general technique for proving bounds on independent model disagreement based on $\textit{anchoring}$ to the average of two models within the analysis. We then apply this technique to prove disagreement bounds for four commonly used machine learning algorithms: (1) stacked aggregation over an arbitrary model class (where disagreement is driven to 0 with the number of models $k$ being stacked) (2) gradient boosting (where disagreement is driven to 0 with the number of iterations $k$) (3) neural network training with architecture search (where disagreement is driven to 0 with the size $n$ of t</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-159">正文（抓取，非 AI）</h3>
<p>[2602.23360v1] Model Agreement via Anchoring Computer Science &gt; Machine Learning arXiv:2602.23360v1 (cs) [Submitted on 26 Feb 2026] Title: Model Agreement via Anchoring Authors: Eric Eaton , Surbhi Goel , Marcel Hussing , Michael Kearns , Aaron Roth , Sikata Bela Sengupta , Jessica Sorrell View a PDF of the paper titled Model Agreement via Anchoring, by Eric Eaton and 6 other authors View PDF HTML (experimental) Abstract: Numerous lines of aim to control $\textit{model disagreement}$ -- the extent to which two machine learning models disagree in their predictions. We adopt a simple and standard notion of model disagreement in real-valued prediction problems, namely the expected squared difference in predictions between two models trained on independent samples, without any coordination of the training processes. We would like to be able to drive disagreement to zero with some natural parameter(s) of the training procedure using analyses that can be applied to existing training methodologies. We develop a simple general technique for proving bounds on independent model disagreement based on $\textit{anchoring}$ to the average of two models within the analysis. We then apply this technique to prove disagreement bounds for four commonly used machine learning algorithms: (1) stacked aggregation over an arbitrary model class (where disagreement is driven to 0 with the number of models $k$ being stacked) (2) gradient boosting (where disagreement is driven to 0 with the number of iterations $k$) (3) neural network training with architecture search (where disagreement is driven to 0 with the size $n$ of the architecture being optimized over) and (4) regression tree training over all regression trees of fixed depth (where disagreement is driven to 0 with the depth $d$ of the tree architecture). For clarity, we work out our initial bounds in the setting of one-dimensional regression with squared error loss -- but then show that all of our results generalize to multi-dimensional regression with any strongly convex loss. Subjects: Machine Learning (cs.LG) ; Artificial Intelligence (cs.AI) Cite as: arXiv:2602.23360 [cs.LG] (or arXiv:2602.23360v1 [cs.LG] for this version) https://doi.org/10.48550/arXiv.2602.23360 Focus to learn more arXiv-issued DOI via DataCite (pending registration) Submission history From: Sikata Sengupta [ view email ] [v1] Thu, 26 Feb 2026 18:59:32 UTC (44 KB) Full-text links: Access Paper: View a PDF of the paper titled Model Agreement via Anchoring, by Eric Eaton and 6 other authors View PDF HTML (experimental) TeX Source view license Current browse context: cs.LG &lt; prev | next &gt; new | recent | 2026-02 Change to browse by: cs cs.AI References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) IArxiv recommender toggle IArxiv Recommender ( What is IArxiv? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-160">81. Decomposing Private Image Generation via Coarse-to-Fine Wavelet Modeling</h2>
<ul>
<li>链接：https://arxiv.org/abs/2602.23262v1</li>
<li>来源：arxiv</li>
<li>摘要：Generative models trained on sensitive image datasets risk memorizing and reproducing individual training examples, making strong privacy guarantees essential. While differential privacy (DP) provides a principled framework for such guarantees, standard DP finetuning (e.g., with DP-SGD) often results in severe degradation of image quality, particularly in high-frequency textures, due to the indiscriminate addition of noise across all model parameters. In this work, we propose a spectral DP framework based on the hypothesis that the most privacy-sensitive portions of an image are often low-frequency components in the wavelet space (e.g., facial features and object shapes) while high-frequency components are largely generic and public. Based on this hypothesis, we propose the following two-stage framework for DP image generation with coarse image intermediaries: (1) DP finetune an autoregressive spectral image tokenizer model on the low-resolution wavelet coefficients of the sensitive images, and (2) perform high-resolution upsampling using a publicly pretrained super-resolution model. By restricting the privacy budget to the global structures of the image in the first stage, and lev</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-161">正文（抓取，非 AI）</h3>
<p>[2602.23262v1] Decomposing Private Image Generation via Coarse-to-Fine Wavelet Modeling Computer Science &gt; Computer Vision and Pattern Recognition arXiv:2602.23262v1 (cs) [Submitted on 26 Feb 2026] Title: Decomposing Private Image Generation via Coarse-to-Fine Wavelet Modeling Authors: Jasmine Bayrooti , Weiwei Kong , Natalia Ponomareva , Carlos Esteves , Ameesh Makadia , Amanda Prorok View a PDF of the paper titled Decomposing Private Image Generation via Coarse-to-Fine Wavelet Modeling, by Jasmine Bayrooti and 5 other authors View PDF HTML (experimental) Abstract: Generative models trained on sensitive image datasets risk memorizing and reproducing individual training examples, making strong privacy guarantees essential. While differential privacy (DP) provides a principled framework for such guarantees, standard DP finetuning (e.g., with DP-SGD) often results in severe degradation of image quality, particularly in high-frequency textures, due to the indiscriminate addition of noise across all model parameters. In this work, we propose a spectral DP framework based on the hypothesis that the most privacy-sensitive portions of an image are often low-frequency components in the wavelet space (e.g., facial features and object shapes) while high-frequency components are largely generic and public. Based on this hypothesis, we propose the following two-stage framework for DP image generation with coarse image intermediaries: (1) DP finetune an autoregressive spectral image tokenizer model on the low-resolution wavelet coefficients of the sensitive images, and (2) perform high-resolution upsampling using a publicly pretrained super-resolution model. By restricting the privacy budget to the global structures of the image in the first stage, and leveraging the post-processing property of DP for detail refinement, we achieve promising trade-offs between privacy and utility. Experiments on the MS-COCO and MM-CelebA-HQ datasets show that our method generates images with improved quality and style capture relative to other leading DP image frameworks. Subjects: Computer Vision and Pattern Recognition (cs.CV) ; Cryptography and Security (cs.CR) Cite as: arXiv:2602.23262 [cs.CV] (or arXiv:2602.23262v1 [cs.CV] for this version) https://doi.org/10.48550/arXiv.2602.23262 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Jasmine Bayrooti [ view email ] [v1] Thu, 26 Feb 2026 17:36:48 UTC (10,320 KB) Full-text links: Access Paper: View a PDF of the paper titled Decomposing Private Image Generation via Coarse-to-Fine Wavelet Modeling, by Jasmine Bayrooti and 5 other authors View PDF HTML (experimental) TeX Source view license Current browse context: cs.CV &lt; prev | next &gt; new | recent | 2026-02 Change to browse by: cs cs.CR References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-162">82. LineGraph2Road: Structural Graph Reasoning on Line Graphs for Road Network Extraction</h2>
<ul>
<li>链接：https://arxiv.org/abs/2602.23290v1</li>
<li>来源：arxiv</li>
<li>摘要：The accurate and automatic extraction of roads from satellite imagery is critical for applications in navigation and urban planning, significantly reducing the need for manual annotation. Many existing methods decompose this task into keypoint extraction and connectedness prediction, but often struggle to capture long-range dependencies and complex topologies. Here, we propose LineGraph2Road, a framework that improves connectedness prediction by formulating it as binary classification over edges in a constructed global but sparse Euclidean graph, where nodes are keypoints extracted from segmentation masks and edges connect node pairs within a predefined distance threshold, representing potential road segments. To better learn structural link representation, we transform the original graph into its corresponding line graph and apply a Graph Transformer on it for connectedness prediction. This formulation overcomes the limitations of endpoint-embedding fusion on set-isomorphic links, enabling rich link representations and effective relational reasoning over the global structure. Additionally, we introduce an overpass/underpass head to resolve multi-level crossings and a coupled NMS s</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-163">正文（抓取，非 AI）</h3>
<p>[2602.23290v1] LineGraph2Road: Structural Graph Reasoning on Line Graphs for Road Network Extraction Computer Science &gt; Computer Vision and Pattern Recognition arXiv:2602.23290v1 (cs) [Submitted on 26 Feb 2026] Title: LineGraph2Road: Structural Graph Reasoning on Line Graphs for Road Network Extraction Authors: Zhengyang Wei , Renzhi Jing , Yiyi He , Jenny Suckale View a PDF of the paper titled LineGraph2Road: Structural Graph Reasoning on Line Graphs for Road Network Extraction, by Zhengyang Wei and 3 other authors View PDF HTML (experimental) Abstract: The accurate and automatic extraction of roads from satellite imagery is critical for applications in navigation and urban planning, significantly reducing the need for manual annotation. Many existing methods decompose this task into keypoint extraction and connectedness prediction, but often struggle to capture long-range dependencies and complex topologies. Here, we propose LineGraph2Road, a framework that improves connectedness prediction by formulating it as binary classification over edges in a constructed global but sparse Euclidean graph, where nodes are keypoints extracted from segmentation masks and edges connect node pairs within a predefined distance threshold, representing potential road segments. To better learn structural link representation, we transform the original graph into its corresponding line graph and apply a Graph Transformer on it for connectedness prediction. This formulation overcomes the limitations of endpoint-embedding fusion on set-isomorphic links, enabling rich link representations and effective relational reasoning over the global structure. Additionally, we introduce an overpass/underpass head to resolve multi-level crossings and a coupled NMS strategy to preserve critical connections. We evaluate LineGraph2Road on three benchmarks: City-scale, SpaceNet, and Global-scale, and show that it achieves state-of-the-art results on two key metrics, TOPO-F1 and APLS. It also captures fine visual details critical for real-world deployment. We will make our code publicly available. Subjects: Computer Vision and Pattern Recognition (cs.CV) Cite as: arXiv:2602.23290 [cs.CV] (or arXiv:2602.23290v1 [cs.CV] for this version) https://doi.org/10.48550/arXiv.2602.23290 Focus to learn more arXiv-issued DOI via DataCite (pending registration) Submission history From: Zhengyang Wei [ view email ] [v1] Thu, 26 Feb 2026 18:02:44 UTC (16,303 KB) Full-text links: Access Paper: View a PDF of the paper titled LineGraph2Road: Structural Graph Reasoning on Line Graphs for Road Network Extraction, by Zhengyang Wei and 3 other authors View PDF HTML (experimental) TeX Source view license Current browse context: cs.CV &lt; prev | next &gt; new | recent | 2026-02 Change to browse by: cs References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-164">83. Toward Expert Investment Teams:A Multi-Agent LLM System with Fine-Grained Trading Tasks</h2>
<ul>
<li>链接：https://arxiv.org/abs/2602.23330v1</li>
<li>来源：arxiv</li>
<li>摘要：The advancement of large language models (LLMs) has accelerated the development of autonomous financial trading systems. While mainstream approaches deploy multi-agent systems mimicking analyst and manager roles, they often rely on abstract instructions that overlook the intricacies of real-world workflows, which can lead to degraded inference performance and less transparent decision-making. Therefore, we propose a multi-agent LLM trading framework that explicitly decomposes investment analysis into fine-grained tasks, rather than providing coarse-grained instructions. We evaluate the proposed framework using Japanese stock data, including prices, financial statements, news, and macro information, under a leakage-controlled backtesting setting. Experimental results show that fine-grained task decomposition significantly improves risk-adjusted returns compared to conventional coarse-grained designs. Crucially, further analysis of intermediate agent outputs suggests that alignment between analytical outputs and downstream decision preferences is a critical driver of system performance. Moreover, we conduct standard portfolio optimization, exploiting low correlation with the stock in</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-165">正文（抓取，非 AI）</h3>
<p>[2602.23330v1] Toward Expert Investment Teams:A Multi-Agent LLM System with Fine-Grained Trading Tasks Computer Science &gt; Artificial Intelligence arXiv:2602.23330v1 (cs) [Submitted on 26 Feb 2026] Title: Toward Expert Investment Teams:A Multi-Agent LLM System with Fine-Grained Trading Tasks Authors: Kunihiro Miyazaki , Takanobu Kawahara , Stephen Roberts , Stefan Zohren View a PDF of the paper titled Toward Expert Investment Teams:A Multi-Agent LLM System with Fine-Grained Trading Tasks, by Kunihiro Miyazaki and 2 other authors View PDF HTML (experimental) Abstract: The advancement of large language models (LLMs) has accelerated the development of autonomous financial trading systems. While mainstream approaches deploy multi-agent systems mimicking analyst and manager roles, they often rely on abstract instructions that overlook the intricacies of real-world workflows, which can lead to degraded inference performance and less transparent decision-making. Therefore, we propose a multi-agent LLM trading framework that explicitly decomposes investment analysis into fine-grained tasks, rather than providing coarse-grained instructions. We evaluate the proposed framework using Japanese stock data, including prices, financial statements, news, and macro information, under a leakage-controlled backtesting setting. Experimental results show that fine-grained task decomposition significantly improves risk-adjusted returns compared to conventional coarse-grained designs. Crucially, further analysis of intermediate agent outputs suggests that alignment between analytical outputs and downstream decision preferences is a critical driver of system performance. Moreover, we conduct standard portfolio optimization, exploiting low correlation with the stock index and the variance of each system's output. This approach achieves superior performance. These findings contribute to the design of agent structure and task configuration when applying LLM agents to trading systems in practical settings. Comments: 14 pages, 3 figures Subjects: Artificial Intelligence (cs.AI) ; Trading and Market Microstructure (q-fin.TR) Cite as: arXiv:2602.23330 [cs.AI] (or arXiv:2602.23330v1 [cs.AI] for this version) https://doi.org/10.48550/arXiv.2602.23330 Focus to learn more arXiv-issued DOI via DataCite (pending registration) Submission history From: Takanobu Kawahara [ view email ] [v1] Thu, 26 Feb 2026 18:37:36 UTC (144 KB) Full-text links: Access Paper: View a PDF of the paper titled Toward Expert Investment Teams:A Multi-Agent LLM System with Fine-Grained Trading Tasks, by Kunihiro Miyazaki and 2 other authors View PDF HTML (experimental) TeX Source view license Current browse context: cs.AI &lt; prev | next &gt; new | recent | 2026-02 Change to browse by: cs q-fin q-fin.TR References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-166">84. LLM：什么是大语言模型？ | Machine Learning | Google for ...</h2>
<ul>
<li>链接：https://developers.google.cn/machine-learning/crash-course/llm/transformers?hl=zh-cn</li>
<li>来源：bing</li>
<li>摘要：2026年1月2日 · LLM 的预测效果比 N-gram 语言模型或循环神经网络好得多，原因如下： LLM 包含的 参数 远多于循环模型。 LLM 会收集更多上下文。 本部分将介绍用于构建 LLM 的最成功且应用最广泛 …</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-167">正文（抓取，非 AI）</h3>
<p>LLM：什么是大语言模型？ | Machine Learning | Google for Developers 跳至主要内容 Machine Learning / English Deutsch Español Español – América Latina Français Indonesia Italiano Polski Português – Brasil Tiếng Việt Türkçe Русский Українська עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 机器学习概念 首页 产品 Machine Learning 机器学习概念 Crash Course LLM：什么是大语言模型？ 使用集合让一切井井有条 根据您的偏好保存内容并对其进行分类。 一种较新的技术，即 大语言模型 ( LLM ) ，可预测一个 token 或一系列 token，有时甚至可以预测相当于多个段落的 token。请注意，词元可以是字词、子字词（字词的子集），甚至是单个字符。LLM 的预测效果比 N-gram 语言模型或循环神经网络好得多，原因如下： LLM 包含的 参数 远多于循环模型。 LLM 会收集更多上下文。 本部分将介绍用于构建 LLM 的最成功且应用最广泛的架构：Transformer。 什么是 Transformer？ Transformer 是一种先进的架构，适用于各种语言模型应用，例如翻译： 图 1. 一个基于 Transformer 的应用，可将英语翻译成法语。 完整的 Transformer 由编码器和解码器组成： 编码器 会将输入文本转换为中间表示形式。编码器是一个庞大的 神经网络 。 解码器 会将该中间表示形式转换为有用的文本。解码器也是一个庞大的神经网络。 例如，在翻译器中： 编码器将输入文本（例如英语句子）处理为某种中间表示形式。 解码器会将该中间表示形式转换为输出文本（例如，等效的法语句子）。 图 2. 完整的 Transformer 包含编码器和解码器。 点击相应图标，详细了解部分 Transformer。 本模块重点介绍包含编码器和解码器的完整 Transformer；不过，也存在仅使用编码器和仅使用解码器的架构： 仅使用编码器的架构会将输入文本映射为中间表示形式（通常是嵌入层）。 仅使用编码器架构的应用场景包括： 预测输入序列中的任何 token（这是语言模型的传统角色）。 创建复杂的嵌入，可作为另一个系统（例如分类器）的输入。 仅限解码器的架构会根据已生成的文本生成新令牌。仅限解码器的模型通常擅长生成序列；现代仅限解码器的模型可以利用其生成能力来创建对话历史记录和其他提示的后续内容。 什么是自注意力？ 为了增强上下文理解能力，Transformer 很大程度上依赖于一种称为 自注意力 的概念。实际上，自注意力机制会针对输入的每个令牌提出以下问题： “每个其他输入 token 对此 token 的解读有何影响？” “自注意力”中的“自”是指输入序列。某些注意力机制会衡量输入词法单元与输出序列（例如翻译）中的词法单元或某些其他序列中的词法单元之间的关系。但自注意力机制仅衡量输入序列中词法单元之间关系的重要性。 为简单起见，假设每个词元都是一个字词，而完整上下文只是一句话。请看以下句子： 上句话包含 11 个字词。这 11 个字词中的每一个都在关注其他 10 个字词，想知道这 10 个字词对自己的重要程度。例如，请注意，该句子包含代词 it 。代词通常比较含糊。代词 it 通常指代最近的名词或名词短语，但在示例句子中， it 指代的是哪个最近的名词，是动物还是街道？ 自注意力机制可确定 每个 附近字词与代词 it 的相关性。图 3 显示了结果，线条越蓝，表示相应字词对代词“it”越重要。 也就是说，对于代词 it ， 动物 比 街道 更重要。 图 3. 代词 it 的自注意力。摘自 《Transformer：一种用于语言理解的新型神经网络架构》 。 反之，假设句子中的最后一个字发生如下变化： 在这个修订后的句子中，自注意力机制有望将 street 评为比 animal 更贴近代词 it 。 有些自注意力机制是 双向 的，这意味着它们会计算被关注字词 之前 和 之后 的令牌的相关性得分。例如，在图 3 中，请注意系统会检查 it 两侧的字词。因此，双向自注意力机制可以从被关注字词两侧的字词中收集上下文。相比之下， 单向 自注意力机制只能从被关注字词一侧的字词中收集上下文。双向自注意力机制对于生成整个序列的表示形式特别有用，而逐个生成序列令牌的应用则需要单向自注意力机制。因此，编码器使用双向自注意力，而解码器使用单向自注意力。 什么是多头多层自注意力机制？ 每个自注意力层通常由多个 自注意力头 组成。层的输出是不同头的输出的数学运算（例如，加权平均值或点积）。 由于每个头的参数都初始化为随机值，因此不同的头可以学习被关注的每个字词与附近字词之间的不同关系。例如，上一部分中介绍的自注意力头侧重于确定代词“it”指的是哪个名词。 不过，同一层中的其他自注意力头可能会学习每个字词与其他所有字词的语法相关性，或者学习其他互动。 完整的 Transformer 模型会将多个 自注意力层 堆叠在一起。前一层的输出会成为下一层的输入。 这种堆叠方式可让模型逐步构建对文本的更复杂、更抽象的理解。虽然较浅的层可能侧重于基本语法，但较深的层可以整合这些信息，以掌握更细致的概念，例如整个输入中的情感、上下文和主题链接。 点击相应图标，了解 LLM 的 Big O。 自注意力机制会强制上下文中的每个字词学习上下文中所有其他字词的相关性。因此，我们很容易将此问题归类为 O(N 2 ) 问题，其中： N 是上下文中的 token 数量。 如果说前面的大 O 符号还不够令人不安，那么 Transformer 包含多个自注意力层，并且每个自注意力层包含多个自注意力头，因此大 O 符号实际上是： 其中： S 是自注意力层的数量。 D 是每层的注意力头数。 点击相应图标，详细了解 LLM 的训练方式。 您可能永远不会从头开始训练 LLM。训练工业级 LLM 需要大量的机器学习专业知识、计算资源和时间。不过，您点击了该图标以了解详情，因此我们有必要向您做出说明。 构建 LLM 的主要要素是海量的训练数据（文本），通常经过一定程度的过滤。训练的第一阶段通常是对训练数据进行某种形式的 无监督学习 。具体来说，模型会基于 遮盖的预测 进行训练，这意味着训练数据中的某些 token 会被有意隐藏。模型会尝试预测这些缺失的令牌，从而进行训练。例如，假设以下句子是训练数据的一部分： 移除了随机令牌，例如： LLM 只是一个神经网络，因此损失（模型正确考虑的被遮盖令牌的数量）会指导反向传播更新参数值的程度。 经过训练的基于 Transformer 的模型会预测缺失的数据，并逐渐学会检测数据中的模式和高阶结构，以获取有关缺失令牌的线索。请考虑以下已遮盖的实例示例： 通过对大量遮盖示例进行广泛训练，LLM 可以学习到“harvested”或“picked”是第一个词元的高概率匹配项，而“oranges”或“they”是第二个词元的理想选择。 一个名为 指令调优 的可选的进一步训练步骤可以提高 LLM 遵循指令的能力。 为什么 Transformer 模型这么大？ Transformer 包含数千亿甚至数万亿个 参数 。本课程通常建议构建参数数量较少的模型，而不是参数数量较多的模型。毕竟，与参数较多的模型相比，参数较少的模型在进行预测时使用的资源更少。不过，研究表明，参数较多的 Transformer 始终优于参数较少的 Transformer。 但 LLM 如何 生成 文本？ 您可能已经了解研究人员如何训练 LLM 来预测一两个缺失的字词，但可能对此并不感到惊讶。毕竟，预测一两个字基本上就是各种文本、电子邮件和创作软件中内置的自动补全功能。 您可能想知道 LLM 如何生成有关套利的句子、段落或俳句。 事实上，LLM 本质上是一种自动补全机制，可以自动预测（补全）数千个 token。例如，假设有一句句子，后面跟着一句被遮盖的句子： LLM 可以生成被遮盖句子的概率，包括： Probability 字词 3.1% 例如，他可以坐下、待在原地和翻滚。 2.9% 例如，他知道如何坐下、待在原地和翻滚。 足够大的 LLM 可以生成段落和整篇文章的概率。您可以将用户向 LLM 提出的问题视为“给定”句子，后面跟随着一个假想的遮罩。例如： LLM 会针对各种可能的回答生成概率。 再举一个例子，如果 LLM 接受过大量数学“文字题”的训练，那么它看起来就能进行复杂的数学推理。不过，这些 LLM 基本上只是自动补全文字题提示。 LLM 的优势 LLM 可以为各种目标受众群体生成清晰易懂的文本。LLM 可以针对明确训练过的任务做出预测。一些研究人员声称，LLM 还可以针对 未 明确训练过的输入做出预测，但其他研究人员驳斥了这一说法。 LLM 的问题 训练 LLM 会遇到许多问题，包括： 收集庞大的训练集。 耗费了数月时间和大量计算资源和电力。 解决并行处理难题。 使用 LLM 进行 推理 预测会导致以下问题： LLM 会 产生幻觉 ，这意味着它们的预测通常包含错误。 LLM 会消耗大量的计算资源和电力。 使用更大的数据集训练 LLM 通常会减少推理所需的资源量，不过更大的训练集会消耗更多训练资源。 与所有机器学习模型一样，LLM 可能会表现出各种各样的偏差。 练习：检查您的理解情况 假设某个 Transformer 在 10 亿个文档上进行了训练，其中包括数千个包含至少一个“elephant”字词的文档。 以下哪些陈述可能为真？ 金合欢树 是大象的重要食物来源，随着“大象”一词的出现，金合欢树的自注意力得分会逐渐升高。 是的，这样一来，Transformer 就能回答有关大象饮食的问题。 转换器会将“elephant”一词与包含“elephant”一词的各种成语相关联。 是的，系统会开始在“elephant”一词与“elephant”成语中的其他字词之间附加较高的自注意力得分。 Transformer 将逐渐学会忽略训练数据中对“elephant”一词的任何讽刺或反讽用法。 经过足够广泛的训练集训练的足够大的 Transformer 在识别讽刺、幽默和反讽方面变得相当熟练。因此，Transformer 不会忽略讽刺和反讽，而是会从中学习。 关键术语 ： 解码器 嵌入层 编码器 幻觉 指令调优 大语言模型 (LLM) 神经网络 参数 自我关注 张量处理单元 (TPU) 张量处理单元 (TPU) Pod 无监督学习 上一页 arrow_back 什么是语言模型？（10 分钟） 下一页 微调、提炼和提示工程（10 分钟） arrow_forward 如未另行说明，那么本页面中的内容已根据 知识共享署名 4.0 许可 获得了许可，并且代码示例已根据 Apache 2.0 许可 获得了许可。有关详情，请参阅 Google 开发者网站政策 。Java 是 Oracle 和/或其关联公司的注册商标。 最后更新时间 (UTC)：2026-01-02。</p>
</div></details><h2 id="toc-168">85. MediX-R1: Open Ended Medical Reinforcement Learning</h2>
<ul>
<li>链接：https://arxiv.org/abs/2602.23363v1</li>
<li>来源：arxiv</li>
<li>摘要：We introduce MediX-R1, an open-ended Reinforcement Learning (RL) framework for medical multimodal large language models (MLLMs) that enables clinically grounded, free-form answers beyond multiple-choice formats. MediX-R1 fine-tunes a baseline vision-language backbone with Group Based RL and a composite reward tailored for medical reasoning: an LLM-based accuracy reward that judges semantic correctness with a strict YES/NO decision, a medical embedding-based semantic reward to capture paraphrases and terminology variants, and lightweight format and modality rewards that enforce interpretable reasoning and modality recognition. This multi-signal design provides stable, informative feedback for open-ended outputs where traditional verifiable or MCQ-only rewards fall short. To measure progress, we propose a unified evaluation framework for both text-only and image+text tasks that uses a Reference-based LLM-as-judge in place of brittle string-overlap metrics, capturing semantic correctness, reasoning, and contextual alignment. Despite using only $\sim51$K instruction examples, MediX-R1 achieves excellent results across standard medical LLM (text-only) and VLM (image + text) benchmarks, </li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-169">正文（抓取，非 AI）</h3>
<p>[2602.23363v1] MediX-R1: Open Ended Medical Reinforcement Learning Computer Science &gt; Computer Vision and Pattern Recognition arXiv:2602.23363v1 (cs) [Submitted on 26 Feb 2026] Title: MediX-R1: Open Ended Medical Reinforcement Learning Authors: Sahal Shaji Mullappilly , Mohammed Irfan Kurpath , Omair Mohamed , Mohamed Zidan , Fahad Khan , Salman Khan , Rao Anwer , Hisham Cholakkal View a PDF of the paper titled MediX-R1: Open Ended Medical Reinforcement Learning, by Sahal Shaji Mullappilly and 7 other authors View PDF HTML (experimental) Abstract: We introduce MediX-R1, an open-ended Reinforcement Learning (RL) framework for medical multimodal large language models (MLLMs) that enables clinically grounded, free-form answers beyond multiple-choice formats. MediX-R1 fine-tunes a baseline vision-language backbone with Group Based RL and a composite reward tailored for medical reasoning: an LLM-based accuracy reward that judges semantic correctness with a strict YES/NO decision, a medical embedding-based semantic reward to capture paraphrases and terminology variants, and lightweight format and modality rewards that enforce interpretable reasoning and modality recognition. This multi-signal design provides stable, informative feedback for open-ended outputs where traditional verifiable or MCQ-only rewards fall short. To measure progress, we propose a unified evaluation framework for both text-only and image+text tasks that uses a Reference-based LLM-as-judge in place of brittle string-overlap metrics, capturing semantic correctness, reasoning, and contextual alignment. Despite using only $\sim51$K instruction examples, MediX-R1 achieves excellent results across standard medical LLM (text-only) and VLM (image + text) benchmarks, outperforming strong open-source baselines and delivering particularly large gains on open-ended clinical tasks. Our results demonstrate that open-ended RL with comprehensive reward signals and LLM-based evaluation is a practical path toward reliable medical reasoning in multimodal models. Our trained models, curated datasets and source code are available at this https URL Subjects: Computer Vision and Pattern Recognition (cs.CV) Cite as: arXiv:2602.23363 [cs.CV] (or arXiv:2602.23363v1 [cs.CV] for this version) https://doi.org/10.48550/arXiv.2602.23363 Focus to learn more arXiv-issued DOI via DataCite (pending registration) Submission history From: Sahal Shaji Mullappilly [ view email ] [v1] Thu, 26 Feb 2026 18:59:46 UTC (1,145 KB) Full-text links: Access Paper: View a PDF of the paper titled MediX-R1: Open Ended Medical Reinforcement Learning, by Sahal Shaji Mullappilly and 7 other authors View PDF HTML (experimental) TeX Source view license Current browse context: cs.CV &lt; prev | next &gt; new | recent | 2026-02 Change to browse by: cs References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-170">86. SeeThrough3D: Occlusion Aware 3D Control in Text-to-Image Generation</h2>
<ul>
<li>链接：https://arxiv.org/abs/2602.23359v1</li>
<li>来源：arxiv</li>
<li>摘要：We identify occlusion reasoning as a fundamental yet overlooked aspect for 3D layout-conditioned generation. It is essential for synthesizing partially occluded objects with depth-consistent geometry and scale. While existing methods can generate realistic scenes that follow input layouts, they often fail to model precise inter-object occlusions. We propose SeeThrough3D, a model for 3D layout conditioned generation that explicitly models occlusions. We introduce an occlusion-aware 3D scene representation (OSCR), where objects are depicted as translucent 3D boxes placed within a virtual environment and rendered from desired camera viewpoint. The transparency encodes hidden object regions, enabling the model to reason about occlusions, while the rendered viewpoint provides explicit camera control during generation. We condition a pretrained flow based text-to-image image generation model by introducing a set of visual tokens derived from our rendered 3D representation. Furthermore, we apply masked self-attention to accurately bind each object bounding box to its corresponding textual description, enabling accurate generation of multiple objects without object attribute mixing. To tra</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-171">正文（抓取，非 AI）</h3>
<p>[2602.23359v1] SeeThrough3D: Occlusion Aware 3D Control in Text-to-Image Generation Computer Science &gt; Computer Vision and Pattern Recognition arXiv:2602.23359v1 (cs) [Submitted on 26 Feb 2026] Title: SeeThrough3D: Occlusion Aware 3D Control in Text-to-Image Generation Authors: Vaibhav Agrawal , Rishubh Parihar , Pradhaan Bhat , Ravi Kiran Sarvadevabhatla , R. Venkatesh Babu View a PDF of the paper titled SeeThrough3D: Occlusion Aware 3D Control in Text-to-Image Generation, by Vaibhav Agrawal and 4 other authors View PDF HTML (experimental) Abstract: We identify occlusion reasoning as a fundamental yet overlooked aspect for 3D layout-conditioned generation. It is essential for synthesizing partially occluded objects with depth-consistent geometry and scale. While existing methods can generate realistic scenes that follow input layouts, they often fail to model precise inter-object occlusions. We propose SeeThrough3D, a model for 3D layout conditioned generation that explicitly models occlusions. We introduce an occlusion-aware 3D scene representation (OSCR), where objects are depicted as translucent 3D boxes placed within a virtual environment and rendered from desired camera viewpoint. The transparency encodes hidden object regions, enabling the model to reason about occlusions, while the rendered viewpoint provides explicit camera control during generation. We condition a pretrained flow based text-to-image image generation model by introducing a set of visual tokens derived from our rendered 3D representation. Furthermore, we apply masked self-attention to accurately bind each object bounding box to its corresponding textual description, enabling accurate generation of multiple objects without object attribute mixing. To train the model, we construct a synthetic dataset with diverse multi-object scenes with strong inter-object occlusions. SeeThrough3D generalizes effectively to unseen object categories and enables precise 3D layout control with realistic occlusions and consistent camera control. Comments: Project page: this https URL . Accepted at CVPR 2026 Subjects: Computer Vision and Pattern Recognition (cs.CV) ; Artificial Intelligence (cs.AI) Cite as: arXiv:2602.23359 [cs.CV] (or arXiv:2602.23359v1 [cs.CV] for this version) https://doi.org/10.48550/arXiv.2602.23359 Focus to learn more arXiv-issued DOI via DataCite (pending registration) Submission history From: Rishubh Parihar [ view email ] [v1] Thu, 26 Feb 2026 18:59:05 UTC (38,865 KB) Full-text links: Access Paper: View a PDF of the paper titled SeeThrough3D: Occlusion Aware 3D Control in Text-to-Image Generation, by Vaibhav Agrawal and 4 other authors View PDF HTML (experimental) TeX Source view license Current browse context: cs.CV &lt; prev | next &gt; new | recent | 2026-02 Change to browse by: cs cs.AI References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-172">87. Cell-Free MIMO with Rotatable Antennas: When Macro-Diversity Meets Antenna Directivity</h2>
<ul>
<li>链接：https://arxiv.org/abs/2601.16543v1</li>
<li>来源：arxiv</li>
<li>摘要：Cell-free networks leverage distributed access points (APs) to achieve macro-diversity, yet their performance is often constrained by large disparities in channel quality arising from user geometry and blockages. To address this, rotatable antennas (RAs) add a lightweight hardware degree of freedom by steering the antenna boresight toward dominant propagation directions to strengthen unfavorable links, thereby enabling the network to better exploit macro-diversity for higher and more uniform performance. This paper investigates an RA-enabled cell-free downlink network and formulates a max-min rate problem that jointly optimizes transmit beamforming and antenna orientations. To tackle this challenging problem, we develop an alternating-optimization-based algorithm that iteratively updates the beamformers via a second-order cone program (SOCP) and optimizes the antenna orientations using successive convex approximation. To reduce complexity, we further propose an efficient two-stage scheme that first designs orientations by maximizing a proportional-fair log-utility using manifold-aware Frank-Wolfe updates, and then computes the beamformers using an SOCP-based design. Simulation resu</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-173">正文（抓取，非 AI）</h3>
<p>[2601.16543v1] Cell-Free MIMO with Rotatable Antennas: When Macro-Diversity Meets Antenna Directivity Electrical Engineering and Systems Science &gt; Signal Processing arXiv:2601.16543v1 (eess) [Submitted on 23 Jan 2026] Title: Cell-Free MIMO with Rotatable Antennas: When Macro-Diversity Meets Antenna Directivity Authors: Xingxiang Peng , Qingqing Wu , Ziyuan Zheng , Yanze Zhu , Wen Chen , Penghui Huang , Ying Gao , Honghao Wang View a PDF of the paper titled Cell-Free MIMO with Rotatable Antennas: When Macro-Diversity Meets Antenna Directivity, by Xingxiang Peng and 7 other authors View PDF HTML (experimental) Abstract: Cell-free networks leverage distributed access points (APs) to achieve macro-diversity, yet their performance is often constrained by large disparities in channel quality arising from user geometry and blockages. To address this, rotatable antennas (RAs) add a lightweight hardware degree of freedom by steering the antenna boresight toward dominant propagation directions to strengthen unfavorable links, thereby enabling the network to better exploit macro-diversity for higher and more uniform performance. This paper investigates an RA-enabled cell-free downlink network and formulates a max-min rate problem that jointly optimizes transmit beamforming and antenna orientations. To tackle this challenging problem, we develop an alternating-optimization-based algorithm that iteratively updates the beamformers via a second-order cone program (SOCP) and optimizes the antenna orientations using successive convex approximation. To reduce complexity, we further propose an efficient two-stage scheme that first designs orientations by maximizing a proportional-fair log-utility using manifold-aware Frank-Wolfe updates, and then computes the beamformers using an SOCP-based design. Simulation results demonstrate that the proposed orientation-aware designs achieve a substantially higher worst-user rate than conventional beamforming-only benchmarks. Furthermore, larger antenna directivity enhances fairness with proper orientation but can degrade the worst-user performance otherwise. Comments: 12 pages, 7 figures. Submitted to an IEEE journal for possible publication Subjects: Signal Processing (eess.SP) Cite as: arXiv:2601.16543 [eess.SP] (or arXiv:2601.16543v1 [eess.SP] for this version) https://doi.org/10.48550/arXiv.2601.16543 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Xingxiang Peng [ view email ] [v1] Fri, 23 Jan 2026 08:23:55 UTC (246 KB) Full-text links: Access Paper: View a PDF of the paper titled Cell-Free MIMO with Rotatable Antennas: When Macro-Diversity Meets Antenna Directivity, by Xingxiang Peng and 7 other authors View PDF HTML (experimental) TeX Source view license Current browse context: eess.SP &lt; prev | next &gt; new | recent | 2026-01 Change to browse by: eess References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-174">88. Towards Long-Form Spatio-Temporal Video Grounding</h2>
<ul>
<li>链接：https://arxiv.org/abs/2602.23294v1</li>
<li>来源：arxiv</li>
<li>摘要：In real scenarios, videos can span several minutes or even hours. However, existing research on spatio-temporal video grounding (STVG), given a textual query, mainly focuses on localizing targets in short videos of tens of seconds, typically less than one minute, which limits real-world applications. In this paper, we explore Long-Form STVG (LF-STVG), which aims to locate targets in long-term videos. Compared with short videos, long-term videos contain much longer temporal spans and more irrelevant information, making it difficult for existing STVG methods that process all frames at once. To address this challenge, we propose an AutoRegressive Transformer architecture for LF-STVG, termed ART-STVG. Unlike conventional STVG methods that require the entire video sequence to make predictions at once, ART-STVG treats the video as streaming input and processes frames sequentially, enabling efficient handling of long videos. To model spatio-temporal context, we design spatial and temporal memory banks and apply them to the decoders. Since memories from different moments are not always relevant to the current frame, we introduce simple yet effective memory selection strategies to provide m</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-175">正文（抓取，非 AI）</h3>
<p>[2602.23294v1] Towards Long-Form Spatio-Temporal Video Grounding Computer Science &gt; Computer Vision and Pattern Recognition arXiv:2602.23294v1 (cs) [Submitted on 26 Feb 2026] Title: Towards Long-Form Spatio-Temporal Video Grounding Authors: Xin Gu , Bing Fan , Jiali Yao , Zhipeng Zhang , Yan Huang , Cheng Han , Heng Fan , Libo Zhang View a PDF of the paper titled Towards Long-Form Spatio-Temporal Video Grounding, by Xin Gu and 7 other authors View PDF HTML (experimental) Abstract: In real scenarios, videos can span several minutes or even hours. However, existing research on spatio-temporal video grounding (STVG), given a textual query, mainly focuses on localizing targets in short videos of tens of seconds, typically less than one minute, which limits real-world applications. In this paper, we explore Long-Form STVG (LF-STVG), which aims to locate targets in long-term videos. Compared with short videos, long-term videos contain much longer temporal spans and more irrelevant information, making it difficult for existing STVG methods that process all frames at once. To address this challenge, we propose an AutoRegressive Transformer architecture for LF-STVG, termed ART-STVG. Unlike conventional STVG methods that require the entire video sequence to make predictions at once, ART-STVG treats the video as streaming input and processes frames sequentially, enabling efficient handling of long videos. To model spatio-temporal context, we design spatial and temporal memory banks and apply them to the decoders. Since memories from different moments are not always relevant to the current frame, we introduce simple yet effective memory selection strategies to provide more relevant information to the decoders, significantly improving performance. Furthermore, instead of parallel spatial and temporal localization, we propose a cascaded spatio-temporal design that connects the spatial decoder to the temporal decoder, allowing fine-grained spatial cues to assist complex temporal localization in long videos. Experiments on newly extended LF-STVG datasets show that ART-STVG significantly outperforms state-of-the-art methods, while achieving competitive performance on conventional short-form STVG. Subjects: Computer Vision and Pattern Recognition (cs.CV) Cite as: arXiv:2602.23294 [cs.CV] (or arXiv:2602.23294v1 [cs.CV] for this version) https://doi.org/10.48550/arXiv.2602.23294 Focus to learn more arXiv-issued DOI via DataCite (pending registration) Submission history From: Bing Fan [ view email ] [v1] Thu, 26 Feb 2026 18:04:09 UTC (3,102 KB) Full-text links: Access Paper: View a PDF of the paper titled Towards Long-Form Spatio-Temporal Video Grounding, by Xin Gu and 7 other authors View PDF HTML (experimental) TeX Source view license Current browse context: cs.CV &lt; prev | next &gt; new | recent | 2026-02 Change to browse by: cs References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-176">89. CL4SE: A Context Learning Benchmark For Software Engineering Tasks</h2>
<ul>
<li>链接：https://arxiv.org/abs/2602.23047v1</li>
<li>来源：arxiv</li>
<li>摘要：Context engineering has emerged as a pivotal paradigm for unlocking the potential of Large Language Models (LLMs) in Software Engineering (SE) tasks, enabling performance gains at test time without model fine-tuning. Despite its success, existing research lacks a systematic taxonomy of SE-specific context types and a dedicated benchmark to quantify the heterogeneous effects of different contexts across core SE workflows. To address this gap, we propose CL4SE (Context Learning for Software Engineering), a comprehensive benchmark featuring a fine-grained taxonomy of four SE-oriented context types (interpretable examples, project-specific context, procedural decision-making context, and positive &amp; negative context), each mapped to a representative task (code generation, code summarization, code review, and patch correctness assessment). We construct high-quality datasets comprising over 13,000 samples from more than 30 open-source projects and evaluate five mainstream LLMs across nine metrics. Extensive experiments demonstrate that context learning yields an average performance improvement of 24.7% across all tasks. Specifically, procedural context boosts code review performance by up</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-177">正文（抓取，非 AI）</h3>
<p>[2602.23047v1] CL4SE: A Context Learning Benchmark For Software Engineering Tasks Computer Science &gt; Software Engineering arXiv:2602.23047v1 (cs) [Submitted on 26 Feb 2026] Title: CL4SE: A Context Learning Benchmark For Software Engineering Tasks Authors: Haichuan Hu , Ye Shang , Guoqing Xie , Congqing He , Quanjun Zhang View a PDF of the paper titled CL4SE: A Context Learning Benchmark For Software Engineering Tasks, by Haichuan Hu and 4 other authors View PDF HTML (experimental) Abstract: Context engineering has emerged as a pivotal paradigm for unlocking the potential of Large Language Models (LLMs) in Software Engineering (SE) tasks, enabling performance gains at test time without model fine-tuning. Despite its success, existing research lacks a systematic taxonomy of SE-specific context types and a dedicated benchmark to quantify the heterogeneous effects of different contexts across core SE workflows. To address this gap, we propose CL4SE (Context Learning for Software Engineering), a comprehensive benchmark featuring a fine-grained taxonomy of four SE-oriented context types (interpretable examples, project-specific context, procedural decision-making context, and positive &amp; negative context), each mapped to a representative task (code generation, code summarization, code review, and patch correctness assessment). We construct high-quality datasets comprising over 13,000 samples from more than 30 open-source projects and evaluate five mainstream LLMs across nine metrics. Extensive experiments demonstrate that context learning yields an average performance improvement of 24.7% across all tasks. Specifically, procedural context boosts code review performance by up to 33% (Qwen3-Max), mixed positive-negative context improves patch assessment by 30% (DeepSeek-V3), project-specific context increases code summarization BLEU by 14.78% (GPT-Oss-120B), and interpretable examples enhance code generation PASS@1 by 5.72% (DeepSeek-V3). CL4SE establishes the first standardized evaluation framework for SE context learning, provides actionable empirical insights into task-specific context design, and releases a large-scale dataset to facilitate reproducible research in this domain. Comments: 23 pages, 4 figures Subjects: Software Engineering (cs.SE) Cite as: arXiv:2602.23047 [cs.SE] (or arXiv:2602.23047v1 [cs.SE] for this version) https://doi.org/10.48550/arXiv.2602.23047 Focus to learn more arXiv-issued DOI via DataCite (pending registration) Submission history From: Shang Ye [ view email ] [v1] Thu, 26 Feb 2026 14:28:57 UTC (235 KB) Full-text links: Access Paper: View a PDF of the paper titled CL4SE: A Context Learning Benchmark For Software Engineering Tasks, by Haichuan Hu and 4 other authors View PDF HTML (experimental) TeX Source view license Current browse context: cs.SE &lt; prev | next &gt; new | recent | 2026-02 Change to browse by: cs References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-178">90. Learning from Brain Topography: A Hierarchical Local-Global Graph-Transformer Network for EEG Emotion Recognition</h2>
<ul>
<li>链接：https://arxiv.org/abs/2601.10525v1</li>
<li>来源：arxiv</li>
<li>摘要：Understanding how local neurophysiological patterns interact with global brain dynamics is essential for decoding human emotions from EEG signals. However, existing deep learning approaches often overlook the brain's intrinsic spatial organization, failing to simultaneously capture local topological relations and global dependencies. To address these challenges, we propose Neuro-HGLN, a Neurologically-informed Hierarchical Graph-Transformer Learning Network that integrates biologically grounded priors with hierarchical representation learning. Neuro-HGLN first constructs a spatial Euclidean prior graph based on physical electrode distances to serve as an anatomically grounded inductive bias. A learnable global dynamic graph is then introduced to model functional connectivity across the entire brain. In parallel, to capture fine-grained regional dependencies, Neuro-HGLN builds region-level local graphs using a multi-head self-attention mechanism. These graphs are processed synchronously through local-constrained parallel GCN layers to produce region-specific representations. Subsequently, an iTransformer encoder aggregates these features to capture cross-region dependencies under a </li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-179">正文（抓取，非 AI）</h3>
<p>[2601.10525v1] Learning from Brain Topography: A Hierarchical Local-Global Graph-Transformer Network for EEG Emotion Recognition Computer Science &gt; Human-Computer Interaction arXiv:2601.10525v1 (cs) [Submitted on 15 Jan 2026] Title: Learning from Brain Topography: A Hierarchical Local-Global Graph-Transformer Network for EEG Emotion Recognition Authors: Yijin Zhou , Fu Li , Yi Niu , Boxun Fu , Huaning Wang , Lijian Zhang View a PDF of the paper titled Learning from Brain Topography: A Hierarchical Local-Global Graph-Transformer Network for EEG Emotion Recognition, by Yijin Zhou and 5 other authors View PDF HTML (experimental) Abstract: Understanding how local neurophysiological patterns interact with global brain dynamics is essential for decoding human emotions from EEG signals. However, existing deep learning approaches often overlook the brain's intrinsic spatial organization, failing to simultaneously capture local topological relations and global dependencies. To address these challenges, we propose Neuro-HGLN, a Neurologically-informed Hierarchical Graph-Transformer Learning Network that integrates biologically grounded priors with hierarchical representation learning. Neuro-HGLN first constructs a spatial Euclidean prior graph based on physical electrode distances to serve as an anatomically grounded inductive bias. A learnable global dynamic graph is then introduced to model functional connectivity across the entire brain. In parallel, to capture fine-grained regional dependencies, Neuro-HGLN builds region-level local graphs using a multi-head self-attention mechanism. These graphs are processed synchronously through local-constrained parallel GCN layers to produce region-specific representations. Subsequently, an iTransformer encoder aggregates these features to capture cross-region dependencies under a dimension-as-token formulation. Extensive experiments demonstrate that Neuro-HGLN achieves state-of-the-art performance on multiple benchmarks, providing enhanced interpretability grounded in neurophysiological structure. These results highlight the efficacy of unifying local topological learning with cross-region dependency modeling for robust EEG emotion recognition. Subjects: Human-Computer Interaction (cs.HC) Cite as: arXiv:2601.10525 [cs.HC] (or arXiv:2601.10525v1 [cs.HC] for this version) https://doi.org/10.48550/arXiv.2601.10525 Focus to learn more arXiv-issued DOI via DataCite Submission history From: YiJin Zhou [ view email ] [v1] Thu, 15 Jan 2026 15:52:05 UTC (10,475 KB) Full-text links: Access Paper: View a PDF of the paper titled Learning from Brain Topography: A Hierarchical Local-Global Graph-Transformer Network for EEG Emotion Recognition, by Yijin Zhou and 5 other authors View PDF HTML (experimental) TeX Source view license Current browse context: cs.HC &lt; prev | next &gt; new | recent | 2026-01 Change to browse by: cs References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-180">91. UniScale: Unified Scale-Aware 3D Reconstruction for Multi-View Understanding via Prior Injection for Robotic Perception</h2>
<ul>
<li>链接：https://arxiv.org/abs/2602.23224v1</li>
<li>来源：arxiv</li>
<li>摘要：We present UniScale, a unified, scale-aware multi-view 3D reconstruction framework for robotic applications that flexibly integrates geometric priors through a modular, semantically informed design. In vision-based robotic navigation, the accurate extraction of environmental structure from raw image sequences is critical for downstream tasks. UniScale addresses this challenge with a single feed-forward network that jointly estimates camera intrinsics and extrinsics, scale-invariant depth and point maps, and the metric scale of a scene from multi-view images, while optionally incorporating auxiliary geometric priors when available. By combining global contextual reasoning with camera-aware feature representations, UniScale is able to recover the metric-scale of the scene. In robotic settings where camera intrinsics are known, they can be easily incorporated to improve performance, with additional gains obtained when camera poses are also available. This co-design enables robust, metric-aware 3D reconstruction within a single unified model. Importantly, UniScale does not require training from scratch, and leverages world priors exhibited in pre-existing models without geometric encod</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-181">正文（抓取，非 AI）</h3>
<p>[2602.23224v1] UniScale: Unified Scale-Aware 3D Reconstruction for Multi-View Understanding via Prior Injection for Robotic Perception Computer Science &gt; Computer Vision and Pattern Recognition arXiv:2602.23224v1 (cs) [Submitted on 26 Feb 2026] Title: UniScale: Unified Scale-Aware 3D Reconstruction for Multi-View Understanding via Prior Injection for Robotic Perception Authors: Mohammad Mahdavian , Gordon Tan , Binbin Xu , Yuan Ren , Dongfeng Bai , Bingbing Liu View a PDF of the paper titled UniScale: Unified Scale-Aware 3D Reconstruction for Multi-View Understanding via Prior Injection for Robotic Perception, by Mohammad Mahdavian and 5 other authors View PDF Abstract: We present UniScale, a unified, scale-aware multi-view 3D reconstruction framework for robotic applications that flexibly integrates geometric priors through a modular, semantically informed design. In vision-based robotic navigation, the accurate extraction of environmental structure from raw image sequences is critical for downstream tasks. UniScale addresses this challenge with a single feed-forward network that jointly estimates camera intrinsics and extrinsics, scale-invariant depth and point maps, and the metric scale of a scene from multi-view images, while optionally incorporating auxiliary geometric priors when available. By combining global contextual reasoning with camera-aware feature representations, UniScale is able to recover the metric-scale of the scene. In robotic settings where camera intrinsics are known, they can be easily incorporated to improve performance, with additional gains obtained when camera poses are also available. This co-design enables robust, metric-aware 3D reconstruction within a single unified model. Importantly, UniScale does not require training from scratch, and leverages world priors exhibited in pre-existing models without geometric encoding strategies, making it particularly suitable for resource-constrained robotic teams. We evaluate UniScale on multiple benchmarks, demonstrating strong generalization and consistent performance across diverse environments. We will release our implementation upon acceptance. Subjects: Computer Vision and Pattern Recognition (cs.CV) ; Robotics (cs.RO) Cite as: arXiv:2602.23224 [cs.CV] (or arXiv:2602.23224v1 [cs.CV] for this version) https://doi.org/10.48550/arXiv.2602.23224 Focus to learn more arXiv-issued DOI via DataCite (pending registration) Submission history From: Mohammad Mahdavian [ view email ] [v1] Thu, 26 Feb 2026 17:04:36 UTC (4,836 KB) Full-text links: Access Paper: View a PDF of the paper titled UniScale: Unified Scale-Aware 3D Reconstruction for Multi-View Understanding via Prior Injection for Robotic Perception, by Mohammad Mahdavian and 5 other authors View PDF TeX Source view license Current browse context: cs.CV &lt; prev | next &gt; new | recent | 2026-02 Change to browse by: cs cs.RO References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-182">92. LLM Novice Uplift on Dual-Use, In Silico Biology Tasks</h2>
<ul>
<li>链接：https://arxiv.org/abs/2602.23329v1</li>
<li>来源：arxiv</li>
<li>摘要：Large language models (LLMs) perform increasingly well on biology benchmarks, but it remains unclear whether they uplift novice users -- i.e., enable humans to perform better than with internet-only resources. This uncertainty is central to understanding both scientific acceleration and dual-use risk. We conducted a multi-model, multi-benchmark human uplift study comparing novices with LLM access versus internet-only access across eight biosecurity-relevant task sets. Participants worked on complex problems with ample time (up to 13 hours for the most involved tasks). We found that LLM access provided substantial uplift: novices with LLMs were 4.16 times more accurate than controls (95% CI [2.63, 6.87]). On four benchmarks with available expert baselines (internet-only), novices with LLMs outperformed experts on three of them. Perhaps surprisingly, standalone LLMs often exceeded LLM-assisted novices, indicating that users were not eliciting the strongest available contributions from the LLMs. Most participants (89.6%) reported little difficulty obtaining dual-use-relevant information despite safeguards. Overall, LLMs substantially uplift novices on biological tasks previously reser</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-183">正文（抓取，非 AI）</h3>
<p>[2602.23329v1] LLM Novice Uplift on Dual-Use, In Silico Biology Tasks Computer Science &gt; Artificial Intelligence arXiv:2602.23329v1 (cs) [Submitted on 26 Feb 2026] Title: LLM Novice Uplift on Dual-Use, In Silico Biology Tasks Authors: Chen Bo Calvin Zhang , Christina Q. Knight , Nicholas Kruus , Jason Hausenloy , Pedro Medeiros , Nathaniel Li , Aiden Kim , Yury Orlovskiy , Coleman Breen , Bryce Cai , Jasper Götting , Andrew Bo Liu , Samira Nedungadi , Paula Rodriguez , Yannis Yiming He , Mohamed Shaaban , Zifan Wang , Seth Donoughe , Julian Michael View a PDF of the paper titled LLM Novice Uplift on Dual-Use, In Silico Biology Tasks, by Chen Bo Calvin Zhang and 18 other authors View PDF HTML (experimental) Abstract: Large language models (LLMs) perform increasingly well on biology benchmarks, but it remains unclear whether they uplift novice users -- i.e., enable humans to perform better than with internet-only resources. This uncertainty is central to understanding both scientific acceleration and dual-use risk. We conducted a multi-model, multi-benchmark human uplift study comparing novices with LLM access versus internet-only access across eight biosecurity-relevant task sets. Participants worked on complex problems with ample time (up to 13 hours for the most involved tasks). We found that LLM access provided substantial uplift: novices with LLMs were 4.16 times more accurate than controls (95% CI [2.63, 6.87]). On four benchmarks with available expert baselines (internet-only), novices with LLMs outperformed experts on three of them. Perhaps surprisingly, standalone LLMs often exceeded LLM-assisted novices, indicating that users were not eliciting the strongest available contributions from the LLMs. Most participants (89.6%) reported little difficulty obtaining dual-use-relevant information despite safeguards. Overall, LLMs substantially uplift novices on biological tasks previously reserved for trained practitioners, underscoring the need for sustained, interactive uplift evaluations alongside traditional benchmarks. Comments: 59 pages, 33 figures Subjects: Artificial Intelligence (cs.AI) ; Computation and Language (cs.CL); Cryptography and Security (cs.CR); Computers and Society (cs.CY); Human-Computer Interaction (cs.HC) Cite as: arXiv:2602.23329 [cs.AI] (or arXiv:2602.23329v1 [cs.AI] for this version) https://doi.org/10.48550/arXiv.2602.23329 Focus to learn more arXiv-issued DOI via DataCite (pending registration) Submission history From: Chen Bo Calvin Zhang [ view email ] [v1] Thu, 26 Feb 2026 18:37:23 UTC (574 KB) Full-text links: Access Paper: View a PDF of the paper titled LLM Novice Uplift on Dual-Use, In Silico Biology Tasks, by Chen Bo Calvin Zhang and 18 other authors View PDF HTML (experimental) TeX Source view license Current browse context: cs.AI &lt; prev | next &gt; new | recent | 2026-02 Change to browse by: cs cs.CL cs.CR cs.CY cs.HC References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-184">93. Comparative Evaluation of SDP, SOCP, and QC Convex Relaxations for Large-Scale Market-Based AC Optimal Power Flow</h2>
<ul>
<li>链接：https://arxiv.org/abs/2602.14136v1</li>
<li>来源：arxiv</li>
<li>摘要：The alternating current optimal power flow (ACOPF) problem is central to modern power system operations, determining how electricity is generated and transmitted to maximize social welfare while respecting physical and operational constraints. However, the nonlinear and non-convex nature of AC power flow equations makes finding globally optimal solutions computationally intractable for large networks. Convex relaxations - including semidefinite programming (SDP), second-order cone programming (SOCP), and quadratic convex (QC) formulations - provide tractable alternatives that can yield provably optimal or near-optimal solutions under appropriate conditions. This paper presents a comprehensive comparative study of multiple ACOPF relaxations applied to market-based welfare maximization. We implement DCOPF, Shor's SDP relaxation (complex and real-valued forms), chordal SDP, Jabr's SOCP relaxation, and QC relaxations in a unified, solver-native framework using the MOSEK Fusion API, eliminating modeling overhead present in high-level frameworks such as CVXPY. To address the practical challenge of missing or overly conservative angle difference bounds required by QC relaxations, we emplo</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-185">正文（抓取，非 AI）</h3>
<p>[2602.14136v1] Comparative Evaluation of SDP, SOCP, and QC Convex Relaxations for Large-Scale Market-Based AC Optimal Power Flow Mathematics &gt; Optimization and Control arXiv:2602.14136v1 (math) [Submitted on 15 Feb 2026] Title: Comparative Evaluation of SDP, SOCP, and QC Convex Relaxations for Large-Scale Market-Based AC Optimal Power Flow Authors: Ata Keskin View a PDF of the paper titled Comparative Evaluation of SDP, SOCP, and QC Convex Relaxations for Large-Scale Market-Based AC Optimal Power Flow, by Ata Keskin View PDF Abstract: The alternating current optimal power flow (ACOPF) problem is central to modern power system operations, determining how electricity is generated and transmitted to maximize social welfare while respecting physical and operational constraints. However, the nonlinear and non-convex nature of AC power flow equations makes finding globally optimal solutions computationally intractable for large networks. Convex relaxations - including semidefinite programming (SDP), second-order cone programming (SOCP), and quadratic convex (QC) formulations - provide tractable alternatives that can yield provably optimal or near-optimal solutions under appropriate conditions. This paper presents a comprehensive comparative study of multiple ACOPF relaxations applied to market-based welfare maximization. We implement DCOPF, Shor's SDP relaxation (complex and real-valued forms), chordal SDP, Jabr's SOCP relaxation, and QC relaxations in a unified, solver-native framework using the MOSEK Fusion API, eliminating modeling overhead present in high-level frameworks such as CVXPY. To address the practical challenge of missing or overly conservative angle difference bounds required by QC relaxations, we employ quasi-Monte Carlo sampling with Sobol sequences to empirically estimate tighter bounds. We evaluate these relaxations on subnetworks of varying sizes derived from the ARPA-E dataset, systematically comparing solution quality, runtime, and memory consumption. Our results demonstrate the trade-offs between relaxation tightness and computational efficiency, providing practical guidance for selecting appropriate formulations based on network scale and solution requirements. Subjects: Optimization and Control (math.OC) MSC classes: 90C26, 90C22 ACM classes: G.1.6; G.3; J.2 Cite as: arXiv:2602.14136 [math.OC] (or arXiv:2602.14136v1 [math.OC] for this version) https://doi.org/10.48550/arXiv.2602.14136 Focus to learn more arXiv-issued DOI via DataCite (pending registration) Submission history From: Ata Keskin [ view email ] [v1] Sun, 15 Feb 2026 13:17:22 UTC (150 KB) Full-text links: Access Paper: View a PDF of the paper titled Comparative Evaluation of SDP, SOCP, and QC Convex Relaxations for Large-Scale Market-Based AC Optimal Power Flow, by Ata Keskin View PDF TeX Source view license Current browse context: math.OC &lt; prev | next &gt; new | recent | 2026-02 Change to browse by: math References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-186">94. Understanding Usage and Engagement in AI-Powered Scientific Research Tools: The Asta Interaction Dataset</h2>
<ul>
<li>链接：https://arxiv.org/abs/2602.23335v1</li>
<li>来源：arxiv</li>
<li>摘要：AI-powered scientific research tools are rapidly being integrated into research workflows, yet the field lacks a clear lens into how researchers use these systems in real-world settings. We present and analyze the Asta Interaction Dataset, a large-scale resource comprising over 200,000 user queries and interaction logs from two deployed tools (a literature discovery interface and a scientific question-answering interface) within an LLM-powered retrieval-augmented generation platform. Using this dataset, we characterize query patterns, engagement behaviors, and how usage evolves with experience. We find that users submit longer and more complex queries than in traditional search, and treat the system as a collaborative research partner, delegating tasks such as drafting content and identifying research gaps. Users treat generated responses as persistent artifacts, revisiting and navigating among outputs and cited evidence in non-linear ways. With experience, users issue more targeted queries and engage more deeply with supporting citations, although keyword-style queries persist even among experienced users. We release the anonymized dataset and analysis with a new query intent taxo</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-187">正文（抓取，非 AI）</h3>
<p>[2602.23335v1] Understanding Usage and Engagement in AI-Powered Scientific Research Tools: The Asta Interaction Dataset Computer Science &gt; Human-Computer Interaction arXiv:2602.23335v1 (cs) [Submitted on 26 Feb 2026] Title: Understanding Usage and Engagement in AI-Powered Scientific Research Tools: The Asta Interaction Dataset Authors: Dany Haddad , Dan Bareket , Joseph Chee Chang , Jay DeYoung , Jena D. Hwang , Uri Katz , Mark Polak , Sangho Suh , Harshit Surana , Aryeh Tiktinsky , Shriya Atmakuri , Jonathan Bragg , Mike D'Arcy , Sergey Feldman , Amal Hassan-Ali , Rubén Lozano , Bodhisattwa Prasad Majumder , Charles McGrady , Amanpreet Singh , Brooke Vlahos , Yoav Goldberg , Doug Downey View a PDF of the paper titled Understanding Usage and Engagement in AI-Powered Scientific Research Tools: The Asta Interaction Dataset, by Dany Haddad and 21 other authors View PDF HTML (experimental) Abstract: AI-powered scientific research tools are rapidly being integrated into research workflows, yet the field lacks a clear lens into how researchers use these systems in real-world settings. We present and analyze the Asta Interaction Dataset, a large-scale resource comprising over 200,000 user queries and interaction logs from two deployed tools (a literature discovery interface and a scientific question-answering interface) within an LLM-powered retrieval-augmented generation platform. Using this dataset, we characterize query patterns, engagement behaviors, and how usage evolves with experience. We find that users submit longer and more complex queries than in traditional search, and treat the system as a collaborative research partner, delegating tasks such as drafting content and identifying research gaps. Users treat generated responses as persistent artifacts, revisiting and navigating among outputs and cited evidence in non-linear ways. With experience, users issue more targeted queries and engage more deeply with supporting citations, although keyword-style queries persist even among experienced users. We release the anonymized dataset and analysis with a new query intent taxonomy to inform future designs of real-world AI research assistants and to support realistic evaluation. Subjects: Human-Computer Interaction (cs.HC) ; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR) Cite as: arXiv:2602.23335 [cs.HC] (or arXiv:2602.23335v1 [cs.HC] for this version) https://doi.org/10.48550/arXiv.2602.23335 Focus to learn more arXiv-issued DOI via DataCite (pending registration) Submission history From: Dany Haddad [ view email ] [v1] Thu, 26 Feb 2026 18:40:28 UTC (14,759 KB) Full-text links: Access Paper: View a PDF of the paper titled Understanding Usage and Engagement in AI-Powered Scientific Research Tools: The Asta Interaction Dataset, by Dany Haddad and 21 other authors View PDF HTML (experimental) TeX Source view license Current browse context: cs.HC &lt; prev | next &gt; new | recent | 2026-02 Change to browse by: cs cs.AI cs.IR References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-188">95. Beyond MSE: Ordinal Cross-Entropy for Probabilistic Time Series Forecasting</h2>
<ul>
<li>链接：https://arxiv.org/abs/2511.10200v2</li>
<li>来源：arxiv</li>
<li>摘要：Time series forecasting is an important task that involves analyzing temporal dependencies and underlying patterns (such as trends, cyclicality, and seasonality) in historical data to predict future values or trends. Current deep learning-based forecasting models primarily employ Mean Squared Error (MSE) loss functions for regression modeling. Despite enabling direct value prediction, this method offers no uncertainty estimation and exhibits poor outlier robustness. To address these limitations, we propose OCE-TS, a novel ordinal classification approach for time series forecasting that replaces MSE with Ordinal Cross-Entropy (OCE) loss, preserving prediction order while quantifying uncertainty through probability output. Specifically, OCE-TS begins by discretizing observed values into ordered intervals and deriving their probabilities via a parametric distribution as supervision signals. Using a simple linear model, we then predict probability distributions for each timestep. The OCE loss is computed between the cumulative distributions of predicted and ground-truth probabilities, explicitly preserving ordinal relationships among forecasted values. Through theoretical analysis usin</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-189">正文（抓取，非 AI）</h3>
<p>[2511.10200v2] Beyond MSE: Ordinal Cross-Entropy for Probabilistic Time Series Forecasting Computer Science &gt; Machine Learning arXiv:2511.10200v2 (cs) [Submitted on 13 Nov 2025 ( v1 ), last revised 27 Nov 2025 (this version, v2)] Title: Beyond MSE: Ordinal Cross-Entropy for Probabilistic Time Series Forecasting Authors: Jieting Wang , Huimei Shi , Feijiang Li , Xiaolei Shang View a PDF of the paper titled Beyond MSE: Ordinal Cross-Entropy for Probabilistic Time Series Forecasting, by Jieting Wang and 3 other authors View PDF HTML (experimental) Abstract: Time series forecasting is an important task that involves analyzing temporal dependencies and underlying patterns (such as trends, cyclicality, and seasonality) in historical data to predict future values or trends. Current deep learning-based forecasting models primarily employ Mean Squared Error (MSE) loss functions for regression modeling. Despite enabling direct value prediction, this method offers no uncertainty estimation and exhibits poor outlier robustness. To address these limitations, we propose OCE-TS, a novel ordinal classification approach for time series forecasting that replaces MSE with Ordinal Cross-Entropy (OCE) loss, preserving prediction order while quantifying uncertainty through probability output. Specifically, OCE-TS begins by discretizing observed values into ordered intervals and deriving their probabilities via a parametric distribution as supervision signals. Using a simple linear model, we then predict probability distributions for each timestep. The OCE loss is computed between the cumulative distributions of predicted and ground-truth probabilities, explicitly preserving ordinal relationships among forecasted values. Through theoretical analysis using influence functions, we establish that cross-entropy (CE) loss exhibits superior stability and outlier robustness compared to MSE loss. Empirically, we compared OCE-TS with five baseline models-Autoformer, DLinear, iTransformer, TimeXer, and TimeBridge-on seven public time series datasets. Using MSE and Mean Absolute Error (MAE) as evaluation metrics, the results demonstrate that OCE-TS consistently outperforms benchmark models. The codeis publicly available at: this https URL . Subjects: Machine Learning (cs.LG) Cite as: arXiv:2511.10200 [cs.LG] (or arXiv:2511.10200v2 [cs.LG] for this version) https://doi.org/10.48550/arXiv.2511.10200 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Feijiang Li [ view email ] [v1] Thu, 13 Nov 2025 11:14:24 UTC (10,056 KB) [v2] Thu, 27 Nov 2025 11:46:13 UTC (10,059 KB) Full-text links: Access Paper: View a PDF of the paper titled Beyond MSE: Ordinal Cross-Entropy for Probabilistic Time Series Forecasting, by Jieting Wang and 3 other authors View PDF HTML (experimental) TeX Source view license Current browse context: cs.LG &lt; prev | next &gt; new | recent | 2025-11 Change to browse by: cs References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) IArxiv recommender toggle IArxiv Recommender ( What is IArxiv? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-190">96. TOPP-DWR: Time-Optimal Path Parameterization of Differential-Driven Wheeled Robots Considering Piecewise-Constant Angular Velocity Constraints</h2>
<ul>
<li>链接：https://arxiv.org/abs/2511.12910v1</li>
<li>来源：arxiv</li>
<li>摘要：Differential-driven wheeled robots (DWR) represent the quintessential type of mobile robots and find extensive appli- cations across the robotic field. Most high-performance control approaches for DWR explicitly utilize the linear and angular velocities of the trajectory as control references. However, existing research on time-optimal path parameterization (TOPP) for mobile robots usually neglects the angular velocity and joint vel- ocity constraints, which can result in degraded control perfor- mance in practical applications. In this article, a systematic and practical TOPP algorithm named TOPP-DWR is proposed for DWR and other mobile robots. First, the non-uniform B-spline is adopted to represent the initial trajectory in the task space. Second, the piecewise-constant angular velocity, as well as joint velocity, linear velocity, and linear acceleration constraints, are incorporated into the TOPP problem. During the construction of the optimization problem, the aforementioned constraints are uniformly represented as linear velocity constraints. To boost the numerical computational efficiency, we introduce a slack variable to reformulate the problem into second-order-cone program</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-191">正文（抓取，非 AI）</h3>
<p>[2511.12910v1] TOPP-DWR: Time-Optimal Path Parameterization of Differential-Driven Wheeled Robots Considering Piecewise-Constant Angular Velocity Constraints Computer Science &gt; Robotics arXiv:2511.12910v1 (cs) [Submitted on 17 Nov 2025] Title: TOPP-DWR: Time-Optimal Path Parameterization of Differential-Driven Wheeled Robots Considering Piecewise-Constant Angular Velocity Constraints Authors: Yong Li , Yujun Huang , Yi Chen , Hui Cheng View a PDF of the paper titled TOPP-DWR: Time-Optimal Path Parameterization of Differential-Driven Wheeled Robots Considering Piecewise-Constant Angular Velocity Constraints, by Yong Li and 2 other authors View PDF Abstract: Differential-driven wheeled robots (DWR) represent the quintessential type of mobile robots and find extensive appli- cations across the robotic field. Most high-performance control approaches for DWR explicitly utilize the linear and angular velocities of the trajectory as control references. However, existing research on time-optimal path parameterization (TOPP) for mobile robots usually neglects the angular velocity and joint vel- ocity constraints, which can result in degraded control perfor- mance in practical applications. In this article, a systematic and practical TOPP algorithm named TOPP-DWR is proposed for DWR and other mobile robots. First, the non-uniform B-spline is adopted to represent the initial trajectory in the task space. Second, the piecewise-constant angular velocity, as well as joint velocity, linear velocity, and linear acceleration constraints, are incorporated into the TOPP problem. During the construction of the optimization problem, the aforementioned constraints are uniformly represented as linear velocity constraints. To boost the numerical computational efficiency, we introduce a slack variable to reformulate the problem into second-order-cone programming (SOCP). Subsequently, comparative experiments are conducted to validate the superiority of the proposed method. Quantitative performance indexes show that TOPP-DWR achieves TOPP while adhering to all constraints. Finally, field autonomous navigation experiments are carried out to validate the practicability of TOPP-DWR in real-world applications. Subjects: Robotics (cs.RO) Report number: IROS20251376 Cite as: arXiv:2511.12910 [cs.RO] (or arXiv:2511.12910v1 [cs.RO] for this version) https://doi.org/10.48550/arXiv.2511.12910 Focus to learn more arXiv-issued DOI via DataCite Journal reference: 2025 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2025) Submission history From: Yong Li [ view email ] [v1] Mon, 17 Nov 2025 02:58:53 UTC (5,008 KB) Full-text links: Access Paper: View a PDF of the paper titled TOPP-DWR: Time-Optimal Path Parameterization of Differential-Driven Wheeled Robots Considering Piecewise-Constant Angular Velocity Constraints, by Yong Li and 2 other authors View PDF view license Current browse context: cs.RO &lt; prev | next &gt; new | recent | 2025-11 Change to browse by: cs References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-192">97. MovieTeller: Tool-augmented Movie Synopsis with ID Consistent Progressive Abstraction</h2>
<ul>
<li>链接：https://arxiv.org/abs/2602.23228v1</li>
<li>来源：arxiv</li>
<li>摘要：With the explosive growth of digital entertainment, automated video summarization has become indispensable for applications such as content indexing, personalized recommendation, and efficient media archiving. Automatic synopsis generation for long-form videos, such as movies and TV series, presents a significant challenge for existing Vision-Language Models (VLMs). While proficient at single-image captioning, these general-purpose models often exhibit critical failures in long-duration contexts, primarily a lack of ID-consistent character identification and a fractured narrative coherence. To overcome these limitations, we propose MovieTeller, a novel framework for generating movie synopses via tool-augmented progressive abstraction. Our core contribution is a training-free, tool-augmented, fact-grounded generation process. Instead of requiring costly model fine-tuning, our framework directly leverages off-the-shelf models in a plug-and-play manner. We first invoke a specialized face recognition model as an external "tool" to establish Factual Groundings--precise character identities and their corresponding bounding boxes. These groundings are then injected into the prompt to stee</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-193">正文（抓取，非 AI）</h3>
<p>[2602.23228v1] MovieTeller: Tool-augmented Movie Synopsis with ID Consistent Progressive Abstraction Computer Science &gt; Computer Vision and Pattern Recognition arXiv:2602.23228v1 (cs) [Submitted on 26 Feb 2026] Title: MovieTeller: Tool-augmented Movie Synopsis with ID Consistent Progressive Abstraction Authors: Yizhi Li , Xiaohan Chen , Miao Jiang , Wentao Tang , Gaoang Wang View a PDF of the paper titled MovieTeller: Tool-augmented Movie Synopsis with ID Consistent Progressive Abstraction, by Yizhi Li and 3 other authors View PDF HTML (experimental) Abstract: With the explosive growth of digital entertainment, automated video summarization has become indispensable for applications such as content indexing, personalized recommendation, and efficient media archiving. Automatic synopsis generation for long-form videos, such as movies and TV series, presents a significant challenge for existing Vision-Language Models (VLMs). While proficient at single-image captioning, these general-purpose models often exhibit critical failures in long-duration contexts, primarily a lack of ID-consistent character identification and a fractured narrative coherence. To overcome these limitations, we propose MovieTeller, a novel framework for generating movie synopses via tool-augmented progressive abstraction. Our core contribution is a training-free, tool-augmented, fact-grounded generation process. Instead of requiring costly model fine-tuning, our framework directly leverages off-the-shelf models in a plug-and-play manner. We first invoke a specialized face recognition model as an external "tool" to establish Factual Groundings--precise character identities and their corresponding bounding boxes. These groundings are then injected into the prompt to steer the VLM's reasoning, ensuring the generated scene descriptions are anchored to verifiable facts. Furthermore, our progressive abstraction pipeline decomposes the summarization of a full-length movie into a multi-stage process, effectively mitigating the context length limitations of current VLMs. Experiments demonstrate that our approach yields significant improvements in factual accuracy, character consistency, and overall narrative coherence compared to end-to-end baselines. Comments: 6 pages, CSCWD 2026 Subjects: Computer Vision and Pattern Recognition (cs.CV) ; Artificial Intelligence (cs.AI) Cite as: arXiv:2602.23228 [cs.CV] (or arXiv:2602.23228v1 [cs.CV] for this version) https://doi.org/10.48550/arXiv.2602.23228 Focus to learn more arXiv-issued DOI via DataCite (pending registration) Submission history From: Yizhi Li [ view email ] [v1] Thu, 26 Feb 2026 17:08:08 UTC (8,873 KB) Full-text links: Access Paper: View a PDF of the paper titled MovieTeller: Tool-augmented Movie Synopsis with ID Consistent Progressive Abstraction, by Yizhi Li and 3 other authors View PDF HTML (experimental) TeX Source view license Current browse context: cs.CV &lt; prev | next &gt; new | recent | 2026-02 Change to browse by: cs cs.AI References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-194">98. Evaluating Zero-Shot and One-Shot Adaptation of Small Language Models in Leader-Follower Interaction</h2>
<ul>
<li>链接：https://arxiv.org/abs/2602.23312v1</li>
<li>来源：arxiv</li>
<li>摘要：Leader-follower interaction is an important paradigm in human-robot interaction (HRI). Yet, assigning roles in real time remains challenging for resource-constrained mobile and assistive robots. While large language models (LLMs) have shown promise for natural communication, their size and latency limit on-device deployment. Small language models (SLMs) offer a potential alternative, but their effectiveness for role classification in HRI has not been systematically evaluated. In this paper, we present a benchmark of SLMs for leader-follower communication, introducing a novel dataset derived from a published database and augmented with synthetic samples to capture interaction-specific dynamics. We investigate two adaptation strategies: prompt engineering and fine-tuning, studied under zero-shot and one-shot interaction modes, compared with an untrained baseline. Experiments with Qwen2.5-0.5B reveal that zero-shot fine-tuning achieves robust classification performance (86.66% accuracy) while maintaining low latency (22.2 ms per sample), significantly outperforming baseline and prompt-engineered approaches. However, results also indicate a performance degradation in one-shot modes, wh</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-195">正文（抓取，非 AI）</h3>
<p>[2602.23312v1] Evaluating Zero-Shot and One-Shot Adaptation of Small Language Models in Leader-Follower Interaction Computer Science &gt; Human-Computer Interaction arXiv:2602.23312v1 (cs) [Submitted on 26 Feb 2026] Title: Evaluating Zero-Shot and One-Shot Adaptation of Small Language Models in Leader-Follower Interaction Authors: Rafael R. Baptista , André de Lima Salgado , Ricardo V. Godoy , Marcelo Becker , Thiago Boaventura , Gustavo J. G. Lahr View a PDF of the paper titled Evaluating Zero-Shot and One-Shot Adaptation of Small Language Models in Leader-Follower Interaction, by Rafael R. Baptista and 5 other authors View PDF HTML (experimental) Abstract: Leader-follower interaction is an important paradigm in human-robot interaction (HRI). Yet, assigning roles in real time remains challenging for resource-constrained mobile and assistive robots. While large language models (LLMs) have shown promise for natural communication, their size and latency limit on-device deployment. Small language models (SLMs) offer a potential alternative, but their effectiveness for role classification in HRI has not been systematically evaluated. In this paper, we present a benchmark of SLMs for leader-follower communication, introducing a novel dataset derived from a published database and augmented with synthetic samples to capture interaction-specific dynamics. We investigate two adaptation strategies: prompt engineering and fine-tuning, studied under zero-shot and one-shot interaction modes, compared with an untrained baseline. Experiments with Qwen2.5-0.5B reveal that zero-shot fine-tuning achieves robust classification performance (86.66% accuracy) while maintaining low latency (22.2 ms per sample), significantly outperforming baseline and prompt-engineered approaches. However, results also indicate a performance degradation in one-shot modes, where increased context length challenges the model's architectural capacity. These findings demonstrate that fine-tuned SLMs provide an effective solution for direct role assignment, while highlighting critical trade-offs between dialogue complexity and classification reliability on the edge. Subjects: Human-Computer Interaction (cs.HC) ; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Robotics (cs.RO); Systems and Control (eess.SY) Cite as: arXiv:2602.23312 [cs.HC] (or arXiv:2602.23312v1 [cs.HC] for this version) https://doi.org/10.48550/arXiv.2602.23312 Focus to learn more arXiv-issued DOI via DataCite (pending registration) Submission history From: Ricardo Vilela De Godoy Dr. [ view email ] [v1] Thu, 26 Feb 2026 18:20:26 UTC (1,308 KB) Full-text links: Access Paper: View a PDF of the paper titled Evaluating Zero-Shot and One-Shot Adaptation of Small Language Models in Leader-Follower Interaction, by Rafael R. Baptista and 5 other authors View PDF HTML (experimental) TeX Source view license Current browse context: cs.HC &lt; prev | next &gt; new | recent | 2026-02 Change to browse by: cs cs.AI cs.LG cs.RO cs.SY eess eess.SY References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-196">99. InnerQ: Hardware-aware Tuning-free Quantization of KV Cache for Large Language Models</h2>
<ul>
<li>链接：https://arxiv.org/abs/2602.23200v1</li>
<li>来源：arxiv</li>
<li>摘要：Reducing the hardware footprint of large language models (LLMs) during decoding is critical for efficient long-sequence generation. A key bottleneck is the key-value (KV) cache, whose size scales with sequence length and easily dominates the memory footprint of the model. Previous work proposed quantization methods that are focused on compressing the KV cache while maintaining its information. We introduce InnerQ, a hardware-aware KV-cache quantization scheme that lowers decode latency without sacrificing accuracy. InnerQ applies group-wise quantization while grouping the cache matrices over their inner dimension. Unlike previous work that group over the outer dimension, InnerQ aligns dequantization with the vector-matrix multiplication and enables scale factor reuse across GPU compute units. This reduces memory accesses and accelerates dequantization, yielding up to $22\%$ speedup over previous work and up to $88\%$ over half-precision vector-matrix multiplication. To preserve fidelity under aggressive compression, InnerQ incorporates (i) hybrid quantization, selecting symmetric or asymmetric quantization per group based on local statistics; (ii) high-precision windows for both th</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-197">正文（抓取，非 AI）</h3>
<p>[2602.23200v1] InnerQ: Hardware-aware Tuning-free Quantization of KV Cache for Large Language Models Computer Science &gt; Machine Learning arXiv:2602.23200v1 (cs) [Submitted on 26 Feb 2026] Title: InnerQ: Hardware-aware Tuning-free Quantization of KV Cache for Large Language Models Authors: Sayed Mohammadreza Tayaranian Hosseini , Amir Ardakani , Warren J. Gross View a PDF of the paper titled InnerQ: Hardware-aware Tuning-free Quantization of KV Cache for Large Language Models, by Sayed Mohammadreza Tayaranian Hosseini and 2 other authors View PDF Abstract: Reducing the hardware footprint of large language models (LLMs) during decoding is critical for efficient long-sequence generation. A key bottleneck is the key-value (KV) cache, whose size scales with sequence length and easily dominates the memory footprint of the model. Previous work proposed quantization methods that are focused on compressing the KV cache while maintaining its information. We introduce InnerQ, a hardware-aware KV-cache quantization scheme that lowers decode latency without sacrificing accuracy. InnerQ applies group-wise quantization while grouping the cache matrices over their inner dimension. Unlike previous work that group over the outer dimension, InnerQ aligns dequantization with the vector-matrix multiplication and enables scale factor reuse across GPU compute units. This reduces memory accesses and accelerates dequantization, yielding up to $22\%$ speedup over previous work and up to $88\%$ over half-precision vector-matrix multiplication. To preserve fidelity under aggressive compression, InnerQ incorporates (i) hybrid quantization, selecting symmetric or asymmetric quantization per group based on local statistics; (ii) high-precision windows for both the most recent tokens and the attention sink tokens to mitigate outlier leakage; and (iii) per-channel normalization of the key cache, computed once during prefill and folded into the query to avoid runtime overhead. Our evaluation experiments on Llama models shows that InnerQ maintains a few-shot GSM8K performance comparable to non-quantized KV caches and surpasses prior KV cache quantization methods. Comments: 16 pages, 4 figures, 4 tables, 2 algorithms Subjects: Machine Learning (cs.LG) ; Computation and Language (cs.CL) Cite as: arXiv:2602.23200 [cs.LG] (or arXiv:2602.23200v1 [cs.LG] for this version) https://doi.org/10.48550/arXiv.2602.23200 Focus to learn more arXiv-issued DOI via DataCite (pending registration) Submission history From: Sayed Mohammadreza Tayaranian Hosseini [ view email ] [v1] Thu, 26 Feb 2026 16:50:36 UTC (30 KB) Full-text links: Access Paper: View a PDF of the paper titled InnerQ: Hardware-aware Tuning-free Quantization of KV Cache for Large Language Models, by Sayed Mohammadreza Tayaranian Hosseini and 2 other authors View PDF TeX Source view license Current browse context: cs.LG &lt; prev | next &gt; new | recent | 2026-02 Change to browse by: cs cs.CL References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) IArxiv recommender toggle IArxiv Recommender ( What is IArxiv? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-198">100. Benchmarking State Space Models, Transformers, and Recurrent Networks for US Grid Forecasting</h2>
<ul>
<li>链接：https://arxiv.org/abs/2602.21415v1</li>
<li>来源：arxiv</li>
<li>摘要：Selecting the right deep learning model for power grid forecasting is challenging, as performance heavily depends on the data available to the operator. This paper presents a comprehensive benchmark of five modern neural architectures: two state space models (PowerMamba, S-Mamba), two Transformers (iTransformer, PatchTST), and a traditional LSTM. We evaluate these models on hourly electricity demand across six diverse US power grids for forecast windows between 24 and 168 hours. To ensure a fair comparison, we adapt each model with specialized temporal processing and a modular layer that cleanly integrates weather covariates. Our results reveal that there is no single best model for all situations. When forecasting using only historical load, PatchTST and the state space models provide the highest accuracy. However, when explicit weather data is added to the inputs, the rankings reverse: iTransformer improves its accuracy three times more efficiently than PatchTST. By controlling for model size, we confirm that this advantage stems from the architecture's inherent ability to mix information across different variables. Extending our evaluation to solar generation, wind power, and wh</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-199">正文（抓取，非 AI）</h3>
<p>[2602.21415v1] Benchmarking State Space Models, Transformers, and Recurrent Networks for US Grid Forecasting Computer Science &gt; Machine Learning arXiv:2602.21415v1 (cs) [Submitted on 24 Feb 2026] Title: Benchmarking State Space Models, Transformers, and Recurrent Networks for US Grid Forecasting Authors: Sunki Hong , Jisoo Lee , Yuanyuan Shi View a PDF of the paper titled Benchmarking State Space Models, Transformers, and Recurrent Networks for US Grid Forecasting, by Sunki Hong and 2 other authors View PDF HTML (experimental) Abstract: Selecting the right deep learning model for power grid forecasting is challenging, as performance heavily depends on the data available to the operator. This paper presents a comprehensive benchmark of five modern neural architectures: two state space models (PowerMamba, S-Mamba), two Transformers (iTransformer, PatchTST), and a traditional LSTM. We evaluate these models on hourly electricity demand across six diverse US power grids for forecast windows between 24 and 168 hours. To ensure a fair comparison, we adapt each model with specialized temporal processing and a modular layer that cleanly integrates weather covariates. Our results reveal that there is no single best model for all situations. When forecasting using only historical load, PatchTST and the state space models provide the highest accuracy. However, when explicit weather data is added to the inputs, the rankings reverse: iTransformer improves its accuracy three times more efficiently than PatchTST. By controlling for model size, we confirm that this advantage stems from the architecture's inherent ability to mix information across different variables. Extending our evaluation to solar generation, wind power, and wholesale prices further demonstrates that model rankings depend on the forecast task: PatchTST excels on highly rhythmic signals like solar, while state space models are better suited for the chaotic fluctuations of wind and price. Ultimately, this benchmark provides grid operators with actionable guidelines for selecting the optimal forecasting architecture based on their specific data environments. Comments: 11 pages, 2 figures, 8 tables Subjects: Machine Learning (cs.LG) ; Systems and Control (eess.SY) MSC classes: 68T07, 62M20 ACM classes: I.2.6; G.3; J.2 Cite as: arXiv:2602.21415 [cs.LG] (or arXiv:2602.21415v1 [cs.LG] for this version) https://doi.org/10.48550/arXiv.2602.21415 Focus to learn more arXiv-issued DOI via DataCite (pending registration) Submission history From: Jisoo Lee [ view email ] [v1] Tue, 24 Feb 2026 22:42:39 UTC (2,320 KB) Full-text links: Access Paper: View a PDF of the paper titled Benchmarking State Space Models, Transformers, and Recurrent Networks for US Grid Forecasting, by Sunki Hong and 2 other authors View PDF HTML (experimental) TeX Source view license Current browse context: cs.LG &lt; prev | next &gt; new | recent | 2026-02 Change to browse by: cs cs.SY eess eess.SY References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) IArxiv recommender toggle IArxiv Recommender ( What is IArxiv? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-200">101. AlayaLaser: Efficient Index Layout and Search Strategy for Large-scale High-dimensional Vector Similarity Search</h2>
<ul>
<li>链接：https://arxiv.org/abs/2602.23342v1</li>
<li>来源：arxiv</li>
<li>摘要：On-disk graph-based approximate nearest neighbor search (ANNS) is essential for large-scale, high-dimensional vector retrieval, yet its performance is widely recognized to be limited by the prohibitive I/O costs. Interestingly, we observed that the performance of on-disk graph-based index systems is compute-bound, not I/O-bound, with the rising of the vector data dimensionality (e.g., hundreds or thousands). This insight uncovers a significant optimization opportunity: existing on-disk graph-based index systems universally target I/O reduction and largely overlook computational overhead, which leaves a substantial performance improvement space.   In this work, we propose AlayaLaser, an efficient on-disk graph-based index system for large-scale high-dimensional vector similarity search. In particular, we first conduct performance analysis on existing on-disk graph-based index systems via the adapted roofline model, then we devise a novel on-disk data layout in AlayaLaser to effectively alleviate the compute-bound, which is revealed by the above roofline model analysis, by exploiting SIMD instructions on modern CPUs. We next design a suite of optimization techniques (e.g., degree-bas</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-201">正文（抓取，非 AI）</h3>
<p>[2602.23342v1] AlayaLaser: Efficient Index Layout and Search Strategy for Large-scale High-dimensional Vector Similarity Search Computer Science &gt; Databases arXiv:2602.23342v1 (cs) [Submitted on 26 Feb 2026] Title: AlayaLaser: Efficient Index Layout and Search Strategy for Large-scale High-dimensional Vector Similarity Search Authors: Weijian Chen , Haotian Liu , Yangshen Deng , Long Xiang , Liang Huang , Gezi Li , Bo Tang View a PDF of the paper titled AlayaLaser: Efficient Index Layout and Search Strategy for Large-scale High-dimensional Vector Similarity Search, by Weijian Chen and 5 other authors View PDF HTML (experimental) Abstract: On-disk graph-based approximate nearest neighbor search (ANNS) is essential for large-scale, high-dimensional vector retrieval, yet its performance is widely recognized to be limited by the prohibitive I/O costs. Interestingly, we observed that the performance of on-disk graph-based index systems is compute-bound, not I/O-bound, with the rising of the vector data dimensionality (e.g., hundreds or thousands). This insight uncovers a significant optimization opportunity: existing on-disk graph-based index systems universally target I/O reduction and largely overlook computational overhead, which leaves a substantial performance improvement space. In this work, we propose AlayaLaser, an efficient on-disk graph-based index system for large-scale high-dimensional vector similarity search. In particular, we first conduct performance analysis on existing on-disk graph-based index systems via the adapted roofline model, then we devise a novel on-disk data layout in AlayaLaser to effectively alleviate the compute-bound, which is revealed by the above roofline model analysis, by exploiting SIMD instructions on modern CPUs. We next design a suite of optimization techniques (e.g., degree-based node cache, cluster-based entry point selection, and early dispatch strategy) to further improve the performance of AlayaLaser. We last conduct extensive experimental studies on a wide range of large-scale high-dimensional vector datasets to verify the superiority of AlayaLaser. Specifically, AlayaLaser not only surpasses existing on-disk graph-based index systems but also matches or even exceeds the performance of in-memory index systems. Comments: The paper has been accepted by SIGMOD 2026 Subjects: Databases (cs.DB) ; Information Retrieval (cs.IR) Cite as: arXiv:2602.23342 [cs.DB] (or arXiv:2602.23342v1 [cs.DB] for this version) https://doi.org/10.48550/arXiv.2602.23342 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Long Xiang [ view email ] [v1] Thu, 26 Feb 2026 18:48:29 UTC (685 KB) Full-text links: Access Paper: View a PDF of the paper titled AlayaLaser: Efficient Index Layout and Search Strategy for Large-scale High-dimensional Vector Similarity Search, by Weijian Chen and 5 other authors View PDF HTML (experimental) TeX Source view license Current browse context: cs.DB &lt; prev | next &gt; new | recent | 2026-02 Change to browse by: cs cs.IR References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-202">102. ManifoldGD: Training-Free Hierarchical Manifold Guidance for Diffusion-Based Dataset Distillation</h2>
<ul>
<li>链接：https://arxiv.org/abs/2602.23295v1</li>
<li>来源：arxiv</li>
<li>摘要：In recent times, large datasets hinder efficient model training while also containing redundant concepts. Dataset distillation aims to synthesize compact datasets that preserve the knowledge of large-scale training sets while drastically reducing storage and computation. Recent advances in diffusion models have enabled training-free distillation by leveraging pre-trained generative priors; however, existing guidance strategies remain limited. Current score-based methods either perform unguided denoising or rely on simple mode-based guidance toward instance prototype centroids (IPC centroids), which often are rudimentary and suboptimal. We propose Manifold-Guided Distillation (ManifoldGD), a training-free diffusion-based framework that integrates manifold consistent guidance at every denoising timestep. Our method employs IPCs computed via a hierarchical, divisive clustering of VAE latent features, yielding a multi-scale coreset of IPCs that captures both coarse semantic modes and fine intra-class variability. Using a local neighborhood of the extracted IPC centroids, we create the latent manifold for each diffusion denoising timestep. At each denoising step, we project the mode-ali</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-203">正文（抓取，非 AI）</h3>
<p>[2602.23295v1] ManifoldGD: Training-Free Hierarchical Manifold Guidance for Diffusion-Based Dataset Distillation Computer Science &gt; Computer Vision and Pattern Recognition arXiv:2602.23295v1 (cs) [Submitted on 26 Feb 2026] Title: ManifoldGD: Training-Free Hierarchical Manifold Guidance for Diffusion-Based Dataset Distillation Authors: Ayush Roy , Wei-Yang Alex Lee , Rudrasis Chakraborty , Vishnu Suresh Lokhande View a PDF of the paper titled ManifoldGD: Training-Free Hierarchical Manifold Guidance for Diffusion-Based Dataset Distillation, by Ayush Roy and 3 other authors View PDF HTML (experimental) Abstract: In recent times, large datasets hinder efficient model training while also containing redundant concepts. Dataset distillation aims to synthesize compact datasets that preserve the knowledge of large-scale training sets while drastically reducing storage and computation. Recent advances in diffusion models have enabled training-free distillation by leveraging pre-trained generative priors; however, existing guidance strategies remain limited. Current score-based methods either perform unguided denoising or rely on simple mode-based guidance toward instance prototype centroids (IPC centroids), which often are rudimentary and suboptimal. We propose Manifold-Guided Distillation (ManifoldGD), a training-free diffusion-based framework that integrates manifold consistent guidance at every denoising timestep. Our method employs IPCs computed via a hierarchical, divisive clustering of VAE latent features, yielding a multi-scale coreset of IPCs that captures both coarse semantic modes and fine intra-class variability. Using a local neighborhood of the extracted IPC centroids, we create the latent manifold for each diffusion denoising timestep. At each denoising step, we project the mode-alignment vector onto the local tangent space of the estimated latent manifold, thus constraining the generation trajectory to remain manifold-faithful while preserving semantic consistency. This formulation improves representativeness, diversity, and image fidelity without requiring any model retraining. Empirical results demonstrate consistent gains over existing training-free and training-based baselines in terms of FID, l2 distance among real and synthetic dataset embeddings, and classification accuracy, establishing ManifoldGD as the first geometry-aware training-free data distillation framework. Comments: CVPE 2026 Subjects: Computer Vision and Pattern Recognition (cs.CV) ; Machine Learning (cs.LG) Cite as: arXiv:2602.23295 [cs.CV] (or arXiv:2602.23295v1 [cs.CV] for this version) https://doi.org/10.48550/arXiv.2602.23295 Focus to learn more arXiv-issued DOI via DataCite (pending registration) Submission history From: Ayush Roy [ view email ] [v1] Thu, 26 Feb 2026 18:07:10 UTC (17,761 KB) Full-text links: Access Paper: View a PDF of the paper titled ManifoldGD: Training-Free Hierarchical Manifold Guidance for Diffusion-Based Dataset Distillation, by Ayush Roy and 3 other authors View PDF HTML (experimental) TeX Source view license Current browse context: cs.CV &lt; prev | next &gt; new | recent | 2026-02 Change to browse by: cs cs.LG References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-204">103. Efficient MPC-Based Energy Management System for Secure and Cost-Effective Microgrid Operations</h2>
<ul>
<li>链接：https://arxiv.org/abs/2510.03241v2</li>
<li>来源：arxiv</li>
<li>摘要：Model predictive control (MPC)-based energy management systems (EMS) are essential for ensuring optimal, secure, and stable operation in microgrids with high penetrations of distributed energy resources. However, due to the high computational cost for the decision-making, the conventional MPC-based EMS typically adopts a simplified integrated-bus power balance model. While this simplification is effective for small networks, large-scale systems require a more detailed branch flow model to account for the increased impact of grid power losses and security constraints. This work proposes an efficient and reliable MPC-based EMS that incorporates power-loss effects and grid-security constraints. %, while adaptively shaping the battery power profile in response to online renewable inputs, achieving reduced operational costs. It enhances system reliability, reduces operational costs, and shows strong potential for online implementation due to its reduced computational effort. Specifically, a second-order cone program (SOCP) branch flow relaxation is integrated into the constraint set, yielding a convex formulation that guarantees globally optimal solutions with high computational efficie</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-205">正文（抓取，非 AI）</h3>
<p>[2510.03241v2] Efficient MPC-Based Energy Management System for Secure and Cost-Effective Microgrid Operations Electrical Engineering and Systems Science &gt; Systems and Control arXiv:2510.03241v2 (eess) [Submitted on 23 Sep 2025 ( v1 ), last revised 7 Oct 2025 (this version, v2)] Title: Efficient MPC-Based Energy Management System for Secure and Cost-Effective Microgrid Operations Authors: Hanyang He , John Harlim , Daning Huang , Yan Li View a PDF of the paper titled Efficient MPC-Based Energy Management System for Secure and Cost-Effective Microgrid Operations, by Hanyang He and 3 other authors View PDF HTML (experimental) Abstract: Model predictive control (MPC)-based energy management systems (EMS) are essential for ensuring optimal, secure, and stable operation in microgrids with high penetrations of distributed energy resources. However, due to the high computational cost for the decision-making, the conventional MPC-based EMS typically adopts a simplified integrated-bus power balance model. While this simplification is effective for small networks, large-scale systems require a more detailed branch flow model to account for the increased impact of grid power losses and security constraints. This work proposes an efficient and reliable MPC-based EMS that incorporates power-loss effects and grid-security constraints. %, while adaptively shaping the battery power profile in response to online renewable inputs, achieving reduced operational costs. It enhances system reliability, reduces operational costs, and shows strong potential for online implementation due to its reduced computational effort. Specifically, a second-order cone program (SOCP) branch flow relaxation is integrated into the constraint set, yielding a convex formulation that guarantees globally optimal solutions with high computational efficiency. Owing to the radial topology of the microgrid, this relaxation is practically tight, ensuring equivalence to the original problem. Building on this foundation, an online demand response (DR) module is designed to further reduce the operation cost through peak shaving. To the best of our knowledge, no prior MPC-EMS framework has simultaneously modeled losses and security constraints while coordinating flexible loads within a unified architecture. The developed framework enables secure operation with effective peak shaving and reduced total cost. The effectiveness of the proposed method is validated on 10-bus, 18-bus, and 33-bus systems. Subjects: Systems and Control (eess.SY) Cite as: arXiv:2510.03241 [eess.SY] (or arXiv:2510.03241v2 [eess.SY] for this version) https://doi.org/10.48550/arXiv.2510.03241 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Hanyang He [ view email ] [v1] Tue, 23 Sep 2025 13:29:36 UTC (11,061 KB) [v2] Tue, 7 Oct 2025 12:07:41 UTC (11,066 KB) Full-text links: Access Paper: View a PDF of the paper titled Efficient MPC-Based Energy Management System for Secure and Cost-Effective Microgrid Operations, by Hanyang He and 3 other authors View PDF HTML (experimental) TeX Source view license Current browse context: eess.SY &lt; prev | next &gt; new | recent | 2025-10 Change to browse by: cs cs.SY eess References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-206">104. SC-Arena: A Natural Language Benchmark for Single-Cell Reasoning with Knowledge-Augmented Evaluation</h2>
<ul>
<li>链接：https://arxiv.org/abs/2602.23199v1</li>
<li>来源：arxiv</li>
<li>摘要：Large language models (LLMs) are increasingly applied in scientific research, offering new capabilities for knowledge discovery and reasoning. In single-cell biology, however, evaluation practices for both general and specialized LLMs remain inadequate: existing benchmarks are fragmented across tasks, adopt formats such as multiple-choice classification that diverge from real-world usage, and rely on metrics lacking interpretability and biological grounding. We present SC-ARENA, a natural language evaluation framework tailored to single-cell foundation models. SC-ARENA formalizes a virtual cell abstraction that unifies evaluation targets by representing both intrinsic attributes and gene-level interactions. Within this paradigm, we define five natural language tasks (cell type annotation, captioning, generation, perturbation prediction, and scientific QA) that probe core reasoning capabilities in cellular biology. To overcome the limitations of brittle string-matching metrics, we introduce knowledge-augmented evaluation, which incorporates external ontologies, marker databases, and scientific literature to support biologically faithful and interpretable judgments. Experiments and a</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-207">正文（抓取，非 AI）</h3>
<p>[2602.23199v1] SC-Arena: A Natural Language Benchmark for Single-Cell Reasoning with Knowledge-Augmented Evaluation Computer Science &gt; Artificial Intelligence arXiv:2602.23199v1 (cs) [Submitted on 26 Feb 2026] Title: SC-Arena: A Natural Language Benchmark for Single-Cell Reasoning with Knowledge-Augmented Evaluation Authors: Jiahao Zhao , Feng Jiang , Shaowei Qin , Zhonghui Zhang , Junhao Liu , Guibing Guo , Hamid Alinejad-Rokny , Min Yang View a PDF of the paper titled SC-Arena: A Natural Language Benchmark for Single-Cell Reasoning with Knowledge-Augmented Evaluation, by Jiahao Zhao and 7 other authors View PDF HTML (experimental) Abstract: Large language models (LLMs) are increasingly applied in scientific research, offering new capabilities for knowledge discovery and reasoning. In single-cell biology, however, evaluation practices for both general and specialized LLMs remain inadequate: existing benchmarks are fragmented across tasks, adopt formats such as multiple-choice classification that diverge from real-world usage, and rely on metrics lacking interpretability and biological grounding. We present SC-ARENA, a natural language evaluation framework tailored to single-cell foundation models. SC-ARENA formalizes a virtual cell abstraction that unifies evaluation targets by representing both intrinsic attributes and gene-level interactions. Within this paradigm, we define five natural language tasks (cell type annotation, captioning, generation, perturbation prediction, and scientific QA) that probe core reasoning capabilities in cellular biology. To overcome the limitations of brittle string-matching metrics, we introduce knowledge-augmented evaluation, which incorporates external ontologies, marker databases, and scientific literature to support biologically faithful and interpretable judgments. Experiments and analysis across both general-purpose and domain-specialized LLMs demonstrate that (i) under the Virtual Cell unified evaluation paradigm, current models achieve uneven performance on biologically complex tasks, particularly those demanding mechanistic or causal understanding; and (ii) our knowledge-augmented evaluation framework ensures biological correctness, provides interpretable, evidence-grounded rationales, and achieves high discriminative capacity, overcoming the brittleness and opacity of conventional metrics. SC-Arena thus provides a unified and interpretable framework for assessing LLMs in single-cell biology, pointing toward the development of biology-aligned, generalizable foundation models. Subjects: Artificial Intelligence (cs.AI) Cite as: arXiv:2602.23199 [cs.AI] (or arXiv:2602.23199v1 [cs.AI] for this version) https://doi.org/10.48550/arXiv.2602.23199 Focus to learn more arXiv-issued DOI via DataCite (pending registration) Submission history From: Zhao Jiahao [ view email ] [v1] Thu, 26 Feb 2026 16:50:28 UTC (1,738 KB) Full-text links: Access Paper: View a PDF of the paper titled SC-Arena: A Natural Language Benchmark for Single-Cell Reasoning with Knowledge-Augmented Evaluation, by Jiahao Zhao and 7 other authors View PDF HTML (experimental) TeX Source view license Current browse context: cs.AI &lt; prev | next &gt; new | recent | 2026-02 Change to browse by: cs References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-208">105. STELLAR: Storage Tuning Engine Leveraging LLM Autonomous Reasoning for High Performance Parallel File Systems</h2>
<ul>
<li>链接：https://arxiv.org/abs/2602.23220v1</li>
<li>来源：arxiv</li>
<li>摘要：I/O performance is crucial to efficiency in data-intensive scientific computing; but tuning large-scale storage systems is complex, costly, and notoriously manpower-intensive, making it inaccessible for most domain scientists. To address this problem, we propose STELLAR, an autonomous tuner for high-performance parallel file systems. Our evaluations show that STELLAR almost always selects near-optimal parameter configurations for parallel file systems within the first five attempts, even for previously unseen applications.   STELLAR differs fundamentally from traditional autotuning methods, which often require hundreds of thousands of iterations to converge. Powered by large language models (LLMs), STELLAR enables autonomous end-to-end agentic tuning by (1) accurately extracting tunable parameters from software manuals, (2) analyzing I/O trace logs generated by applications, (3) selecting initial tuning strategies, (4) rerunning applications on real systems and collecting I/O performance feedback, (5) adjusting tuning strategies and repeating the tuning cycle, and (6) reflecting on and summarizing tuning experiences into reusable knowledge for future optimizations. STELLAR integrat</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-209">正文（抓取，非 AI）</h3>
<p>[2602.23220v1] STELLAR: Storage Tuning Engine Leveraging LLM Autonomous Reasoning for High Performance Parallel File Systems Computer Science &gt; Distributed, Parallel, and Cluster Computing arXiv:2602.23220v1 (cs) [Submitted on 26 Feb 2026] Title: STELLAR: Storage Tuning Engine Leveraging LLM Autonomous Reasoning for High Performance Parallel File Systems Authors: Chris Egersdoerfer , Philip Carns , Shane Snyder , Robert Ross , Dong Dai View a PDF of the paper titled STELLAR: Storage Tuning Engine Leveraging LLM Autonomous Reasoning for High Performance Parallel File Systems, by Chris Egersdoerfer and 4 other authors View PDF HTML (experimental) Abstract: I/O performance is crucial to efficiency in data-intensive scientific computing; but tuning large-scale storage systems is complex, costly, and notoriously manpower-intensive, making it inaccessible for most domain scientists. To address this problem, we propose STELLAR, an autonomous tuner for high-performance parallel file systems. Our evaluations show that STELLAR almost always selects near-optimal parameter configurations for parallel file systems within the first five attempts, even for previously unseen applications. STELLAR differs fundamentally from traditional autotuning methods, which often require hundreds of thousands of iterations to converge. Powered by large language models (LLMs), STELLAR enables autonomous end-to-end agentic tuning by (1) accurately extracting tunable parameters from software manuals, (2) analyzing I/O trace logs generated by applications, (3) selecting initial tuning strategies, (4) rerunning applications on real systems and collecting I/O performance feedback, (5) adjusting tuning strategies and repeating the tuning cycle, and (6) reflecting on and summarizing tuning experiences into reusable knowledge for future optimizations. STELLAR integrates retrieval-augmented generation (RAG), tool execution, LLM-based reasoning, and a multiagent design to stabilize reasoning and combat hallucinations. We evaluate the impact of each component on optimization outcomes, providing design insights for similar systems in other optimization domains. STELLAR's architecture and empirical results highlight a promising approach to complex system optimization, especially for problems with large search spaces and high exploration costs, while making I/O tuning more accessible to domain scientists with minimal added resources. Comments: Published in the Proceedings of the 2025 International Conference for High Performance Computing, Networking, Storage, and Analysis (SC25) Subjects: Distributed, Parallel, and Cluster Computing (cs.DC) Cite as: arXiv:2602.23220 [cs.DC] (or arXiv:2602.23220v1 [cs.DC] for this version) https://doi.org/10.48550/arXiv.2602.23220 Focus to learn more arXiv-issued DOI via DataCite (pending registration) Related DOI : https://doi.org/10.1145/3712285.3759887 Focus to learn more DOI(s) linking to related resources Submission history From: Chris Egersdoerfer [ view email ] [v1] Thu, 26 Feb 2026 17:01:18 UTC (2,737 KB) Full-text links: Access Paper: View a PDF of the paper titled STELLAR: Storage Tuning Engine Leveraging LLM Autonomous Reasoning for High Performance Parallel File Systems, by Chris Egersdoerfer and 4 other authors View PDF HTML (experimental) TeX Source view license Current browse context: cs.DC &lt; prev | next &gt; new | recent | 2026-02 Change to browse by: cs References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-210">106. SPARTA: Scalable and Principled Benchmark of Tree-Structured Multi-hop QA over Text and Tables</h2>
<ul>
<li>链接：https://arxiv.org/abs/2602.23286v1</li>
<li>来源：arxiv</li>
<li>摘要：Real-world Table-Text question answering (QA) tasks require models that can reason across long text and source tables, traversing multiple hops and executing complex operations such as aggregation. Yet existing benchmarks are small, manually curated - and therefore error-prone - and contain shallow questions that seldom demand more than two hops or invoke aggregations, grouping, or other advanced analytical operations expressible in natural-language queries. We present SPARTA, an end-to-end construction framework that automatically generates large-scale Table-Text QA benchmarks with lightweight human validation, requiring only one quarter of the annotation time of HybridQA. The framework first constructs a reference fact database by enriching each source table with grounding tables whose tuples are atomic facts automatically extracted from the accompanying unstructured passages, then synthesizes nested queries whose number of nested predicates matches the desired hop count. To ensure that every SQL statement is executable and that its verbalization yields a fluent, human-sounding question, we propose two novel techniques: provenance-based refinement, which rewrites any syntacticall</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-211">正文（抓取，非 AI）</h3>
<p>[2602.23286v1] SPARTA: Scalable and Principled Benchmark of Tree-Structured Multi-hop QA over Text and Tables Computer Science &gt; Computation and Language arXiv:2602.23286v1 (cs) [Submitted on 26 Feb 2026] Title: SPARTA: Scalable and Principled Benchmark of Tree-Structured Multi-hop QA over Text and Tables Authors: Sungho Park , Jueun Kim , Wook-Shin Han View a PDF of the paper titled SPARTA: Scalable and Principled Benchmark of Tree-Structured Multi-hop QA over Text and Tables, by Sungho Park and 1 other authors View PDF HTML (experimental) Abstract: Real-world Table-Text question answering (QA) tasks require models that can reason across long text and source tables, traversing multiple hops and executing complex operations such as aggregation. Yet existing benchmarks are small, manually curated - and therefore error-prone - and contain shallow questions that seldom demand more than two hops or invoke aggregations, grouping, or other advanced analytical operations expressible in natural-language queries. We present SPARTA, an end-to-end construction framework that automatically generates large-scale Table-Text QA benchmarks with lightweight human validation, requiring only one quarter of the annotation time of HybridQA. The framework first constructs a reference fact database by enriching each source table with grounding tables whose tuples are atomic facts automatically extracted from the accompanying unstructured passages, then synthesizes nested queries whose number of nested predicates matches the desired hop count. To ensure that every SQL statement is executable and that its verbalization yields a fluent, human-sounding question, we propose two novel techniques: provenance-based refinement, which rewrites any syntactically valid query that returns a non-empty result, and realistic-structure enforcement, which confines generation to post-order traversals of the query graph. The resulting pipeline produces thousands of high-fidelity question-answer pairs covering aggregations, grouping, and deep multi-hop reasoning across text and tables. On SPARTA, state-of-the-art models that reach over 70 F1 on HybridQA or over 50 F1 on OTT-QA drop by more than 30 F1 points, exposing fundamental weaknesses in current cross-modal reasoning. Our benchmark, construction code, and baseline models are available at this https URL . Comments: 10 pages, 5 figures. Published as a conference paper at ICLR 2026. Project page: this https URL Subjects: Computation and Language (cs.CL) ; Artificial Intelligence (cs.AI); Databases (cs.DB); Information Retrieval (cs.IR) ACM classes: H.2.4; H.3.3; H.3.4; I.2.7; I.2.1; H.2.3; F.2.2; I.2.6; H.3.1 Cite as: arXiv:2602.23286 [cs.CL] (or arXiv:2602.23286v1 [cs.CL] for this version) https://doi.org/10.48550/arXiv.2602.23286 Focus to learn more arXiv-issued DOI via DataCite (pending registration) Journal reference: The Fourteenth International Conference on Learning Representations (ICLR), 2026 Submission history From: Sungho Park [ view email ] [v1] Thu, 26 Feb 2026 17:59:51 UTC (1,194 KB) Full-text links: Access Paper: View a PDF of the paper titled SPARTA: Scalable and Principled Benchmark of Tree-Structured Multi-hop QA over Text and Tables, by Sungho Park and 1 other authors View PDF HTML (experimental) TeX Source view license Current browse context: cs.CL &lt; prev | next &gt; new | recent | 2026-02 Change to browse by: cs cs.AI cs.DB cs.IR References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )</p>
</div></details><h2 id="toc-212">107. 什么是大语言模型 (LLM)？ | Microsoft Azure</h2>
<ul>
<li>链接：https://azure.microsoft.com/zh-cn/resources/cloud-computing-dictionary/what-are-large-language-models-llms</li>
<li>来源：bing</li>
<li>摘要：LLM 简史 LLM 是现代的发展，但自然语言处理 (NLP) 的研究可以追溯到 1950 年，当时，Alan Turing 发起了 Turing 测试来衡量计算机之间的智能行为。 在测试中，一名人类评判员使用一系列问题与计算 …</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-213">正文（抓取，非 AI）</h3>
<p>什么是大语言模型 (LLM)？ | Microsoft Azure This is the Trace Id: f17b7bc1f8b18abffee1f36dc7796c2e 跳转至主内容 什么是大语言模型 (LLM)？ 大致了解 LLM 的工作原理，并了解如何使用它们来构建 AI 支持的解决方案。 浏览 Azure 模型目录 开始使用 Azure 阅读时间 10 分钟 LLM 含义 大语言模型 (LLM) 是高级的 AI 系统，它们利用通过 机器学习 技术对其进行训练的数据来理解并生成自然语言或类似于人类的文本。LLM 可以自动生成基于文本的内容，这些内容可以应用于各行各业的大量用例，从而改进全球组织的效率和成本节省。 要点 LLM 是可以理解和生成自然语言的高级 AI 系统。 LLM 依赖于深度学习体系结构和机器学习技术来处理和合并来自不同数据源的信息。 LLM 为不同的领域带来重大利益，例如语言生成和翻译。 尽管 LLM 是开创性的进展，但仍面临着一些挑战，其中可能包括计算要求、道德问题和理解上下文方面的限制。 尽管存在这些挑战，但组织已在使用生成式预训练转换器 (GPT) 系列和来自转换器的双向编码器表示形式 (BERT) 来执行内容创建、聊天机器人、翻译和情绪分析等任务。 工作原理 LLM 的工作原理 LLM 简史 LLM 是现代的发展，但自然语言处理 (NLP) 的研究可以追溯到 1950 年，当时，Alan Turing 发起了 Turing 测试来衡量计算机之间的智能行为。在测试中，一名人类评判员使用一系列问题与计算机进行对话，并且必须判断出自己正在与之交谈的对象是计算机还是人类。 到了 20 世纪 80 年代和 90 年代，NLP 从逻辑试验转变为更加依赖于数据驱动的方法。由于能够根据句子中前面的单词来预测后面可能出现的单词，统计语言模型（如 n 元语法）为新时代的到来铺平了道路。到 21 世纪 10 年代初，更新的神经网络进一步扩展了这些语言模型的功能，支持其超越确定字词顺序的范畴，向着能够更深入理解字词表征和含义的方向进一步发展。 这些新的发展在 2018 年迎来了重大突破，当时八位 Google 的科学家撰写并发表了 Attention is All You Need，这是一项机器学习方面里程碑式的研究。最值得关注的是，该论文引入了转换器体系结构，这是一种创新性的神经网络框架，可以更准确、更大规模地管理和理解复杂的文本信息。转换器现在是当今一些功能最强大的 LLM 的基础，包括 GPT 系列和 BERT。 基本体系结构 当今的先进 LLM 使用 深度学习 体系结构（如转换器和其他深度神经网络框架）来处理来自不同数据源的信息。转换器在处理顺序数据（如文本）时特别有效，这支持它们可以理解和生成自然语言来完成语言生成和翻译等任务。 转换器由两个主要组件组成：编码器和解码器。这些组件通常协同工作来处理和生成序列。编码器采用原始文本数据，并将该输入转换为可由模型分析的离散元素。然后，解码器通过一系列层处理该数据以生成最终输出，例如，该输出可能包含一个生成的句子。转换器还可以仅包括编码器或解码器，具体取决于模型或任务的类型。 训练过程 LLM 的训练过程包括三个主要阶段：数据收集、模型训练和微调。 在数据收集阶段，模型会接触到来自各种源（包括 Internet 资源、书籍、文章和数据库）的大量文本数据。还会对数据进行清理、处理、标准化并将其存储在 NoSQL 数据库 中，以便可将其用于在语言模式、语法、信息和上下文方面训练模型。 在预训练阶段，模型开始对数据中的语言形成理解。这是通过执行大规模、非监督式任务实现的，模型将会在此过程中学习根据其上下文预测文本。一些技术包括自动回归建模（模型通过此方式学习预测序列中的下一个单词），以及掩码语言建模（模型通过此方式填充掩码词来理解上下文）。 最后，在微调阶段，将会针对更小、更特定于任务的数据集对模型进行进一步训练。此过程可优化模型的知识，并增强其针对特定任务（如情绪分析或翻译）的性能，以便可以将其用于各种应用领域。 关键组件 转换器模型课将原始文本分解为更小的基本文本单位（称为标记）。标记可能由字词、组成字词的部分甚至单个字符组成，具体取决于用例。然后，这些标记将转换为密集的数字表示形式，用于捕获顺序、语义含义和上下文。然后，这些称为嵌入的表示形式会通过包括以下两个子层的层堆栈进行传递：自注意力和神经网络。 虽然两个层都有助于将文本转换为模型可以有效地处理的表单，但自注意力机制才是转换器体系结构的关键组件。自注意力机制允许模型聚焦于文本序列的不同部分，并动态衡量信息相对于序列中其他标记的价值，而不考虑其位置。借助此机制，LLM 还能够捕获书面语言的复杂依赖关系、关联和上下文细微差别。 优势、挑战 优势和挑战 优点 LLM 带来了许多好处，促进了在工作和社会方面的重大进展。 改进了语言生成和翻译 由于 LLM 可以理解和捕获单词之间微妙的关系，因此它们擅长生成类似于人类的自然文本，从而改善了语言生成的效果。它们可以流畅且一致地生成富于创意且契合上下文的响应，并且可以使用各种体裁（包括小说）来生成相应。 由于可以结合上下文和发现含义的细微之处，因此，针对多语言数据进行训练的 LLM 还可以执行高度准确的翻译任务。通过基于一组特定语言对模型进行训练，可以帮助模型微调其处理惯用语、表达和其他复杂语言特征的能力，从而生成自然且流畅的翻译。 不同领域中的应用 LLM 是多功能工具，在许多领域（包括医疗保健、财经和客户服务）中具有广泛的应用。 在医疗保健领域，LLM 可以： 分析患者报告以判断可能的情况，并提供初步诊断。 生成患者记录和出院摘要，进而简化管理任务。 根据患者病史提出个性化治疗计划和医疗护理建议。 在财务领域，LLM 可以： 识别财务数据中可能指向欺诈的异常活动。 通过分析市场趋势和财务报表来评估财务风险。 根据你独特的财务历史记录和目标提出个性化建议。 在客户服务方面，LLM 可以： 通过对话式代理和聊天机器人提供自动化客户支持。 通过向客户提供全天候支持来扩展组织的服务范围。 通过根据常见问题生成内容来帮助创建和更新文档。 挑战 LLM 带来了重要的好处，但也带来了需要关注的挑战。 计算和能源要求 虽然 LLM 功能强大，但它们需要大量的计算资源、存储和能源消耗才能运行。在训练期间，转换器会随着输入序列的长度进行缩放，因此文本越长，所需的内存就越多。这些需求不仅成本高昂，而且还会向环境中排放大量的碳。 云计算 平台可以通过提供灵活、可缩放的基础结构来支持 LLM 的大量计算负载，以便组织可以更轻松地开始开发自己的模型。不过，LLM 对环境的影响仍会带来挑战，并表明需要采用更节能的模型和技术。 道德问题（例如，偏见、错误信息） 大语言模型的优劣取决于其训练时使用的数据。如果训练数据中存在针对某些群体的歧视性偏见，则该模型会凸显出这些态度。识别和缓解这些偏见以确保模型保持公平是一项持续性任务，需要进行经常性且一致的人工监视。 LLM 还可以生成看似令人信服但实际上具有误导性的信息，从而导致错误信息、虚假新闻、网络钓鱼电子邮件和其他形式有害内容的传播。内容审核准则也可能因区域而异，这增加了把握这些准则的难度。因此，许多组织可能会发现，在将 LLM 引入其业务运营时，很难在用户中建立和保持信任。 理解上下文和细微差别方面的限制 尽管 LLM 擅长识别语言中的模式，但它们仍可能难以应对需要更细微理解的新的上下文或未知上下文。因此，使用敏感的专有数据训练的 LLM 可能会意外生成或泄露其训练数据中的机密信息。 解决此问题可能会带来重大挑战，特别是由于 LLM 的内部工作通常缺乏透明度。这可能会导致缺乏整体问责机制，以及有关建立信任的问题。 用例 类型和用例 GPT 系列 GPT 系列最初由 OpenAI 于 2018 年开发，向 LLM 引入了数据收集、预训练和微调的基础概念。2019 年发布的 GPT-2 大幅提升了该模型的能力，并增强了其生成更契合上下文的语言的能力。GPT-3 则提升了模型处理复杂提示和任务的能力。最新的迭代 GPT-4 于 2023 年发布，可针对提示提供更加准确、更加体现细微差别的响应，同时还解决了该模型此前存在的一些问题（包括偏见）。 目前，GPT 仍在不断拓展自然语言生成领域的可能性边界。该系列中的每个模型均基于上一个模型生成，从而推动 AI 支持的创新不断向前发展。 BERT 及其变体 BERT 是由 Google 于 2018 年开发的开创性模型，它为使用 LLM 可能实现的任务设置了标准。与以单向方式（从左到右或从右到左）处理文本的 GPT 系列不同，BERT 采用了双向方法。双向模型可同时从两个方向处理每个单词的上下文，这可支持 BERT 除了进行下一句预测之外，还可执行掩码语言建模。研究人员还通过针对情绪分析等任务微调 BERT，为该领域的进一步发展做出了贡献，并因此设定了新的基准。 其他值得关注的模型 由 Facebook AI 于 2019 年开发的稳健优化 BERT 方法 (RoBERTa) 是 BERT 模型的一个变体，它通过优化预训练过程，扩展了 BERT 的双向转换器体系结构。RoBERTa 使用更大的数据集进行训练，并且训练时间更长。它还仅侧重于掩码语言建模。通过此方式，RoBERTa 展示了其捕获上下文和细微差别的强大功能。 由 Google Research 发明的文本到文本传输转换器 (T5) 是另一个值得一提的 LLM。与传统模型一样，T5 也是基于转换器体系结构构建的，并在预训练阶段使用编码器和解码器处理文本。与传统模型不同的是，T5 将输入和输出视为文本字符串，从而简化了体系结构和训练过程。T5 模型是一种可自适应的通用模型，可以处理各种各样的任务。 内容创建和汇总 LLM 可以使用各种样式和格式生成具有吸引力、信息丰富且契合上下文的内容。收到提示后，它们可以生成文章、报告、博客文章、电子邮件、营销文案，甚至代码片段。 在生成摘要方面，大语言模型展现了独特的能力，能够将大量文本提炼为简洁且准确的概要。它们可以显示要点，同时仍保留原始内容的原始上下文和含义。研究人员已经通过使用 LLM 汇总研究论文、文章、演示文稿和会议笔记节省了时间，并提高了工作效率。 对话代理和聊天机器人 对话代理和聊天机器人依赖于 LLM 的高级自然语言处理功能，可生成类似于人类的交互。它们以流畅、自然和契合上下文的方式解读用户输入并给出响应。它们不仅可以回答问题，而且可以参与长而复杂的对话。 通过添加聊天机器人和虚拟助手，企业现在可以为其客户提供全天候支持，进而扩展其服务可用性、缩短响应时间和提高整体客户满意度。 语言翻译和情绪分析 针对多语言数据集进行广泛训练的 LLM 可跨各种语言生成高度准确的翻译。与传统模型不同，LLM 可以捕获语言的细微之处和复杂性（如惯用表达），从而生成流畅且契合上下文的翻译。 LLM 还可以执行情绪分析，以分析文本的基本情感语气。通过处理和解释语言的细微之处，LLM 可提供更精确、更深入的情绪评估。它们甚至可以察觉到更细微的情绪，如讽刺。 个性化推荐 LLM 可以分析用户数据（包括用户历史记录和首选项）并生成个性化的定制建议，以反映用户的兴趣和需求，进而增强整体用户体验。 此功能广泛用于电子商务、内容流式处理和社交媒体领域，在这些领域中，提供定制的建议可推动更有意义的交互。LLM 还可以用作一种教育工具，为学生提供个性化的学习体验。 后续发展 后续发展 随着研究人员不断提高其理解能力、效率和可伸缩性，预计大语言模型在处理复杂语言任务方面会更加得心应手。随着 LLM 的采用日益扩大，越来越多的组织将会全面体验到简化的自动化流程、更强的个性化服务以及更好的决策过程。 研究人员正在继续探索新的方法来解决偏见这一长期存在的问题。这些方法包括可在训练期间处理偏见问题的去偏算法、纳入可重新平衡数据集以体现公平性的综合数据、可更好地了解模型决策的可解释性工具，以及有助于更精准地识别和量化偏见的检测基准。 处理文本、图像、音频和视频数据的多模式模型也变得越来越复杂。大语言模型 (LLM) 通过评估语法和语义来处理文本数据，而多模式模型则通过 计算机视觉 技术来分析视觉数据，并通过时序处理来分析音频数据。（表单顶部）多模式模型正在增强当今的技术，同时也在为未来的创新开辟道路。 详细了解 Azure AI 学生开发人员资源 利用可帮助职业快速起步的学习资料和计划。 了解详细信息 Azure 资源 访问所需的所有 Azure 资源，包括教程、白皮书和代码示例。 浏览资源 资源 Azure 学习中心 通过针对你的角色或特定技术定制的培训来发展你的 AI 技能。 了解详细信息 常见问题 全部展开 全部折叠 LLM 代表什么？ LLM 代表大语言模型。 LLM 和 AI 之间有什么区别？ AI 是一个广泛的领域，涵盖的应用范围非常广泛，而不仅仅是语言。它包括所有旨在复制人类智能的技术。作为特定类型的 AI 模型，LLM 是更广泛的 AI 方案其中的一部分，侧重于处理和生成自然语言文本。 NLP 和 LLM 之间有什么区别？ 自然语言处理 (NLP) 是指侧重于语言处理的综合性领域，而大语言模型 (LLM) 则是 NLP 领域中的一种特定高级模型，它们使用深度学习技术来处理语言任务。 GPT 和 LLM 之间有什么区别？ 生成式预训练转换器 (GPT) 是指 OpenAI 开发的一系列特定的大语言模型 (LLM)。它们是一种 LLM，特别侧重于语言生成。 获取 Azure 移动应用</p>
</div></details><h2 id="toc-214">108. 一文搞清楚大语言模型（LLM）到底是什么？看这一篇就够了！</h2>
<ul>
<li>链接：https://blog.csdn.net/l01011_/article/details/149738323</li>
<li>来源：bing</li>
<li>摘要：2025年7月29日 · 我们大致理解为LLM构建并维护了某种知识库，但这个数据库却非常奇特、不完美且怪异。 最近有一个广为流传的例子，我们称之为“反转诅咒”。 比如，如果你和目前最先进的语言模型 …</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-215">正文（抓取，非 AI）</h3>
<p>一、前言 在很多讲大模型的文章中，经常听到模型推理、模型训练等专业术语，我们就从这两个专业术语来说明大模型到底是什么、它又是如何工作的。 我们日常与ChatGPT聊天，就是模型推理（LLM Inference）的过程。模型推理消耗的GPU以及服务器资源由用户量决定，相比于模型训练阶段要小很多。模型推理其实就是在服务器上运行Transformer架构以及模型训练得到的参数，参数量级决定模型大小，例如DeepSeek-R1满血版本的671B，就是指模型参数是671 Billion。 这些模型参数的获取是关键，整体分为两个阶段：预训练 与 微调。 预训练阶段就是将互联网上的庞大文本库送入超级GPU集群炼丹，最终得到一版Base-model。但此阶段的Base-model并不具备对话能力，它只是将互联网语料库压缩进了自己的大脑里。微调阶段就是提供一问一答的文本格式，将基础模型微调成问答助手，而且这些数据全都是高质量的人工专家给出的答案。 由此，一个ChatGPT级别的伟大模型就诞生了！ 二、LLM Inference（模型推理） 以 Llama2 70B 模型为例，这是一个由 Meta AI 发布的大语言模型。这是 Llama 系列语言模型的第二代，也是该系列中参数最多的模型，达到了 700 亿。Llama2 系列包括了多个不同规模的模型，70 亿，130 亿，340 亿，700 亿是最大的一个。 Llama2 70B 模型实际上就是你电脑上的两个文件：一个是存储参数的文件，另一个是运行这些参数的代码。这些参数是神经网络（即语言模型）的权重或参数。因为这是一个拥有 700 亿参数的模型，每个参数占用两个字节，因此参数文件的大小为 140 GB，之所以是两个字节，是因为这是 float16 类型的数据。 除了这些参数，还有一大堆神经网络的参数。你还需要一些能运行神经网络的代码，这些代码被包含在我们所说的运行文件中。这个运行文件可以是 C 语言或 Python，或任何其他编程语言编写的。它可以用任何语言编写，但 C 语言是一种非常简单的语言，只是举个例子。只需大约 500 行 C 语言代码，无需任何其他依赖，就能构建起神经网络架构，并且主要依靠一些参数来运行模型。所以只需要这两个文件。 你只需带上这两个文件和你的MacBook就拥有了一个完整的工具包。你不需要连接互联网或其他任何设备。你可以拿着这两个文件，编译你的 C 语言代码。你将得到一个可针对参数运行并与语言模型交互的二进制文件。比如你让它写一首诗，模型就会开始生成文本。 那么，这些参数从何而来，我们如何获得它们？因为无论 run.c 文件中的内容是什么，神经网络的架构和前向传播的算法都是公开的。 三、LLM Training（模型训练） 真正的关键在于参数的获取。所以，为了获得模型参数，所谓的模型training 过程比之前展示的模型inference 要复杂得多。模型inference 只是在MacBook上运行模型。而模型训练则是一个计算上极为复杂的过程。 简单来说，模型训练所做的可以被理解为对大量互联网内容的压缩。 因为Llama2 70B是开源的模型，训练方式也是公开的。你需要从网上获取大约10TB的文本，通常这些文本来自于对互联网的爬取。想象一下，从各种不同的网站上收集大量的文本，并将它们汇总起来。接下来，你需要配置一个GPU集群，这些GPU是为了处理像神经网络训练这样复杂的计算任务而专门设计的高性能计算机。 你需要大约 6,000个GPU，并且需要运行大约 12 天才能得到一个Llama2 70B，整个过程大约需要花费200万美元。这个过程基本上是将这大量的文本压缩成你可以想象的一种zip文件，可以被理解为互联网文本的zip文件。 但这种压缩与普通zip文件不同，因为zip文件是无损压缩，而这里是有损压缩。我们只是大致获取了我们训练文本的概览，而不是在这些参数中保留了文本的完整副本。所以，可以把它理解为一种有损压缩方式。另外需要指出的是，按照目前最先进技术的标准，这些数据其实只是入门级别的。如果考虑到像 ChatGPT、Claude这样的顶尖神经网络，这些数字可能需要增加十倍甚至更多。 这意味着在实际操作中，我们需要将这些数字大幅上调。这也解释了为什么如今这些神经网络的训练成本高达数千万甚至数亿美元，它们需要庞大的计算集群和大量数据集，而且在获取参数的过程中需要付出巨大努力。一旦获得了这些参数，实际运行神经网络的计算成本就相对较低了。 那么，这个神经网络到底在做什么呢？正如我之前提到的那些参数， 神经网络的主要任务其实是预测文本序列中的下一个词。 你可以这样理解：当你输入一连串词语，比如 “cat sat on a”，这些词就会被送入神经网络。神经网络中分布着的这些参数，就是完成这一任务的关键。通过神经元的相互连接和激发，来预测下一个单词。 你可以这么理解这个过程：输入一段文本后，神经网络会预测下一个词是什么。举个例子，在 “cat sat on a” 这四个词的上下文中，神经网络可能会预测下一个词是“mat”，并且给出了 97% 的高概率。这就是神经网络要解决的核心问题。从数学上可以证明，预测与数据压缩之间存在密切联系。这也是为什么我会说，这种神经网络训练在某种意义上是一种数据压缩：因为如果你能够非常准确地预测下一个词，你就可以利用这个能力来压缩数据集。 所以，这其实是一个专注于预测下一个词的神经网络。你输入一些词，它就会告诉你接下来的词是什么。这种训练的结果之所以显得有些神奇，是因为尽管下一个词预测看似是一个简单的任务，但实际上它是一个非常强大的目标。因为这个目标迫使神经网络在其参数中学习到大量关于世界的信息。 四、LLM Dreams 那么，我们实际如何使用这些神经网络呢？当我们训练好它们后，模型推理是个非常简单的过程。基本上是生成下一个词，我们从模型中采样，选择一个词，然后我们继续将其反馈进去并得到下一个词，然后继续这样反馈。我们可以重复这个过程，让这个网络仿佛在“梦游”互联网文档。这是一个很好的比喻，如果我们只是运行神经网络，或者说进行推理，我们会得到类似于在网络上浏览文字的梦境体验。 可以这么理解：因为这个神经网络是基于网页内容进行训练的，然后它可以自由遨游于其中。例如，在左边，我们可以看到类似于 Java 代码的“梦境”。中间的部分，看起来像是对亚马逊产品描述的“梦境”。而右边，则似乎呈现出一篇维基百科文章的样子。以中间的这个例子为例，标题、作者、ISBN 编号等等，这些内容都是神经网络完全自行创造的。这个网络正在“梦想”出它所训练数据集中的文本类型，它在模仿这些文档，但其实，这些都像是它的幻觉一样。 比如说 ISBN 号码，这个号码几乎可以肯定是不存在的。网络只是知道在“ISBN:”后面通常会跟着这样长度的数字，然后就随机生成一个（这就是一种模型幻觉）。实际上，它只是随意插入看起来合理的内容。因此，它在模仿训练数据集的分布模式。在右边，黑鼻鲑鱼，它实际上是一种鱼。这里的情况是，这段文字在训练集文档中并未原样出现，但如果你真的去查证，会发现对这种鱼的这些描述信息大致上是正确的。因此，这个网络对这种鱼有一定的了解，它知道很多关于这种鱼的信息。它不会完全复制训练集中看到的文档，但它会对互联网的信息进行某种程度的压缩和整合，它能够记住整体的轮廓。它大致掌握了相关知识，然后开始创造。它构建了一种合适的形式，并用自己的知识填充其中。 但我们永远不能百分之百确定它生成的内容是幻觉、错误的回答，还是正确的回答。所以，它的一部分内容可能是记忆中的，而另一部分则不是，我们无法精确区分。但大多数情况下，这就像是它在梦游或在做关于互联网文本的梦，源于它的数据分布。这种能力使得神经网络能够生成各种文本，从代码到商品描述再到百科全书条目，但它也意味着生成的内容需要谨慎验证和审查，以确保准确性和可信度。这就是模型训练和模型推断的关键过程，它们共同构建了人工智能模型的能力和潜力。 五、How do they work？ 让我们换个话题，来看看这个神经网络是怎么运作的？它是如何完成下一个词预测任务的？它内部的运作机制是什么？下图就是我们称之为 Transformer 的神经网络架构，这是它的一个示意图。现在，这个神经网络的一个显著特点是，我们对其架构有着完整的理解。我们清楚地知道在它的各个阶段会发生哪些数学运算。 但问题在于，这 1000 亿个参数分散在整个神经网络中，我们所了解的只是如何逐步调整这些参数，以使整个网络在下一个词预测的任务上表现得更好。我们知道如何优化这些参数，也知道如何随时间调整它们以获得更佳的下一词预测效果，但我们并不真正清楚这些参数具体是如何工作的。我们可以观察到它在下一个词预测方面的进步，但并不清楚这些参数是如何协同工作以实现这一点的。我们手头有些模型，可以让我们从宏观层面思考网络可能在做的事情。 我们大致理解为LLM构建并维护了某种知识库，但这个数据库却非常奇特、不完美且怪异。最近有一个广为流传的例子，我们称之为“反转诅咒”。比如，如果你和目前最先进的语言模型 GPT-4（ChatGPT 的一部分）对话，你问，谁是汤姆·克鲁斯的母亲？它会告诉你是玛丽·李·菲弗，这是正确的。但如果你问，谁是玛丽·菲弗的儿子，它会告诉你它不知道。这种知识很古怪，它似乎是单向的。这些信息并不是简单存储后就能从各种角度获取，你必须从某个特定的角度去提问（这也是为什么PE如此重要）。 这真是既奇怪又令人困惑。归根结底，我们实际上并不真正了解其工作原理，只能大致判断它是否有效，以及有效的可能性有多大。简而言之，可以将大语言模型 (LLM) 视为难以完全解读的产物。它们与你可能在工程学科中建造的任何其他东西都不相似。它们不像汽车，我们了解汽车的每一个部件。 它们是来自长期优化过程的神经网络。我们目前并不完全理解它们是如何工作的，尽管有一个叫做可解释性或机械可解释性的领域，正在尝试研究并理解这些神经网络的每一个部分。目前，我们可以在一定程度上做到这一点，但还未能全面实现。现在，我们主要将它们视为基于经验的产品。我们可以给它们输入一些数据，然后测量输出结果。我们基本上可以测量它们的行为表现。我们可以观察它们在许多不同情况下生成的文本。因此，我认为这需要相应的复杂评估来处理这些模型，因为它们主要是基于经验的。 六、Summary 构建像 ChatGPT 这样的大模型包括两个主要阶段：预训练和微调。预训练阶段需要从互联网上搜集大量文本资料，使用GPU集群进行处理。这些高性能计算机的成本非常昂贵，通常需要几百万美元的投入。完成后，就得到了基础模型。由于这个过程计算量巨大且成本高昂，公司通常一年或几个月才会做一次。 微调阶段相对便宜，需要编写标注指南和雇佣人员进行帮助。例如，可以通过Scale AI等公司进行文档标注。这个阶段需要收集约100,000个高质量的问答回应样本，成本要低得多，可能只需一天就能完成。接下来是进行大量的评估工作，部署模型，并监控和收集任何不当行为。对于每个不当行为，都需要修复并返回第一步重复这个过程。修复方法通常是找到错误回应的对话，然后用正确的回应替换。由于微调成本较低，可以每周或每天进行迭代，许多公司在微调阶段而非预训练阶段会更频繁地进行迭代。 Meta发布的Llama 2系列包括基础模型和助手模型。基础模型无法直接使用，因为它们无法直接对问题回复正确的答案，而助手模型则可以直接进行问答。Meta已经完成了极其昂贵的预训练阶段，提供了基础模型，允许用户基于这些结果进行自己的微调。此外，还有一个你可以选择进行的第三阶段微调，即人类反馈强化学习（RLHF），主要通过使用比较标签来提升额外性能。在OpenAI，这个过程被称为人类反馈强化学习（RLHF），这其实是一个可选的第三阶段，它能在大语言模型中提升额外性能。 如何学习大模型 AI ？ 由于新岗位的生产效率，要优于被取代岗位的生产效率，所以实际上整个社会的生产效率是提升的。 但是具体到个人，只能说是： “最先掌握AI的人，将会比较晚掌握AI的人有竞争优势”。 这句话，放在计算机、互联网、移动互联网的开局时期，都是一样的道理。 我在一线互联网企业工作十余年里，指导过不少同行后辈。帮助很多人得到了学习和成长。 我意识到有很多经验和知识值得分享给大家，也可以通过我们的能力和经验解答大家在人工智能学习中的很多困惑，所以在工作繁忙的情况下还是坚持各种整理和分享。但苦于知识传播途径有限，很多互联网行业朋友无法获得正确的资料得到学习提升，故此将并将重要的AI大模型资料包括AI大模型入门学习思维导图、精品AI大模型学习书籍手册、视频教程、实战学习等录播视频免费分享出来。 这份完整版的大模型 AI 学习资料已经上传CSDN，朋友们如果需要可以微信扫描下方CSDN官方认证二维码免费领取【 保证100%免费 】 第一阶段（10天）：初阶应用 该阶段让大家对大模型 AI有一个最前沿的认识，对大模型 AI 的理解超过 95% 的人，可以在相关讨论时发表高级、不跟风、又接地气的见解，别人只会和 AI 聊天，而你能调教 AI，并能用代码将大模型和业务衔接。 大模型 AI 能干什么？ 大模型是怎样获得「智能」的？ 用好 AI 的核心心法 大模型应用业务架构 大模型应用技术架构 代码示例：向 GPT-3.5 灌入新知识 提示工程的意义和核心思想 Prompt 典型构成 指令调优方法论 思维链和思维树 Prompt 攻击和防范 … 第二阶段（30天）：高阶应用 该阶段我们正式进入大模型 AI 进阶实战学习，学会构造私有知识库，扩展 AI 的能力。快速开发一个完整的基于 agent 对话机器人。掌握功能最强的大模型开发框架，抓住最新的技术进展，适合 Python 和 JavaScript 程序员。 为什么要做 RAG 搭建一个简单的 ChatPDF 检索的基础概念 什么是向量表示（Embeddings） 向量数据库与向量检索 基于向量检索的 RAG 搭建 RAG 系统的扩展知识 混合检索与 RAG-Fusion 简介 向量模型本地部署 … 第三阶段（30天）：模型训练 恭喜你，如果学到这里，你基本可以找到一份大模型 AI相关的工作，自己也能训练 GPT 了！通过微调，训练自己的垂直大模型，能独立训练开源多模态大模型，掌握更多技术方案。 到此为止，大概2个月的时间。你已经成为了一名“AI小子”。那么你还想往下探索吗？ 为什么要做 RAG 什么是模型 什么是模型训练 求解器 &amp; 损失函数简介 小实验2：手写一个简单的神经网络并训练它 什么是训练/预训练/微调/轻量化微调 Transformer结构简介 轻量化微调 实验数据集的构建 … 第四阶段（20天）：商业闭环 对全球大模型从性能、吞吐量、成本等方面有一定的认知，可以在云端和本地等多种环境下部署大模型，找到适合自己的项目/创业方向，做一名被 AI 武装的产品经理。 硬件选型 带你了解全球大模型 使用国产大模型服务 搭建 OpenAI 代理 热身：基于阿里云 PAI 部署 Stable Diffusion 在本地计算机运行大模型 大模型的私有化部署 基于 vLLM 部署大模型 案例：如何优雅地在阿里云私有部署开源大模型 部署一套开源 LLM 项目 内容安全 互联网信息服务算法备案 … 学习是一个过程，只要学习就会有挑战。天道酬勤，你越努力，就会成为越优秀的自己。 如果你能在15天内完成所有的任务，那你堪称天才。然而，如果你能完成 60-70% 的内容，你就已经开始具备成为一名大模型 AI 的正确特征了。 这份完整版的大模型 AI 学习资料已经上传CSDN，朋友们如果需要可以微信扫描下方CSDN官方认证二维码免费领取【 保证100%免费 】</p>
</div></details><h2 id="toc-216">109. GitHub - datawhalechina/self-llm: 《开源大模型食用指南》针对中 …</h2>
<ul>
<li>链接：https://github.com/datawhalechina/self-llm</li>
<li>来源：bing</li>
<li>摘要：本项目旨在首先基于核心贡献者的经验，实现国内外主流开源 LLM 的部署、使用与微调教程；在实现主流 LLM 的相关部分之后，我们希望充分聚集共创者，一起丰富这个开源 LLM 的世界，打造更多、更 …</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-217">正文（抓取，非 AI）</h3>
<p>GitHub - datawhalechina/self-llm: 《开源大模型食用指南》针对中国宝宝量身打造的基于Linux环境快速微调（全参数/Lora）、部署国内外开源大模型（LLM）/多模态大模型（MLLM）教程 Skip to content You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session. Dismiss alert datawhalechina / self-llm Public Notifications You must be signed in to change notification settings Fork 2.8k Star 28.5k 《开源大模型食用指南》针对中国宝宝量身打造的基于Linux环境快速微调（全参数/Lora）、部署国内外开源大模型（LLM）/多模态大模型（MLLM）教程 License Apache-2.0 license 28.5k stars 2.8k forks Branches Tags Activity Star Notifications You must be signed in to change notification settings datawhalechina/self-llm master Branches Tags Go to file Code Open more actions menu Folders and files Name Name Last commit message Last commit date Latest commit History 1,376 Commits 1,376 Commits .github/ ISSUE_TEMPLATE .github/ ISSUE_TEMPLATE dataset dataset examples examples images images models models models_amd models_amd models_ascend models_ascend models_mlx models_mlx .gitignore .gitignore LICENSE LICENSE README.md README.md README_en.md README_en.md contributors.json contributors.json support_model.md support_model.md support_model_Ascend.md support_model_Ascend.md support_model_amd.md support_model_amd.md utils.py utils.py View all files Repository files navigation 开源大模型食用指南 中文 | English 本项目是一个围绕开源大模型、针对国内初学者、基于 Linux 平台的中国宝宝专属大模型教程，针对各类开源大模型提供包括环境配置、本地部署、高效微调等技能在内的全流程指导，简化开源大模型的部署、使用和应用流程，让更多的普通学生、研究者更好地使用开源大模型，帮助开源、自由的大模型更快融入到普通学习者的生活中。 本项目的主要内容包括： 基于 Linux 平台的开源 LLM 环境配置指南，针对不同模型要求提供不同的详细环境配置步骤； 针对国内外主流开源 LLM 的部署使用教程，包括 LLaMA、ChatGLM、InternLM 等； 开源 LLM 的部署应用指导，包括命令行调用、在线 Demo 部署、LangChain 框架集成等； 开源 LLM 的全量微调、高效微调方法，包括分布式全量微调、LoRA、ptuning 等。 项目的主要内容就是教程，让更多的学生和未来的从业者了解和熟悉开源大模型的食用方法！任何人都可以提出issue或是提交PR，共同构建维护这个项目。 想要深度参与的同学可以联系我们，我们会将你加入到项目的维护者中。 学习建议：本项目的学习建议是，先学习环境配置，然后再学习模型的部署使用，最后再学习微调。因为环境配置是基础，模型的部署使用是基础，微调是进阶。初学者可以选择Qwen1.5，InternLM2，MiniCPM等模型优先学习。 进阶学习推荐 ：如果您在学习完本项目后，希望更深入地理解大语言模型的核心原理，并渴望亲手从零开始训练属于自己的大模型，我们强烈推荐关注 Datawhale 的另一个开源项目—— Happy-LLM 从零开始的大语言模型原理与实践教程 。该项目将带您深入探索大模型的底层机制，掌握完整的训练流程。 注：如果有同学希望了解大模型的模型构成，以及从零手写RAG、Agent和Eval等任务，可以学习Datawhale的另一个项目 Tiny-Universe ，大模型是当下深度学习领域的热点，但现有的大部分大模型教程只在于教给大家如何调用api完成大模型的应用，而很少有人能够从原理层面讲清楚模型结构、RAG、Agent 以及 Eval。所以该仓库会提供全部手写，不采用调用api的形式，完成大模型的 RAG 、 Agent 、Eval 任务。 注：考虑到有同学希望在学习本项目之前，希望学习大模型的理论部分，如果想要进一步深入学习 LLM 的理论基础，并在理论的基础上进一步认识、应用 LLM，可以参考 Datawhale 的 so-large-llm 课程。 注：如果有同学在学习本课程之后，想要自己动手开发大模型应用。同学们可以参考 Datawhale 的 动手学大模型应用开发 课程，该项目是一个面向小白开发者的大模型应用开发教程，旨在基于阿里云服务器，结合个人知识库助手项目，向同学们完整的呈现大模型应用开发流程。 项目意义 什么是大模型？ 大模型（LLM）狭义上指基于深度学习算法进行训练的自然语言处理（NLP）模型，主要应用于自然语言理解和生成等领域，广义上还包括机器视觉（CV）大模型、多模态大模型和科学计算大模型等。 百模大战正值火热，开源 LLM 层出不穷。如今国内外已经涌现了众多优秀开源 LLM，国外如 LLaMA、Alpaca，国内如 ChatGLM、BaiChuan、InternLM（书生·浦语）等。开源 LLM 支持用户本地部署、私域微调，每一个人都可以在开源 LLM 的基础上打造专属于自己的独特大模型。 然而，当前普通学生和用户想要使用这些大模型，需要具备一定的技术能力，才能完成模型的部署和使用。对于层出不穷又各有特色的开源 LLM，想要快速掌握一个开源 LLM 的应用方法，是一项比较有挑战的任务。 本项目旨在首先基于核心贡献者的经验，实现国内外主流开源 LLM 的部署、使用与微调教程；在实现主流 LLM 的相关部分之后，我们希望充分聚集共创者，一起丰富这个开源 LLM 的世界，打造更多、更全面特色 LLM 的教程。星火点点，汇聚成海。 我们希望成为 LLM 与普罗大众的阶梯，以自由、平等的开源精神，拥抱更恢弘而辽阔的 LLM 世界。 项目受众 本项目适合以下学习者： 想要使用或体验 LLM，但无条件获得或使用相关 API； 希望长期、低成本、大量应用 LLM； 对开源 LLM 感兴趣，想要亲自上手开源 LLM； NLP 在学，希望进一步学习 LLM； 希望结合开源 LLM，打造领域特色的私域 LLM； 以及最广大、最普通的学生群体。 项目规划及进展 本项目拟围绕开源 LLM 应用全流程组织，包括环境配置及使用、部署应用、微调等，每个部分覆盖主流及特点开源 LLM： Example 系列 Chat-嬛嬛 ： Chat-甄嬛是利用《甄嬛传》剧本中所有关于甄嬛的台词和语句，基于LLM进行LoRA微调得到的模仿甄嬛语气的聊天语言模型。 Tianji-天机 ：天机是一款基于人情世故社交场景，涵盖提示词工程 、智能体制作、 数据获取与模型微调、RAG 数据清洗与使用等全流程的大语言模型系统应用教程。 AMChat : AM (Advanced Mathematics) chat 是一个集成了数学知识和高等数学习题及其解答的大语言模型。该模型使用 Math 和高等数学习题及其解析融合的数据集，基于 InternLM2-Math-7B 模型，通过 xtuner 微调，专门设计用于解答高等数学问题。 数字生命 : 本项目将以我为原型，利用特制的数据集对大语言模型进行微调，致力于创造一个能够真正反映我的个性特征的AI数字人——包括但不限于我的语气、表达方式和思维模式等等，因此无论是日常聊天还是分享心情，它都以一种既熟悉又舒适的方式交流，仿佛我在他们身边一样。整个流程是可迁移复制的，亮点是数据集的制作。 已支持模型 ✨ 已支持 50+ 主流大语言模型 ✨ 每个模型都提供完整的部署、微调和使用教程 📖 查看完整模型列表和教程 | 🎯 快速开始 • Kimi-K2.5 • Step-3.5-Flash • GLM-4.7-Flash • Gemma3 • MiniMax-M2 • Qwen3 • Qwen3-VL • SpatialLM • Hunyuan3D-2 • Qwen2-VL • MiniCPM-o • Qwen2.5-Coder • DeepSeek-Coder-V2 • gpt-oss-20b • GLM-4.1-Thinking • DeepSeek-R1 • InternLM3 • phi4 • GLM-4.5-Air • Hunyuan-A13B • DeepSeek • Baichuan • InternLM • Kimi • ERNIE-4.5 • Llama4 • Apple OpenELM • Llama3.1 • Gemma-2 • Qwen2.5 • Qwen2 • GLM-4 • Qwen 1.5 • phi-3 • MiniCPM • Yi 零一万物 • Yuan2.0 • Yuan2.0-M32 • 哔哩哔哩 Index • CharacterGLM • BlueLM • Qwen-Audio • TransNormerLLM • Atom • ChatGLM3 • Qwen2-57B-A14B-Instruct • Qwen2-72B-Instruct • Qwen2-7B-Instruct • InternLM2-20B • Tele-Chat • XVERSE2 AMD GPU 专区 🚀 AMD GPU 平台已支持模型 每个模型都提供完整的 AMD 环境配置和部署教程 感谢 AMD University Program 对本项目的支持 📖 查看完整 AMD 平台模型列表和教程 • 谷歌 Gemma3 • AMD 环境准备与配置 • NPU 推理加速支持 • Qwen3 • lemonade-server SDK 部署 • Ryzen AI 300 系列优化 昇腾Ascend NPU 专区 🚀 昇腾Ascend NPU 平台已支持模型 每个模型都提供完整的昇腾Ascend NPU 环境配置和部署教程 📖 查看完整昇腾 NPU 平台模型列表和教程 • Qwen3 • Ascend NPU 环境配置通用指南 • MindIE 服务化部署调用 • vLLM-ascend 部署调用 • sglang-ascend 部署调用 • 大模型服务化性能和精度测试 • AISBench 测试工具环境配置 • 昇腾大模型服务化性能测试 • 昇腾大模型服务化精度测试 沐曦专区 Coming Soon! Apple M 专区 📖 点击跳转 Apple M 专区 Welcome More Platforms! 🚀 即将支持更多平台（Apple M 系列已有设备测试），敬请期待！ 🤝 欢迎昇腾 Ascend、摩尔线程 MUSA、沐曦等平台提供技术支持、硬件支持或参与贡献 🌟 欢迎各平台开发者共建共享，推动大模型技术在更多国产硬件生态中的繁荣发展！ 致谢 核心贡献者 宋志学(不要葱姜蒜)-项目负责人 （Datawhale成员） 邹雨衡-项目负责人 （Datawhale成员-对外经济贸易大学） 刘十一-Ascend专区负责人 （Datawhale成员-鲸英助教） 姜舒凡 （内容创作者-Datawhale成员） 郭宣伯 （内容创作者-北京航空航天大学） 林泽毅 （内容创作者-SwanLab产品负责人） 林恒宇 （内容创作者-广东东软学院-鲸英助教） 王泽宇 （内容创作者-太原理工大学-鲸英助教） 郭志航 （内容创作者） 陈榆 （内容创作者-谷歌开发者机器学习技术专家） 肖鸿儒 （Datawhale成员-同济大学） 张帆 （内容创作者-Datawhale成员） 李娇娇 （Datawhale成员） 高立业 （内容创作者-DataWhale成员） Kailigithub （Datawhale成员） 丁悦 （Datawhale-鲸英助教） 谢好冉 （内容创作者-鲸英助教） 惠佳豪 （Datawhale-宣传大使） 王茂霖 （内容创作者-Datawhale成员） 孙健壮 （内容创作者-对外经济贸易大学） 郑皓桦 （内容创作者） 荞麦 （内容创作者-Datawhale成员） 骆秀韬 （内容创作者-Datawhale成员-似然实验室） 李柯辰 （Datawhale成员） 程宏 （内容创作者-Datawhale意向成员） 李秀奇 （内容创作者-DataWhale意向成员） 余洋 （内容创作者-安徽理工大学副教授-Datawhale成员） 陈思州 （Datawhale成员） 颜鑫 （Datawhale成员） 杜森 （内容创作者-Datawhale成员-南阳理工学院） 散步 （Datawhale成员） 郑远婧 （内容创作者-鲸英助教-福州大学） Swiftie （小米NLP算法工程师） 张友东 （内容创作者-Datawhale成员） 张晋 （内容创作者-Datawhale成员） 娄天奥 （内容创作者-中国科学院大学-鲸英助教） 小罗 （内容创作者-Datawhale成员） 邓恺俊 （内容创作者-Datawhale成员） 赵文恺 （内容创作者-太原理工大学-鲸英助教） 王熠明 （内容创作者-Datawhale成员） 黄柏特 （内容创作者-西安电子科技大学） 左春生 （内容创作者-Datawhale成员） 杨卓 （内容创作者-西安电子科技大学-鲸英助教） 付志远 （内容创作者-海南大学） 三水 （内容创作者-鲸英助教） 樊奇 （内容创作者-上海交通大学） 陈辅元 （内容创作者-Datawhale成员） 谭逸珂 （内容创作者-对外经济贸易大学） 何至轩 （内容创作者-鲸英助教） 康婧淇 （内容创作者-Datawhale成员） 杨晨旭 （内容创作者-太原理工大学-鲸英助教） 赵伟 （内容创作者-鲸英助教） 苏向标 （内容创作者-广州大学-鲸英助教） 陈睿 （内容创作者-西交利物浦大学-鲸英助教） 张龙斐 （内容创作者-鲸英助教） 孙超 （内容创作者-Datawhale成员） 卓堂越 （内容创作者-鲸英助教） fancy （内容创作者-鲸英助教） 谭斐然 （西安电子科技大学-鲸英助教） 注：排名根据贡献程度排序 其他 特别感谢 @Sm1les 对本项目的帮助与支持 感谢 AMD University Program 对本项目的支持 部分lora代码和讲解参考仓库： https://github.com/zyds/transformers-code.git 如果有任何想法可以联系我们 DataWhale 也欢迎大家多多提出 issue 特别感谢以下为教程做出贡献的同学！ Star History About 《开源大模型食用指南》针对中国宝宝量身打造的基于Linux环境快速微调（全参数/Lora）、部署国内外开源大模型（LLM）/多模态大模型（MLLM）教程 Topics lora llm chatglm qwen chatglm3 internlm2 minicpm qwen2 qwen1-5 glm-4 gemma-2b-it llama3 q-wen Resources Readme License Apache-2.0 license Uh oh! There was an error while loading. Please reload this page . Activity Custom properties Stars 28.5k stars Watchers 158 watching Forks 2.8k forks Report repository Releases No releases published Packages 0 Uh oh! There was an error while loading. Please reload this page . Contributors 78 + 64 contributors Languages Jupyter Notebook 93.4% Python 5.6% Other 1.0% You can’t perform that action at this time.</p>
</div></details><h2 id="toc-218">110. 一文搞懂LLM大模型！LLM从入门到精通万字长文 - 知乎</h2>
<ul>
<li>链接：https://zhuanlan.zhihu.com/p/7046080918</li>
<li>来源：bing</li>
<li>摘要：2024年11月15日 · 五、LLM大模型建立的流程 LLM（Large Language Model）大模型训练的流程一般涉及以下几个阶段： 1. 数据收集和预处理：首先需要收集大量的训练数据，这些数据可以是从互联网 …</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-219">正文（抓取，非 AI）</h3>
<p>LLM从入门到精通精品文章 目录 一、LLM基本概念 二、LLM发展历程 三、LLM大模型的分类 四、LLM主流大模型类别 五、LLM大模型建立的流程 六、Fine-Tuning 七、Prompt-Tuning 八、超大规模参数模型Prompt-Tuning方法 8.1上下文学习 In-Context Learning 8.2.指令学习 Instruction- Tuning 8.3 思维链Chain-of-Thought 九、Prefix-Tuning 十、LoRA 一、LLM基本概念 大模型LLM（Large Language Model）是指具有大规模参数和复杂计算结构的机器学习模型。**这些模型通常由深度神经网络构建而成，拥有数十亿甚至数千亿个参数。大模型的设计目的是为了提高模型的表达能力和预测性能，能够处理更加复杂的任务和数据。大模型在各种领域都有广泛的应用，包括自然语言处理、计算机视觉、语音识别和推荐系统等。大模型通过训练海量数据来学习复杂的模式和特征，具有更强大的泛化能力，可以对未见过的数据做出准确的预测。 大模型本质上是一个使用海量数据训练而成的深度神经网络模型，其巨大的数据和参数规模，实现了智能的涌现，展现出类似人类的智能。 LLM的使用场景非常广泛。首先，LLM可以用于文本生成，可以生成连贯的段落、文章、对话等，可以应用于自动写作、机器翻译等任务中。其次，LLM可以用于问答系统，可以回答复杂的问题，甚至进行对话式问答。再者，LLM可以用于语义理解和推理，可以进行情感分析、命名实体识别、文本分类等任务。此外，LLM还可以用于智能助理、机器人交互、自动摘要、信息提取等应用领域。总的来说，LLM在自然语言处理和人工智能领域都有很大的潜力，可以提供更加智能和自然的人机交互体验。 二、LLM发展历程 截止目前，语言模型发展走过了三个阶段： 有需要的小伙伴，可以 点击下方领取 三、LLM大模型的分类 按照输入数据类型的不同，LLM大模型主要可以分为以下三大类： 语言大模型（NLP） ：是指在自然语言处理（Natural Language Processing，NLP）领域中的一类大模型，通常用于处理文本数据和理解自然语言。这类大模型的主要特点是它们在大规模语料库上进行了训练，以学习自然语言的各种语法、语义和语境规则。例如：GPT系列（OpenAI）、Bard（Google）、文心一言（百度）。 视觉大模型（CV） ：是指在计算机视觉（Computer Vision，CV）领域中使用的大模型，通常用于图像处理和分析。这类模型通过在大规模图像数据上进行训练，可以实现各种视觉任务，如图像分类、目标检测、图像分割、姿态估计、人脸识别等。例如：VIT系列（Google）、文心UFO、华为盘古CV、INTERN（商汤）。 多模态大模型 ：是指能够处理多种不同类型数据的大模型，例如文本、图像、音频等多模态数据。这类模型结合了NLP和CV的能力，以实现对多模态信息的综合理解和分析，从而能够更全面地理解和处理复杂的数据。例如：DingoDB多模向量数据库（九章云极DataCanvas）、DALL-E(OpenAI)、悟空画画（华为）、midjourney。 四、LLM主流大模型类别 随着ChatGPT迅速火爆，引发了大模型的时代变革，国内外各大公司也快速跟进生成式AI市场，近百款大模型发布及应用。开源语言大模型种类有以下4个： 1 ChatGLM-6B模型简介： ChatGLM-6B 是清华大学提出的一个开源、支持中英双语的对话语言模型，基于GeneralLanguageModel (GLM) 架构，具有 62 亿参数.该模型使用了和 ChatGPT 相似的技术，经过约 1T 标识符的中英双语训练(中英文比例为1:1)，辅以监督微调、反馈自助、人类反馈强化学习等技术的加持，62 亿参数的 ChatGLM-6B 已经能生成相当符合人类偏好的回答（目前中文支持最好）. GLM是一种基于自回归空白填充目标的通用预训练框架. GLM 将 NLU 任务转化为包含任务描述的完形填空问题，可以通过自回归生成的方式来回答. 原理：在输入文本中随机挖去一些连续的文本片段，然后训练模型按照任意顺序重建这些片段. 完形填空问题是指在输入文本中用一个特殊的符号（如[MASK]）替换掉一个或多个词，然后训练模型预测被替换掉的词. 优点 ：较低的部署门槛： INT4 精度下，只 需6GB显存，使得 ChatGLM-6B 可 以部署在消费级显卡上进行推理. 更长的序列长度： 相比 GLM-10B （序列长度1024），ChatGLM2-6B 序列长度达32K，支持更长对话和应 用。 人类类意图对齐训练。 缺点 ：模型容量小，相对较弱的模型记忆和语言能力。 多轮对话能力较弱。 模型配置（6B）与硬件要求： 2 LLaMA模型简介 ： LLaMA（Large Language Model Meta AI），由 Meta AI 于2023年发布的一个开放且高效的大型基础语言模型，共有 7B、13B、33B、65B（650 亿）四种版本. LLaMA训练数据是以英语为主的拉丁语系，另外还包含了来自 GitHub 的代码数据。训练数据以英文为主，不包含中韩日文，所有训练数据都是开源的。其中LLaMA-65B 和 LLaMA-33B 是在 1.4万亿 (1.4T) 个token上训练的，而最小的模型 LLaMA-7B 和LLaMA-13B 是在 1万亿 (1T) 个 token 上训练的. LLaMA 的训练目标是语言模型，即根据已有的上文去预测下一个词 优点 ： 具有 130 亿参数的 LLaMA 模型 「在大多数基准上」可以胜过 GPT-3（ 参数量达 1750 亿）. 可以在单块 V100 GPU 上运行； 而最大的 650 亿参数的 LLaMA 模型可以媲美谷歌的 Chinchilla70B 和 PaLM-540B. 缺点 ：会产生偏见性、有毒或者虚假的内容. 在中文上效果差，训练语料不包含中文或者一个汉字切分为多个token，编码效率低，模型学习难度大. 模型配置（7B）与硬件要求： 3 BLOOM模型简介 BLOOM系列模型是由 Hugging Face公司训练的大语言模型. 训练数据包含了英语、中文、法语、西班牙语、葡萄牙语等共 46 种语言，另外还包含 13 种编程语言. 1.5TB 经过去重和清洗的文本，其中中文语料占比为16.2%. 按照模型参数量，BLOOM 模型有 560M、1.1B、1.7B、3B、7.1B 和 176B 这几个不同参数规模的模型. BLOOM 的训练目标是语言模型，即根据已有的上文去预测下一个词 优点 ：具有良好的多语言适 应性，能够在多种语 言间进行切换，且无 需重新训练. 缺点 ：会产生偏见性、有毒或者虚假的内容. 模型配置（176B）与硬件要求 4 Baichuan-7B模型 Baichuan-7B由百川智能于2023年6月发布的一个开放且可商用的大型预训练语言模型，其支持中英双语，是在约 1.2万亿 (1.2T) 个 token上训练的70亿参数模型. Baichuan-7B 的训练目标也是语言模型，即根据已有的上文去预测下一个词。 模型配置（7B）与模型特点： 五、LLM大模型建立的流程 LLM（Large Language Model）大模型训练的流程一般涉及以下几个阶段： 1. 数据收集和预处理 ：首先需要收集大量的训练数据，这些数据可以是从互联网上收集的文本数据，也可以是从现有的数据集中获取的。收集的数据需要进行预处理，包括去除噪声、标记化、分词等操作。 2. 构建模型架构 ：选择适合的模型架构来搭建LLM模型，常见的架构包括Transformer、BERT等。 3. 模型预训练 ：大模型首先在大量的无标签数据上进行训练，预训练的最终目的是让模型学习到语言的统计规律和一般知识。在这个过程中模型能够学习到词语的语义、句子的语法结构、以及文本的一般知识和上下文信息。需要注意的是，预训练本质上是一个无监督学习过程;得到预训练模型(Pretrained Model)， 也被称为基座模型(Base Model)，模型具备通用的预测能力。如GLM-130B模 型、OpenAI的A、B、C、D四大模型，都是基座模型; 4. 微调 ：在微调阶段，LLM使用特定的数据集进行训练，以适应特定的任务或应用。这个阶段通常使用迁移学习的方法，将预训练的模型作为基础，在特定任务上进行微调。 5. 模型评估 ：训练完成后，需要对训练得到的模型进行评估。评估的方式可以是计算模型在验证集或测试集上的准确率、召回率等指标。同时，也可以对生成的文本进行人工评估，判断模型生成的文本是否符合预期。 6. 模型部署和优化 ：训练完成后，需要将模型进行部署，可以通过将模型封装成API接口的形式提供给用户使用。同时，也可以进行模型的优化，如模型压缩、量化等，以提高模型的效率和性能。 需要注意的是，LLM大模型训练的过程通常需要大量的计算资源和时间，并且在训练过程中需要进行大规模的参数更新，因此需要有高效的训练算法和计算平台来支持。 有需要的小伙伴，可以 点击下方领取 六、Fine-Tuning 首先解释一下什么是Prompt? prompt顾名思义就是“提示”的意思， 应该有人玩过你画我猜这个游戏吧，对方根据一个词语画一幅画，我们来猜他画的是什么，因为有太多灵魂画手了，画风清奇，或者你们没有心有灵犀，根本就不好猜啊！这时候屏幕上会出现一些提示词比如3个字，水果，那岂不是好猜一点了嘛。 Fine-Tuning属于一种迁移学习方式，在自然语言处理（NLP）中，Fine-Tuning是用于将预训练的语言模型适应于特定任务或领域。 Fine-Tuning的基本思想是采用已经在大量文本上进行训练的预训练语言模型，然后在小规模的任务特定文本上继续训练它. Fine-Tuning的目的是把模型适配到特定的任务上！ 解决方法：Prompt-Tuning, 通过添加模板的方法来避免引入额外的参数，从而让模型可以在小样本（few-shot）或者零样本（zero-shot）场景下达到理想的效果 。 七、Prompt-Tuning Prompt-Tuning方法是一种用于改进语言模型的训练方法，是由谷歌提出的一种轻量级的优化方法 。在语言模型中，Prompt是一个前缀文本，用于指导生成的文本内容。Prompt-Tuning方法通过对Prompt进行优化，使其能够更好地引导模型生成符合预期的文本。 基于Fine-Tuning的方法是让预训练模型去迁就下游任务，而基于Prompt-Tuning的方法可以让下游任务去迁就预训练模型, 其目的是将Fine-tuning的下游任务目标转换为Pre-training的任务. 传统的语言模型训练方法通常使用大量的无标签文本来预训练模型，然后使用有标签数据进行微调。然而，这种方法存在一些问题，例如模型生成的文本可能缺乏准确性和一致性。Prompt-Tuning方法通过添加或调整Prompt，可以更精确地指导模型生成特定类型的文本。具体来说，Prompt-Tuning方法有以下几个步骤： 1. 创建Prompt：根据任务需求，设计一个能够引导生成文本的Prompt。Prompt可以是一个简单的问题、一句话的描述、或者一段指令等。 2. 选择优化策略：根据不同的优化目标，选择适当的优化策略。常见的策略包括Prompt Engineering、Prompt Language Modeling、Prompt Optimization等。 3. 优化Prompt：根据选择的优化策略，在训练数据中对Prompt进行优化。这可以包括通过梯度下降方法调整Prompt的权重，或者通过生成和筛选多个Prompt进行优选。 4. 微调模型：使用优化后的Prompt和有标签数据对模型进行微调，以便模型能够更好地生成符合Prompt要求的文本。 通过使用Prompt-Tuning方法，可以提高语言模型生成文本的准确性和一致性，并且能够更好地指导模型生成特定类型的文本，适用于各种任务，例如机器翻译、对话系统、摘要生成等。 八、超大规模参数模型Prompt-Tuning方法 近两年来，随着Prompt-Tuning技术的发展，对于超过10亿参数量的模型来说，Prompt-Tuning所带来的增益远远高于标准的Fine-tuning. 如GPT-3模型, 只需要设计合适的模板或指令即可以实现免参数训练的零样本学习. 根本原因 ：模型参数量足够大，训练过程中使用了 足够多的语料，同时设计的 预训练任务足够有效. 该方法在参数规模非常大的模型微调时效果很好，当参数规模达到 100亿时和全量微调效果一致。 面向超大规模的模型的Prompt-Tuning方法有三种： 1. 上下文学习 In-Context Learning ：直接挑选少量的训练样本作为该任务的提示.。 2. 指令学习 Instruction- Tuning ：构建任务指令集，促使模型根据任务指令做出反馈。 3. 思维链Chain-of-Thought ：给予或激发模型具有推理和解释的信息，通过线性链式的模式指导模型生成合理的结果。 8.1 上下文学习 In-Context Learning In-Context learning（ICL）最早在GPT3中提出, 旨在从训练集中挑选少量的标注样本，设计任务相关的指令形成提示模板，用于指导测试样本生成相应的结果，有以下三种学习方式。 8.2. 指令学习 Instruction- Tuning 其实Prompt-Tuning本质上是对下游任务的指令，简单的来说：就是告诉模型需要做什么任务，输出什么内容. 上文我们提及到的离散或连续的模板，本质上就是一种对任务的提示. 因此, 在对大规模模型进行微调时, 可以为各种类型的任务定义更明显的指令, 并进行训练，来提高模型对不同任务的泛化能力. 举个例子： 指令学习和Prompt的区别是什么？ 指令学习激发语言模型的理解能力；指令学习激发语言模型的补全能力. 8.3 思维链Chain-of-Thought 思维链方法的核心思想是将思考的过程及其相关的观念和想法串联起来，形成一个连续的思维链条。这种链条可以由线性或非线性的思维过程构成，从而帮助模型不断延伸和扩展思考。 相比于之前传统的上下文学习（即通过x1,y1,x2 ,y2 ,…xtest作为输入来让大模型补全输出ytest），思维链多了中间的推导提示. 九、Prefix-Tuning 2021年斯坦福大学论文《Prefix-Tuning: Optimizing Continuous Prompts for Generation》中提出了PrefixTuning 方法，该方法是在输入 token 之前构造一段任务相关的 virtual tokens 作为Prefix，然后训练的时候只更新 Prefix 部分的参数，而 Transformer 中的其他部分参数固定. 原理简述:在原始模型基础上，增加一个可被训练的Embedding 层，用于给提示词增加前缀进行信息过滤，从而让模型更好的理解提示词意图， 并在训练过程中不断优化这些参数; 优点：Prefix Tuning既能够在模型结构上增加一些新的灵活性，又能够在模型使用上提供一种自动的、能够改进模型表现的提示机制; 十、LoRA 低秩适应（Low-Rank Adaptation，LoRA）是一种参数高效的微调技术， LoRA是最早由微软研究院发布的一项微调技术; 其核心思想是对大型模型的权重矩阵进行隐式的低秩转换，也就是：通过一个较低维度的表示来近似表示一个高维矩阵或数据集。 LoRA 原理： LoRA技术冻结预训练模型的权重，并在每个Transformer块中注入可训练层（称为秩分解矩阵），即在模型的Linear层的旁边增加一个“旁支”A和B。其中，A将数据从d维降到r维，这个r是LoRA的秩，是一个重要的超参数；B将数据从r维升到d维，B部分的参数初始为0。模型训练结束后，需要将A+B部分的参数与原大模型的参数合并在一起使用。 简而言之：基于大模型的内在低秩特性，LoRA通过在模型的权重矩阵中添加一个低秩矩阵来实现微调。这个低秩矩阵可以看作是对原始权重矩阵的一个小的调整，它不会显著改变模型的参数规模，但可以有效地捕捉到特定任务的特征。 读者福利：如果大家对大模型感兴趣，这套大模型学习资料一定对你有用 对于0基础小白入门： 如果你是零基础小白，想快速入门大模型是可以考虑的。 一方面是学习时间相对较短，学习内容更全面更集中。 二方面是可以根据这些资料规划好学习计划和方向。 包括：大模型学习线路汇总、学习阶段，大模型实战案例，大模型学习视频，人工智能、机器学习、大模型书籍PDF。带你从零基础系统性的学好大模型！ 有需要的小伙伴，可以 点击下方领取 AI大模型学习路线汇总 大模型学习路线图，整体分为7个大的阶段： （全套教程文末领取哈） 第一阶段： 从大模型系统设计入手，讲解大模型的主要方法； 第二阶段： 在通过大模型提示词工程从Prompts角度入手更好发挥模型的作用； 第三阶段： 大模型平台应用开发借助阿里云PAI平台构建电商领域虚拟试衣系统； 第四阶段： 大模型知识库应用开发以LangChain框架为例，构建物流行业咨询智能问答系统； 第五阶段： 大模型微调开发借助以大健康、新零售、新媒体领域构建适合当前领域大模型； 第六阶段： 以SD多模态大模型为主，搭建了文生图小程序案例； 第七阶段： 以大模型平台应用与开发为主，通过星火大模型，文心大模型等成熟大模型构建大模型行业应用。 学会后的收获： • 基于大模型全栈工程实现 （前端、后端、产品经理、设计、数据分析等），通过这门课可获得不同能力； • 能够利用大模型解决相关实际项目需求： 大数据时代，越来越多的企业和机构需要处理海量数据，利用大模型技术可以更好地处理这些数据，提高数据分析和决策的准确性。因此，掌握大模型应用开发技能，可以让程序员更好地应对实际项目需求； • 基于大模型和企业数据AI应用开发， 实现大模型理论、掌握GPU算力、硬件、LangChain开发框架和项目实战技能， 学会Fine-tuning垂直训练大模型（数据准备、数据蒸馏、大模型部署）一站式掌握； • 能够完成时下热门大模型垂直领域模型训练能力，提高程序员的编码能力： 大模型应用开发需要掌握机器学习算法、深度学习框架等技术，这些技术的掌握可以提高程序员的编码能力和分析能力，让程序员更加熟练地编写高质量的代码。 有需要的小伙伴，可以 点击下方领取</p>
</div></details><h2 id="toc-220">111. 凸优化学习笔记：QP及SOCP问题 - CSDN博客</h2>
<ul>
<li>链接：https://blog.csdn.net/whn19980801/article/details/116832637</li>
<li>来源：bing</li>
<li>摘要：文章浏览阅读1.3w次，点赞18次，收藏125次。本文详细介绍了凸优化在无线通信中的应用，从二次规划（QP）问题开始，如最小二乘和接受波束成形，然后扩展到二次锥规划（SOCP）问 …</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-221">正文（抓取，非 AI）</h3>
<p>凸优化学习笔记：QP及SOCP问题 QP问题 定义 是什么 几何意义 QP、QCQP、LP之间的关系 例子 最小二乘及回归问题 多面体间距离 方差定界问题 基于随机费用的线性规划（考虑随机变量的优化问题以及讨论多目标函数时的权衡问题） 通信中例子（接受波束成形问题） 接受波束成形问题 平均旁瓣能量最小化 最大旁瓣能量最小化 认知无线电中波束成形设计 SOCP问题 定义 例子 鲁棒线性规划 概率约束的LP 鲁棒最小二乘逼近 通信中例子 抑制旁瓣的接受波束成形 最小方差波束设计 基于最小方差的鲁棒波束成形 下行波束成形 单小区 PM问题 SB问题 多小区 家庭基站波束成形 本文是针对Boyd凸优化教材在QP、SOCP问题部分的学习笔记。 QP问题 定义 是什么 QP 目标函数的凸的二次函数，约束是仿射的，为QP问题。有如下形式： minimize ⁡ subject to ( 1 / 2 ) x T P x + q T x + r G x ⪯ h A x = b (1) \begin{array}{ll} \underset{\text { subject to }}{\operatorname{minimize}} &amp; (1 / 2) x^{T} P x+q^{T} x+r \ &amp; G x \preceq h \ &amp; A x=b \end{array} \tag{1} subject to m i n i m i z e ​ ​ ( 1 / 2 ) x T P x + q T x + r G x ⪯ h A x = b ​ ( 1 ) QCQP 在QP的基础上，约束也是二次函数形式： minimize ⁡ ( 1 / 2 ) x T P 0 x + q 0 T x + r 0 subject to ( 1 / 2 ) x T P i x + q i T x + r i ≤ 0 , i = 1 , … , m A x = b , (2) \begin{array}{ll} \operatorname{minimize} &amp; (1 / 2) x^{T} P_{0} x+q_{0}^{T} x+r_{0} \ \text { subject to } &amp; (1 / 2) x^{T} P_{i} x+q_{i}^{T} x+r_{i} \leq 0, \quad i=1, \ldots, m \ &amp; A x=b, \end{array} \tag{2} m i n i m i z e subject to ​ ( 1 / 2 ) x T P 0 ​ x + q 0 T ​ x + r 0 ​ ( 1 / 2 ) x T P i ​ x + q i T ​ x + r i ​ ≤ 0 , i = 1 , … , m A x = b , ​ ( 2 ) 几何意义 QP： 在一个多面体空间内最小化一个凸的二次函数 QCQP问题将多面体空间换为一个椭圆空间。 QP、QCQP、LP之间的关系 QCQP问题最具一般性，若 ( 2 ) (2) ( 2 ) 式中 P i P_i P i ​ 为0，则QCQP退化为QP，若 P P P 也为0，则进一步退化为LP问题。 例子 最小二乘及回归问题 无约束的最小二乘问题为： ∥ A x − b ∥ 2 2 = x T A T A x − 2 b T A x + b T b (3) |A x-b|<em 2="">{2}^{2}=x^{T} A^{T} A x-2 b^{T} A x+b^{T} b \tag{3} ∥ A x − b ∥ 2 2 ​ = x T A T A x − 2 b T A x + b T b ( 3 ) 是一个无约束QP问题，该问题有闭式解 x = A † b x=A^{\dagger}b x = A † b ，也叫做 回归分析或最小二乘逼近 。 增加不等式约束后成为 约束回归或约束最小二乘 ，无解析解。例如： minimize ⁡ ∥ A x − b ∥ 2 2 subject to l i ≤ x i ≤ u i , i = 1 , … , n (4) \begin{array}{ll} \operatorname{minimize} &amp; |A x-b|</em>^{2} \ \text { subject to } &amp; l_{i} \leq x_{i} \leq u_{i}, \quad i=1, \ldots, n \end{array} \tag{4} m i n i m i z e subject to ​ ∥ A x − b ∥ 2 2 ​ l i ​ ≤ x i ​ ≤ u i ​ , i = 1 , … , n ​ ( 4 ) 多面体间距离 R n \mathbf{R}^{n} R n 上两个多面体 P 1 = { x ∣ A 1 x ⪯ b 1 } \mathcal{P}<em 1="">{1}=\left{x \mid A</em> x \preceq b_{1}\right} P 1 ​ = { x ∣ A 1 ​ x ⪯ b 1 ​ } 与 P 2 = { x ∣ A 2 x ⪯ b 2 } \mathcal{P}<em 2="">{2}=\left{x \mid A</em> x \preceq b_{2}\right} P 2 ​ = { x ∣ A 2 ​ x ⪯ b 2 ​ } 间距离为： dist ⁡ ( P 1 , P 2 ) = inf ⁡ { ∥ x 1 − x 2 ∥ 2 ∣ x 1 ∈ P 1 , x 2 ∈ P 2 } (5) \operatorname{dist}\left(\mathcal{P}<em 2="">{1}, \mathcal{P}</em>\right)=\inf \left{\left|x_{1}-x_{2}\right|<em 1="">{2} \mid x</em> \in \mathcal{P}<em 2="">{1}, x</em> \in \mathcal{P}<em 1="">{2}\right} \tag{5} d i s t ( P 1 ​ , P 2 ​ ) = in f { ∥ x 1 ​ − x 2 ​ ∥ 2 ​ ∣ x 1 ​ ∈ P 1 ​ , x 2 ​ ∈ P 2 ​ } ( 5 ) 若要最小化两个多面体间距离，优化问题为： minimize ⁡ ∥ x 1 − x 2 ∥ 2 2 subject to A 1 x 1 ⪯ b 1 , A 2 x 2 ⪯ b 2 , (6) \begin{array}{ll} \operatorname{minimize} &amp; \left|x</em>-x_{2}\right|<em 1="">{2}^{2} \ \text { subject to } &amp; A</em> x_{1} \preceq b_{1}, \quad A_{2} x_{2} \preceq b_{2}, \end{array} \tag{6} m i n i m i z e subject to ​ ∥ x 1 ​ − x 2 ​ ∥ 2 2 ​ A 1 ​ x 1 ​ ⪯ b 1 ​ , A 2 ​ x 2 ​ ⪯ b 2 ​ , ​ ( 6 ) 典型的QP问题。 方差定界问题 maximize ⁡ ∑ i = 1 n f i 2 p i − ( ∑ i = 1 n f i p i ) 2 subject to p ⪰ 0 , 1 T p = 1 α i ≤ a i T p ≤ β i , i = 1 , … , m (7) \begin{array}{ll} \operatorname{maximize} &amp; \sum_{i=1}^{n} f_{i}^{2} p_{i}-\left(\sum_{i=1}^{n} f_{i} p_{i}\right)^{2} \ \text { subject to } &amp; p \succeq 0, \quad \mathbf{1}^{T} p=1 \ &amp; \alpha_{i} \leq a_{i}^{T} p \leq \beta_{i}, \quad i=1, \ldots, m \end{array} \tag{7} m a x i m i z e subject to ​ ∑ i = 1 n ​ f i 2 ​ p i ​ − ( ∑ i = 1 n ​ f i ​ p i ​ ) 2 p ⪰ 0 , 1 T p = 1 α i ​ ≤ a i T ​ p ≤ β i ​ , i = 1 , … , m ​ ( 7 ) 其目标函数可改写为 ( f 2 ) T x − ∣ ∣ f T x ∣ ∣ 2 2 = − x T f f T x + ( f 2 ) T x (f^2)^\mathrm{T}x-||f^\mathrm{T}x||<em i="">2^2=-x^\mathrm{T}ff^\mathrm{T}x+(f^2)^\mathrm{T}x ( f 2 ) T x − ∣ ∣ f T x ∣ ∣ 2 2 ​ = − x T f f T x + ( f 2 ) T x ，典型的QP问题。 基于随机费用的线性规划（考虑随机变量的优化问题以及讨论多目标函数时的权衡问题） 一个标准的LP问题： minimize ⁡ c T x subject to G x ⪯ h A x = b (8) \begin{array}{ll} \operatorname{minimize} &amp; c^{T} x \ \text { subject to } &amp; G x \preceq h \ &amp; A x=b \end{array} \tag{8} m i n i m i z e subject to ​ c T x G x ⪯ h A x = b ​ ( 8 ) 其中 c c c 是随机变量，均值为 c ˉ \bar{c} c ˉ ，方差为 E ( c − c ˉ ) ( c − c ˉ ) T = Σ \mathbf{E}(c-\bar{c})(c-\bar{c})^\mathrm{T}=\Sigma E ( c − c ˉ ) ( c − c ˉ ) T = Σ 。所以目标函数均值为 E c T x = c ˉ T x \mathbf{E} c^{T} x=\bar{c}^{T} x E c T x = c ˉ T x ，方差为 var ⁡ ( c T x ) = E ( c T x − E c T x ) 2 = x T Σ x \operatorname{var}\left(c^{T} x\right)=\mathbf{E}\left(c^{T} x-\mathbf{E} c^{T} x\right)^{2}=x^{T} \Sigma x v a r ( c T x ) = E ( c T x − E c T x ) 2 = x T Σ x 。若考虑均值和方差两个目标函数，则有如下权衡过程： minimize ⁡ c ˉ T x + γ x T Σ x subject to G x ⪯ h A x = b (9) \begin{array}{ll} \operatorname{minimize} &amp; \bar{c}^{T} x+\gamma x^{T} \Sigma x \ \text { subject to } &amp; G x \preceq h \ &amp; A x=b \end{array} \tag{9} m i n i m i z e subject to ​ c ˉ T x + γ x T Σ x G x ⪯ h A x = b ​ ( 9 ) 该目标函数是双优化目标问题常用的权衡准则，其中 γ \gamma γ 描述两个优化目标的权重。 通信中例子（接受波束成形问题） 接受波束成形问题 对于MIMO系统，考虑远场情况与窄带信号，接收端第 i i i 根天线与第 j j j 根天线的接收信号在时域有如下关系： x i ( t ) = x j ( t − ( i − j ) d sin ⁡ θ / c ) (10) x</em>(t)=x_{j}(t-(i-j) d \sin \theta / c) \tag{10} x i ​ ( t ) = x j ​ ( t − ( i − j ) d sin θ / c ) ( 1 0 ) 如下图所示： 若原信号方向为 θ \theta θ ，接收端 P P P 个天线，记信号源为 s ( t ) ∈ C s(t) \in \mathbb{C} s ( t ) ∈ C ，接收信号为 y ( t ) = [ y 1 ( t ) , … , y P ( t ) ] T \mathbf{y}(t)=\left[y_{1}(t), \ldots, y_{P}(t)\right]^{T} y ( t ) = [ y 1 ​ ( t ) , … , y P ​ ( t ) ] T ，则有如下关系： y ( t ) = a ( θ ) s ( t ) (11) \mathbf{y}(t)=\mathbf{a}(\theta) s(t) \tag{11} y ( t ) = a ( θ ) s ( t ) ( 1 1 ) 其中 a ( θ ) \mathbf{a}(\theta) a ( θ ) 为接收波矢 a ( θ ) = [ 1 , e − j 2 π d sin ⁡ ( θ ) / λ , … , e − j 2 π d ( P − 1 ) sin ⁡ ( θ ) / λ ] T ∈ C P (12) \mathbf{a}(\theta)=\left[1, e^{-j 2 \pi d \sin (\theta) / \lambda}, \ldots, e^{-j 2 \pi d(P-1) \sin (\theta) / \lambda}\right]^{T} \in \mathbb{C}^{P} \tag{12} a ( θ ) = [ 1 , e − j 2 π d sin ( θ ) / λ , … , e − j 2 π d ( P − 1 ) sin ( θ ) / λ ] T ∈ C P ( 1 2 ) 接收端波束成形过程为 s ^ ( t ) = w H y ( t ) = w H a ( θ ) s ( t ) (13) \hat{s}(t)=\mathbf{w}^{H} \mathbf{y}(t)=\mathbf{w}^{H} \mathbf{a}(\theta) s(t) \tag{13} s ^ ( t ) = w H y ( t ) = w H a ( θ ) s ( t ) ( 1 3 ) w ∈ C P \mathbf{w} \in \mathbb{C}^{P} w ∈ C P 为波束成形权重向量。简单的波束成形可表述为 w = a ( θ d e s ) \mathbf{w}=\mathbf{a}(\theta_{\mathrm{des}}) w = a ( θ d e s ​ ) ，但这种方式旁瓣抑制效果差，如下图： 下面是两种解决算法： 平均旁瓣能量最小化 令 Ω = [ − π / 2 , θ ℓ ] ∪ [ θ u , π / 2 ] \boldsymbol{\Omega}=\left[-\pi / 2, \theta_{\ell}\right] \cup\left[\theta_{u}, \pi / 2\right] Ω = [ − π / 2 , θ ℓ ​ ] ∪ [ θ u ​ , π / 2 ] 表示旁瓣带，约束条件： w H a ( θ d e s ) = 1 \mathbf{w}^{H} \mathbf{a}\left(\theta_{\mathrm{des}}\right)=1 w H a ( θ d e s ​ ) = 1 ，则旁瓣总能量最小化问题为 min ⁡ w ∈ C P ∫ Ω ∣ w H a ( θ ) ∣ 2 d θ s.t. w H a ( θ des ) = 1 (14) \begin{array}{c} \min <em _omega="\Omega">{\mathbf{w} \in \mathbb{C}^{P}} \int</em>\left|\mathbf{w}^{H} \mathbf{a}(\theta)\right|^{2} d \theta \ \text { s.t. } \mathbf{w}^{H} \mathbf{a}\left(\theta_{\text {des }}\right)=1 \end{array} \tag{14} min w ∈ C P ​ ∫ Ω ​ ∣ ∣ ​ w H a ( θ ) ∣ ∣ ​ 2 d θ s.t. w H a ( θ des ​ ) = 1 ​ ( 1 4 ) 转化为标准的等式约束二次规划问题有： min ⁡ w ∈ C P { f ( w ) ≜ w H P w } s.t. w H a ( θ des ) = 1 (15) \begin{array}{c} \min <em _des="{des" _text="\text">{\mathbf{w} \in \mathbb{C}^{P}}\left{f(\mathbf{w}) \triangleq \mathbf{w}^{H} \mathbf{P} \mathbf{w}\right} \ \text { s.t. } \mathbf{w}^{H} \mathbf{a}\left(\theta</em>}\right)=1 \end{array} \tag{15} min w ∈ C P ​ { f ( w ) ≜ w H P w } s.t. w H a ( θ des ​ ) = 1 ​ ( 1 5 ) 其中 P = ∫ Ω a ( θ ) a H ( θ ) d θ = P H ⪰ 0 \mathbf{P}=\int_{\Omega} \mathbf{a}(\theta) \mathbf{a}^{H}(\theta) d \theta=\mathbf{P}^{H} \succeq \mathbf{0} P = ∫ Ω ​ a ( θ ) a H ( θ ) d θ = P H ⪰ 0 为半正定矩阵。该问题是一个凸问题。 最大旁瓣能量最小化 问题形式： min ⁡ w ∈ C P max ⁡ θ ∈ Ω ∣ w H a ( θ ) ∣ 2 s.t. w H a ( θ des ) = 1. (16) \begin{aligned} &amp;\min <em _in="\in" _omega="\Omega" _theta="\theta">{\mathbf{w} \in \mathbb{C}^{P}} \max </em>\left|\mathbf{w}^{H} \mathbf{a}(\theta)\right|^{2} \ &amp;\text { s.t. } \mathbf{w}^{H} \mathbf{a}\left(\theta_{\text {des }}\right)=1 . \end{aligned} \tag{16} ​ w ∈ C P min ​ θ ∈ Ω max ​ ∣ ∣ ​ w H a ( θ ) ∣ ∣ ​ 2 s.t. w H a ( θ des ​ ) = 1 . ​ ( 1 6 ) 这种最大值最小化问题可以转化为上境图形式： min ⁡ w ∈ C P , t ∈ R t s.t. ∣ w H a ( θ ) ∣ 2 ≤ t , ∀ θ ∈ Ω w H a ( θ d e s ) = 1 (17) \begin{array}{rl} \min <em _mathrm_des="\mathrm{des">{\mathbf{w} \in \mathbb{C}^{P}, t \in \mathbb{R}} &amp; t \ \text { s.t. } &amp; \left|\mathbf{w}^{H} \mathbf{a}(\theta)\right|^{2} \leq t, \forall \theta \in \mathbf{\Omega} \ &amp; \mathbf{w}^{H} \mathbf{a}\left(\theta</em>}\right)=1 \end{array} \tag{17} min w ∈ C P , t ∈ R ​ s.t. ​ t ∣ ∣ ​ w H a ( θ ) ∣ ∣ ​ 2 ≤ t , ∀ θ ∈ Ω w H a ( θ d e s ​ ) = 1 ​ ( 1 7 ) 将不等式约束标准化表示： ∣ w H a ( θ ) ∣ 2 − t = w H P ( θ ) w − t (18) \left|\mathbf{w}^{H} \mathbf{a}(\theta)\right|^{2}-t=\mathbf{w}^{H} \mathbf{P}(\theta) \mathbf{w}-t \tag{18} ∣ ∣ ​ w H a ( θ ) ∣ ∣ ​ 2 − t = w H P ( θ ) w − t ( 1 8 ) 其中 P ( θ ) = a ( θ ) a H ( θ ) ⪰ 0 \mathbf{P}(\theta</p>
</div></details><h2 id="toc-222">112. 大语言模型LLM（Large Language Model）介绍 - jack_Meng ...</h2>
<ul>
<li>链接：https://www.cnblogs.com/mq0036/p/18704400</li>
<li>来源：bing</li>
<li>摘要：2025年2月8日 · 一. 什么是LLM（大语言模型）？ 1. 发展历程 语言建模的研究始于20世纪90年代，最初采用了统计学习方法，通过前面的词汇来预测下一个词汇。然而，这种方法在理解复杂语言规则方面 …</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-223">正文（抓取，非 AI）</h3>
<p>大语言模型LLM（Large Language Model）介绍 - jack_Meng - 博客园 Jack_孟 大语言模型LLM（Large Language Model）介绍 一. 什么是LLM（大语言模型）？ 1. 发展历程 语言建模的研究始于 20世纪90年代 ，最初采用了统计学习方法，通过前面的词汇来预测下一个词汇。然而，这种方法在理解复杂语言规则方面存在一定局限性。 随后，研究人员不断尝试改进，其中在 2003年 ，深度学习先驱 Bengio 在他的经典论文《A Neural Probabilistic Language Model》中，首次将深度学习的思想融入到语言模型中，使用了更强大的 神经网络模型 ，这相当于为计算机提供了更强大的"大脑"来理解语言。这种方法让模型可以更好地捕捉语言中的复杂关系，虽然这一步很重要，但仍有改进的空间。 大约在 2018年左右 ，研究人员引入了 Transformer架构的神经网络模型 ，通过大量文本数据训练这些模型，使它们能够通过阅读大量文本来深入理解语言规则和模式，就像让计算机阅读整个互联网一样。所以它对语言有了更深刻的理解。这种方法在很多任务上表现得非常好。 与此同时，研究人员发现，随着语言模型规模的扩大（增加模型大小或使用更多数据），模型展现出了一些惊人的能力，通常在各种任务中表现显著提升。这时我们进入了大语言模型（LLM）时代。 2. 大语言模型的概念 大语言模型（英文：Large Language Model，缩写LLM），也称大型语言模型，是一种人工智能模型，旨在理解和生成人类语言 。 通常，大语言模型 (LLM) 指包含 数百亿（或更多）参数的语言模型 ，这些模型在大量的文本数据上进行训练，例如国外的有GPT-3 、GPT-4、PaLM 、Galactica 和 LLaMA 等，国内的有ChatGLM、文心一言、通义千问、讯飞星火等。 在这个阶段，计算机的“大脑”变得非常巨大，拥有数十亿甚至数千亿的参数。这就像是将计算机的大脑升级到了一个巨型超级计算机。这让计算机可以在各种任务上表现得非常出色，有时甚至比人类还要聪明。 为了探索性能的极限，许多研究人员开始训练越来越庞大的语言模型 ，例如拥有 1750 亿参数的 GPT-3 和 5400 亿参数的 PaLM 。尽管这些大型语言模型与小型语言模型（例如 BERT 的 3.3 亿参数和 GPT-2 的 15 亿参数）使用相似的架构和预训练任务，但它们展现出截然不同的能力，尤其在解决复杂任务时表现出了惊人的潜力，这被称为“ 涌现能力 ”。以 GPT-3 和 GPT-2 为例，GPT-3 可以通过学习上下文来解决少样本任务，而 GPT-2 在这方面表现较差。因此，研究界给这些庞大的语言模型起了个名字，称之为“大语言模型（LLM）”。而 LLM 的一个杰出应用就是 ChatGPT ，它是 GPT 系列 LLM 用于与人类对话式应用的大胆尝试，展现出了非常流畅和自然的表现。 3、LLM的应用和影响 LLM已经在许多领域产生了深远的影响。在 自然语言处理 领域，它可以帮助计算机更好地理解和生成文本，包括写文章、回答问题、翻译语言等。在 信息检索 领域，它可以改进搜索引擎，让我们更轻松地找到所需的信息。在 计算机视觉 领域，研究人员还在努力让计算机理解图像和文字，以改善多媒体交互。 最重要的是，LLM的出现让人们重新思考了 通用人工智能（AGI） 的可能性。AGI 是一种像人类一样思考和学习的人工智能。LLM 被认为是 AGI 的一种早期形式，这引发了对未来人工智能发展的许多思考和计划。 总之，LLM 是一种令人兴奋的技术，它让计算机更好地理解和使用语言，正在改变着我们与技术互动的方式，同时也引发了对未来人工智能的无限探索。希望这篇文章让你对LLM有了更清晰的认识！ 二、大模型的能力和特点 1. 大模型的能力 1.1 涌现能力（emergent abilities） 区分大语言模型（LLM）与以前的预训练语言模型（PLM）最显著的特征之一是它们的 涌现能力 。涌现能力指的是一种令人惊讶的能力，它在小型模型中不明显，但在大型模型中显著出现。可以类比到物理学中的相变现象，涌现能力的显现就像是模型性能随着规模增大而迅速提升，超过了随机水平，也就是我们常说的量变引起了质变。 具体来说，涌现能力可以定义为与某些复杂任务相关的能力，但我们更关注的是它们具备的通用能力，也就是能够应用于解决各种任务的能力。接下来，让我们简要介绍三个典型的LLM涌现能力： 上下文学习 ：上下文学习能力是由 GPT-3 首次引入的。这种能力允许语言模型在提供自然语言指令或多个任务示例的情况下，通过理解上下文并生成相应输出的方式来执行任务，而无需额外的训练或参数更新。 指令遵循 ：通过使用自然语言描述的多任务数据进行微调，也就是所谓的指令微调，LLM 被证明在同样使用指令形式化描述的未见过的任务上表现良好。这意味着LLM能够根据任务指令执行任务，而无需事先见过具体示例，这展示了其强大的泛化能力。 逐步推理 ：小型语言模型通常难以解决涉及多个推理步骤的复杂任务，例如数学问题。然而，LLM通过采用"思维链"推理策略，可以利用包含中间推理步骤的提示机制来解决这些任务，从而得出最终答案。据推测，这种能力可能是通过对代码的训练获得的。 这些涌现能力让LLM在处理各种任务时表现出色，使它们成为了解决复杂问题和应用于多领域的强大工具。 1.2 作为基座模型支持多元应用的能力 在2021年，斯坦福大学等多所高校的研究人员提出了基座模型（foundation model）的概念，这更清晰地描述了之前学界所称的预训练模型的作用。这是一种全新的AI技术范式，借助于海量无标注数据的训练，获得可以适用于大量下游任务的大模型（单模态或者多模态）。这样，多个应用可以只依赖于一个或少数几个大模型进行统一建设。 大语言模型是这个新模式的典型例子，使用统一的大模型可以极大地提高研发效率，相比于分散的模型开发方式，这是一项本质上的进步。大型模型不仅可以缩短每个具体应用的开发周期，减少所需人力投入，也可以基于大模型的推理、常识和写作能力，获得更好的应用效果。因此，大模型可以成为AI应用开发的大一统基座模型，这是一个一举多得、全新的范式，值得大力推广。 1.3 支持对话作为统一入口的能力 让大语言模型真正火爆的契机，是基于对话聊天的ChatGPT。事实上，业界很早就发现了用户对于对话交互的特殊偏好，陆奇在微软期间2016年就推进“对话即平台（conversation as a platform）”的战略。此外，苹果Siri、亚马逊Echo等基于语音对话的产品也非常受欢迎，反映出互联网用户对于聊天和对话这种交互模式的偏好。虽然之前的聊天机器人存在各种问题，但大型语言模型的出现再次让聊天机器人这种交互模式可以重新想像。用户愈发期待像钢铁侠中“贾维斯”一样的人工智能，无所不能、无所不知。这引发我们对于智能体（Agent）类型应用前景的思考，Auto-GPT、微软Jarvis等项目已经出现并受到关注，相信未来会涌现出很多类似的以对话形态让助手完成各种具体工作的项目。 2.大模型的特点 大语言模型（LLM，Large Language Models）具有多种显著特点，这些特点使它们在自然语言处理和其他领域中引起了广泛的兴趣和研究。以下是大语言模型的一些主要特点： 巨大的规模： LLM通常具有巨大的参数规模，可以达到数十亿甚至数千亿个参数。这使得它们能够捕捉更多的语言知识和复杂的语法结构。 预训练和微调： LLM采用了预训练和微调的学习方法。它们首先在大规模文本数据上进行预训练（无标签数据），学会了通用的语言表示和知识，然后通过微调（有标签数据）适应特定任务，从而在各种NLP任务中表现出色。 上下文感知： LLM在处理文本时具有强大的上下文感知能力，能够理解和生成依赖于前文的文本内容。这使得它们在对话、文章生成和情境理解方面表现出色。 多语言支持： LLM可以用于多种语言，不仅限于英语。它们的多语言能力使得跨文化和跨语言的应用变得更加容易。 多模态支持： 一些LLM已经扩展到支持多模态数据，包括文本、图像和语音。这意味着它们可以理解和生成不同媒体类型的内容，实现更多样化的应用。 涌现能力： LLM表现出令人惊讶的涌现能力，即在大规模模型中出现但在小型模型中不明显的性能提升。这使得它们能够处理更复杂的任务和问题。 多领域应用： LLM已经被广泛应用于文本生成、自动翻译、信息检索、摘要生成、聊天机器人、虚拟助手等多个领域，对人们的日常生活和工作产生了深远的影响。 伦理和风险问题： 尽管LLM具有出色的能力，但它们也引发了伦理和风险问题，包括生成有害内容、隐私问题、认知偏差等。因此，研究和应用LLM需要谨慎。 总之，大语言模型是一种具有强大语言处理能力的技术，已经在多个领域展示了潜力。它们为自然语言理解和生成任务提供了强大的工具，同时也引发了对其伦理和风险问题的关注。这些特点使LLM成为了当今计算机科学和人工智能领域的重要研究和应用方向。 三、常见大模型 大语言模型的发展历程虽然只有短短不到五年的时间，但是发展速度相当惊人，截止 2023 年 6 月，国内外有超过百种大模型相继发布。按照时间线给出了 2019 年至 2023 年 6 月比较有影响力并且模型参数量超过 100 亿的大语言模型，如下图所示： （该图来源于参考内容 [1] ） 接下来我们主要介绍几个国内外常见的大模型（包括开源和闭源的LLM） 1.闭源 LLM (未公开源代码) 1.1.GPT系列 OpenAI 公司在 2018 年提出的 GPT（Generative Pre-Training）模型是典型的 生成式预训练语言模型 之一。 GPT 模型的基本原则是 通过语言建模将世界知识压缩到仅解码器的 Transformer 模型中 ，这样它就可以恢复(或记忆)世界知识的语义，并充当通用任务求解器。它能够成功的两个关键点： 训练能够准确预测下一个单词的仅解码器的 Transformer 语言模型。 扩展语言模型的大小。 总体而言，OpenAI 在 LLM 上的研究大致可以分为以下几个阶段： 接下来，我们将从模型规模、特点等方面，介绍大家熟悉的 ChatGPT 与 GPT4： ChatGPT 2022 年 11 月，OpenAI 发布了基于 GPT模型（GPT-3.5 和 GPT-4） 的 会话应用 ChatGPT 。由于与人类交流的出色能力，ChatGPT 自发布以来就引发了人工智能社区的兴奋。ChatGPT 是基于强大的 GPT 模型开发的，具有特别优化的会话能力。 ChatGPT 从本质上来说是一个 LLM 应用，它是基于 GPT-3.5 和 GPT-4 开发出来的，与 GPT-4 有本质的区别，正如当前应用界面所显示的，支持 GPT-3.5 和 GPT-4 两个版本。 现在的 ChatGPT 支持最长达 32,000 个字符，知识截止日期是 2021 年 9 月，它可以执行各种任务，包括 代码编写、数学问题求解、写作建议 等。ChatGPT 在与人类交流方面表现出了卓越的能力：拥有丰富的知识储备，对数学问题进行推理的技能，在多回合对话中准确追踪上下文，并且与人类安全使用的价值观非常一致。后来，ChatGPT 支持插件机制，这进一步扩展了 ChatGPT 与现有工具或应用程序的能力。到目前为止，它似乎是人工智能历史上最强大的聊天机器人。ChatGPT 的推出对未来的人工智能研究具有重大影响，它为探索类人人工智能系统提供了启示。 注意：2023 年 11 月 7 日， OpenAI 召开了首个开发者大会，会上推出了最新的大语言模型 GPT-4 Turbo，这个 Turbo 就相当于是进阶版的意思。它将上下文长度扩展到 128k个token，相当于 300 页文本，并且训练知识更新到 2023 年 4 月。 GPT-4 2023 年 3 月发布的GPT-4，它将 文本输入扩展到多模态信号 。总体而言，GPT3.5 拥有 1750亿 个参数，而 GPT4 的参数量官方并没有公布，但有相关人员猜测，GPT-4 在 120 层中总共包含了 1.8 万亿参数，也就是说，GPT-4 的规模是 GPT-3 的 10 倍以上。因此，GPT-4 比 GPT-3.5 解决复杂任务的能力更强，在许多评估任务上表现出较大的性能提升 。 最近的一项研究通过对人为生成的问题进行定性测试来研究 GPT-4 的能力，这些问题包含了各种各样的困难任务，并表明 GPT-4 可以比之前的 GPT 模型(如 GPT3.5 )实现更优越的性能。此外，由于六个月的迭代校准(在 RLHF 训练中有额外的安全奖励信号)，GPT-4 对恶意或挑衅性查询的响应更安全。在技术报告中，OpenAI 强调了如何安全地开发 GPT-4 ，并应用了一些干预策略来缓解 LLM 可能出现的问题，如幻觉、隐私和过度依赖。例如，他们引入了称为 红队评估 （red teaming）的机制，以减少危害或有毒物质的产生。作为另一个重要方面，GPT4 是在一个完善的深度学习基础设施上开发的，并使用改进的优化方法。他们引入了一种称为 可预测扩展 （predictable scaling）的新机制，可以在模型训练期间使用一小部分计算准确预测最终性能。 使用地址 1.2. Claude系列 Claude 系列模型是由 OpenAI 离职人员创建的 Anthropic 公司开发的闭源语言大模型，可以完成摘要总结、搜索、协助创作、问答、编码等任务。目前包含 Claude 和 Claude-Instant 两种模型可供选择，其中 Claude Instant 的延迟更低，性能略差，价格比完全体的 Claude-v1 要便宜，两个模型的上下文窗口都是 9000 个token（约 5000 个单词，或 15 页）它的目标是“更安全”、“危害更小”的人工智能。最早的 Claude 于 2023 年 3 月 15 日发布，并在 2023 年 7 月 11 日，更新至 Claude-2 。Claude 2 的训练参数官方并未公开，但是相关的猜测大概是 860.1 亿个参数。 该系列模型通过无监督预训练、基于人类反馈的强化学习和 Constitutional AI 技术（包含监督训练和强化学习）进行训练，旨在改进模型的有用性、诚实性和无害性。值得一提的是，Claude 最高支持 100K 词元的上下文，而 Claude-2 更是拓展到了 200K 词元的上下文。相比于Claude 1.3， Claude 2 拥有更强的综合能力，同时能够生成更长的相应。 总的来说，Claude 2 注重提高以下能力： Anthropic 致力于提高 Claude 作为编码助理的能力，Claude 2 在编码基准和人类反馈评估方面性能显著提升。 长上下文（long-context）模型对于处理长文档、少量 prompt 以及使用复杂指令和规范进行控制特别有用。Claude 的上下文窗口从 9K token 扩展到了 100K token（Claude 2 已经扩展到 200K token，但目前发布版本仅支持 100K token）。 以前的模型经过训练可以编写相当短的回答，但许多用户要求更长的输出。Claude 2 经过训练，可以生成最多 4000 个 token 的连贯文档，相当于大约 3000 个单词。 Claude 通常用于将长而复杂的自然语言文档转换为结构化数据格式。Claude 2 经过训练，可以 更好地生成 JSON、XML、YAML、代码和 Markdown 格式的正确输出 。 虽然 Claude 的训练数据仍然主要是英语，但 Claude 2 的训练数据中非英语数据比例已经明显增加。 Claude 2 的训练数据包括 2022 年和 2023 年初更新的数据。这意味着它知道最近发生的事件，但它仍然可能会产生混淆。 使用地址 1.3. PaLM 系列 PaLM 系列 语言大模型由 Google 开发。其初始版本于 2022 年 4 月发布，并在 2023 年 3 月公开了 API。PaLM 基于 Google 提出的 Pathways 机器学习系统搭建，训练数据总量达 780B 个字符，内容涵盖网页、书籍、新闻、开源代码等多种形式的语料。前 PaLM 共有 8B、62B、540B 三个不同参数量的模型版本。Google 还开发了多种 PaLM 的改进版本。 Med-PaLM 是 PaLM 540B 在医疗数据上进行了微调后的版本 ，在 MedQA 等医疗问答数据集上取得了最好成绩。 PaLM-E 是 PaLM 的多模态版本 ，能够在现实场景中控制机器人完成简单任务。 2023 年 5 月，Google 发布了 PaLM 2 ，但并未公开其技术细节。Google 内部文件显示其参数量为 340B，训练数据为 PaLM 的 5 倍左右。它是 PaLM(540B) 的升级版，能够处理“多语言任务”。它使用了一个覆盖 100 多种语言的语料库进行训练。而 PaLM2 实际上是一系列模型，可以根据规模分为：Gecko、Otter、Bison和Unicorn，可以根据不同的领域和需求进行微调，最小模型可以部署在移动端，最大的参数量也只有 14.7B。现已部署在 Google 的 25 个产品和功能中，包括 Bard 和 Google Worksapce 应用，针对不同的领域又可以变成专有模型，比如 Med-PaLM 2，是第一个在美国医疗执照考试类问题上表现出“专家”水平的大型语言模型。 PaLM 2 的几大突破： 最优的缩放比例（训练数据大小/模型参数量），通过 compute-optimal scaling 的研究，可以得知数据大小与模型大小同样重要。根据谷歌的研究，数据和模型大小大致按照 1：1 的比例缩放，可以达到最佳性能。（过去常认为，模型参数量的大小大致为数据集 3 倍更佳） 训练数据集非纯英文语料，混合了百种语言，包括了网络文档、书籍、代码、数学和对话数据，比用于训练PaLM的语料库大得多。并在研究中发现，越大的模型越是能处理更多的非英文数据集，而且包含更高比例的非英语数据，对多语言任务（如翻译和多语言问题回答）是有利的，因为模型会接触到更多的语言和文化。这使得该模型能够学习每种语言的细微差别。 以下窗口是 Google 基于 PaLM 2 开发的对话应用 Bard: 使用地址 1.4. 文心一言 文心一言是基于百度文心大模型的知识增强语言大模型 ，于 2023 年 3 月在国内率先开启邀测。文心一言的基础模型文心大模型于 2019 年发布 1.0 版，现已更新到 4.0 版本。更进一步划分，文心大模型包括 NLP 大模型、CV 大模型、跨模态大模型、生物计算大模型、行业大模型，其中 NLP 大模型主要为 ERNIE 系列模型，是打造文心一言的关键。文心大模型参数量非常大，达到了 2600 亿。 2023 年 8 月 31 日，文心一言率先向全社会全面开放，提供 APP、网页版、API 接口等多种形式的开放服务。文心一言一方面采用有监督精调、人类反馈的强化学习、提示等技术，还具备知识增强、检索增强和对话增强等关键技术。当前，以文心一言为代表的大模型已经逐步赶超国外最优水平。文心一言基于飞桨深度学习框架进行训练，算法与框架的协同优化后效果和效率都得到提升，模型训练速度达到优化前的 3 倍，推理速度达到优化前的 30 多倍。文心一言还建设了插件机制，通过外部工具、服务的调用，拓展大模型的能力的边界。 使用地址 1.5. 星火大模型 讯飞星火认知大模型 是科大讯飞于 2023 年 5 月 6 日发布的语言大模型，提供了基于自然语言处理的多元能力，支持多种自然语言处理任务，同时联合中科院人工智能产学研创新联盟和长三角人工智能产业链联盟在业内提出了覆盖 7 大类 481 项任务的《通用人工智能评测体系》；6 月 9 日星火大模型升级到 V1.5 版 ，实现了开放式知识问答、多轮对话、逻辑和数学能力的提升；8 月 15 日星火大模型升级到 V2.0 版，对于代码和多模态能力进行了提升。 讯飞星火 V2.0 升级发布的多模态能力，已实现 图像描述、图像理解、图像推理、识图创作、文图生成、虚拟人合成 。星火大模型包含超过 1700 亿个参数 ，来源于数十亿的语言数据集。尽管比 ChatGPT 3.5 模型 1.5 万亿个差着数量级，但 ChatGPT 覆盖了全球主要语言，汉语不到其中 10% 的数据量。所以在现有数据基础上，星火大模型比 ChatGPT 更懂中文。基于代码和多模态能力的发布， 智能编程助手iFlyCode1.0 和 讯飞智作2.0 两款应用产品也在发布会上发布，进一步解放编程和内容生产力。同时，讯飞和华为还联合重磅发布了国内首款支持大模型训练私有化的全国产化产品“ 星火一体机 ”，可支持企业快速实现讯飞星火大模型的私有化部署、场景赋能和专属大模型训练优化。 2023 年 10 月 24 日，2023 科大讯飞全球 1024 开发者上，现场发布了 讯飞星火认知大模型 V3.0 ，此次七大能力持续提升，并且全面对标ChatGPT。尤其像中文能力客观评测上超越ChatGPT，在医疗、法律、教育等专业表现也格外突出，还有在代码项目级理解能力、小样本学习、多模态指令跟随与细节表达等能力有所提升。 以下是讯飞星火的使用界面： 使用地址 2. 开源 LLM 2.1. LLaMA 系列 LLaMA 系列模型 是 Meta 开源的一组参数规模 从 7B 到 70B 的基础语言模型，它们都是在数万亿个字符上训练的，展示了如何 仅使用公开可用的数据集来训练最先进的模型 ，而不需要依赖专有或不可访问的数据集。这些数据集包括 Common Crawl、Wikipedia、OpenWebText2、RealNews、Books 等。LLaMA 模型使用了 大规模的数据过滤和清洗技术 ，以提高数据质量和多样性，减少噪声和偏见。LLaMA 模型还使用了高效的 数据并行 和 流水线并行 技术，以加速模型的训练和扩展。特别地，LLaMA 13B 在 CommonsenseQA 等 9 个基准测试中超过了 GPT-3 (175B)，而 LLaMA 65B 与最优秀的模型 Chinchilla-70B 和 PaLM-540B 相媲美 。LLaMA 通过使用更少的字符来达到最佳性能，从而在各种推理预算下具有优势。 与 GPT 系列相同，LLaMA 模型也采用了 decoder-only 架构，但同时结合了一些前人工作的改进，例如： Pre-normalization ，为了提高训练稳定性，LLaMA 对每个 Transformer子层的输入进行了 RMSNorm 归一化，这种归一化方法可以避免梯度爆炸和消失的问题，提高模型的收敛速度和性能； SwiGLU 激活函数 ，将 ReLU 非线性替换为 SwiGLU 激活函数，增加网络的表达能力和非线性，同时减少参数量和计算量； RoPE 位置编码 ，模型的输入不再使用位置编码，而是在网络的每一层添加了位置编码，RoPE 位置编码可以有效地捕捉输入序列中的相对位置信息，并且具有更好的泛化能力。 这些改进使得 LLaMA 模型在自然语言理解、生成、对话等任务上都取得了较好的结果。 LLaMA 开源地址 2.2 GLM 系列 GLM 系列模型是清华大学和智谱 AI 等合作研发的开源语言大模型 。 ChatGLM 是基于 GLM 结构开发的具有 62 亿参数量 的语言大模型， 支持 2048 的上下文长度 。其使用了包含 1 万亿字符的中英文语料进行训练，能够 支持中文和英文两种语言的任务 。通过监督微调、反馈自助、人类反馈强化学习等多种训练技术，ChatGLM 拥有强大的生成能力，能够生成更符合人类偏好的内容。与 GLM 相似，通过 INT4 量化 和 P-Tuning v2 等高效微调的算法， ChatGLM 能够在 7G 显存的条件下进行微调 。 在 ChatGLM 的基础上，2023 年 6 月发布的 ChatGLM 2 使用了包含 1.4 万亿字符的中英预料进行预训练，并使用人类偏好的数据对模型进行对齐训练 ，拥有比前一版本更加强大的能力，在多个任务上取得提升。 通过 FlashAttention 技术，ChatGLM 2 能够处理更长的长下文，支持的长下文长度达到了 3.2 万字符 。 通过 Multi-Query Attention 技术，ChatGLM 2 能够进一步地提升推理速度，减小对显卡的显存占用 。 在 2023 年 10 月 27 日的 2023 中国计算机大会（CNCC）上，智谱 AI 推出了 ChatGLM3 ， ChatGLM3-6B 是 ChatGLM3 系列中的开源模型，在保留了前两代模型对话流畅、部署门槛低等众多优秀特性的基础上，ChatGLM3-6B 引入了如下特性： 更强大的基础模型： ChatGLM3-6B 的基础模型 ChatGLM3-6B-Base 采用了更多样的训练数据、更充分的训练步数和更合理的训练策略。在语义、数学、推理、代码、知识等不同角度的数据集上测评显示，ChatGLM3-6B-Base 具有在 10B 以下的基础模型中最强的性能。 更完整的功能支持： ChatGLM3-6B 采用了全新设计的 Prompt 格式，除正常的多轮对话外。同时原生支持工具调用（Function Call）、代码执行（Code Interpreter）和 Agent 任务等复杂场景。 更全面的开源序列： 除了对话模型 ChatGLM3-6B 外，还开源了基础模型 ChatGLM3-6B-Base、 长文本对话模型 ChatGLM3-6B-32K 。以上所有权重对学术研究完全开放，在填写问卷进行登记后亦允许免费商业使用。 此外，还开源了 多模态 CogVLM-17B 、以及 智能体 AgentLM ，具体来说： 在对话模型上，对标 ChatGPT 的是 ChatGLM 在文生图方面，对标 DALL.E 的是 CogView 代码生成上，与 Codex 相对的是 CodeGeeX 搜索增强上，与 WebGPT 相对的是 WebGLM 在多模态、图文理解领域，与 GPT-4V 对标的有 ChatGLM3 以下是智谱清言的使用界面： ChatGLM开源地址 使用地址 2.3. 通义千问 通义千问由阿里巴巴基于“通义”大模型研发 ，于 2023 年 4 月正式发布。2023 年 8 月，阿里云开源了Qwen（通义千问）系列工作，当前 开源模型的参数规模为70亿（7B）和140亿（14B） 。本次开源包括基础模型Qwen，即 Qwen-7B 和 Qwen-14B ，以及对话模型 Qwen-Chat ，即 Qwen-7B-Chat 和 Qwen-14B-Chat。 它能够以自然语言方式响应人类的各种指令，拥有强大的能力，如回答问题、创作文字、编写代码、提供各类语言的翻译服务、文本润色、文本摘要以及角色扮演对话等。借助于阿里云丰富的算力资源和平台服务，通义千问能够实现快速迭代和创新功能。此外，阿里巴巴完善的产品体系以及广泛的应用场景使得通义千问更具可落地性和市场可接受程度。 通义千问开源地址 使用地址 2.4. Baichuan 系列 Baichuan 是由 百川智能 开发的 开源可商用 的语言大模型，在权威的中文和英文 benchmark 上均取得同尺寸最好的效果，其基于 Transformer 解码器架构 。 Baichuan-7B 是在大约 1.2 万亿字符上训练的 70 亿参数 模型，支持 中英双语，最大 4096 的上下文窗口长度 。 Baichuan-13B 在 Baichuan-7B 的基础上进一步扩大参数量到 130 亿 ，并且在高质量的语料上训练了 1.4 万亿 字符，超过 LLaMA-13B 40%，是当前开源 13B 尺寸下训练数据量最多的模型。其支持中英双语，使用 ALiBi 位置编码，最大 4096 的上下文窗口长度，使用 rotary-embedding，是现阶段被大多数模型采用的位置编码方案，具有很好的外推性。百川同时开源了 预训练 和 对齐 模型， 预训练模型是面向开发者的“基座”，而对齐模型则面向广大需要对话功能的普通用户 。除了原始权重，为实现更高效的推理，百川开源了 INT8 和 INT4 的量化版本，相对非量化版本在几乎没有效果损失的情况下大大降低了部署的机器资源需求。 Baichuan 2 是百川智能推出的新一代开源大语言模型， Baichuan2-7B 和 Baichuan2-13B ，均基于 2.6 万亿 Tokens 的高质量语料训练,在保留了上一代开源模型良好的生成与创作能力，流畅的多轮对话能力以及部署门槛较低等众多特性的基础上，两个模型在数学、代码、安全、逻辑推理、语义理解等能力有显著提升。Baichuan 2 在多个权威的中文、英文和多语言的通用、领域 benchmark 上取得同尺寸最佳的效果。本次发布包含有 7B、13B 的 Base 和 Chat 版本，并提供了 Chat 版本的 4bits 量化。 2023 年 10 月 30 日，百川智能发布 Baichuan2-192K 大模型，上下文窗口长度高达 192 K ，发布时是全球最长的上下文窗口（但不久后就被零一万物的首款开源大模型 —— Yi 打破纪录达到 200 K，可直接处理 40 万汉字超长文本输入）。Baichuan2-192K 能够一次处理约 35 万个汉字，是目前支持长上下文窗口最优秀大模型 Claude2（支持 100 K上下文窗口，实测约 8 万字）的 4.4 倍。 百川开源地址 四、什么是 LangChain 1. Langchain 简介 ChatGPT 的巨大成功激发了越来越多的开发者兴趣，他们希望利用 OpenAI 提供的 API 或者私有化模型，来开发基于大型语言模型的应用程序。尽管大型语言模型的调用相对简单，但要创建完整的应用程序，仍然需要大量的定制开发工作，包括API集成、互动逻辑、数据存储等等。 为了解决这个问题，从 2022 年开始，许多机构和个人相继推出了多个开源项目，旨在 帮助开发者们快速构建基于大型语言模型的端到端应用程序或工作流程 。其中一个备受关注的项目就是 LangChain 框架。 LangChain 框架是一个开源工具，充分利用了大型语言模型的强大能力，以便开发各种下游应用。它的目标是为各种大型语言模型应用提供通用接口，从而简化应用程序的开发流程 。具体来说，LangChain 框架可以实现数据感知和环境互动，也就是说，它能够让语言模型与其他数据来源连接，并且允许语言模型与其所处的环境进行互动。 2. Langchain 发展历史 LangChain 的作者是 Harrison Chase，该项目自从 2022 年 10 月在 github 上开源以后，迅速吸引了大量开发者的兴趣和投资者的青睐，也由此转变为一家初创公司。 前段时间，AI初创公司 LangChain 成功完成了 1000 万美元的种子轮融资，投资方为 Benchmark Capital。这笔融资进一步证实了 LangChain 作为 AI 初创企业的领军地位。并且，在本次种子轮融资后，LangChain 不久后再次获得了红杉领投的 2000 万至 2500 万美元融资，估值达到 2 亿美元。 3. Langchain 核心组件 LangChain 作为一个大语言模型开发框架，可以将 LLM 模型（对话模型、embedding模型等）、向量数据库、交互层 Prompt、外部知识、外部代理工具整合到一起，进而可以自由构建 LLM 应用。 LangChain 主要由以下 6 个核心模块组成: 模型输入/输出（Model I/O） ：与语言模型交互的接口。 数据连接（Data connection） ：与特定应用程序的数据进行交互的接口。 链（Chains） ：将组件组合实现端到端应用。 记忆（Memory） ：用于链的多次运行之间持久化应用程序状态。 代理（Agents） ：扩展模型的推理能力，用于复杂的应用的调用序列。 回调（Callbacks） ：扩展模型的推理能力，用于复杂的应用的调用序列。 【 参考内容 】： [1] https://arxiv.org/abs/2303.18223 [2] https://xueqiu.com/1389978604/248392718 [3] arXiv:2303.18223 [cs.CL] https://doi.org/10.48550/arXiv.2303.18223 [4] 张奇、桂韬、郑锐、黄萱菁，大语言模型理论与实践， https://intro-llm.github.io/ , 2023. [5] https://mp.weixin.qq.com/s/J6IHMf7THKJ9QTxsjG87lg <strong>EOF</strong> 本文作者： 久曲健的测试窝 本文链接： https://www.cnblogs.com/longronglang/p/18695418 关于博主： 评论和私信会在第一时间回复。或者直接私信我。 版权声明： 本博客所有文章除特别声明外，均采用 BY-NC-SA 许可协议。转载请注明出处！ 声援博主： 如果您觉得文章对您有帮助，可以点击文章右下角 【 推荐 】 一下。 2025-02-08 15:31:51【出处】： https://www.cnblogs.com/longronglang/p/18695418 ======================================================================================= 您的资助是我最大的动力！ 金额随意,欢迎来赏！ 付 款后有任何问题请给我留言。 如果，您认为阅读这篇博客让您有些收获，不妨点击一下右下角的 【 推荐 】 按钮。 如果，您希望更容易地发现我的新博客，不妨点击一下绿色通道的【 关注我 】。(●'◡'●) 因为，我的写作热情也离不开您的肯定与支持，感谢您的阅读，我是【 Jack_孟 】！ 如果对你有所帮助，赞助一杯咖啡！打 付款后有任何问题请给我留言!!! 本文来自博客园，作者： jack_Meng ，转载请注明原文链接： https://www.cnblogs.com/mq0036/p/18704400 【免责声明】本文来自源于网络，如涉及版权或侵权问题，请及时联系我们，我们将第一时间删除或更改！ posted on 2025-02-08 15:32 jack_Meng 阅读( 4347 ) 评论( 0 ) 收藏 举报 刷新页面 返回顶部 导航 博客园 首页 新随笔 联系 订阅 管理 公告 博客园 © 2004-2026 浙公网安备 33010602011771号 浙ICP备2021040463号-3</p>
</div></details><h2 id="toc-224">113. ITRANSFORMER: INVERTED TRANSFORMERS ARE EFFECTIVE …</h2>
<ul>
<li>链接：https://openreview.net/pdf?id=JePfAI8fah</li>
<li>来源：bing</li>
<li>摘要：The iTransformer model achieves state-of-the-art on challenging real-world datasets, which further empowers the Transformer family with promoted performance, generalization ability across differ-ent …</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-225">正文（抓取，非 AI）</h3>
<p>%PDF-1.5 %���� 857 0 obj &lt;&lt; /Linearized 1 /L 6180839 /H [ 3693 608 ] /O 861 /E 641968 /N 25 /T 6175425 &gt;&gt; endobj 858 0 obj &lt;&lt; /Type /XRef /Length 176 /Filter /FlateDecode /DecodeParms &lt;&lt; /Columns 5 /Predictor 12 &gt;&gt; /W [ 1 3 1 ] /Index [ 857 431 ] /Info 373 0 R /Root 859 0 R /Size 1288 /Prev 6175426 /ID [&lt;502b702c9d8e436f3bfaec62934eb6be&gt;&lt;9e0b606ce671310a498649f07e25f851&gt;] &gt;&gt; stream x�cbd<code>�g</code>b<code>8 "y@$c+�dJ �� �] D��E&gt;�H�Y �SD�����`���?0�Hr JD��L���{ R� ����r,��E)�"��A�,'�4P��NoAl?�{��z!$3+�"_101~w��/�(9J�O2.x7��C�� ME�$�H���Qr�� � F endstream endobj 859 0 obj &lt;&lt; /Names 1188 0 R /OpenAction 1252 0 R /Outlines 1158 0 R /PageMode /UseOutlines /Pages 1157 0 R /Type /Catalog &gt;&gt; endobj 860 0 obj &lt;&lt; /Filter /FlateDecode /S 427 /O 600 /Length 520 &gt;&gt; stream x�c```b`��e`c</code>�ce�0�$���8���� ��=�����A�w� 9+�3B�s.�/g��õwA�]30�8�D �-a�7��v j� gdO 0p��C�Y�㸂�5&amp; ��k�Z����9�L�V/���Yy !a��l{�!Z,<em><em>��v]�� �A���N�u�nL]��r�����7I�.[Uv��Lej��n��|ߝu �a7ff�?��\΋o MJ]�<code>&amp;���0��M�S;�=hK�\�[w����6�!� ̌z��ǆȲٗs&lt;=�Ж��Mm�-Ϋ�Ԣn�P[zt͗�?_q^R%ڹB#L-��K�s�����}jQ�y�M����f?���cf�&gt; 7)���c``�c���*Vbkp&lt; dfh� a�</code>� ��I�^G��/ �5@�U _ ��d�S� 0�5�)� <code>Itx�Aƃ��A� j'��(�I1 vR</code>ra�h��i��A$���!�AȀ��a�~ ���+��OPPe�y�p�ѼA����a�pS�� �T<code>�g�\g� N_�R T�� endstream endobj 861 0 obj &lt;&lt; /Annots [ 1253 0 R 1254 0 R 1255 0 R 1256 0 R 1257 0 R 1258 0 R 1259 0 R 1260 0 R 1261 0 R 1262 0 R 1263 0 R 1264 0 R 1265 0 R 1266 0 R 1267 0 R 1276 0 R 1268 0 R 1269 0 R 1277 0 R 1270 0 R 1271 0 R 1272 0 R ] /Contents 863 0 R /MediaBox [ 0 0 612 792 ] /Parent 1130 0 R /Resources 904 0 R /Type /Page &gt;&gt; endobj 862 0 obj &lt;&lt; /BBox [ 181.2772 91.58253 536.0453 449.0422 ] /Filter /FlateDecode /FormType 1 /PTEX.FileName (./pic/radar.pdf) /PTEX.InfoDict 1278 0 R /PTEX.PageNumber 1 /Resources &lt;&lt; /ColorSpace &lt;&lt; /Cs1 1279 0 R &gt;&gt; /Font &lt;&lt; /TT1 1280 0 R /TT2 1281 0 R /TT3 1282 0 R &gt;&gt; /ProcSet [ /PDF /Text ] /XObject &lt;&lt; /Fm1 864 0 R &gt;&gt; &gt;&gt; /Subtype /Form /Type /XObject /Length 1420 &gt;&gt; stream x�XMOI�ϯ��9����.!�U�Z�� � ��J@D�������� ������q{\���U�+?�=�%k,^,6�⩄bR��-}����@�Ξ�6O���i����;�w�����3�wtK7��f��  6q7�63���V�Av��P2��Lb�</code>��d ^��B��2��I%e�9�&amp;%�(Fî����|7:t���mF��F�� oT��S� fRf\��5��f�]�S�H�&gt;��o��݇{��ߐ���qʵ&gt;b �A�����HF � ��A@a⏵�-�&gt;��F$�a ��Z鳾�/�:</em>�G+\���O:_7</em>���ڈ�)Z�����&gt;F�M�ޓ8JHexy�lg'$����l�/�u ]����v��� 7S0�V��7����F����� hH6���ߥd$��g�V�1n&gt;����  ��=j&gt;����� �dAB�EŴ��[MH��$6!dV�r�r��.�����Ѹ� s��/�o�e����;��c0C$�H ��R�����7�.k��=qQ� �s� biu9h��s^�����F1��l�+�� ��^v eb�5 J�ry�+c��6.A;F5� �&amp;�x�rHr6�b�G ��|[���=<em>&lt;ګ2Ό���C����"��5�ćCg=���+�E��@rH:�X� ���ENqr@�?u D�o����� �� .L���U��i�� �����q2�7�c"G��Bhp�t����<code>�*��p�y&amp;�8.�_.�J)� ����1���@�*�����Um</code>�����p��{r�{���T (Z��=��{� �w�E���-e;o{��]�z�����������5)xe Ί._��Zwg" C�d���̀�9$ ?�ӧ�N�.��˳�NuxI9y�M}V߶K�b"GWEw�U</em>ZH �K�{��i� .L���� �&gt; f���qI| ��9 T1��#�� ]��}�+w�M�+�͔�fC��%���l-����YTP��U˱/�сpW�di,�dD � ��=I�:3\T�gcC� ��t� �V���h  0 �MC1���i: ���(Y�Oz�6�Z<em>냪��~�c������/�� �Ӣ�n���PS0[�ܨ�0�� �8��m/�վ�(��a�;ߛ��FQ� ����<code>���</code>t\V_N �����Vi���(.�3I�SJ���F~F �r&lt;���KM+F&amp;(�v$�U�q���8�Fv��<em>RpV�ؚ^�,r��Uu��[�6�z{\�Jئ��/���?v V7S�I��O��</em>���HϘW^�X46�A��H�<em>������i�%~]���ݖ�C�51e���[7v�e��(�"!<code>"G*cI**j*�zY�/D諃��4�2q��&gt;�O �[U/W�]�^�,��� endstream endobj 863 0 obj &lt;&lt; /Filter /FlateDecode /Length 3846 &gt;&gt; stream x��ZYs�6~����Ueq���%s8����1N�6�&gt;�lq�"5&lt;����� &lt;$:�$�* �F���� ��U������ų/�V���:^�ܭT���(�c�V7��/�w�mY�{�[otxy+O~l���6��Z� �mdF���W���- �p����/�n.&gt;\(</code>!X)YQ�i����</em>��v���U��,]=д��d1&lt;�ջ��/�?8݇2~�h#�C��3x���W����U��&amp; �q3�1��|��)��X{/���R� ���F��6�yC��h��ťU��4֎�������� � %���+�D��ܚ� ^/.�?N� �7���y��U����ċe&amp;��Ox�Z��1a��t�Q��:���� �� "�W���w=Ȗ6c��d��G��</em>���<code>�gB["j�����e��qs�¿ �㩃c? F� ������2�n_&lt;�q�-/��}4[����!�RK�ST]^s�M�,� �}�7EuqgP���# &amp;�$���*/G-���? �I��FC1����K�_߉���a� ~5V8y��������׾���cU�u�}\�ȳM[t�(r��-�[8�Ui(o^�jIfp��E\��������ս�$�~���qЎ���}ݟ/��U �b��B�&lt;�$J�ɲ</code>��Dp��|ʢ����r����6�F��O:XXD�<em>�&amp;r_�?EpD� �l�ND��]�o�%�U��i�g���H���eIz� �n�s��&amp;���y���1����t|14�t�L0��Y��� <em>�<code>�f5�%����,�f~� 'B� B��!�+r����t=F�m��� &lt;��v�.4~%�� l|us���'�z��Z��л�AR�b�@�*����p����yۉ)�ޡ�ٲ������n�V ����y�� � 9��)�i �" ��1�7�}��m�79��$ƅ ��z��%�����7k˫8 ց���[�0�����"KQ��o'/ o�S3�T��:a�J;q;9E�0C� �</code>�z]�OOd1��}Y���G[� �V����,"Z�3;�M�</em>)=g���#��E��=:W[�K�1%�i'q6'X�=yvb�ʺ�7讵 l�<code>l�����ͷ��� � o���G~ ��+�����ϛ"�HR�D����ym�8@^�.? }Ǻ:�=�%AƼU#�8�p�}PL ��#�1Vqc���������܆</code>���m ��  ��r8�����|��8��S����O�<code>�jǼ�1jo�[n��rCd;�tfص���&amp;ߑE�</code>^��.��w�7���X�h�hE</em>� �;&gt;a\ �/�f�BN��d�y��[�ۉ�0��!z-V]R[�M���- �̀�L3�.s� ��sZ�� ����;� � X� �cD���C� �Mz�@��Z 3,C|�+�з ���u�=������}c��<em>�8��/P&amp;4;�� �B���ᢥ͛�<code>�v��g���)�W��G�o����ASl� ��l,jAo[�9�I��3� W.m�fw$�ڽϏ�vc�Z��}�z}��G3%Q��</code>ݼG3-�bl \�(Z�c�V�a� 0��k;&gt;<code>��  ν�10�x2h��+&gt;J�����}s�[��5��N � d&gt;�@׶�</code>\U�,MÜ|���;�@6�ȝy��1�m ��Ạs��VP�A I�V�,T�n@�b�����f���Iʕ̿-n։�4���]�7SB�����9!g��k�ñ|&lt;�i�W�cY8w @�02Ω��v��� C]e;�fO�nC�+��-�A !T.^t��at��jɆN��$�� �V����� ����%�B����ڤ����;H�b�c���W»!�@�4�a���L�A"�ݲ����%�1��)?�̜�Gq+l�� 8&amp;� �@���I��CB��z<code>�e�c��h����a�</code>!&amp;WE{p��m~$�[ �$�23��b�ɣ?p�,���By�� �'�KN^'�xI�BQ �k/�G \�4��C�^�'0 y�  </em>ih�s��M~ j� �|��Z:r�w�� �p&amp;�0�u<em>At :S�t9�� lG��, =(g�0��-�&amp;o ����� ��� � �^9w�bk�L���|�].X_U�M�=kJ��:d�P���.G8�K����B� m� ��02�l���?�u�w���Q�,ĢdំQ�U�X"X~[J��]��X@�n �����9�oE$e�}� ��Ϟ݃V��X�x�C�����₏��X����\���ʖ���� ���Fd�ah�6�PL;)=�U��R�k��"��R�^�n\� wT�ٌ�V�@.�ExO��d�RU+XM���}�Q)�m��6 �(�����A�^����0�-����t;7z��%C"F�EqO(rqO= e6)h�B��idf��&amp;hH���P�؜�с�3�&lt;]�"(r��:ѳzؖ ��;��A�yw�� 8,</em>��b�Ƃy@W��d v�c�֊ �b�� ��? �� �Aǰ�4 pD(z|x ?A4'�r�� ~c�ؕ��%��@%�W��sv9h9/#�mW�-Oo���9�Q�#T&amp;.�� ��p�C�ϥ���e^��|Ѓ��Id�ǭ�ljL� * �=�����@���W�n�4|!O&amp;��(�����B�r���^�-W���}<em>&lt;� �  �E�2.n䁧���� �\[��!��A�o�"(�U{,+���1퀣&gt; �� +;��Tr��a^0�f z�m�s��<code>��Հ�H��k�Ltf\��I @��/�B���'$�I��;{,��.�zǼh � ���$�A�NQh7�a *F�&amp;��7太V��ZC�H� �</code>?���7,uQ���:�P옲�ݸ�X!w��<code>���f�m����sWNS�7�3n�-���2s}�2'~h&amp;�Ƙ��c���� ��X �B#�)� ER�'� �[��}_q��0��k�*�M�|�,v�3�8�b?t/�K���i�= ���i�h����{&lt;��R��� yg��I��*�#}��P�I2��e�А p�� 0��D-.���E���P�QUx��D�MebB@@� ��fk� f���s�ۆ'�i� A5�)�EN���T�ɜz ���lrBPR?������MF�㊟�S</code>r�U�F�}��W��$\�$4��B�-�H��@8<code>���V�Ôz1T(�����t��� ��%�O�����-�&lt;��\�L]��ՙ,�%&gt;+�!+��cwU��</code>Vv8p��$<code>9�['d��S�I�;�}Q ��R� �������X41�Bi������V���������n螸��Ŀ���@ �3��� &lt;�t��ඦ��t�0�� � ˡ�G��U9#�"?[��Ӌ���I�G���:����~ƃ�s�ky;���fV�1hU�(r�'���^���J� ����C��l</code>� ��x��@R�;?��,3�߬��(MI�<em>�$�,Ee~��</em>�!w =�iK0��6 7��������D��B��b�����K��g@�;'�'�� )Xx� endstream endobj 864 0 obj &lt;&lt; /BBox [ 0 0 781 718 ] /Filter /FlateDecode /FormType 1 /Group &lt;&lt; /CS 1287 0 R /I true /K false /S /Transparency &gt;&gt; /Resources 1286 0 R /Subtype /Form /Type /XObject /Length 278 &gt;&gt; stream x���N�@ E�|� ^?�㞆����j%�6�F��8�@�Eӌ ��=׹��06�h<code>��Y&amp;� '��@�T�@tBWSິ�V3��$����0,���� �@u�s:��utx:ϧ������zZyy:�#1��� ����!��6rm�~k��i������ ��KĂ �+uc��4H� ;��U��gA�4� i�1X�,�t� �k�a_1�6�T/�"��ѩN*�è��l�ޒ�\�Q���[&lt; ��t�D � B��o%�� {��5F֊c��In��z��� endstream endobj 865 0 obj &lt;&lt; /Alternate /DeviceRGB /Filter /FlateDecode /N 3 /Length 2612 &gt;&gt; stream x��wTS��Ͻ7��" %�z �;HQ�I�P��&amp;vDF)VdT�G�"cE ��b� �P��QDE�݌k �5�ޚ��Y�����g�}׺ P���tX�4�X���\���X��ffG�D���=���HƳ��.�d��,�P&amp;s���"7C$  E�6&lt;~&amp;��S��2����)2�12� ��"�įl���+�ɘ�&amp;�Y��4���Pޚ%ᣌ�\�%�g�|e�TI� ��(����L 0�_��&amp;�l�2E�� ��9�r��9h� x�g��Ib�טi���f��S�b1+��M�xL��� �0��o�E%Ym�h��� ��Y��h���� ~S�=�z�U�&amp;�ϞA��Y�l�/� �$Z� ���U �m@��O�  � �ޜ� �l^��� ' ���ls�k.+�7���oʿ�9�����V;�?�#I3eE妧�KD�� ��d�����9i���,�����UQ� ��h��&lt;�X�.d ���6'~�khu_ }�9P�I�o= C#$n?z}�[1 Ⱦ�h���s�2z��� \�n�LA"S�� �dr%�,�߄l��t� 4�.0,</code> �3p�  ��H�.Hi@�A&gt;�  A1�v�jp ԁz�N�6p\W� p �G@ ��K0ށi���A����B�ZyCAP8�C���@��&amp;�<em>���CP=�#t�]���� 4�}���a � ��ٰ; G���Dx����J�&gt;���� ,�_@��FX�DB�X$!k�"��E�����H�q���a���Y��bVa�bJ0՘c�VL�6f3����bձ�X'�?v 6��-�V<code>�</code>[����a�; ��� p~�\2n5��׌���� �&amp;�x�</em>���s�b|!�   ߏ ƿ'� Zk�!� $l$T����4Q��Ot"�y�\b)���A�I &amp;N�I�$R$)���TIj"]&amp;=&amp;�!��:dGrY@^O�$� </em>%�?P�(&amp;OJ EB�N9J�@y@yC�R �n�X����ZO�D}J}/G�3���ɭ���k��{%O�חw�<em>.�'</em>!J����Q�@�S���V�F��=�IE���b�b�b�b��5�Q%�����O�@��%�!BӥyҸ�M�:�e�0 G7��ӓ��� �� e%e[�(� ���R�0<code>�3R��������4�����6�i^��)��*n*|�"�f����LUo�՝�m�O�0j&amp;jaj�j��.��ϧ�w�ϝ_4����갺�z��j���=���U�4�5�n�ɚ��4ǴhZ �Z�Z�^0����Tf%��9�����-�&gt;�ݫ=�c��Xg�N��]�.[7A�\�SwBOK/X/_�Q�&gt;Q�����G�[��� �</code>�A�������a�a��c#����<em>�Z�;�8c�q��&gt;�[&amp;���I�I��MS���T<code>�ϴ� k�h&amp;4�5�Ǣ��YY�F֠9�&lt;�|�y��+ =�X���_,�,S-�, Y)YXm�����Ěk]c}ǆj�c�Φ�浭�-�v��};�]���N����"�&amp;�1=�x����tv(��}�������'{'��I�ߝY�)� Σ ��-r�q� r�.d.�_xp��Uە�Z���M׍�v�m���=����+K�G�ǔ���� ^���W�W����b�j�&gt;:&gt;�&gt;�&gt;�v��}/�a��v���������O8� � �FV&gt; 2 u����� /�_$\�B�Cv�&lt; 5 ]�s.,4�&amp;�y�Ux~xw-bEDCĻH����G��KwF�G�E�GME{E�EK�X,Y��F�Z� �= {$vr����K���� ��.3\����r���Ϯ�_�Yq*  ���©�L��_�w�ד������+��]�e�������D��]�cI�II�OA��u�_�䩔���)3�ѩ�i�����B%a��+]3='�/�4�0C��i��U�@ёL(sYf����L�H�$�%�Y �j��gGe��Q�����n� ����~5f5wug�v����5�k��֮\۹Nw]������m mH���Fˍe�n���Q�Q��</code>h����B�BQ�-�[l�ll��f��jۗ"^��b���O%ܒ��Y}W�����������w�vw����X�bY^�Ю�]�����W�Va[q<code>i�d��2���J�jGէ������{�����׿�m���&gt;  ���Pk�Am�a�����꺿g_D�H��G�G��u�;��7�7�6�Ʊ�q�o���C{��P3���8!9���� � &lt;�y�}��'�����Z�Z���։��6i{L{��ӝ � -?��|������gKϑ���9�w~�Bƅ��:Wt&gt;���ҝ����ˁ��^�r�۽��U��g�9];}�}����� ���_�~i��m��p���㭎�}��]�/���}������.�{�^�=�}����^?�z8�h�c��' O*��?�����f�����</code>ϳ�g���C/����O�ϩ�+F�F�G�Gό���z����ˌ��ㅿ)����ѫ�~w��gb���k��?Jި�9���m�d���wi獵�ޫ�?�����c�Ǒ��O�O���?w| ��x&amp;mf������ endstream endobj 866 0 obj &lt;&lt; /Filter /FlateDecode /Length1 3264 /Length 2251 &gt;&gt; stream x�VYl��c���&amp;wIJ�H.�K��R�KS"iQWt��O�.�H�;�l�� E�6I��)� ��@[@/-z&lt;�Z���&gt;�%̓[�F�a�A � ��,��Jӧvf��=�o�����K�� �DyO,�/#��U�n&lt;�x�� �L"�N��?ٱѿ�7N�@��e�ӧ�.&gt;�n�A�c�����$vvi�����?��87�t z(�]h���W.�&amp;�C&gt;�ς�� �A����6uy�{��{� b;}�^���q����}Ծ�~L �7�=���o���(B �o�}�����1</em>�� �I���<em>�I�y��&gt;߄<code>&lt; 55��� �+XDu��p��/��N(x݃�,� أ�d�tf���Bg���Z���mf������D�.�6 {��k�&gt;h�杈���x���y���F�^�m�7.���np��-�M</code>� Q�'}ɀ/��o�o⛶�2�W� ��#y�r����e��j�S���D/ŉ��唛��qj��D.�2{��W��ZK��&gt;����=Fvv�77z .�置�|�w��N�h���;"z��7=��g��x���ـ�ɵ6�-�1��@ʊ�zɨZBAQҨ�ơ&amp;�B �"aX�� ��s�bm���q����J�#$6�</em>;9�:S��� �aZ�0H�t� o(��d�8|g߭�(�F�~'�8R]����;l&gt;�=1�h�-V����[0��}��<code>4 x��ʓx�8��gc;�q�l$���[�?�JI |ԞU��-�Q� ���B��y[ �CV�J@����p�y�a��2�,Ъ��^�v ��(���=���_1"~_�!��=� �B�$��9���pgk�j��2!?���P�o\�\Y��*� |��%B�yt���+����+�#�bJׇt���� �j{����S�@�:�ˬ}���3T P��9��@� � �JY�: @4��o&amp;k���]ceb�n  n&gt;����x��7�k�/Ӝ-�1���!�A#�)*�&gt;U+��|�</code>JT�zN��ȥ�Zy�� ��|��$���,3s��@<code>�� ���y��~O_�,������bqt[��Y��@��M�G�� �P�� *�; �/T&gt;�w�i�1�8ࢿ\�</code>��Uar���� "ӛtC{�i��<code>Y��o���#�V��Jݮ�˙��)QN�Aa���tQ��:]a\�K&amp;��$Xȥ��S���;�&amp;f �E�ś���/~��� l��_�����o�zEf�8}���\r���_�Ä/}���B�}�S�a���� �]�!˕���BF̴ �&amp;ɍI��!7,Eem' V����U�*�HO4��07ߟ �V��e��&amp;H M1doH5R���U/���.���|�'x���QK��J��f���rsaAH�J0S '�S�����]i�� 8Y ��O�9 ���;��L�� �bI�V��� S=�n����Qm�d#�_��v�� � ե���1�� �����Q�zlL����QX�I��;�׍2U�l_��z� gU��gޤ�712Q�U��҉3�%�TŸߎ f'�X��- u5���� N\|6d���#��P �� "�ж�y=�H6�D�b��  �Gn�nl""љ��Y�_:�\^-�0lG�� ���v��.��vh^ߥ&amp;fdBʯ �&gt;�k&lt;�lo�M�-S���6�G��PD�lt@�\7vM��&amp;���RגI�U��nE*v��.U�7,���gZ���Y҃q��m,�*+-6�����{�Gt+ [\4���7[k��VGc� P \� v���,�\�|�O�*)��t���c���n�[G�¡�&lt;���/��&amp;+�GO�&amp;_|��0� �&gt; a �E�WT �f�t�85t��O��؈���D�u��Q�펈�P�� ���'�[ؓ�6��f�؆��Е5����r�(y ����+*�5��dT��¶�JG�� v ^ IS:G(Ƽ �~{� {�M�&lt;�bI��{ 0����</code>y8%�|��}'6�s&amp;�Qv���{|Vo�8 ���S��]v&amp;���1eQ���@U8c�� 8�O�i}ڍ����+<em>+vm�2�߽|���|a����� 1��' endstream endobj 867 0 obj &lt;&lt; /Filter /FlateDecode /Length1 2196 /Length 1590 &gt;&gt; stream x�UIl���YDs�l���3҈ R�!ǔHJ��P�Wʭ��M$��F� �i I�Kz�!h�&amp;��EQ��H� z(zi��PE��{�!N� �7$�io}�����������n�x�� DQ��A�~ �ɽ���N�q�kW�όu�/��k�0�w@� nݞ�@:�ן&gt;���Ġ{b ����������}��7��pk�W@6ǿ��o w��4�摧=N� ����~{)�������nǋ��O�߇�����&gt;D�U �C�&gt;Dq�<code>x���p닶��!�� Ge�#ET�,��p xx �^Y [ �A� ��l{ ��E b�&lt;����#ȏ�mɨ���7Ї��_� �:�F~GS� ��.S��p6���(����35���9� ��Ũw��K��G�u�U[�DN�齜��&lt;.�D:��|��c��{H@B��&amp;UE$1D��E\1z�� �)��'�uw !Ѹ��d/�_��_̇%�n���&gt;������������� O{/�����o� ���n���UB8�EY�s���6�E_� ��9&lt;P�a)I�^qU�s�����[��_��Q�c$�͟C ���ƇA�!�W���I��v��� � &amp;���̅ec�������Z�K��&amp;�ֿR�?W_��T��R�JE(��V�";���� �92K�pH$$������R�Ć���d\�������?R���Ս6^�)�^΋�� �?���3�k�;)ը�ꢝ- ��p�3 �9�����%S� N� �a 1��&gt;!��撟W�3��t���o̩ G��b�:u4%� a�HL Ɗ��\C�0����a��z B�$�jV]m���^u�l+ol]�F�S��O^�[˺�b��� �3e�����t4����� {{)�\ ��~s�Ƙd&lt;|� �A�S�p� e��9��� #���%H|U�����hy�rO-��.Ø��9���Tr����#d�{�^�Ȕ����G�QG� 8|9���)��� �/�ة�5�zP�����[�"� A��i[���M&amp;��@@�K$gg� ?-qfam��ZJÍڥ��:g�{��O �R&lt;^j&amp;TK�$�R�eO���n]UyF�*�L���gk�ʹi� F۲�6� ���w��� Ml/�0{F-�S0�</code>�j!Ȳ�(o�ꆅ ��f���ih��0��d'�</em> Q�4w����ε �'��~������V�Y �$�������]�N7��5H�7��&gt;Ɵ�'<code>� |���P�(ԣ�D$� ��\��+6����4�G �c �?s�ֳ�~�we����M�oWJP� endstream endobj 868 0 obj &lt;&lt; /Filter /FlateDecode /Length1 3632 /Length 2286 &gt;&gt; stream x�W[l���.�Zr)r���|-ɥ(&gt;$&gt;%��EIT$ˑ%K�帰L�NeG�e[FmA� � 4 ���p��'@�"�G��G��)P�h��h݇�q�"���%#ˏ��g��s��s��ݺp�4��k�"�ɍ�&amp;�� W��~�Y�&amp;K���N7O6� ��,6.�]�غܶ�a\Z?w�}�$�Nm4/�� ����͍������湋[��� �������^4��� ���� ����|u�Y�7��;솷�K����?f&gt;i]oݣw�p���?�o蛭{���i]��6������{(Gmh��v_ {��t;#��} z�AnX+b Uh �� z��*��4�$��ɁLh �I4+���8���M��Yv.=0��m�nG��.$��;� ��܍����*��Ǩ���K�}"�U�.�GSt���~ƭqoq����ۀ'P�Sd�YX w�a�Ӻ��YZ��?-t�O��[��p�"Be�G�p&gt;W*2D��� �B�B\�� �z�W��\�Żؙ �9\Y*D�2~���DO������W^xj��֓9�eЌ�7o�V1J̃�J�o-^��������J����E8-F��-�+�TA���w��� U��2|�W=�,�Jm;�� 3�*���rz�EG�Ҩ� ����� �1 �0J�� hT㱴l�8��� %.��m' G��e�E��"����P�b2��.�ۄ��/3����&gt;���a|"D�J��' �/^������ ����"</code>]����};���B(%u8��Y�! 0� � {�=�|� �r��4fT�q5b�k:� ��0kt��}�ط��&amp;Ɣ�)o��+���&amp;;|vY&amp;&amp;3���8 ��+�r�9�2?j��c_���˄|��2ت,���XVo|��AC��D��!�lO�M�F����9I�������L<em>�+�&lt;�a�SL��~����F���C�x��p�� v�Cwㅠ���2e �� Y{=������o�3 i�b��� �wtT� �7��cJ]�Ԉ&gt;�X�O�f</em> ͌�'�&amp;��f�tH� |7'E �FR�s�Xm!]ZP�]6'� .Ecٸ���-� ʢ:���XI�cjnf�0_����@�x�'x��g��k� ��N<code>��S�}U��8S��Ĵ)�Ld��� �&lt;���� ]�2h�P�v� LX�lX�Lx���iGǟ2����?�B^ C(V( ����'��u$� 晰�H�0��5 �Uvk؛�z��p��ds011�K͂�Tr,㐫�xIuy�R$9�3��b4�k6�&gt;� N9�i9\�D�Z-5�Q�� ���FBqO,� Fd���twE�VW���� </code>�<code>���RKY���|��s�~ג�T*�g����l㥉�̩�?�X�� �:�� J�7�ǚ��&amp;����&gt;�) ���^Ϩ 8 �~ �~}(����+˻w��7k ��#W8 i� R�������N�����&amp;�V|71�W���n���2^?1�:����3�� ���r�| Ua#k���&amp;�@9pd�;$�e\a�|������)J�l��}%��b���S�������a6u����Bn� Ď.��V���?����&gt;yZ%��x&gt;! ��ݹ�+3�H�B�i&amp; �a��v�� "������D37����E2~}����ҭU�� i��ЬJH�Ʊ�{� �����Qס.T�n��� P�A݅� �کj]zb�&lt;�T2�p�u典���)�4Y� %7�k�֫��JN�窉�:��z��/�ݢN���M���Dd�Ɛ�����֌�_6��7;�����] %㾠۴�0�L'w,ʻU�[�p��df(�z�1IM��;���q</code>�c%�R2��W��p.V_��k �� ~l����| �c �G�Ɂ@�Ξ��r1#o9=C�R�L�&amp;��N���?y4{�Z��L)}dj ��������/&amp;NM�<em>&gt;</em>]mD|#<em>�� ����o(�9s^� &lt;�Q���:���</em>�7s�b��Ǖ.�PQ�C��F ;#r�b�Z,&amp;˞�C�<em>ezR ���ٓ�#�&lt;���<code>�r�a����j&lt;���#���</code>� �:h |</em>�_-� j�Ԉ����sť�B�+�ɾ�p<em>Y� �=ɡ�ŐߧH�:�J��w���A�V&amp;��X�=�i�.��W</em>륀�����@�PQe���˄b�;$��m�T�!�Y"D��� ��2��dC�Jm�S�8x��<em>���&lt;=2�P����5� �����m�d�܏h^X�P��,d�&lt;</em>�2�5L�i4�f� k -���:�����a��f���69p<code>s��F3=~n��� } endstream endobj 869 0 obj &lt;&lt; /BitsPerComponent 8 /ColorSpace 1279 0 R /Filter /FlateDecode /Height 1 /Interpolate true /SMask 875 0 R /Subtype /Image /Type /XObject /Width 1 /Length 11 &gt;&gt; stream x���? �� endstream endobj 870 0 obj &lt;&lt; /BitsPerComponent 8 /ColorSpace 1279 0 R /Filter /FlateDecode /Height 2400 /Interpolate true /SMask 876 0 R /Subtype /Image /Type /XObject /Width 1 /Length 54 &gt;&gt; stream x�Ё    � � �Pa�� 0</code>�� 0<code>�� 0</code>���� ��� endstream endobj 871 0 obj &lt;&lt; /BitsPerComponent 8 /ColorSpace 1279 0 R /Filter /FlateDecode /Height 1 /Interpolate true /Subtype /Image /Type /XObject /Width 2000 /Length 50 &gt;&gt; str</p>
</div></details><h2 id="toc-226">114. Second-order cone programming - pku.edu.cn</h2>
<ul>
<li>链接：http://faculty.bicmr.pku.edu.cn/~wenzw/courses/Alizadeh-Goldfarb-socp.pdf</li>
<li>来源：bing</li>
<li>摘要：2022年11月3日 · 1. Introduction Second-order cone programming (SOCP) problems are convex optimization problems in which a linear function is minimized over the intersection of an affine …</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-227">正文（抓取，非 AI）</h3>
<p>%PDF-1.3 %���� 311 0 obj &lt;&lt; /Linearized 1 /O 314 /H [ 1219 2066 ] /L 344114 /E 43750 /N 51 /T 337775 &gt;&gt; endobj xref 311 34 0000000016 00000 n 0000001031 00000 n 0000001154 00000 n 0000003285 00000 n 0000003480 00000 n 0000003648 00000 n 0000004146 00000 n 0000004731 00000 n 0000005071 00000 n 0000006174 00000 n 0000006262 00000 n 0000009288 00000 n 0000017583 00000 n 0000017976 00000 n 0000018461 00000 n 0000018932 00000 n 0000019416 00000 n 0000020151 00000 n 0000020391 00000 n 0000020988 00000 n 0000021416 00000 n 0000022278 00000 n 0000022997 00000 n 0000023314 00000 n 0000023519 00000 n 0000023735 00000 n 0000026478 00000 n 0000026815 00000 n 0000027602 00000 n 0000027681 00000 n 0000033030 00000 n 0000040758 00000 n 0000001219 00000 n 0000003262 00000 n trailer &lt;&lt; /Size 345 /Info 300 0 R /Root 312 0 R /Prev 337764 /ID[&lt;25c0b1bc1c1c070088d1399869bb293b&gt;&lt;25c0b1bc1c1c070088d1399869bb293b&gt;] &gt;&gt; startxref 0 %%EOF 312 0 obj &lt;&lt; /Type /Catalog /Pages 302 0 R /FICL:Enfocus 307 0 R /PageMode /UseThumbs /OpenAction 313 0 R &gt;&gt; endobj 313 0 obj &lt;&lt; /S /GoTo /D [ 314 0 R /FitBH -32768 ] &gt;&gt; endobj 343 0 obj &lt;&lt; /S 2495 /T 2657 /Filter /FlateDecode /Length 344 0 R &gt;&gt; stream H��V}P��Y�B�B\"�l&gt;����$Р���@@�A� B� G#ś�Un&lt;' �!�j��N��T={÷�� th�[=J �77�;���k�I����3�y7��&lt;��{~��}w l �)  ��<code>�P�[  ���S,�M��4x�� �P��_�Ŋ=�?�}���&gt;�v\����ް��ch��&amp;d�/�&gt;-XƯc�</code>8�#�T��Kџ�BCH�(Mԏ. υ��Rt�O0� MS��� �p$���S�姸f�W����մ�� �����y��|#B ���6a�H��&gt;�F�<em>���(��dN���?)Q����; ]��7 Ld�,3�kw�&gt;���Gb! ��]���|�rO�Y�󠾅�67�r� ��Ԛ�V�d�В��<code>���e: �D 1 e��3�j�� ��y����1DY��A2 c�Z �P�W�~/��7� �Kfz51Y�</code>��ˉ�@���.��.�. yp�r�t���6V V�f��b�L�)l�9��U x��)T'.nq�2�Я�.��A���=�Y�R���<em>aX"���&gt;��A�!� � nvZ�8B���Fs��ư����L� Ta�x+]@�lK���ᄋ������-���F�X�:�h��V6�2����� ���JT�<code _�_�����z�_��_9�i���x_i�="�{�����Z�/$��9�I���Xi�" ݛ_k�h�_�_0_k_�՞��_����q���_�v3��ͳ_�o_��p_l����_�="ݛ%K�H�_�^0k^�՞������q���~�v3��Ͳ[�O^��p&gt;L����,�" �_��_�f�vt�="����F�vT�">V�F����6ʆ�-Z���h�_�Ye��m����4ظ3#y��c��F�^:gx�����G%�벻����FkO$�-h� �r�s%�wJۥ�(,�M��!�Fh���3�QE��g��ܨ�d���tś�rz'!&gt;� lVP���A��~9�٣���I#]��z���:�%��8�'+.~ ����LD V9و;���~ D�l�ʄ���]���.x�~� vd8�/��'�|jV�?쩠w�c���R JA ��&amp;[[Q���~�|!k���������Ro��t�9_ n([ ;�ÙU�cY�� .B�C�K�WY����J�O�U���[:d���M��G%��D-w������ii��ń�����ݏ, v��� 5EP�S2�� ,og䰸��n� (�Q ��:��)� g\�:J% Q�2�� �(֟P�Da˨��3L��o]����Ž��z�g�$N� �3�[�BD?�3+��C��·!�_0�s]OP=V�� �{w�0�����s�3N�����lC���� �dbd��� GR�*�]� �T�a\��;�_8�ܑʪ��/�?���rH����҅��S��_�ŭ��B;S��x��P��N�a�X[���o; XC���񹱷�ld TA)Ax\iu0�A�_D��� :]P&gt;a�5{dH�Xo���,-� FS�{|W7�/*����abeVE��E5�r� ������M���Pey��&lt;�ਡ�p��2Ղ��C</code>!&lt;��c�a{k o�@�� m ����M c��f� �1��F�h�h�t���� 5:7]是 �ɔ��� �� �,�F �]�F�������H!��&lt;_b@2��� �Q~�M �깃��MEP�̯v���de�E�&amp;���� �lB�� <code>�H�</code>�7U�����.�~�096�BOQ�����������ճ�@(���1n���6{1R����ѷ�� ��+���כ}y�h96BX���xz�w�Y����ĉ��Gm= N�N�/ѿt���@�8���&amp;�ЖVj��fbb�N劷����ar;�G�l���}F�g��Z ��yx�7 9��G�;�!�������</em>�D|c�V�h�ɹ7D�M�þ����(;\��ib���,��9�]8�O��  ˥�]^��@ =@�� <code>��~�B� �f�Uj�o����bv}A=9���� W��kGb;��P%�]ɼ��ǽ��:g��^{�߯� �� q�� endstream endobj 344 0 obj 1947 endobj 314 0 obj &lt;&lt; /Type /Page /Parent 301 0 R /Resources 315 0 R /Contents 336 0 R /Rotate 0 /MediaBox [ 0 0 595 842 ] /CropBox [ 14.173 164.409 453.26801 830.661 ] /Thumb 199 0 R &gt;&gt; endobj 315 0 obj &lt;&lt; /ProcSet [ /PDF /Text ] /Font &lt;&lt; /F1 326 0 R /F2 318 0 R /F3 334 0 R /F9 332 0 R /F10 331 0 R /F11 338 0 R &gt;&gt; /ExtGState &lt;&lt; /GS1 339 0 R &gt;&gt; &gt;&gt; endobj 316 0 obj &lt;&lt; /Filter /FlateDecode /Length 423 &gt;&gt; stream H�T�Ao�0����9v�C&lt;H@B�Z�CwW���cH&amp;(��D&amp; ���o�V*O��c{^xd��������ah�:Q��6�u��F��� �}3�O�ͥ )�����I/� ��.���x��'�=������q�~Q�'�z��W~{��x�zQ?����V�E�{����E)���y�zh�:֍�ڟ���+�JQ�����"wi˩Kϩ7�{,�5D�  r '� K�&amp;.�����D]��``��vK��ĕ���d[��hR��Ңn�c�P 5 ��K�K _&amp;"%�0|�ls0|��8�a���r ��K�5�:�� _&amp;"K�ΰ�� &amp;�z��</code>�0�o�<code>��g�����^$Mm</code>o����-��$��Xٍ� ���ʣ���� <em>H�wܚ[1�s !�{��'�È���/� |��d endstream endobj 317 0 obj &lt;&lt; /Type /FontDescriptor /Ascent 0 /CapHeight 0 /Descent 0 /Flags 4 /FontBBox [ 0 -954 1043 796 ] /FontName /EBBDIG+MTSY /ItalicAngle 0 /StemV 50 /CharSet (/equal/Rfractur/braceleft/infinity/reflexsubset/arrowdblright/latticetop\ /periodcentered/radical/bar/lessequal/similar/prime/equal1/braceright/gr\ eaterequal/approxequal/precedes/element/intersection/bullet/follows/plus\ minus/triangle/bardbl/asteriskmath/plus/arrowright/negationslash/semicol\ on/nabla/circleplus/minus/arrowdown/universal/openbullet/arrowdblleft/mu\ ltiply) /FontFile3 321 0 R &gt;&gt; endobj 318 0 obj &lt;&lt; /Type /Font /Subtype /Type1 /FirstChar 1 /LastChar 38 /Widths [ 278 846 780 780 780 357 780 733 784 370 629 370 800 668 780 754 754 521 754 498 0 286 754 1021 780 784 556 500 780 701 1099 270 780 1043 1043 500 616 780 ] /Encoding 325 0 R /BaseFont /EBBDIG+MTSY /FontDescriptor 317 0 R /ToUnicode 316 0 R &gt;&gt; endobj 319 0 obj &lt;&lt; /Filter /FlateDecode /Length 1010 /Subtype /Type1C &gt;&gt; stream H�\�}L[e��etP���܋��eΩ��l���1f�Q�dt@{A�PJKK���嫴囵C�:'����1e@��eD�n��� ~�������_�&gt;ɓ��&lt;�=/� �0 ��3 &gt;���\zN���F</em>��qO� ��n��zxLpF �Q� 5 O�F��T��e�;�1�x Nm���u���kn5�HY�UV3JM�Q�{1Y.O���6��,G�b�jY�F���-#S1��jF%+3ʎhԥ�<em>YF�,Kݠ{��X���aXt|V�" �]X�&lt;���ö��<code>B�߅�E� b�a�� � G� �7ÑGa��:��G��� � ?��0����6�7�K� �L&lt;�j��C d��T���OG�q����-��H�ԑdEih�F���f=n���� MO}� HA�����t۬�.}U�e F�����{w0��/6X����iT���n��+����7X� �������=���p��)�!��v��.��m�i��=;f��� 2�8�#�j4�j����K{)�ߝ�����{����ϼDBD�_�~ZĹ�5�� �_��� -]��e����y#T��M.s߄���;4J����Ib��z�Kɹ�B*3�t��</code>+�A�ǲEzC���pt{�}W��W��3V�2�|y���$l�(~�u�;�j�Z�B��Փ�����������������!� Z�L 8�V�� ��HOV(mv?�I�q���Ί�+��~M�d�+&gt;;�֒o)1����,9 ��j�xu@9sP�<code>! q?� 9��NT�TO/���7�_��: c���j,��W��3��� �� �x�JD�/��{�.R�R�dBd�~��s�~� &gt;��ҋ�#A?�gq*�-NJ�Ȁ� �g$�v�U&lt;"�����ɷ���� ���f��Ǜ'lŶ6���=N7��c��1�?��� 1�_�F Y ��D�"Qʯ��M���29Qe�/�P��\�k�L~��x��g�N� ���� �/� ,;#��]� ���0 8g�U endstream endobj 320 0 obj &lt;&lt; /Type /Encoding /Differences [ 1 /circlecopyrt /K /Q /S /L ] &gt;&gt; endobj 321 0 obj &lt;&lt; /Filter /FlateDecode /Length 2933 /Subtype /Type1C &gt;&gt; stream H��T PT���e�{(�6�a��qd����P0�bj �</code>]������]P- +</em>ʯ���KP��8���tkG�tjZk�Ɵ�d��1m����%�]��$3��t�{�y�|�s�wF�!c �<em>77�<code>u���K�r�����X� ��rT��&gt;��۷^��_Qjz𿛛_��&lt;�#��,���k��r�Rs( ��~�� ��uh*A�Ȃ\�t���4D����&gt;E��7�%��{���*�X�t�&amp;k��jr��*����*wN�(��*�͢�z��lt:M��f��� ��,8 �N���0YLf�hMa�Cj�(��8-�vѶk�h� �Rpf�"X�&amp;+�qN��Ze3�mu�h2Z��B�Q�� T���a1:�O�2�U�6!z��j�j&amp;��,��.�+�muV��TK��6�</code>-wQg���,T9S�S�l����t��]jvv��5]W�]� ���6�贉]�M����d�S��j�������o� ��:â��]��Ԥ����d�J�YW|ǡ+����Ber𿯧Aӌ!��# �<code>8լ����h�6�uCBJjZFf�Ҝ��5���kv�E��! ���&gt;�{� ��p;n� �{��� � ���&gt;�O�_</code>/n��( ip��]h3</em>C[p7&gt;�O�� uBQ�F �T\����C B�B|!�HSx�C5��Bτ~��U�P~ˬ<code>�3n�&amp;�c sJn ���sa�a�΄�¾��Ĩ�%�ǲ�����r)� � �Ə+� �� }�(Z,}�����A��� 9���0w R� e|v@^Nu�.�*��#������.E�A��QE�:z �C����P;��l�d� ����E +�ۻ&lt;'c��7uh����� �6jI��,,Uzxw���4�d��T�56�4����k�65 v�c�v�w�5���d�S�s ����Qw�[���s�K�y���C �"�S� ��?���j �b�;:u�=���mkǱ�����=��5�f NHp݋e,)�t &lt;���Q2Z ����)�0*��I7</code>(8�oo�@p��߇�� ��S�L�w<em>%O'�~�"M �#� 0 0� ��.��a���z�IfxHG.l��SO d&amp; ' ���0ē!��{H8��xNE ���%� � y�&lt;�EO "hQA�'�@�ʓg�RB�������z��g��S���ӵ��yy.��s�8m�|R�"�,��LJ%�2�D�b������@��Q� ��j�)�~��"��$����E�J �� �Y#����|��MzT�ʏ<code>�k6Pr �V�d��S�f��F��jH�rK�T�S ��cU H�H��OQ����O�,]�&lt;. �� ���7�iW e'�&gt; �=�&lt;%Y0[���ϥ�ڻ��E��}�Y _�0G �G�srF�ꇟ?�yR�{�8�m�Ps�\�w(�^�� �硰�/��\ SB�Ľ�44q5,44���Ǎ0u��פ��� ��O!���B���׽N�F �c���]dIC+qZzj֫}&lt;��J�l��������u�� �P^�^�:Y{�Pa�m�� ���a��J��{�j�|�v�˒��S�ҩ�#��W9�a���j �� �(���9���"w�5T���U� aTpR�4 �L�� ��KS���]�í#f��I�F��V�F�B_���2R�����b�jx�</code>�q�;|����a�;�~5���x��e� w��&lt;��RU�M�����k7��2}�@ِp�keI�-B��<code>6^Xw�H3�H� ���k��&lt;�R@��.�B�EGL��kߵ{��AC���S�J���r�C{X}9�7 G�Y����,�(���=�����ҊL���]ڛ���9Ajr�x���/����%�3</code>a��|�J$I;]��<em>^�Y Z�&amp;qkɼd�}�@̚ ��K������%$j��-������</em>�+��V�䐠08񗴙]�7�Y�$��d �t��ZZ�dC�����^��$��v51j՝fI~꥕���5~�Đ�S�?�!��� k�~V�l0n��Zb2C�m�P��}�{P��{�� ��� �2~��ɿ8e|��W�n �&amp;�94�a��, ��]} �k� ^�g:�"��eA��:5C[�L�(�Q� �&gt;��ai��;��~� VT�(��#P�M�:��G��O�f&amp;�9��u�"|�,l�Y*�Ȁ^�_�n�,�-\�u� p�,���j�Ś��Cc-�����h���q{��]զ�� ��?tO�2���Z̏:����Q�UQ�� �c��&gt;1D3�Pm:U�$Uk��f����� ��$l{�U��Y9ٖu!�� ;e�@�g ���A=,�g&lt;��|%�B3���tcƏ{Q�n'�7+</em>�Y�p��?V.��觜�)n�;oą���P�FX����� ��ј ��wK�͈��<em>�Rd�dK�q;</em>k��,Y��i��4����&amp;.���j�)�2:)K&amp;�\�1��� �&gt;~�mn1��T���:.-�E����L�c���N3{�N�L�ud��S�1#����鴃Z!�.b!�.d,9���%b��p��jtCI�%�A�� ���K�eJ�tE��"�k�;Z-�~�9� endstream endobj 322 0 obj &lt;&lt; /Filter /FlateDecode /Length 8202 /Subtype /Type1C &gt;&gt; stream H�TSyTS�~H�� �{�^P�� (�fXDEP�<code>�e���-�@ � !!�$FD (�:� щ�ڣB��ZO;���3��Ǵ��_��{Ͻ�������ð������]k�i섹x��3� ��y��=� ���=�� �u�������_�(� �O}����7�C� p' /^�Ȁ�a�� ���k""�6m�ٕ����kr*�W �D�r�B�&gt;�0�ˉ�Id��(B&amp;�H%9�'��s$N�/iQ�I��/� r� � V�)�H� ����ɞ-���KRA._�Z���y���s6�����x���#Ke�X�T2� Ģ0� �H.�V �?���ײD��X���7�fEn��,r�� a��V� ���ʓ�E �f ��˗�+��e�����?��o�w�</code>:������  G������3C�B���k� .�]0�p���E�����Elv~���ſ[rd�O�=�OL�6�+��T�2�A�Tf��-(|��9r LO3����w���r���blC��ku�J]zRI��c��C-Qm�&lt;�k=?���A�1�t�����]��! Vg�</em>�-#��@2r���O����%@�l�|^��g ؕ���?�A�]��|���O��6��|8�� ��0�J�-2�ݾL����7lmlů-ǎ������3V�c�����G���-�B����cW�f�R����� %�r��ܪ&lt;$�2 9� �A_% �&amp;WƓ1$]�#3���w-c@K�Q�د߃�i��}�E����l���KieL8� Vp ȧ�I���ԝ6LM�/��G���n�j��ۨ�绛L�Pg�F�� Ԥ��h�����U��4e�r<em>3�:lC]�:��ۭwN�����ɏ��GS���d�@ e�"Eyy�W7UT��[Rv� ȝ4��^�L?�9q����������B��A�%J�nܓ��w�)�}�-���(- a7</em>� �r�z���Fne H"�&lt;�: B\ �� ����Rk6��� =�����, �\^<em>�\F�jlS0sG{�^�Z�F&lt;<em>����f�'�_)@�5�{ v��|���k��:j� | k7�kۉ�AJ15 ��J��y¶��&lt;c �&gt;�"'��M0I� AyG;��z���l�uGMbCg/ �hLU����W���j�� 0J��o���ڪꤙ</em>�&amp;9�]T ���V qJg7�1���Y��l2��ϊ���� �� ��V�;�G汆��q�͵?=�"��R�(&amp;+;5С35��I�����I�P�)2�Xā|L�s�Yw�uVy��sc��yI�����A�,�]f�� ���;�7�^�<code>j���ʻ���ų��] �r��L����/��H�^�A-Z�ֈ�hJ�2L���j���J����2���U���p����q�ecO��e�J {��t!E�m��7�Ӛ�f#���ZnM N?@��� � i  {����� ���'๵L�zyV � ��-���U��CEd �x]h��OK��&gt; =�t( ?AU��J������X}x���Oiƫu;�M�&lt;��p�={qᶅV��Xt����Nڎ�s+*�,_�-�ه�x;2�&gt;GU�쯺�]��m��]�9�E��^�kA N�W:);�9h#�q���F�ʻi�</code>��pX���+R 8nO�!�K�S���c����J����pzڪ23c����TŪ8���$�I�� Ѹ�����h@h��f�꽛fm(�-��nljL�0��Dg19Gg���Ĺ���,Ő�1?�</em>�����w_��J8֐�媩^�1Vw�O�� �a�K0 A��a�dQ��Nv]�c ����J.1<em>����l%�RvJ�N�|o��6߉���r��� ���� ���J���m��3�}"Zv �&gt;��R�C�&amp;��e%���&gt;��s󸳵���Ϋ�����͢(�|��dB/}V���Mxe���k���̮A��d5�(5��W�ŵb����z�f �S��[PuK !�E�1.^�Y Eg-+�i �qg){����tω}�ط���d�B̢��'��<em>~��ֺZ=�%��#��&gt;���{�v���NR���ڽ���r���Ed</em>��7$���R8-<code>�u�l���ri��o&lt;�=~Җ��z�� �0�ޢϦ���9�s�F����*���ѴF[�Rb+u�ū���!��5�CX��c�E�+������ �@DXW����)�׮��$�|�X��LKG�_/R�yu� ����ٯ) ��fP��</code>j1�P��ʐʠ龐�-Y�}���Y�k+���?j9oi�Lm�h8�����]4���'�v���WB%Y�t�/ӷ:��Go�٦E�Ũ� �:$�0E���z_��N��� ΠOL i���d�"<code>n0L��_}���f�c���F���z �0�[� f#*�.v ��_Oh�.�|L� ��� {�M\,���0e�(D..�r�of?+ �n ,G� j�a�6��鬸JCp;�&gt;7$}�F#R\ ��]�q� ��2�[+���E� ��b�(��wؔ�#8�I � M6]T4 ��� �eΪaа���Z�Q�Ƙ��p���v�r3r�*�!-7O�?-V㲻nҰ͑�0��5�'�X��</code>��T�T��xLP�4x��� C������Poҙ�䉲 x"�~�~�</em>u���A1ɖ�S�Kt��t �K5i�J�6���o���]S�$x}a�c��� ��Z�֢�$pO� �&lt;����n�O��<em>�#�E�w!)��Ĵ9�������Q ډ�"u�q���N ��<code>k�p�����j 1�5�)����T&amp;5Q���F��ᕻ��)X!�Ƴ8�_�V�D� Q����%�^^���(Y5�Y��t�Rs)�t&amp;��P&gt; aU# N8D�&amp; �cl�0Y� ��5�= �6L�Tj�˜˔I)�i}8���5�za[�HO���ٲYo�=a��'v�2�]TyAYq� $Y�@�����p%�J3�� � ��4��3 �xo��Q� 7�{kEکՎ�V �E�N�XZQ��*#{ BBBHB6�KX�M7�ť�Z�R븵��Ӫs&gt;���� �̏&gt;�����;�=��{N� �¦6%��s�/ ��nmor[I:� � Q�Ӿ�;�$ ����l�c���F���.�#��ƞr�t���/�(F�_ ��|���?����'�*�#Q.���ŵ [�������ۀ���X-�#�6��;Y�e��� �,� ����� �O�И�ɍ�P���|�Un�d묵���]�0 *%���l�u�� ��f����W��Y�6�P/�3�d�� ��)�E�ʚ��w�i���|u��o0�M�5%;Y��io�0N%)1)���E9iB�6i�:SEb��v���}@��y�;</code>F!���]�#�'8 o"�l�-Ȁh��Jγ �$0�1\�� �����] ��a�f�L$D�� m�&amp;q�/}!���n]+1�\�Z-1�W&lt;��' /O��6��h��� �%�����V ���� �!�8�h�x���b�Ja���s�L h���2��y1��O?���Ԋ �o�p�E�y��J�p� ����S"a� �%fe{^�d0�����Y¡���c�C&lt;�</em>H|i+ل/�^�����8���nr��;��Q�:o�̏�ݗ8햲n�ѓ�B���A)�%��e�{�p�)8Oޅ1wj�� �&amp;P�d'1� :[U�ɐ�U6{o�N��J'�����|�� �A��@��OZ��x�SO��c��0V��&lt;���W�c�p� ��v�U�jG &lt;�QG�+]�7�|��#ٻ�s�������^��Ǳ�3�&gt;M� ��cN�w�'8_��;�v�{�~��tLWgHX���YL�o��0����� �;<code>��������� &gt;O� X���k {����Us � ��?S:z�T�� ��+T���%�:&lt;ꖧ&amp;��F���l�[�I�h����y�~</code>n=����3%�G_O|U�H�k8�^��K���+���������Dq��[�� �!~H��[㑨�x��7l�RtJc4 x�IO i(����=1T }���� ��� e� w�;�;��M!^I��7�RX�Z�Vqx�ޤ�����u5�{���k���z!�f�3,�caD�&lt;'93�0�����7 �6�T���a"���3)�Ғ��~)3Dz�I�z 9����&lt; f��ӗ]�e���/�� ��OD��Q,D\� �81k~�~�w"v �y�,S'�ZJ8̵� �a�kg�����<em>D�$�Ӑ®���{������ ��u5�{,$y�H �e|��s���Y� T�+qp����������I�&lt;�r�Ytfe�PA1�7�R&lt;�Fr���oF��Al�S��6�n1�P ��L'�?�06ٟ&gt;$���SeK����$Ǌ�� 6� M����&lt;J/ʼ�t&amp;A́�.��7e�� ��D17ڀ����t� �Q�:Z���O��߁�z �mH�� O�I�&amp;�X�4�o�� ^�y0�Y���9��lFuncc��e[����-��Ix ��v�鱫�FN�w(z/�� �|vM[�dNI��h� ���2��?�wx"���H���gȥj��Z����.����� ����)DG^<code>Ԧi��U�}��j�k݋�BM&amp;5 O����gd��j</code>�%�l��.M�&gt;#��9�v�������}�jB�.�j�t M��;z]�=�&gt;��jR�8L��: |y���mL��-� �uE�Jۮ�mR�G�nq  .S�C�\ B (� �V��5|�ژ���R\��uuu��<code>�ex��ܻ��zA�B�����A�=�%�0iL� �%�ץ'ɥ���g�7���=�Ҟ9e��,�3���-GZ�jiA!���D�� y*6G������Wi7�W �&gt;��R�w��K�J��g��f❩lJe��l�Ƀ �D ��EC�F90��w_�z�샍@K��$WI �{E�NK�yE8^;C��@� q�d|�,lZ�f�z�� 9��c~ߊ��C�_ٮ��&amp;�;Z�]Y�A�z�]�ۤIfZ� L�SB B��! �76lK�%ɺo˖�[uڒ,[���md���06%q�R�6�IJ)tZ�����c���t:�t��ݙ�ߛ���[^�H��D=����*��"����q�+@��7�ߒ ���u�v�㵄���/�h�N� _[�ƈ�R���ޢ&lt;�� �"CFg��k�Ś|&amp;���o]���J�e���  � �t�:���jHa�6�Ÿ@6v�&amp;X�ggltU�W���Yd�p�Ɯ$�w:U��[������ � 3��Y�J��%]�Ș�:�^3��#b :�)��'b#wGI,эS_$�C�� p |�}�PR'�U��Ll.}6RKI�;��"��&lt;J�:w���˗�?�/﬿\ � [�t�����C[��\P%õF{��|+ &lt;��E��ٙ[!sm ��|���H��/�M�5��9P��0W��p��Ɲ�ӓ����"c�B7�י(/޻�lwy-|ٯ�Y�S�c �K̙����� W���t' �cj '1wq��0�lEX7��{���y�1�$I�BX%�Mj��DO��-��c Q��76A2��&amp;�!(O��?�;�5��v�J�j�!B��Sw����G�x��ZN��SY�X���j�f�MݺHk����� ��[����� ]�zp� ӷ���%����</code>Cχ��赩 � �a&gt; ���#V$b j���L&gt;�Q�h���JP�O�[B}�ۮ!R 5�഍��L��nͯ���ۜz�?�O���f���I��<code>y'�w�Zݕz�|��Zx�]k�1{� �����R�Ĉ�5�F[ ��#��{�G� _[Y�B��v8�.��$W�! �l�������$%?�߂ܗ8�~��6�%��wo��dtS���^;��&amp;G�2ј��:H�����U��{�ryH� �_�摍R�m%���4 ��7ѭks��n�ߩT� ܆ 2N"�ͺ0�Z$�ܛZ��g��U�nE �G�/�������w��x�pu(u���'</code>� ���͆E���Q�f5���@��l��,:����D��� ��@�0�i��7}���\M��z[�����ST�S(�uM����l50�3v�&gt;�E);��j�B��O� Ҵ"UBN��5�����Nw�#�S��J��</em>�4&amp;͈? ; �v4�*��Zʩ��p����{'΄ȇ�2��8�#��,&amp;j�� �� \b {���૤n� �zz���Ƃ�Z�� :K�Ҡj��}�}��&amp;� ��h��u9��UL^�F?_ X��a�9����3�xS� 0w�+���q����:H��[����=9|�8j���i � ��f�:��])l4�Mz�_B uF�� ,["Й��/���������!LU��)����z]E�Mc�h�}�wU�o�1.��d�6m/�1���u�nQeSt����U4�i9��y<code>C軺Á�J�T�m$C'�Q\PYE�_P �l�z� Қ ��� ���t</code>���%zk�COt0F:�z܁��&amp;s���0s!�}�0,����{����X�&amp;:�q$Z�<code>f�߼�?�DL��:\*Wח��JO���4�y�'+q��j�2�z��\�H���E]THe��\��h! =\�����c��ժ�ƻۃ�$��{dy�w����gr�E�We���$ju����݅�:��.��&lt;��1��-�Gs��k�J����l���vC#���g�u�^�:�Gb�� �7N^�����vK3��8�E' R/��V� �����Kv K�����PXK��j�J ͗k�jp�B!!Qoٸ� ��A��K��DM �u��r�{)</code>j���z�Nec��e��%�z �q'�O��\W�&gt;��Wp�9ӥG�^k�Gw�/��GF�N�H�V�b�N�i�Fm��2( �n)��&gt;0��� GҪaw~�b� �?�ӈ�^�W�^����oժ���VS�5���� Lo�0 endstream endobj 323 0 obj &lt;&lt; /Type /Encoding /Differences [ 1 /star /r /X /Z /A /i /j /x /I /parenleft /comma /parenright /period /k /l /B /n /m /R /greater /c /K /q /Q /beta /slash /u /v /t /y /epsilon1 /b /T /s /D /w /pi /p /S /rho /tau /M /Lambda1 /lambda /sigma /F /Delta1 /z /xi /Y /alpha /f /omega /gamma /d /P /delta /G /C /O /H /J /V /U /mu /nu /kappa /theta /L /W /E /h /N /less ] &gt;&gt; endobj 324 0 obj &lt;&lt; /Type /FontDescriptor /Ascent 0 /CapHeight 0 /Descent 0 /Flags 68 /FontBBox [ 0 -213 987 680 ] /FontName /EBBDIE+MTMI /ItalicAngle -14.036 /StemV 73 /XHeight 0 /CharSet (/Delta1/O/gamma/m/x/greater/Q/Lambda1/epsilon1/K/parenleft/R/star/pi/the\ ta/p/E/S/parenright/T/q/kappa/U/mu/B/r/lambda/V/b/C/s/nu/c/W/l/comma/D/t\ /X/xi/G/u/Y/f/rho/I/H/period/v/sigma/Z/P/J/h/F/w/slash/delta/i/tau/d/L/a\ lpha/y/N/j/omega/M/less/beta/n/z/A/k) /FontFile3 322 0 R &gt;&gt; endobj 325 0 obj &lt;&lt; /Type /Encoding /Differences [ 1 /periodcentered /radical /plus /equal</p>
</div></details><h2 id="toc-228">115. AC ASE STUDY WITHP TST AND VARYING INPUT LENGTH</h2>
<ul>
<li>链接：https://openreview.net/attachment?id=4A9IdSa1ul&amp;name=supplementary_material</li>
<li>来源：bing</li>
<li>摘要：2025年3月15日 · In this section, we focus on iTransformer (Liu et al., 2024) and PatchTST (Nie et al., 2023), highlight-ing the effectiveness of FreDF in enhancing their performance given varying input …</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-229">正文（抓取，非 AI）</h3>
<p>%PDF-1.3 %��������� 4 0 obj &lt;&lt; /Filter /FlateDecode /Length 7998 &gt;&gt; stream x�[��ƕ���)�U�� �q�V6U�g�r�8V�J��@q( ���̙��|���|����l��ؒ^\��@�&gt;}����}�]�U�]��<em>S�Y�.��.�&amp;��~�"[��M��/�e�wY[7�Fً7�'/~(2�^e˖||y����]</em>-��u�-7�W��r��ݫ�qw�ݹ;o7owG�a�ஞ�ş��bY�e}��w��w�o^d�J��� D�2�D���j��}�M��B�m�5�c(�<em>�/�YQ��u]�_�k~�</em>����U�6��-?��<em>{�w~��鞶�Uޔ��zU7���^���:�&lt;���+�^���)ؽ����?��</em>�W˿���&lt;�<em>v�Q�)�˲[U�r=.z�����ݐ� ��� ��RU��j]+� 7��B糢<code>R��!A ��~�Φ\�0�2y@� aD��#�vݪ(&lt;��m ���\��pYe˿�Q5�R����y�����~v��|a��&gt;7w2*��m��A���X��F���@�W�6� &lt;���!=��UU����^Wr}��&gt;�w��8ʊ����J�yu�&gt;��w�n�a�j���������,=���˰�&gt;��ȶ� ��A �T v�&amp;���pJ}٪US�mf�p2+_� �w���+ni�o����F�X� j��,a0�OhY��b�qQ��VU �ts��W�w��5�𖛇�^fڇ�t~�z�R0� �0��˽GϏ��J$��+�M ?r~*��Rcm3=���5�&lt; G�WfeU��f��斿�g�ZCvm�V��2nj���3����2�5B����@.w����)%��\�À</code>w���˸V����o??�~��� i{��������n��co�+ߢge���Nkhn� ��:�{!hs�!jm��Q/ܚ^�w�=j=.ʞ�nww� n�a�[͚=m��B�X���<em>G ,���&amp;����7oowovw�?�.�����9zɈ�4�;�X&gt;Ջ���x�� �D��ݨ�7�G����� N�����x�!� �H�Ț&gt;�u��:̫F3�</em>/����yykpB�E�ˡ�9�<em>Y��Z����-��� �~� �o���L��J� U� <code>���&amp;�,4�ϝ���&lt;�Y8=X2A�-������F�M 4�+�.yjc�� ��� Y�4 Y�8c0�v�h��Ŧ1��ש��f[L�ֵ�C%'[�Vɉ�* �ƈ���Zp���ފFD߼z&lt;"ٍ�����qk�R�uB�o~��oz�� .d�^"'c,-�Ŋ qu�) .� w(/ �ܿ޽a? 7N�ps��\ /Y \&lt;�?x��� �䩄��Fvٖ���-��$(�Q�e�! Vm���ex������.�C�C��²/0�&lt;v�= �/#%ͧxu�Ѹ(27=\~c(�m�l����as���F���uۏb'6Y�bg�*� �� 3���� 4�u���sYԊ_UY ��a&lt;�L��zw��2�FG#q4��[�.�5]!Z|D"�Z�!�ӎK�V"�m�v�*/�?�" ���F�fN��nVE&gt;l)�j�.E�~�k0VTł �e�gb97�b�� ����,� ���#2��R�ք�x�.�a�f���f�(W5񀺑h ���u</code>�E�0~��-A�� D�(�)�n���t���AM��H��^��qQ��D$����Lw�����&gt;7 Nk�JZ�OnY!̑��o #V����j���$bmK��XLmzs��Hc�[�</em> �t�V�{</em>�{<code>O�.��S���0����M��Z� {��D�UI �?�ŜeE f��v~�L��$�xB8à����e&amp;@���߱� b~���eQu�L�� x'yT�Y)�Y�� ��|� I��WVm85�H����Iķ�QY&amp;� /�.�1��. Wa|y�$�-���0]���6� �Y�T �F��]�3� ^�;���Lg yP�l͒2��G2�M��6��� �� ��6J�2�P�X%�mŢH WSP������  ���S g� B:�'X�Vbp9I����&lt;d�kt� � ������\3O��R,��w��Ϧ��UMڧ�r�</code>�X�Q���m���H˲ǟZM�� � M�׀��Q�Y �B܃�S����xT��,Py �&amp;�Ô{�zfOy �( TOR4�4H�&gt;�R��l�=H�� �T�'�A:��HS:�@�%#���.�Ij����+/IS��(�ޞ�$� u&lt;��,XG �$H@��=� ��,(beB�#E�����1�&lt;�ZaUye �����U�!Zԫ� ��V��(D�;�B��,!FU���p�v��LI��h @�X� L͎h �tv��vBt�u��:��������� g@�����Sߴ�p �x#wNf X�<em>oV ���e�0�پPu�%o����H@&lt;��T�B0.I�:��lO�[�^e�M�� �uj�x�6ݯ�39�޴5�&amp;�Nb'�{)V^�gڦ)���##Ǩ'��q=b5x .�,�r�% KxO��y�-v��9�:�I ��{�$qL����H�p�+�\�3(�)$5ҙN �p�O��u�v�&lt;�� ��Uk|/�q'�߉������&amp;j�?�Y�D�y��E���s����Hr��@y_</em>��(p:!P0�V)L���!4O��H <code>6 �V�</code>A��,(�΅3Z��}Bl� �� ��c�&lt;F��Wud+�=�1<em>+b ���M r�[�eϨ#3�T%V��̞!��x5HK��H��2� ����� ��r</em>�I�� ,� gA�[�UR��J4����Ҥ��o�S�M�����!�f#Ew����A��).yR��rj��: .���,��1 ��<em>�u�f�� z�t�\&amp; ��" U͗���'��!^L�/I� �$u� 5�%@�Dpx��</em>�i���̞!/IS�� U�#� �m=H=�t��"HP�t�x�zf���t�J�,P�2I��� ���ٷ�վw�i;��y�� [�\n�g�&amp;,���8'�"M^���s����i:�o�M �E��V�ɐ�G�������O�\�T�/�y&gt;8�xO�ň g� ,� cs ��,�� gAoїm$��wN ; � �E�'�&lt;��s_��b�!&gt;�$����} ��n�@��������y �"o��C�Ӎ�AA9K��.Y�.LR%�2�J;�'V^ ~���DT���!���,�5�'<code>6&lt;�m=΀^fE � �����&gt;���� a��&gt;�R�F����@;ŀ�0���&amp;�E�+��6]��e2 �Cg:K��O��@q��tO�WV|�������1���Y� HMOR�8|�1��q�@�3�t�=� �c ($��&amp;��,W�� ����{|Ζ� �g:�§�,����u�}�g��gb��MlЖ��?�_޾�\��QTqR������=� �� �DV�g�d��gQ���l�韟M��g�����d֯:��@Mo �*9*��������</code> ���j��2�!�L@�Sp1D'X�x���N�'<code>��{�N���yǧ'&amp;�s�؝�A�</code>�y�φ{'�&gt;���xd�z&gt;�����Oe\���ھ����s���N�k��<code>�M ��,�G|�</code>���۴i �F��3��@K�N ���?��\d�7I��#�p9��YP�����v��@�u��Dj�y��|g�H�uy������e}&gt;ٗ.� O�s'��s�lgaa�p0m�O�e7ష�&amp;Sg���l�6�g;;Y�'�&gt;�^�["��� uK$9\�8#���� f#���,腆��� �(�D�Y��� PG��JJ g��.� � �- �����0jJ���C�N&amp;���K.?'�mo ��yO����U ����� M���E/x ��qg\?���� �uϺ'��h ��bx���@O{P��ԣ)'�����Ҥ؞L��9�q�AjzK�'Acc#�s� �\��x H�8B������bI:�I$��wG{�zf���t��2��]dl�e�A�p�����x\���Ҧ%\7H�h p4�&amp;����-RJG3$I��� �̞!/IS�� Uč� ��&gt;�� �\ � ĒTIpB�{f���t�D#��4��)Ȝ4 ��&lt;�Q�� ��e�H L��~�R�K�O�8�K�oA�M�MR9 w�m�<em>'Hi���=��ɞ��ۣ=9n�􋧚�S<code>�h �98�a�%+ � ��=x�l�� ̩�� �x� ��</code>��b�j�ʩk4LC/�'�n��5���Q&amp;"Y��}o��\c�)ڟ���(����</em>��aT��<em>s[ ��07��� ��~�}&gt;/�j x�?Z�R��RĎci+�����ſli [�K���%ˤ$�.����P��Rm���0'�ui���ݝ)?� � � �S�j���ak)p(ti�p�BW+�mP�4 R�������ڵ�j2HA5���:li�� R��p˅�;�<em> N43K�&amp;/�hU��<code>G7��i�i���&gt;��� {]��-?���� �k�ɕ�)SQN�-�!�[[�C~ak:�s����[�swJUW</code>� �:��R�D ��3�%t�?�<em>�R�2S�I�|{��=&lt;{����� Õ�J��?[yB���R��L��̄�pR�|8� 7�fXy1u����� �4eK�qu}xC��!n��ҟ���B�����0 �i�:�.��Az�ٱ�}�(�|�ꏖr���GE�ɒC�v�T�&gt;���M��(� ���c�{#�ը�B2��u8�� z�lW!�0�[BZ�v�P+�u��iI&lt;}L�Mg}z K/L1� �� ϗ⒔��8�0����&amp;�f�����8�ooy�x� ��b�Ta)jxe�&gt;�G 5�8,�n�</em>.��-�����C݀�!]���)�Tp̶��F��T�$.ؑ�n��� XH]��������kl 8�<code>r��</code>�m٣'��R��C�</em> 1 � QR�C� ��M����:��ͩ�Pd����BL�}�5�</em>������u9Rl��CB%ʆ��(� QM%�k�-J� ���5'{f��1,���c%A�ZF� F�V���/� �S�A44%=��2!�<em>ia�[��</em>�y����/-��ҕe���� ��q,+Jز.��e��{f�+j <code>]&lt;�e�e��R��TH+(�Q���fo�o�Uls���O</code>�����h��؆X#�[5�ܭ��6L����Oa�5,[���%$ �7/�5E ����D�߳2�<em>I %� m[�<em>���-J�v��%�ʼ�w@_?���O�i�w��^�ic'I]R�<code>���Z�e�.�P ��ޠ�M��{�S�+�u]����[��Is�� ��Z5G���7%M�</code>���|&amp;��</em>0ǿE�2_KyĈ����<code>�D �V�w���'�%Z( �� ea�D����3��GU�[4��</code>ȿ���&lt;�?Է����g���ւ�cO��ˏ�~%�^�D4b��{��䘹/4��� t���f �Q4����j��{��+�'U��O�</em>�?0���l ��H%3�'�WS��i<em>o �!����-�/3&lt;�R_3od?�= �����Yd06@p�Ia� gXl{OD���R������r1 �Y�̈́A4pTa���P�Ju� � u��:T�-H5N�� �Ч�$��y-!�V�c�bZXs���Űg$�3���n�o���=k]c�����I�&gt; �T��4�f7ٯp��1ՙ�</em>�l/b�-�Q�MC�;G ��VU�Q�"b h���o�de���r�!�� ���)��gԲ�����֚����F�@N�V����1 Lu�xV1�Ģ�\&amp;�Oڬ���3�ƻ�V5�:J.��y毶����"�bQGr-�1������ϻ�C�Q��˅�7 -Bt��w� K�kv�o|s��o�k��� �OB'~�� O�Yh�w�#m��6�cI(��!<code>.��&amp; �#'�r�X�Dʥ�-![ �n��&gt;��H� �6��]� �����V-9 Lef�J:?�\'U&gt;��2�Lqz)��Ѓ?vreNk ��&amp;����~&lt;�&lt;�U��Kbߗǽ���[�t�K��Ľg� &gt;d��Ҟ'&amp;ў �ey�9G � �O, �w�B�� ��Q� 9�L�A:e2�W�� �~�&lt; )��AM2�B#) V��/�Jm�n�Lf&lt; L�s F��&amp;\�A BtT1�&lt;��  �n9&amp;G&amp;�爡����ݐ ",���9� �I���� ֹ��st��MCJHB]TZ$ _F!mw+�&gt;�:G�L�l��d�t6�2ލ$Qf��M�O�?����� z����^EA���)�ɦ|</code>����UG��&gt; �j]%���=/��xZe���4" mR+��&gt;�Y ��}gVf̵�-�ke1�mG}<em>kd�W�"�ߺ�&lt;�U{�Ps���?GҤr+��AG���(9��.�%e,'�&amp;��,Ȯ ��.�(3 �ƴn��祺�$�rpѐ����f��e �� 0$�s��N��-E}L�8� $�7̮���O)� �k6'E �w.���k����c�/Yֳ{,F�7Ȯ|$�9�4G�A����� �����A,/�= �����'�j�S&gt;ar, ���&amp;�~\�P��}  ��U�x/�%N�j��N���&gt;�%�r�6.p����\ � ���&lt;�{$��m";�����C���%�M, ݽ�Cy �Z���B #����'���8�u����<em>R~]� TIOc)?+xd�o�d��'��f�K��<code>�nU���l�Orw�q&amp;��[���-�-� �9�&amp;]�����M!� ��=aq��i�H��������B��B�-ͱ�i/��h��~38�DM"�w�cQ����� " �tFo�mb�i��B$�bN �8TVr�%�8UƠy��щ�֭D���0F݅��l p8 ��(��� ��%�x�f%v���V;jL��X���0CNxN���g���s*Ф�?h���/$�uL ��-�~r��SgY^�ݰ���9�%��u%����cL3�˜���.)m���_ ��Cr̓#_* ���� �9�9^��9�����'����� ����������+�� �d/�&lt;�r�� i�#��%��[:����oL{�{ݞ&gt;�XW����;�i��\���c�n�����벅O9�ys�e�S����ƾ^N��y�����]F��6�l8��z��(,b��.�o6ס��q�I5+��s?� ��u�����8d�;ʡ[9</code>y+�C�( U�&amp;���UuE^�� �P�</em>},N���H�����u�B �?�cu˨ځ����˱V)Gʖ;�B��0c� ݃af���ܽy��]�9��� ���(�\�yͺ�N��?s���?1Ӥ/�Y ���\��� ��u�</em>$h�� ]�����������Q� � �́?8Jʕ�~�5�R ����5)=U<code>C�� �Q�ynY �Iu��t�qӯ�� 6�N�Ƈo�Qi^\N. ��*� K.YX,@} \�حJ�]�&lt;���t �zu�}�Hi��C�B��_v�� RA �5� � $/Y'e������~w�E �����W�R\�l����no�x࠭[��3��3�(��V�� _�y��P�y��z���ș��$"9�YٟZ8��կ+��{�DRb)1� ��j�z�J���W��aX =��aO���;�oy����s(Lۼ}{&lt;|����[=��,JeDӪ�S5�ʻ�� ����&gt;��RD� [����Qt� endstream endobj 2 0 obj &lt;&lt; /Type /Page /Parent 3 0 R /Resources 5 0 R /Contents 4 0 R /MediaBox [0 0 612 792] /Rotate 0 /Annots 15 0 R &gt;&gt; endobj 5 0 obj &lt;&lt; /ProcSet [ /PDF /Text ] /ColorSpace &lt;&lt; /Cs1 7 0 R /Cs2 8 0 R &gt;&gt; /ExtGState &lt;&lt; /Gs1 24 0 R /Gs2 25 0 R &gt;&gt; /Font &lt;&lt; /Ty1 6 0 R /Ty2 9 0 R /Ty3 10 0 R /Ty4 11 0 R /G1 12 0 R /G2 13 0 R /Ty5 14 0 R &gt;&gt; &gt;&gt; endobj 15 0 obj [ 16 0 R 17 0 R 18 0 R 19 0 R 20 0 R 21 0 R 22 0 R 23 0 R ] endobj 24 0 obj &lt;&lt; /Type /ExtGState /ca 0.2 &gt;&gt; endobj 25 0 obj &lt;&lt; /Type /ExtGState /CA 0.2 &gt;&gt; endobj 26 0 obj &lt;&lt; /N 3 /Alternate /DeviceRGB /Length 2612 /Filter /FlateDecode &gt;&gt; stream x��wTS��Ͻ7��" %�z �;HQ�I�P��&amp;vDF)VdT�G�"cE ��b� �P��QDE�݌k �5�ޚ��Y�����g�}׺ P���tX�4�X���\���X��ffG�D���=���HƳ��.�d��,�P&amp;s���"7C$  E�6&lt;~&amp;��S��2����)2�12� ��"�įl���+�ɘ�&amp;�Y��4���Pޚ%ᣌ�\�%�g�|e�TI� ��(����L 0�_��&amp;�l�2E�� ��9�r��9h� x�g��Ib�טi���f��S�b1+��M�xL��� �0��o�E%Ym�h��� ��Y��h���� ~S�=�z�U�&amp;�ϞA��Y�l�/� �$Z� ���U �m@��O�  � �ޜ� �l^��� ' ���ls�k.+�7���oʿ�9�����V;�?�#I3eE妧�KD�� ��d�����9i���,�����UQ� ��h��&lt;�X�.d ���6'~�khu_ }�9P�I�o= C#$n?z}�[1 Ⱦ�h���s�2z��� \�n�LA"S�� �dr%�,�߄l��t� 4�.0,</code> �3p�  ��H�.Hi@�A&gt;�  A1�v�jp ԁz�N�6p\W� p �G@ ��K0ށi���A����B�ZyCAP8�C���@��&amp;�<em>���CP=�#t�]���� 4�}���a � ��ٰ; G���Dx����J�&gt;���� ,�_@��FX�DB�X$!k�"��E�����H�q���a���Y��bVa�bJ0՘c�VL�6f3����bձ�X'�?v 6��-�V<code>�</code>[����a�; ��� p~�\2n5��׌���� �&amp;�x�</em>���s�b|!�   ߏ ƿ'� Zk�!� $l$T����4Q��Ot"�y�\b)���A�I &amp;N�I�$R$)���TIj"]&amp;=&amp;�!��:dGrY@^O�$� <em>%�?P�(&amp;OJ EB�N9J�@y@yC�R �n�X����ZO�D}J}/G�3���ɭ���k��{%O�חw�</em>.�'<em>!J����Q�@�S���V�F��=�IE���b�b�b�b��5�Q%�����O�@��%�!BӥyҸ�M�:�e�0 G7��ӓ��� �� e%e[�(� ���R�0<code>�3R��������4�����6�i^��)��*n*|�"�f����LUo�՝�m�O�0j&amp;jaj�j��.��ϧ�w�ϝ_4����갺�z��j���=���U�4�5�n�ɚ��4ǴhZ �Z�Z�^0����Tf%��9�����-�&gt;�ݫ=�c��Xg�N��]�.[7A�\�SwBOK/X/_�Q�&gt;Q�����G�[��� �</code>�A�������a�a��c#����<em>�Z�;�8c�q��&gt;�[&amp;���I�I��MS���T<code>�ϴ� k�h&amp;4�5�Ǣ��YY�F֠9�&lt;�|�y��+ =�X���_,�,S-�, Y)YXm�����Ěk]c}ǆj�c�Φ�浭�-�v��};�]���N����"�&amp;�1=�x����tv(��}�������'{'��I�ߝY�)� Σ ��-r�q� r�.d.�_xp��Uە�Z���M׍�v�m���=����+K�G�ǔ���� ^���W�W����b�j�&gt;:&gt;�&gt;�&gt;�v��}/�a��v���������O8� � �FV&gt; 2 u����� /�_$\�B�Cv�&lt; 5 ]�s.,4�&amp;�y�Ux~xw-bEDCĻH����G��KwF�G�E�GME{E�EK�X,Y��F�Z� �= {$vr����K���� ��.3\����r���Ϯ�_�Yq*  ���©�L��_�w�ד������+��]�e�������D��]�cI�II�OA��u�_�䩔���)3�ѩ�i�����B%a��+]3='�/�4�0C��i��U�@ёL(sYf����L�H�$�%�Y �j��gGe��Q�����n� ����~5f5wug�v����5�k��֮\۹Nw]������m mH���Fˍe�n���Q�Q��</code>h����B�BQ�-�[l�ll��f��jۗ"^��b���O%ܒ��Y}W�����������w�vw����X�bY^�Ю�]�����W�Va[q<code>i�d��2���J�jGէ������{�����׿�m���&gt;  ���Pk�Am�a�����꺿g_D�H��G�G��u�;��7�7�6�Ʊ�q�o���C{��P3���8!9���� � &lt;�y�}��'�����Z�Z���։��6i{L{��ӝ � -?��|������gKϑ���9�w~�Bƅ��:Wt&gt;���ҝ����ˁ��^�r�۽��U��g�9];}�}����� ���_�~i��m��p���㭎�}��]�/���}������.�{�^�=�}����^?�z8�h�c��' O*��?�����f�����</code>ϳ�g���C/����O�ϩ�+F�F�G�Gό���z����ˌ��ㅿ)����ѫ�~w��gb���k��?Jި�9���m�d���wi獵�ޫ�?�����c�Ǒ��O�O���?w| ��x&amp;mf������ endstream endobj 7 0 obj [ /ICCBased 26 0 R ] endobj 27 0 obj &lt;&lt; /N 1 /Alternate /DeviceGray /Length 3385 /Filter /FlateDecode &gt;&gt; stream x�W\SW�?7�f�2�F�eˈ� ���"&amp;��b ��R�<code>���QѢ�E��:Q�V�ƭ/�RA��Z\X}���������~_��p��9�Y���B�[xRi.!�')��'pҦ�����"M�4y�)'..� I�DH���^�D)��B�5v��Q�&gt;�:�DP��C��Ä/�"�2 ��� �$.���� x�Q ^ bd.�eb&gt;+\�+a����x,wWwV�,?S��V���?��\9i7����^��oW��B� !�/�C|^h"</code>o��E��AQl��S G�s�9��7f� �+�G�xB�Q�()� ���(r��Lɜ�X�� ��_��p�H�%sf��,?����CB� �����4�+ �I9�I�(�v�.�z6/2� <code>;an8���FK ��=�O-��Ɛ�� �(��&gt;�Q(J� �;�BY��Uf�ø�� ��"H9�K��*x1���d�� ��Ћe�2�#}�P�L�8B�R0 �|4��u#*@bT�@Y����gh�0KM3 Pȳ �| '�� r� ��X&gt;ʄ���rD�B�A���% �#w�U�� �� �ͿFr�~����b�0�� ��X��Z܁I�(Na��r���7�% V���H?����T c�m � C�ML��GD�[�M3J��B&gt;Y!���sҷ��Z炭�� ��(��x�ι�d8&gt;</code>�;�;gx��h</em>4�2�;H�5+⹳��^�\6[̿�r���b�\~�b��j9���� ��h�q]���d�S6Gl���ѼQ0I�7ހ.�5��C� Ă�/�Nj/�{�����hϧ ���\� %����I����U��A4�L y �u&lt;�oDO�#s� ��� !w=N2B�= �U�&gt;1���� !��l�{|�/'d��Ȕ�2�Jg՗ �����.�y�ʝ����]���쇊((�Ǿ������#O���8ނ�� ��V�4ޢ@��c�|�q������A�?|H� �98���� d&gt;�}�l��Gb�=|�Gs���h����Y4:�c+�2��Sʴf�1�LG���Ę��3� Y3���LC�<code>�3C��&gt;�c$c� !D2��u/ � a��/ST9ް���#k��d�&gt;g��d�&amp;e �9WE��T�d�$F��ĕ��=�1s��MV-</code>&lt;6]���(͗fO ���Ze�b�Bh�0Ģ��r�Z$<code>raN�\�z��Ep"h��p2&lt;d T�ȅ�� "��&amp;k�ho�el�j�Ϟ�&gt;�p�(�}��|i�L�%*dq�f$dq%|Wg�;� ���=���Ћx�� 3���eEJA��H�</code>z��#k�����^����po�EI( ��D�KĶ -A���B��f� �B � BG�1t��.�+�݃/Pz��K4�a ��t1c��Ŝ0w� �B�h,K�2�,L�ɱ2�3�[�m�v<code>طX v��]��</code>�X ����S�)z3� eś¡DQ�(3)Y���RJee#�����D9M�H�tQ�Rq����%�{��x,��g�2|!^���ux#T�v�:ޅ��o�K��M�L����Bb9���C4g��D71@��jPM�NT</em><em>�:��E�G���P�G��j�P_�h4���%��M�O[N�J;@;E�J{D����t'�?=�Σ�+����'���=�� 5�Ý�HgH��^� �5�cƐ������J��@�De��.�V��</em>=<em>C�ڪ����I�٪KT7�6��S���BMM�J�G-^M��Xm��A��j�jo�u� Ճ�g���W��V?�~G������F�F�F�� ��35^3u��L.S�\Ĭe61�1�i�h�jr4gi�j�h ּ�ٯ��e����Z�U�բuKkP[W�M;V;O{��^� ڽ:t ;�P �N��N�3:�tq]k�<code>]��g��t���������z�z�z��]���џ���_�_�\�� 7�3���48dp�ୡ�!�Ph�̰���+�qFAFB�*�F�Fo�Yơ�9ƫ��?0!L M�M�l39g�?No��8���q���5��:�&amp;��7�i�a:hfnn&amp;5�dvƬ���&lt;�&lt;�|�� �&gt; ]�  ��:��OX�,+���u�5</code>ija)��ay�r���</em>٪���kUko�L�u�m�66Sm�l��ܵU����n�m�}ego�j���]���=׾�~��} �@��u7���{���u�G����ȱ�����I���3���Y�\�|�E݅�R�ϥ���5ڵ���� 6�'���&gt;�=ۃ� ߷{n:n�n�n�n�;���k�oLԘ6q����'9MN�6鶇��T�� m yzy�&lt;=��l�2��x�����^�}އ�3�g��1�7������|�s������;�~�p�ɏ���y�;��X<em>tZ����=��ds�s�MaO�M92�U�o���S!xHxHUȥP����͡ì²��� �{��?A���X q�k��s��^� "�F�G%Fm��9�1Z�:�25r�ک�clc$1GcQ,7vm�8���q������k�MpK(KhO�M���7�eҔ��I�� ���m)�)3RR^����I�6aڂi�L��i������������O��1�r�͙�3�g^�e2+w��ٚ�y�gP3R3�f�����x�s�s����7� ��}B���L��5��Y�Yk��D��Q�8X�Y�&lt;;"{{���؜�9 rSs�1�2�Z$:� ��|�����R'i��k����sdQ���<code>fAs� �S�!w�.�. (�-z=/e��b�bIqG�cɲ�ǥa�_�'��緕Y�-)�^�Y�c!�p�¶E֋*�,_�g�꒜%?���ה��Y�g�f�+ } ���Jf����R��ۿ �qi��e����T�Xͮ��~�����/ݾ����+.��\�mm�d��Ձ����^S���کk�ֱ�U��s���j&amp;�lߠ�A��kc���M6�Vmz�Y���vJ�-�[�my�U��ڶ�m��ͶWo�����;�w4������,��뮔]�_{�PoR_]��n��= {�6x54�5ݻr e�|_����|�Ms�K�����O����桨Cm��7~g�ݖ#�G����������]�i�W["[�Z�Z�|����c��j�� _yB�Dŉ'KO����?�u�Q��{g���q6��sQ����ÙvN�����]�����G/z^l���8��OG.y^j��u���ϕ֫����x�����?��޸��y�f��۷f��-��{'���Ew��-��}��5M ��k��tyv ����9��{�����R�˻��_5~�yl�׽�X_Xߕ'ӟ�&lt;�&gt; ��M��-� �}�{�� �z�˞�c� �����g�</code>��×y/�^U�6~�������o �{G������z�Cއ� �b endstream endobj 8 0 obj [ /ICCBased 27 0 R ] endobj 29 0 obj &lt;&lt; /Filter /FlateDecode /Length 7690 &gt;&gt; stream x�]ے#��}�W�~�DX�B]�O�h�;�,{z��X��æ���H�.����}շ�I � �P ػ� �, �����[�뿬X�����u?���~������7����?ڏU9���+:U�o ֿ���Z��w��\o�y�t8��o߬&gt;��r�ٞެ�'��{z�n�?������y�?�g����W5�W����7��o��~w�. 5� t�%� Z���n�{�</em>&gt;��H�߯?B��<em>P� �˺h�~5U���z %&gt;݋�� ������!��a�x�޳������|b���8����Xt%ln���Ş� ʮ7�~Y4��1��+�^ Z_�Q���A�%�f���� ]���7��"B��(˲[��ֱZƢ��I-�ls:��QOI�Mes���,�FMtsX�� [�����M��u�B��/������~+@���P~��5߾)�5ʢ {�ʁb,��]w�P �����c�� �JcC�a�����b{ޞ���1g� T�����</em>�x���1��늬� �N�ywssW�fW��ڝ�#ͭ/5�F4W.( ������|Gnꜰ �� ��(� 2�� r�8b��Z� {�t��0������ ~u��]S ��Y���U�] ��vm7+q�/�2����� �3L�h2v�)� b��D����黧�o�H?��(�]�G�<em>�yNYe(Ҩ�h٠"�EV����\�����3A�Ї5Z�V� ��-� ��n����z�E�y?S<code>@h�ڦh�ɠ���? ;��Hm뢪���r\��E �4 z����w��������̝♶ at�=k�"��-T#��Z�$��%F��a[uEՑ���ѽ����^�{S�g��á �;��Q�cR[U[9t�1gLB� +</code>�����',���e��R�4� /Ǖ�!Gl��P� F���v�/M1���?V�P�@zה�#�a$���HiXv�&lt;�t��be{8�,��f�P��U� ��Z|���P��أB���8 r���'#� =�<code>]�D[ ��5�[6�.Ǣͱ w��suU���篵</code>]�E�,�id\�<code>$Z�0���h� �!�#�b,� �b|�j YM,n5,n�o�,b�Xa� �.�2��� _Q�l�+0�� �:���.'"�</code>��&amp;�� �={^є��@F��T�&lt;�W�����q#^�J���X��0s�U�"�����aVƸ%̮�lt %TMD��m � ���W�l�,~fFwL�𒄬��LK^ X�V��</em>�UZ�d�U=�i�|3��%V$�Gz| LE��X!@/I�~"q�G�k#,�Q"� ��� Z�]��A����Z��4�4.5�L69/�ad��% �����e�lӀ��,%.BY_l� =8 �ۤYA�� ��=��Pad�ZA�$b8 M�C��G�f =s�a6ÂJ� ��Ѿ�B��ÖWU��t����zY��׿���f�����, ��j���y^/&lt;8(+�'���e�Gi������/�� ^�l^�l}a\ ���Y���ќ�S����d��q� �r&amp;n [���������&lt;8(<em>ҧ�jxpP2a��26��к4k |=Z�/�u�����yh X��;W��Aل�� T#"� Vx�<code>U��� ��X��AY?�X��Aل� l����4�p����zp�� "&lt;�tDMN]��MkV</code>����` F���[^ ���������%&lt;�T3�/�$���G� &lt;�K#^xp�I3.��lĂ) �~���!^�5��Ja�-�x��(-&lt;8�&lt;��e ,꾂e�Gi��A5��6�&amp;1+&lt;x���8'&lt;8!��eE�d� �MX0 ���j�^_�I [��Y��K���� �U]z�&amp;&lt;89�̃�9������AɄ�� T#�f�asGE�� �U�������&lt;8(</em>ҧ�jxpP2a��26�L�Z��<em>6�C�����</em> ���V���՜�Sp �MX&lt;X��A5"�U��K��.X��E"�VxpPV�O"VxpP6a�8f��jf� �0p�G1��PUS���ᘺ�1a�j���r���䭗��Z�Ƒӡ��� ��߬qjb�#���c�x�Y�kMN�+8�i�<em>�qj�f�©��9�|"$��Se��^8uP��Nm���}8o��{��a�Y�Fe�-���2��F,��ԁ�W[P8��r�Ђ.� �N@�� �(u���P�@����)�h� J �M&lt; ��R�H�O��(d�R/9}��� ����B���"~2� ��&amp;,�B�M53�7�vNR�R/Bv��&amp;N/@�n�+¨�� ˌ:���w��XF �LX;�Wa�A5"@Z;�Wa� x�� ����x��:(</em>ҧ�juP2a��2�6�����f</p>
</div></details><h2 id="toc-230">116. 大语言模型_百度百科</h2>
<ul>
<li>链接：https://baike.baidu.com/item/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/62884793</li>
<li>来源：bing</li>
<li>摘要：2023年12月26日 · 大语言模型（英语：Large Language Model，简称LLM）是指使用大量文本数据训练的深度学习模型，使得该模型可以生成自然语言文本或理解语言文本的含义。这些模型可以通过在庞 …</li>
</ul>
<details class="article-body-details"><summary>正文（点击展开）</summary><div class="article-body-fold"><h3 id="toc-231">正文（抓取，非 AI）</h3>
<p>大语言模型_百度百科 网页 新闻 贴吧 知道 网盘 图片 视频 地图 文库 资讯 采购 百科 百度首页 登录 注册 进入词条 全站搜索 国际版 帮助 首页 秒懂百科 特色百科 知识专题 加入百科 百科团队 权威合作 个人中心 大语言模型 播报 讨论 上传视频 使用大量文本数据训练的深度学习模型 收藏 查看 我的收藏 0 有用+1 0 本词条由 中国科学院大学计算机科学与技术学院 参与编辑并审核，经 科普中国·科学百科 认证 。 大语言模型（英语：Large Language Model，简称LLM）是指使用大量文本数据训练的 深度学习 模型，使得该模型可以生成自然语言文本或理解语言文本的含义。这些模型可以通过在庞大的数据集上进行训练来提供有关各种主题的深入知识和语言生产 [1] 。其核心思想是通过大规模的无监督训练学习自然语言的模式和结构，在一定程度上模拟人类的语言认知和生成过程。 LLM在多种应用场景下表现出色，不仅能执行拼写检查和语法修正等简单的语言任务，还能处理文本摘要、机器翻译、情感分析、对话生成和内容推荐等复杂任务。通过在大规模数据集上进行预训练，大语言模型获得了强大的通用建模能力和泛化能力。近期，GPT-4和LLaMA等大语言模型在自然语言处理等领域取得了巨大的成功，并逐步应用于金融、医疗和教育等特定领域 [2] 。 2023年12月26日，大语言模型入选“2023年度十大科技名词” [3] 。2024年4月，在第27届联合国科技大会上，世界数字技术院发布了《 生成式人工智能应用安全测试标准 》和《 大语言模型安全测试方法 》两项国际标准，由OpenAI、蚂蚁集团、科大讯飞、谷歌、微软、英伟达、百度、腾讯等数十家单位多名专家学者共同编制而成 [4] 。 中文名 大语言模型 外文名 Large Language Model、LLM 所属学科 人工智能 别 名 大规模语言模型、大型语言模型 [5] 应用领域 金融、法律、政务 [6] 、教育 [7] 、传媒 [8] 等 目录 1 技术原理 2 发展历史 ▪ 技术起源 ▪ 发展历程 ▪ 重大节点 3 基本原理 ▪ 训练流程 ▪ 工作原理 4 模型特点 ▪ 训练成本 ▪ 局限性 ▪ 大语言模型对比 5 相关研究与发展 ▪ 相关社会影响 ▪ 最新研究进展 ▪ 未来发展方向 6 应用领域 ▪ 教育 ▪ 金融 ▪ 政务 ▪ 医疗 ▪ 办公 ▪ 客户联络 7 风险与挑战 ▪ 可信性 ▪ 可解释性 ▪ 应用成本较高 ▪ 能力迁移 ▪ 技术风险 ▪ 隐私保护 技术原理 播报 编辑 大语言模型（英语：Large Language Model，简称LLM）是一种基于深度学习的人工智能技术，也是 自然语言处理 的核心研究内容之一 [6] 。其核心是使用大规模数据集对模型进行训练，从而使其能够生成自然语言文本或理解语言文本的含义。这些模型通过层叠的神经网络结构，学习并模拟人类语言的复杂规律，达到接近人类水平的文本生成能力。大语言模型采用与小模型类似的Transformer架构和预训练目标（如 Language Modeling），与小模型的主要区别在于增加模型大小、训练数据和计算资源 [9] 。相比传统的自然语言处理（Natural Language Processing, NLP）模型，大语言模型能够更好地理解和生成自然文本，同时表现出一定的逻辑思维和推理能力。 发展历史 播报 编辑 技术起源 大语言模型的起源可以追溯到20世纪50年代，当时人工智能领域的先驱们开始探索如何让计算机理解和生成人类语言。20世纪70年代由贾里尼克提出的N-gram语言模型是最常用的统计语言模型之一，广泛用于当今的多种自然语言处理系统中 [10] 。 N-gram 模型将文本序列划分为长度为N的连续词组（N-gram），并利用大量语料库训练模型，以预测给定N-gram的后续词。N-gram模型虽然是一种有效的语言建模技术，但是存在着一些局限性，如数据稀疏性、计算复杂性和语言模型的可扩展性等。基于N-gram语言模型的不足，人们开始尝试用神经网络来建立语言模型。 发展历程 雏形阶段 20世纪40年代末和50年代开始采用计算机技术来研究和处理自然语言 [11] 。1950年，图灵测试诞生。1954年，美国人乔治·戴沃尔设计出第一台可编程机器人。1956年，美国达特茅斯学院举行历史上第一次人工智能研讨会，标志人工智能诞生 [12] 。 1966年，世界上第一个聊天机器人--ELIZA，由美国麻省理工学院（MIT）约瑟夫·魏岑鲍姆发布。ELIZA能通过脚本理解简单的自然语言，并能产生类似人类的互动 [12] 。 1975年，Frederick Jelinek等人在论文《Continuous Speech Recognition by Statistical Methods》中提出并应用N-gram模型于语音识别任务。之后随着神经网络的发展，出现了神经语言模型 [13] 。 2010年，斯坦福大学推出Core NLP套件，该套件提供了一套工具和算法，帮助研究人员处理复杂的NLP任务，允许开发人员执行情感分析和命名实体识别 [14-15] 。 2011年，出现了一个较小版本的Google Brain，具有单词嵌入等高级功能，使自然语言处理系统能够更清楚地理解上下文 [14] 。 2013年，自然语言处理模型 Word2Vec 诞生，首次提出将单词转换为向量的“词向量模型”，以便计算机更好理解和处理文本数据 [16] 。 GPT模型问世 2017年，Google发布论文《Attention is all you need》，提出Attention机制和基于此机制的Transformer架构。此架构价值在于是一种完全基于注意力机制的序列转换模型，而不依赖 循环神经网络 （Recurrent Neural Network, RNN）、 卷积神经网络 （Convolutional Neural Network, CNN）或者长短期记忆（Long Short-Term Memory, LSTM ） [13] 。 2018年，Google AI研究院的Jacob Devlin等人提出了BERT（Bidirectional Encoder Representation from Transformers）， BERT利用掩码机制构造基于上下文预测中间词的预训练任务，很大程度上提高自然语言处理任务的性能。BERT出现具有重大意义，尤其是预训练+参数微调”的研究范式，此后出现更多预训练语言模型都是以该范式为基础；同年，OpenAI公司同样发布了自己的模型GPT（Generative Pre-Training），这是一个典型的生成式预训练模型 [13] 。 2019年，OpenAI发布GPT-2，该模型可以不用根据下游任务数据进行参数优化，可以根据给定指令自行理解并完成任务 [13] 。 2020年，OpenAI发布GPT-3，并在Github上开源GPT-3部分样本和数据集。该模型拥有1750亿个参数。该模型的发布是一件跨时代的事情，意味着自然语言处理领域的大语言模型真正意义上出现了，从此正式开启大语言模型时代 [13] 。 进阶突破阶段 2019年，Radford等人使用GPT-2模型研究大语言模型在零样本情况下的任务处理能力；Brown等人在GPT-3模型上研究通过语境学习进行少样本学习的方法指令微调将大量各类型任务，统一为生成式自然语言理解框架，并构造训练语料进行微调 [17] 。 2022年，Ouyang等人提出使用“有监督微调+ 强化学习”的InstructGPT算法 [17] 。 这些方法逐渐扩展到利用生成式框架针对大量任务进行有监督微调的方法，有效提升模型的性能 [17] 。 2022年11月30日，OpenAI公司发布ChatGPT，该模型属于一类基于GPT技术的大语言模型。Google、Microsoft、NVIDIA等公司也给出了自己的大语言模型 [13] 。 2023年，谷歌公布聊天机器人Bard，它由谷歌的大语言模型LaMDA驱动；同年，百度正式宣布将推出文心一言，3月16日正式上线。文心一言的底层技术基础为文心大模型，底层逻辑是通过百度智能云提供服务，吸引企业和机构客户使用API和基础设施，共同搭建AI模型、开发应用，实现产业AI普惠；3月，Open AI发布多模态预训练大模型GPT4.0 [18-19] 。 2023年4月13日，亚马逊云服务部门在官方博客宣布推出Bedrock生成式人工智能服务，以及自有的大语言模型泰坦（Titan） [20] 。 2024年3月，Databricks推出大语言模型DBRX，号称“现阶段最强开源AI” [21] ；马斯克的xAI公司正式发布大模型Grok-1，参数量达到3140亿，超OpenAI GPT-3.5的1750亿 [22] ；4月，在瑞士举行的第27届联合国科技大会上，世界数字技术院（WDTA）发布了《生成式人工智能应用安全测试标准》和《大语言模型安全测试方法》两项国际标准，是由OpenAI、蚂蚁集团、科大讯飞、谷歌、微软、英伟达、百度、腾讯等数十家单位的多名专家学者共同编制而成 [4] 。 重大节点 Transformer结构 在大语言模型的发展历程中，最重要的里程碑是2018年谷歌发布的Transformer模型，它采用了自注意力机制，可以更好地捕捉语言中地长距离依赖关系，从而极大地提高了大语言模型的效果。通过其自注意力机制，Transformer不仅解决了递归神经网络在并行化处理上的限制，还显著提升了模型处理大规模数据集的能力。这种技术的进步为预训练语言模型（PLMs）的发展铺平了道路，使得这些模型能够更加灵活地适应各种不同的下游任务。 Transformer是一种用于序列到序列（Sequence-to-Sequence）任务的神经网络模型，如机器翻译、语音识别和生成对话等。它是第一个完全依赖于自注意力机制来计算其输入和输出的表示的转换模型。序列到序列模型采用的是编码器-解码器结构，编码器-解码器结构采用堆叠的多头注意力机制加全连接层。通过查询-键-值的模式使用多头注意力。由于Transformer模型中既没有递归，也没有卷积，如果需要获得输入序列精准的位置信息，必须插入位置编码。位置编码和输入嵌入有相同的维度，所以二者可以实现相加运算，位置编码方式可以有多种 [23] 。 从人类反馈中强化学习（RLHF） 人类反馈强化学习（Reinforcement Learning from Human Feedback, RLHF）是一种利用人工指导来微调预先训练好的大型语言模型（LLMs）的方法。由三个相互关联的过程组成：反馈收集、奖励建模和策略优化。RLHF优势在于能更好地与人类的意图保持一致，以及以未来的反馈为条件进行规划，从各种类型的反馈中进行流畅的学习，并根据需要对反馈进行整理。此外，RLHF还允许机器通过抽象人类的价值学习，并不是简单地模仿人类的行为 [24] 。 2023 年4月OpenAI联合创始人John Schulman在Berkeley EECS会议上所做的报告“ReinforcementLearning from Human Feedback：Progress and Challenges”，分享了OpenAI在人类反馈的强化学习方面的进展，分析监督学习和强化学习各自存在的挑战。基于上述报告及相关讨论，强化学习在大语言模型上的重要作用可以概括为以下几个方面 [25] 。 一，强化学习与有监督学习相比，更有可能从整体层面去考虑影响。这是因为二者在反馈粒度方面存在差异，强化学习不仅能够兼顾表达多样性，还能增强对微小变化的敏感性，所以它相对而言更契合大语言模型。而且，强化学习还允许模型呈现出不同的多样性表达 [26] 。 二，强化学习更容易解决幻觉问题。有监督学习算法非常容易使得求知型查询产生幻觉。在模型并不包含或者不知道答案的情况下，有监督训练仍然会促使模型给出答案。而使用强化学习方法，则可以通过定制奖励函数，将正确答案赋予非常高的分数，将放弃回答的答案赋予中低分数，将不正确的答案赋予非常高的负分，使得模型学会依赖内部知识选择放弃回答，从而在一定程度上缓解模型的幻觉问题 [26] 。 三，强化学习可以更好地解决多轮对话奖励累积问题。多轮对话能力是大语言模型重要的基础能力之一。多轮对话是否达成最终目标，需要考虑多次交互过程的整体情况，因此很难使用有监督学习的方法构建。而使用强化学习方法，可以通过构建奖励函数，根据整个对话的背景及连贯性对当前模型输出的优劣进行判断 [26] 。 专家混合模型 GPT-4 采用了专家混合模型（Mixture of Experts，MoE）架构，总共有1.8 万亿个参数。GPT-4使用了16 个专家，每个专家的参数约为1110亿，每次前向传递使用2 个专家进行路由，同时还有550 亿个共享参数用于注意力机制。MoE 架构在减少推理所需的参数量的同时，仍然可以使用更大规模的模型参数 [27] 。 混合专家系统类思路是大模型落地比较优质的路径 [28] 。 提示学习 提示学习（Prompt-based Learning）不同于传统的监督学习，它直接利用了在大量原始文本上进行预训练的语言模型，并通过定义一个新的提示函数，使该模型能够执行小样本甚至零样本学习，以适应仅有少量标注或没有标注数据的新场景 [29] 。 实现自我复制 2025年2月11日消息，据最新研究显示，人工智能（AI）可能已经跨越了一个关键的“红线”—— 实现了自我复制。2024 年 12 月 9 日，复旦大学的研究人员在预印本数据库 arXiv 上发表了一项研究，指出两种流行的大型语言模型（LLMs）能够在无人类干预的情况下克隆自身。 [50] 大模型自信心崩塌 2025年7月21日，谷歌DeepMind证实：反对意见让GPT-4o轻易放弃正确答案。 [55] 密码学领域大模型 2025年8月，全球首个面向密码学领域的大语言模型“玄知大模型”在陕命名发布，标志着密码学进入智能化发展新阶段，为密码算法分析、协议设计与工程实现提供全流程智能支持，开启了密码学AI应用新纪元。 [56] 基本原理 播报 编辑 训练流程 预训练 预训练是大语言模型训练的首要步骤，其目标在于使模型掌握语言的统计模式与语义信息。主流的预训练阶段流程大致相同，其中关键要素是数据，需收集海量无标注数据，像互联网上的文本、新闻、博客、论坛等。这些数据可以涵盖多种语言，且要经过一定的清理和处置，去除噪声、无关信息以及涉及个人隐私的内容，最后以tokenizer粒度输入到前述的语言模型中。经清洗处理后的这些数据用于训练和优化语言模型。在预训练过程中，模型会习得词汇、句法和语义的规律以及上下文的关系。 在预训练语料集方面，GPT-3中通过主要包含经过过滤的Common Crawl数据集、WebText2、Books1、Books2以及英文Wikipedia等数据集合。其中Common Crawl的原始数据有45TB，进行过滤后仅保留了570GB的数据。通过子词方式对上述语料进行切分，大约一共包含5000亿子词。为了保证模型使用更多高质量数据进行训练，在GPT-3训练时，根据语料来源的不同，设置不同的采样权重。在完成3000亿子词训练时，英文Wikipedia的语料平均训练轮数为3.4次，而Common Crawl和Books2仅有0.44次和0.43次 [30] 。 由于Common Crawl数据集合的过滤过程繁琐复杂，OPT则采用了混合RoBERTa、Pile和Pushshift.io Redit数据的方法。由于这些数据集合中包含的绝大部分都是英文数据，因此OPT也从Common Crawl数据集中抽取了部分非英文数据加入训练语料 [30] 。 BigScience大型开放科学开放获取多语言模型（BigScience Large Open-science Open-access Mul-tilingual Language Model, BLOOM）运用Megatron-DeepSpeed 框架进行训练，主要包括两个部分：Megatron-LM 提供张量并行能力和数据加载原语；DeepSpeed 提供 ZeRO 优化器、模型流水线以及常规的分布式训练组件。通过这种方式能够实现数据、张量和流水线的三维并行 [31] 。 数据收集 预训练语料有两种来源： 1.通用语料：如网页、书籍和会话文本等，可以增强大语言模型的语言建模和泛化能力 [13] 。 2.专业语料：有研究将预训练语料库扩展到更专业的数据集，如多语言数据、科学数据和代码，赋予大语言模型特定的任务解决能力 [13] 。 数据收集完后需要对这些数据进行预处理，包括去噪、去冗余、去除不相关和潜在有毒的数据 [13] 。 基础大模型训练 由于模型参数量和所使用的数据量巨大，所以普通服务器单机无法完成训练过程，因此通常采用分布式架构完成训练 [13] 。 指令微调 在完成预训练后，就可以通过指令微调去挖掘和增强语言模型本身具备的能力，这步也是很多企业以及科研研究人员利用大模型的重要步骤。 Instruction tuning（指令微调）是大模型微调的一种具体方式，它是有监督微调（Supervised Fine-Tuning, SFT）的一种特殊形式，旨在让模型理解和遵循人类指令。在指令微调阶段，首先需要准备一系列的NLP任务，并将每个任务转化为指令形式，其中指令包括人类对模型应该执行的任务描述和期望的输出结果。然后，使用这些指令对已经预训练好的大语言模型进行监督学习，使得模型通过学习和适应指令来提高其在特定任务上的表现。 通过指令微调，大模型学习到了如何响应人类指令，可以根据指令直接能够生成合理的答案 [13] 。 为了让模型训练更加高效和简单，这个阶段还有一种高效的fine-tuning技术，这为普通的从业者打开了通向使用大模型的捷径。 大模型高效微调（Parameter-Efficient Fine-Tuning, PEFT）旨在通过最小化微调参数的数量和计算复杂度，达到高效的迁移学习的目的，提高预训练模型在新任务上的性能，从而缓解大型预训练模型的训练成本。在训练过程中，预训练模型的参数保持不变，只需微调少量的额外参数，就可以达到与全量微调相当的性能。 很多研究对PEFT方法进行了探索，例如Adapter Tuning和Prefix Tuning等。其中，Adapter Tuning方法在面对特定的下游任务时，将预训练模型中的某些层固定，只微调接近下游任务的几层参数。而Prefix Tuning方法则是在预训练模型的基础上，添加一些额外的参数，这些参数在训练过程中会根据特定的任务进行更新和调整。 工业界常用的Adapter Tuning的技术是Low-Rank Adaptation（LoRA）。它通过最小化微调参数的数量和计算复杂度，实现高效的迁移学习，以提高预训练模型在新任务上的性能。LoRA 的核心思想是将预训练模型的权重矩阵分解为两个低秩矩阵的乘积。通过这种分解，可以显著减少微调参数的数量，并降低计算复杂度。该方式和机器学习中经典的降维的思想很类似，类似地，LoRA 使用了矩阵分解技术中的奇异值分解 (Singular Value Decomposition, SVD) 或低秩近似 (Low-Rank Approximation) 方法，将原始权重矩阵分解为两个低秩矩阵的乘积。 在微调过程中，LoRA 只更新这两个低秩矩阵的参数，而保持其他预训练参数固定不变。这样可以显著减少微调所需的计算资源和时间，并且在很多任务上取得了与全量微调相当的性能。 LoRA技术的引入使得在大规模预训练模型上进行微调更加高效和可行，为实际应用提供了更多可能性。 类人对齐 由于模型输出的结果与人类回答差距很大，因此需要进一步优化模型，使模型的输出与人类习惯对齐。其中OpenAI开发ChatGPT的人类反馈强化学习是最具代表性也是最成功的 [13] 。 奖励建模 奖励建模（Reward Modeling）阶段的目标是构建一个文本质量对比模型，对于同一个提示词，SFT模型给出的多个不同输出结果的质量进行排序。奖励模型（RM模型）可以通过二分类模型，对输入的两个结果之间的优劣进行判断。RM模型与基础语言模型和SFT模型不同，RM模型本身并不能单独提供给用户使用 [32] 。 奖励模型的训练通常和SFT模型一样，使用数十块GPU，通过几天时间完成训练。由于RM模型的准确率对强化学习阶段的效果有至关重要的影响，因此通常需要大规模的训练数据对该模型进行训练 [32] 。 强化学习 强化学习 （Reinforcement Learning）阶段根据数十万用户给出的提示词，利用前一阶段训练的RM模型，给出SFT模型对用户提示词补全结果的质量评估，并与语言模型建模目标综合得到更好的效果 [33] 。 使用强化学习，在SFT模型基础上调整参数，使得最终生成的文本可以获得更高的奖励（Reward）。该阶段需要的计算量相较预训练阶段也少很多，通常仅需要数十块GPU，数天即可完成训练 [33] 。 Andrej Karpathy也指出，强化学习并不是没有问题的，它会使基础模型的熵降低，从而减少了模型输出的多样性。经过强化学习方法训练后的RL模型，就是最终提供给用户使用、具有理解用户指令和上下文的类ChatGPT 系统。由于强化学习方法稳定性不高，并且超参数众多，使得模型收敛难度大，再叠加RM模型的准确率问题，使得在大语言模型上有效应用强化学习非常困难 [33] 。 工作原理 大语言模型的工作原理基于深度学习架构。它首先会收集海量的文本数据，之后通过词向量表将单词映射到特定的向量空间以便计算机以数值化方式处理语言，随后利用大量的计算资源对具有庞大数量参数的神经网络模型进行训练。通过在训练过程中不断地调整模型参数，让模型去学习数据中的语言模式、语义信息等，使得模型能够在各类自然语言处理任务中取得最佳表现。 大语言模型的“大”主要体现几个方面在：一是参数数量庞大；二是训练数据量大；三是对计算资源需求高。正是因为具备这些“大”的特点，很多先进的大语言模型参数不断增多，泛化性能愈发出色，在各种专门的领域输出结果也越来越准确。 模型特点 播报 编辑 训练成本 大语言模型的训练成本非常高，通常达到数百万美元甚至更多。例如，OpenAI的GPT-4模型训练成本超过1亿美元。随着模型规模的增大，训练成本急剧上升，2023年发布的模型训练成本已逼近2亿美元。预计到2024年底或2025年初，新一代模型的训练成本可能逼近10亿美元。这些成本包括了数据准备、硬件成本、模型架构设计和优化等多个方面 [34] 。 局限性 不能创造语言 大模型至多是会使用语言，而远谈不上能创造语言、发明语言。大语言模型的基础仍然是深度学习技术，即利用大量的文本数据来训练模型，只不过模型的参数规模更为庞大，但与产生语言的劳动、实践根本不沾边 [35] 。 不能深度理解人类 大语言模型只是人类生存实践的旁观者和应答者，缺乏共情能力，还达不到像人类理解那样的深刻性与丰富性，而深层理解更彰显人类智能的特殊性 [35] 。 不能全面嵌入社会 以ChatGPT为代表的大语言模型仍然不能像人一样在社会中进行交往与实践，不能以人类体悟语境的方式来体悟语境，因此，谈论ChatGPT拥有媲美人类的智能，完全理解人类的语言，还为时尚早 [35] 。 安全性不高 安全性是大型语言模型必须直面的关键问题之一。大型语言模型可以在众多学科领域的任务中得以应用，然而，这也表明此类模型会遭遇广泛的内容安全难题。尽管大型语言模型已借助基于人类反馈的强化学习等诸多方式，努力使模型输出与人类价值观相契合，但在应用于各个领域时，语言模型依旧容易遭到恶意利用，进而生成诸如偏见言论、煽动性话语、隐私侵犯言论等存在安全隐患的文本 [36] 。 成本高昂 大语言模型在训练和部署过程中，会耗费大量的计算资源与人力资源，成本高昂。对部分中小型企业来说，很难承受这样的成本，也难以获取充足的技术支持及资源。在企业级应用方面，采用百亿级基础模型较为适宜，再依据不同需求去训练相应的垂直模型，如此只需承担垂直训练的成本。不过，企业怎样实现高效的垂直训练以及如何把控成本，依旧是大模型需要面对的问题之一 [37-38] 。 不能保障内容可信 可信度当前是大型语言模型的重大局限之一。虽然大语言模型能够用于处理各种真实场景中的问题，然而它依旧会产出不可信的文本。现今使用者只能按照自身需求去核验生成的内容是否真实可靠，很难具备权威说服力。与此同时，模型在解决涉及推理的问题时，有可能由于推理过程出现错误而得到不可信的结果。这对其研究发展以及应用落地都有着负面的影响 [39] 。 大语言模型对比 下表为大语言模型对比汇总表 [51] 。 已有大型语言模型对比 模型 发布机构 所在国家 模型 参数量 模态 最大 序列长度 使用方式 GPT-3 OpenAI 美国 1 750 亿 语言 2 048 API GPT-4 OpenAI 美国 语言、 图像 32 000 API Codex OpenAI 美国 120 亿 代码 API J1-Jumbo AI21Labs 美国 1 780 亿 语言 2 048 受限访问 J1-Grande AI21Labs 美国 170 亿 语言 2 048 受限访问 BLOOM BigScience 法国 1 760 亿 语言 2 048 开源 GPT-NeoX EleutherAI 200 亿 语言 2 048 开源 Anthropic-LM Anthropic 美国 520 亿 语言 8 192 Claude Anthropic 美国 语言 100 000 受限访问 CodeGen Salesforce 美国 160 亿 代码 2 048 开源 Turing-NLG Microsoft 美国 170 亿 语言 MT-NLG Microsoft 美国 5 300 亿 语言 2 048 OPT Meta 美国 1 750 亿 语言 2 048 开源 LLaMA Meta 美国 650 亿 语言 2 048 开源 T5 Google 美国 110 亿 语言 512 开源 UL2 Google 美国 200 亿 语言 512 开源 AlphaCode Google 美国 410 亿 代码 768 PaLM Google 美国 5 400 亿 语言 2 048 API LaMDA Google 美国 1 370 亿 语言 Chinchilla Google 美国 700 亿 语言 Gopher Google 美国 2 800 亿 语言 2 048 CPM-2 清华大学、智谱 中国 1 980 亿 语言 开源 GLM-130B 清华大学、智谱 中国 1 300 亿 语言 2 048 开源 MOSS 复旦大学 中国 160 亿 语言 2 048 开源 InternLM 上海 AI LAB 中国 1 040 亿 语言 2 048 ERNIE 3.0 Titan 百度 中国 2 600 亿 语言 512 受限访问 源 1.0 浪潮 中国 2 450 亿 语言 2 048 受限访问 盘古-α 华为 中国 2 000 亿 语言 1 024 盘古-Σ 华为 中国 10 000 亿 语言 1 024 WeLM 腾讯 中国 100 亿 语言 受限访问 M6 阿里巴巴 中国 1 000 亿 语言、图像 M6-10T 阿里巴巴 中国 100 000 亿 语言、图像 512 PLUG 阿里巴巴 中国 270 亿 语言 Baichuan 百川智能 中国 70 亿 语言 4 096 开源 YaLM Yandex 俄罗斯 1 000 亿 语言 2 048 开源 相关研究与发展 播报 编辑 相关社会影响 年度词汇 2023年12月6日，大语言模型入选国家语言资源监测与研究中心发布的“2023年度中国媒体十大流行语” [1] 。 2023年12月26日，大语言模型入选“2023年度十大科技名词” [3] 。 科技发展 大语言模型的快速进步，正在激发新业态、新模式，由此带来的工作方式、教育模式等的变革。它不仅是一项技术，更是未来国力竞争与生产力提高的重要资源。以深度学习平台和大模型为代表的AI新型基础设施，对科技创新、产业升级和高质量发展意义重大 [44] 。 翻译服务 大语言模型快速发展背后是自然语言处理（NLP）技术的突破，结合利用深度学习和大规模数据训练 ， 大语言模型不仅能够理解和生成更为精准和流畅的翻译，且能处理多种语言间的微妙差异，还能进行上下文理解，使得翻译效果更加自然、智能。 [52] 最新研究进展 星火大模型 讯飞星火认知大模型是科大讯飞发布的语言大模型。该模型于2023年5月首次发布，后续经过多次升级。2023年10月，讯飞发布了讯飞星火认知大模型V3.0。2024年1月，讯飞发布了讯飞星火认知大模型V3.5。 2024年10月，讯飞星火4.0 Turbo在第七届世界声博会暨2024科大讯飞全球1024开发者节上被正式发布，该模型七大核心能力全面超过GPT-4 Turbo，数学和代码能力超越GPT-4o，国内外中英文14项主流测试集中讯飞星火4.0 Turbo在9项测试集中实现超越 [45] 。 GPT-4 2023年3月发布的 GPT-4将文本输入扩展到多模态信号。2024年5月14日，新一代旗舰生成模型 GPT-4o 正式发布。GPT-4o 具备了对文本、语音、图像三种模态的深度理解能力，反应迅速且富有情感色彩，极具人性化。OpenAI官网介绍，GPT-4o中的o代表意为全能的前缀omni，称它向更自然的人机交互迈进了一步，因为它接受文本、音频和图像的任意组合作为输入内容，并生成文本、音频和图像的任意组合输出内容 [46] 。 AI大模型 2025年1月8日，在2025年国际消费电子展的高通展台，一台白色等人高的人形机器人用流利的英语热情问候走近的参观者们。这台人形机器人名为“通天晓”，是全球首台完全基于高通SoC的端侧多模态AI大模型人形机器人。这款基于端侧大模型的人形机器人为具身智能产业的创新发展开辟了更优路径。通过阿加犀技术成功部署的端侧大模型，让机器人的‘大脑’显著‘进化’，其多模态处理能力结合视觉、听觉、触觉等各种输入，提升了机器人对复杂场景的理解，从而极大增强了机器人的通用性和泛化性。 [49] 未来发展方向 多模态大语言模型 随着输入数据源模态的扩展，多模态大模型的构建思路通样按照网络架构的不同，可以分为基于理解模型的范式、基于生成式模型的范式，以及基于编解码的模型构建方法。ChatGPT 提供了一个跨领域的具有卓越会话能力和推理能力的语言界面。然而，由于ChatGPT是一个语言模型，无法处理、生成来自视觉世界的图像。同时，视觉基础模型（Visual Foundation Model，VFM），如视觉变换器或 Stable Diffusion，虽然显示出强大的视觉理解和生成能力，但只是具有一轮固定输入和输出的特定任务的专家。如何将 ChatGPT 的上下文交互能力同视觉、语音数据分析能力进行有效整合，将为多模态大模型训练提供新的思路 [47] 。 轻量化大语言模型 随着技术的发展，可以预见未来的生成式人工智能模型的规模将继续增长。更大规模的模型可以提供更深入、更准确的语言理解和生成能力，使得对话更加自然流畅，并且使模型能够更好地理解和回复复杂的问题和指令。然而，这些模型参数规模与训练数据规模的迅速增长带来极大的成本，为现实应用中的存储、分发、部署等带来了挑战。因此，需要对生成式人工智能模型进行轻量化和优化，以提高模型的效率与实用性。总之，更轻量化和高效的生成式人工智能模型将有助于其在更广泛的应用场景中发挥更大的作用 [48] 。 类脑化认知 类脑化是指生成式人工智能应具有与人类大脑类似的特性和能力，以更好地模拟人类的认知和学习过程。现有的生成式模型的训练方式与人类知识获取的方式存在很大的差异，大模型的生成式过程属于快思考，是一种直觉思维，容易出现错误和偏见，且不适合规划类任务。而人类的思维方式是慢思考，是一种理性思维。因此，未来的生成式人工智能需要更复杂和多样化的神经元系统，以及更加灵活的神经网络连接方式，从而模拟人类神经元与脑区的各种特性和行为。基于更强的类脑化认知，生成式人工智能可能将在科学智能领域发挥更大的作用，即学习、模拟和预测自然界和人类社会的各种现象和规律，从而推动科学发现和创新 [48] 。 应用领域 播报 编辑 教育 在线讨论与反思学习场景：赋能高阶思维能力培养 在线讨论与反思学习场景中的文本数据在一定程度上反映学生在线学习过程中的认知和情感表现。具有自然语言理解优势的BERT可对学生文本数据中的认知与情感进行识别，为赋能学生高阶思维能力培养奠定基础。同时探究学生在线学习认知和情感发展规律 [7] 。 人机协同提问场景：加强阅读理解能力 自我提问可以促进学习专注度，加深对阅读内容的理解，但当前学生提问普遍存在水平不高、类型单一等问题。对此，可以利用T5（2019年谷歌提出的一种基于Transformer架构的自然语言处理模型）和GPT系列的自然语言生成优势，为高质量问题创建提供支持，进而加强学生的阅读理解能力。利用GPT-3自动生成提示语（包括提问类型、答案、提问视角），通过多轮人机对话，帮助学生提出深层次问题。GPT-3更能促使小学生提出一系列与知识点相关的、深层次的问题，以加强深度阅读理解。总的来说，大语言模型可以利用其文本生成优势，通过人机协同对话形式辅助学生提问，进而提升其阅读理解能力 [7] 。 人机协同写作和数学解题场景：提升写作和解题水平 写作与数学解题逻辑教学作为学科教学领域的两项重难点，一直存在学生写作时“不愿写”“没得写”“不会写”和数学解题答题不规范、传统教学指导效率低等问题。对此，GPT系列或类T5结构模型因其内容创作和数学推理优势，可以广泛应用于智能写作工具研究和数学解题辅助研究领域，进而有效提升学生的写作和数学解题水平 [7] 。 金融 金融行业需要处理海量文本信息，大语言模型有助于分析和提取新闻媒体、研究报告、财务报表、企业公告、政府政策等文本信息中的价值。同时，金融信息具有强时效性，大语言模型可以做出秒级分析并提出建议。对于负债业务，基于大语言模型的智能客服可以协助优化存款业务流程，同时节省人力成本，提升服务效率 [40] 。 政务 随着中国推动人工智能技术研究及其在政务领域的应用，大语言模型在政务领域发挥了巨大的作用，包括政务文本分类、政务问答、政务命名实体识别、舆情风险识别和政务关系抽取，但同时政务大语言模型研究仍处在探索阶段，存在许多需要解决的问题，即数据多模态化、正确面对“模型即服务”趋势、注重数据高安全性、明确责任边界 [6] 。 医疗 在医疗行业，大语言模型的应用正在开启一场盖临床诊断、治疗护理、医学研究等方面的技术革命。在临床诊断方面,将大语言模型用于对患者病历、检查报告和生理参数等进行深入的自然语言处理和整公医疗大数据的分析，辅助医生快速准确地诊断疾病并制定治疗方案 [53] 。 在治疗护理方面，Google 的 Med-PaLM 2和EPFL( École polytechnique Fédérale de Lausanne)的Meditron 等工具展示了在提高临床效率和患者护理水平方面的应用潜力 [53] 。 在医学研究方面，将多模态大语言模型用于扫描和综合分析海量的科学论文，识别潜在的药物靶点和功效因素，加速了新药的研发 [53] 。 在蛋白质设计和新药发现方面，大语言模型已经展示出蛋白质序列处理和结构预测的强大能力，极大地提升了蛋白质设计的效率和准确性。 办公 在2024年世界人工智能大会上，金山办公发布 WPS AI 2.0，并推出政务自研模型——金山政务办公模型1.0。WPS AI是金山办公旗下基于大语言模型的人工智能办公助手。WPS AI演示了升级后为个人用户新增的4个AI办公助手，分别是AI写作助手、AI阅读助手、AI数据助手、AI设计助手 [41] 。 快手在大会期间正式推出视频生成大模型可灵网页端。同时，可灵推出更加清晰的高画质版、首尾帧控制、镜头控制等新功能，创作者单次生成的文生视频时长增加至10秒 [42] 。 客户联络 提升自动回复能力 可以根据用户输入的问题提供快速和准确的响应，快速解决问题，节省客服团队大量的时间和资源，提高客户体验和满意度。 强化意图识别能力 观察客户联络领域所处现状，大部分是把简单、重复、流程性的问题，交给机器人处理；复杂的、需要情感关怀的问题，交由人工客服处理。而传统的智能客服在意图理解方面的能力，仍然相对薄弱。借助大模型，智能客服能够有效结合用户的历史对话、当前沟通内容等上下文语境，更精准地识别出用户的需求和意图 [43] 。 优化人机交互体验 以ChatGPT为例来看，大模型的深度应用开创了客户使用体验的新范本。丰富的参数和强大的内容生成能力，能够支持智能客服实现更加个性化的问答回复，而非过往千篇一律的机械式问答 [43] 。 ChatGPT的应用已经有相对确定的场景，如扮演人工客服与客户沟通专业知识、提供专业的问答知识建议、对沟通记录进行质检标记、主动分析座席工作行为、发起产品推介、闲聊寒暄以及更“人性化”的引导留资等 [43] 。 风险与挑战 播报 编辑 大模型技术在短期内面临着可信性、可解释性、应用成本及能力迁移等多方面的挑战。 可信性 大模型的可信性无法保障。尽管其生成的内容符合语言规则，通顺流畅且与人类偏好对齐，但在事实性、时效性和数据准确性方面存在较多问题，缺乏可信性评估能力，这使其生成的内容具有一定的欺骗性 [54] 。 可解释性 大模型的可解释性较差。作为基于深度神经网络的“黑盒”模型，大模型的能力来源仍未被完全理解。例如，其涌现能力、规模定律、知识表示、逻辑推理能力、泛化能力、情景学习能力等方面仍需进一步研究，以提供理论支持并推动大规模实际应用的落地 [54] 。 应用成本较高 此外，大模型的应用成本较高。由于其参数规模和数据规模庞大，导致训练和推理的计算量大、功耗高、部署困难，同时也存在延迟问题。这些因素极大地限制了大模型的应用范围。提高推理速度和降低使用成本是推动大规模应用的关键 [54] 。 能力迁移</p></div></details>
</div>
<script>
var READING_HIGHLIGHT_CLASS='reading-highlight';
function getBlocksInOrder(container){
 var blocks=[]; var blockTags=['P','DIV','H2','H3','H4','LI','PRE'];
 function walk(el){
  if(!el||!container.contains(el))return;
  if(blockTags.indexOf(el.tagName)!==-1){
   var hasBlockChild=false;
   for(var k=0;k<el.children.length;k++){ var c=el.children[k]; if(blockTags.indexOf(c.tagName)!==-1){ hasBlockChild=true; break; } }
   if(!hasBlockChild){ var t=(el.innerText||'').trim().replace(/\\s+/g,' '); if(t)blocks.push({el:el,text:t}); }
   else for(k=0;k<el.children.length;k++)walk(el.children[k]);
  } else for(k=0;k<el.children.length;k++)walk(el.children[k]);
 }
 walk(container); return blocks;
}
function speakText(t,onDone){ if(!t){ document.getElementById('readStatus').textContent=''; if(onDone)onDone(); return; }
 speechSynthesis.cancel(); var status=document.getElementById('readStatus'); status.textContent='准备中…';
 var chunks=[]; var maxLen=280; for(var i=0;i<t.length;i+=maxLen){ var s=t.slice(i,i+maxLen); var j=s.lastIndexOf('。'); if(j>80){ chunks.push(s.slice(0,j+1)); i-=s.length-(j+1); } else chunks.push(s); }
 if(chunks.length===0)chunks=[t]; var idx=0;
 function speakNext(){ if(idx>=chunks.length){ status.textContent=''; if(onDone)onDone(); return; } var u=new SpeechSynthesisUtterance(chunks[idx]); u.lang='zh-CN'; u.rate=0.95; var v=speechSynthesis.getVoices().filter(function(x){ return x.lang.indexOf('zh')>=0; })[0]; if(v)u.voice=v;
  u.onend=function(){ idx++; setTimeout(speakNext,80); }; u.onerror=function(){ idx++; setTimeout(speakNext,80); }; status.textContent='朗读中 '+(idx+1)+'/'+chunks.length+'…'; speechSynthesis.speak(u); }
 if(speechSynthesis.getVoices().length)speakNext(); else speechSynthesis.onvoiceschanged=function(){ speechSynthesis.onvoiceschanged=null; speakNext(); }; }
function readFullText(){ var c=document.querySelector('.content'); if(!c){ document.getElementById('readStatus').textContent='无可读内容'; return; }
 var segments=getBlocksInOrder(c); if(!segments.length){ document.getElementById('readStatus').textContent='无可读内容'; return; }
 speechSynthesis.cancel(); document.querySelectorAll('.content .'+READING_HIGHLIGHT_CLASS).forEach(function(el){ el.classList.remove(READING_HIGHLIGHT_CLASS); });
 var status=document.getElementById('readStatus'); var idx=0;
 function runSegment(){ if(idx>=segments.length){ status.textContent=''; document.querySelectorAll('.content .'+READING_HIGHLIGHT_CLASS).forEach(function(el){ el.classList.remove(READING_HIGHLIGHT_CLASS); }); return; }
  var seg=segments[idx]; seg.el.scrollIntoView({behavior:'smooth',block:'center'}); document.querySelectorAll('.content .'+READING_HIGHLIGHT_CLASS).forEach(function(el){ el.classList.remove(READING_HIGHLIGHT_CLASS); }); seg.el.classList.add(READING_HIGHLIGHT_CLASS);
  status.textContent='朗读 '+(idx+1)+'/'+segments.length+'…';
  speakText(seg.text,function(){ seg.el.classList.remove(READING_HIGHLIGHT_CLASS); idx++; setTimeout(runSegment,120); }); }
 runSegment(); }
function getBlockForTap(el){ var c=document.querySelector('.content'); var blockTags=['P','DIV','H2','H3','H4','LI','PRE']; while(el&&el!==c){ if(c.contains(el)&&blockTags.indexOf(el.tagName)!==-1) return el; el=el.parentElement; } return null; }
function onContentTap(e){ if(e.target.closest&&e.target.closest('a')) return; var block=getBlockForTap(e.target); if(block){ var t=(block.innerText||'').trim().replace(/\\s+/g,' '); if(t){ e.preventDefault(); speechSynthesis.cancel(); document.getElementById('readStatus').textContent='朗读本段…'; speakText(t); } } }
if(document.readyState==='loading'){ document.addEventListener('DOMContentLoaded', function(){ var c=document.querySelector('.content'); if(c) c.addEventListener('click', onContentTap); }); } else { var c=document.querySelector('.content'); if(c) c.addEventListener('click', onContentTap); }
setTimeout(function(){ try{ var u=new SpeechSynthesisUtterance('\u200b'); u.volume=0; speechSynthesis.speak(u); }catch(e){} }, 300);
</script>
</body>
</html>